A Foundation in Digital Communication.pdf

A FOUNDATION IN DIGITAL COMMUNICATION
Second Edition
Written in the intuitive yet rigorous style that readers of A Foundation in Digital
Communication have come to expect, the second edition adds new chapters on the
Radar Problem (with Lyapunov’s theorem) and Intersymbol Interference channels.
An added bonus is the treatment of the baseband representation of passband noise.
But much more has changed. Most notably, the derivation of the optimal receiver
for the additive white Gaussian noise channel has been completely rewritten. It is
now simpler, more geometric, and more intuitive, and yet the end result is stronger
and more general: it easily generalizes to multiple-antenna channels and to the
Radar Problem.
The deﬁnition of the power spectral density of nonstationary stochastic pro-
cesses such as QAM signals remains, but new are the connections with the average
autocovariance function and its Fourier Transform. Also unchanged is the empha-
sis on the geometry of the space of energy-limited signals and on the isometry
properties of the Fourier Transform, the baseband representation of passband
signals, and complex sampling.
With the additional topics and over 150 new problems, the book can now be
used for a one- or two-semester graduate course on digital communications or for
a course on stochastic processes and detection theory.
A M O S L A P I D OT H is Professor of Information Theory at ETH Zurich, the Swiss
Federal Institute of Technology, and a Fellow of the IEEE. He received his Ph.D.
in Electrical Engineering from Stanford University, and has held the positions of
Assistant and Associate Professor at the Massachusetts Institute of Technology.
A FOUNDATION IN DIGITAL
COMMUNICATION
Second Edition
AMOS LAPIDOTH
ETH Zurich
University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
4843/24, 2nd Floor, Ansari Road, Daryaganj, Delhi – 110002, India
79 Anson Road, #06–04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of
education, learning, and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781107177321
DOI: 10.1017/9781316822708
c⃝Amos Lapidoth 2017
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2017
Printed in the United Kingdom by TJ International Ltd. Padstow Cornwall
A catalogue record for this publication is available from the British Library.
ISBN 978-1-107-17732-1 Hardback
Cambridge University Press has no responsibility for the persistence or accuracy of
URLs for external or third-party Internet Web sites referred to in this publication
and does not guarantee that any content on such Web sites is, or will remain,
accurate or appropriate.
To my family
Contents
Preface to the Second Edition
xvi
Preface to the First Edition
xviii
Acknowledgments for the Second Edition
xxvi
Acknowledgments for the First Edition
xxvii
1
Some Essential Notation
1
2
Signals, Integrals, and Sets of Measure Zero
4
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.2
Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.3
Integrating Complex-Valued Signals . . . . . . . . . . . . . . . . . . .
5
2.4
An Inequality for Integrals . . . . . . . . . . . . . . . . . . . . . . . .
6
2.5
Sets of Lebesgue Measure Zero . . . . . . . . . . . . . . . . . . . . .
7
2.6
Swapping Integration, Summation, and Expectation . . . . . . . . . .
10
2.7
Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
3
The Inner Product
14
3.1
The Inner Product . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.2
When Is the Inner Product Deﬁned?
. . . . . . . . . . . . . . . . . .
17
3.3
The Cauchy-Schwarz Inequality . . . . . . . . . . . . . . . . . . . . .
18
3.4
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.5
The Cauchy-Schwarz Inequality for Random Variables . . . . . . . . .
23
3.6
Mathematical Comments
. . . . . . . . . . . . . . . . . . . . . . . .
23
3.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
4
The Space L2 of Energy-Limited Signals
27
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
4.2
L2 as a Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . .
27
4.3
Subspace, Dimension, and Basis
. . . . . . . . . . . . . . . . . . . .
29
4.4
∥u∥2 as the “length” of the Signal u(·)
. . . . . . . . . . . . . . . .
31
4.5
Orthogonality and Inner Products . . . . . . . . . . . . . . . . . . . .
33
4.6
Orthonormal Bases
. . . . . . . . . . . . . . . . . . . . . . . . . . .
37
4.7
The Space L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
vii
viii
Contents
4.8
Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
51
4.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
5
Convolutions and Filters
54
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
5.2
Time Shifts and Reﬂections . . . . . . . . . . . . . . . . . . . . . . .
54
5.3
The Convolution Expression . . . . . . . . . . . . . . . . . . . . . . .
55
5.4
Thinking About the Convolution
. . . . . . . . . . . . . . . . . . . .
55
5.5
When Is the Convolution Deﬁned?
. . . . . . . . . . . . . . . . . . .
56
5.6
Basic Properties of the Convolution . . . . . . . . . . . . . . . . . . .
58
5.7
Filters
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
5.8
The Matched Filter
. . . . . . . . . . . . . . . . . . . . . . . . . . .
59
5.9
The Ideal Unit-Gain Lowpass Filter . . . . . . . . . . . . . . . . . . .
61
5.10
The Ideal Unit-Gain Bandpass Filter
. . . . . . . . . . . . . . . . . .
62
5.11
Young’s Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
5.12
Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
62
5.13
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
6
The Frequency Response of Filters and Bandlimited Signals
65
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
6.2
Review of the Fourier Transform
. . . . . . . . . . . . . . . . . . . .
65
6.3
The Frequency Response of a Filter . . . . . . . . . . . . . . . . . . .
77
6.4
Bandlimited Signals and Lowpass Filtering . . . . . . . . . . . . . . .
80
6.5
Bandlimited Signals Through Stable Filters . . . . . . . . . . . . . . .
90
6.6
The Bandwidth of a Product of Two Signals . . . . . . . . . . . . . .
91
6.7
Bernstein’s Inequality
. . . . . . . . . . . . . . . . . . . . . . . . . .
94
6.8
Time-Limited and Bandlimited Signals . . . . . . . . . . . . . . . . .
94
6.9
A Theorem by Paley and Wiener . . . . . . . . . . . . . . . . . . . .
97
6.10
Picket Fences and Poisson Summation . . . . . . . . . . . . . . . . .
97
6.11
Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
99
6.12
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
7
Passband Signals and Their Representation
104
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
7.2
Baseband and Passband Signals . . . . . . . . . . . . . . . . . . . . . 104
7.3
Bandwidth around a Carrier Frequency . . . . . . . . . . . . . . . . . 107
7.4
Real Passband Signals . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.5
The Analytic Signal . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
7.6
Baseband Representation of Real Passband Signals
. . . . . . . . . . 119
7.7
Energy-Limited Passband Signals . . . . . . . . . . . . . . . . . . . . 133
7.8
Shifting to Passband and Convolving . . . . . . . . . . . . . . . . . . 141
7.9
Mathematical Comments
. . . . . . . . . . . . . . . . . . . . . . . . 142
7.10
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
8
Complete Orthonormal Systems and the Sampling Theorem
147
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
8.2
Complete Orthonormal System . . . . . . . . . . . . . . . . . . . . . 147
8.3
The Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
Contents
ix
8.4
The Sampling Theorem . . . . . . . . . . . . . . . . . . . . . . . . . 152
8.5
The Samples of the Convolution
. . . . . . . . . . . . . . . . . . . . 156
8.6
Closed Subspaces of L2 . . . . . . . . . . . . . . . . . . . . . . . . . 156
8.7
An Isomorphism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
8.8
Prolate Spheroidal Wave Functions . . . . . . . . . . . . . . . . . . . 161
8.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
9
Sampling Real Passband Signals
168
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
9.2
Complex Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
9.3
Reconstructing xPB from its Complex Samples . . . . . . . . . . . . . 170
9.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
10 Mapping Bits to Waveforms
176
10.1
What Is Modulation?
. . . . . . . . . . . . . . . . . . . . . . . . . . 176
10.2
Modulating One Bit . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
10.3
From Bits to Real Numbers . . . . . . . . . . . . . . . . . . . . . . . 178
10.4
Block-Mode Mapping of Bits to Real Numbers . . . . . . . . . . . . . 179
10.5
From Real Numbers to Waveforms with Linear Modulation . . . . . . 181
10.6
Recovering the Signal Coeﬃcients with a Matched Filter
. . . . . . . 182
10.7
Pulse Amplitude Modulation
. . . . . . . . . . . . . . . . . . . . . . 184
10.8
Constellations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
10.9
Uncoded Transmission . . . . . . . . . . . . . . . . . . . . . . . . . . 187
10.10 Bandwidth Considerations . . . . . . . . . . . . . . . . . . . . . . . . 188
10.11 Design Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . 189
10.12 Some Implementation Considerations . . . . . . . . . . . . . . . . . . 191
10.13 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
11 Nyquist’s Criterion
195
11.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
11.2
The Self-Similarity Function of Energy-Limited Signals
. . . . . . . . 196
11.3
Nyquist’s Criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
11.4
The Self-Similarity Function of Integrable Signals
. . . . . . . . . . . 208
11.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
12 Stochastic Processes: Deﬁnition
213
12.1
Introduction and Continuous-Time Heuristics
. . . . . . . . . . . . . 213
12.2
A Formal Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
12.3
Describing Stochastic Processes . . . . . . . . . . . . . . . . . . . . . 216
12.4
Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . 216
12.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
13 Stationary Discrete-Time Stochastic Processes
219
13.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
13.2
Stationary Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
13.3
Wide-Sense Stationary Stochastic Processes . . . . . . . . . . . . . . 220
13.4
Stationarity and Wide-Sense Stationarity . . . . . . . . . . . . . . . . 221
13.5
The Autocovariance Function . . . . . . . . . . . . . . . . . . . . . . 222
x
Contents
13.6
The Power Spectral Density Function . . . . . . . . . . . . . . . . . . 224
13.7
The Spectral Distribution Function . . . . . . . . . . . . . . . . . . . 228
13.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
14 Energy and Power in PAM
232
14.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
14.2
Energy in PAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
14.3
Deﬁning the Power in PAM . . . . . . . . . . . . . . . . . . . . . . . 235
14.4
On the Mean of Transmitted Waveforms . . . . . . . . . . . . . . . . 237
14.5
Computing the Power in PAM
. . . . . . . . . . . . . . . . . . . . . 238
14.6
A More Formal Account . . . . . . . . . . . . . . . . . . . . . . . . . 249
14.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
15 Operational Power Spectral Density
257
15.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
15.2
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
15.3
Deﬁning the Operational PSD . . . . . . . . . . . . . . . . . . . . . . 262
15.4
The Operational PSD of Real PAM Signals
. . . . . . . . . . . . . . 266
15.5
A More Formal Account . . . . . . . . . . . . . . . . . . . . . . . . . 270
15.6
Operational PSD and Average Autocovariance Function . . . . . . . . 276
15.7
The Operational PSD of a Filtered Stochastic Process . . . . . . . . . 283
15.8
The Operational PSD and Power . . . . . . . . . . . . . . . . . . . . 285
15.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
16 Quadrature Amplitude Modulation
295
16.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
16.2
PAM for Passband? . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
16.3
The QAM Signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
16.4
Bandwidth Considerations . . . . . . . . . . . . . . . . . . . . . . . . 299
16.5
Orthogonality Considerations . . . . . . . . . . . . . . . . . . . . . . 300
16.6
Spectral Eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
16.7
QAM Constellations . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
16.8
Recovering the Complex Symbols via Inner Products . . . . . . . . . . 305
16.9
Filtering QAM Signals . . . . . . . . . . . . . . . . . . . . . . . . . . 309
16.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
17 Complex Random Variables and Processes
314
17.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
17.2
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
17.3
Complex Random Variables . . . . . . . . . . . . . . . . . . . . . . . 316
17.4
Complex Random Vectors . . . . . . . . . . . . . . . . . . . . . . . . 323
17.5
Discrete-Time Complex Stochastic Processes . . . . . . . . . . . . . . 328
17.6
Limits of Proper Complex Random Variables . . . . . . . . . . . . . . 334
17.7
On the Eigenvalues of Large Toeplitz Matrices . . . . . . . . . . . . . 337
17.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
18 Energy, Power, and PSD in QAM
341
18.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
Contents
xi
18.2
The Energy in QAM . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
18.3
The Power in QAM . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
18.4
The Operational PSD of QAM Signals . . . . . . . . . . . . . . . . . 349
18.5
A Formal Account of Power in Passband and Baseband . . . . . . . . 354
18.6
A Formal Account of the PSD in Baseband and Passband . . . . . . . 361
18.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
19 The Univariate Gaussian Distribution
373
19.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
19.2
Standard Gaussian Random Variables . . . . . . . . . . . . . . . . . . 373
19.3
Gaussian Random Variables . . . . . . . . . . . . . . . . . . . . . . . 375
19.4
The Q-Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
19.5
Integrals of Exponentiated Quadratics
. . . . . . . . . . . . . . . . . 382
19.6
The Moment Generating Function
. . . . . . . . . . . . . . . . . . . 383
19.7
The Characteristic Function of Gaussians . . . . . . . . . . . . . . . . 384
19.8
Central and Noncentral Chi-Square Random Variables . . . . . . . . . 386
19.9
The Limit of Gaussians Is Gaussian . . . . . . . . . . . . . . . . . . . 390
19.10 Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . 392
19.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
20 Binary Hypothesis Testing
395
20.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
20.2
Problem Formulation
. . . . . . . . . . . . . . . . . . . . . . . . . . 395
20.3
Guessing in the Absence of Observables
. . . . . . . . . . . . . . . . 397
20.4
The Joint Law of H and Y . . . . . . . . . . . . . . . . . . . . . . . 398
20.5
Guessing after Observing Y . . . . . . . . . . . . . . . . . . . . . . . 400
20.6
Randomized Decision Rules . . . . . . . . . . . . . . . . . . . . . . . 403
20.7
The MAP Decision Rule . . . . . . . . . . . . . . . . . . . . . . . . . 405
20.8
The ML Decision Rule . . . . . . . . . . . . . . . . . . . . . . . . . . 407
20.9
Performance Analysis: the Bhattacharyya Bound . . . . . . . . . . . . 408
20.10 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
20.11 (Nontelepathic) Processing . . . . . . . . . . . . . . . . . . . . . . . 411
20.12 Suﬃcient Statistics
. . . . . . . . . . . . . . . . . . . . . . . . . . . 416
20.13 Implications of Optimality . . . . . . . . . . . . . . . . . . . . . . . . 424
20.14 Multi-Dimensional Binary Gaussian Hypothesis Testing
. . . . . . . . 425
20.15 Guessing in the Presence of a Random Parameter . . . . . . . . . . . 431
20.16 Mathematical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
20.17 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
21 Multi-Hypothesis Testing
441
21.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441
21.2
The Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441
21.3
Optimal Guessing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
21.4
Example: Multi-Hypothesis Testing for 2D Signals . . . . . . . . . . . 447
21.5
The Union-of-Events Bound . . . . . . . . . . . . . . . . . . . . . . . 451
21.6
Multi-Dimensional M-ary Gaussian Hypothesis Testing
. . . . . . . . 458
21.7
Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . 464
21.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
xii
Contents
22 Suﬃcient Statistics
468
22.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
22.2
Deﬁnition and Main Consequence . . . . . . . . . . . . . . . . . . . . 469
22.3
Equivalent Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . 471
22.4
Identifying Suﬃcient Statistics
. . . . . . . . . . . . . . . . . . . . . 481
22.5
Suﬃcient Statistics for the M-ary Gaussian Problem . . . . . . . . . . 485
22.6
Irrelevant Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
22.7
Testing with Random Parameters . . . . . . . . . . . . . . . . . . . . 489
22.8
Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . 491
22.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
23 The Multivariate Gaussian Distribution
494
23.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
23.2
Notation and Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . 495
23.3
Some Results on Matrices . . . . . . . . . . . . . . . . . . . . . . . . 497
23.4
Random Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503
23.5
A Standard Gaussian Vector . . . . . . . . . . . . . . . . . . . . . . . 509
23.6
Gaussian Random Vectors . . . . . . . . . . . . . . . . . . . . . . . . 510
23.7
Jointly Gaussian Vectors . . . . . . . . . . . . . . . . . . . . . . . . . 523
23.8
Moments and Wick’s Formula . . . . . . . . . . . . . . . . . . . . . . 527
23.9
The Limit of Gaussian Vectors Is a Gaussian Vector . . . . . . . . . . 528
23.10 Conditionally-Independent Gaussian Vectors . . . . . . . . . . . . . . 529
23.11 Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . 533
23.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
24 Complex Gaussians and Circular Symmetry
540
24.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540
24.2
Scalars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540
24.3
Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
24.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558
25 Continuous-Time Stochastic Processes
560
25.1
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560
25.2
The Finite-Dimensional Distributions . . . . . . . . . . . . . . . . . . 560
25.3
Deﬁnition of a Gaussian SP . . . . . . . . . . . . . . . . . . . . . . . 563
25.4
Stationary Continuous-Time Processes . . . . . . . . . . . . . . . . . 564
25.5
Stationary Gaussian Stochastic Processes . . . . . . . . . . . . . . . . 566
25.6
Properties of the Autocovariance Function . . . . . . . . . . . . . . . 568
25.7
The Power Spectral Density of a Continuous-Time SP . . . . . . . . . 571
25.8
The Spectral Distribution Function . . . . . . . . . . . . . . . . . . . 573
25.9
The Average Power
. . . . . . . . . . . . . . . . . . . . . . . . . . . 576
25.10 Stochastic Integrals and Linear Functionals . . . . . . . . . . . . . . . 578
25.11 Linear Functionals of Gaussian Processes . . . . . . . . . . . . . . . . 585
25.12 The Joint Distribution of Linear Functionals . . . . . . . . . . . . . . 591
25.13 Filtering WSS Processes . . . . . . . . . . . . . . . . . . . . . . . . . 594
25.14 The PSD Revisited
. . . . . . . . . . . . . . . . . . . . . . . . . . . 600
25.15 White Gaussian Noise . . . . . . . . . . . . . . . . . . . . . . . . . . 603
25.16 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613
Contents
xiii
26 Detection in White Gaussian Noise
620
26.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 620
26.2
Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 620
26.3
From a Stochastic Process to a Random Vector . . . . . . . . . . . . 621
26.4
The Random Vector of Inner Products . . . . . . . . . . . . . . . . . 626
26.5
Optimal Guessing Rule
. . . . . . . . . . . . . . . . . . . . . . . . . 628
26.6
Performance Analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . 632
26.7
The Front-End Filter
. . . . . . . . . . . . . . . . . . . . . . . . . . 634
26.8
Detection in Passband . . . . . . . . . . . . . . . . . . . . . . . . . . 637
26.9
Some Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638
26.10 Detection in Colored Gaussian Noise . . . . . . . . . . . . . . . . . . 652
26.11 Multiple Antennas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 662
26.12 Detecting Signals of Inﬁnite Bandwidth . . . . . . . . . . . . . . . . . 664
26.13 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 665
27 Noncoherent Detection and Nuisance Parameters
670
27.1
Introduction and Motivation
. . . . . . . . . . . . . . . . . . . . . . 670
27.2
The Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672
27.3
From a SP to a Random Vector . . . . . . . . . . . . . . . . . . . . . 673
27.4
The Conditional Law of the Random Vector . . . . . . . . . . . . . . 675
27.5
An Optimal Detector
. . . . . . . . . . . . . . . . . . . . . . . . . . 678
27.6
The Probability of Error . . . . . . . . . . . . . . . . . . . . . . . . . 680
27.7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 681
27.8
Extension to M ≥2 Signals . . . . . . . . . . . . . . . . . . . . . . . 683
27.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 685
28 Detecting PAM and QAM Signals in White Gaussian Noise
688
28.1
Introduction and Setup
. . . . . . . . . . . . . . . . . . . . . . . . . 688
28.2
A Random Vector and Its Conditional Law . . . . . . . . . . . . . . . 689
28.3
Other Optimality Criteria . . . . . . . . . . . . . . . . . . . . . . . . 691
28.4
Consequences of Orthonormality
. . . . . . . . . . . . . . . . . . . . 693
28.5
Extension to QAM Communications
. . . . . . . . . . . . . . . . . . 696
28.6
Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . 703
28.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 703
29 Linear Binary Block Codes with Antipodal Signaling
707
29.1
Introduction and Setup
. . . . . . . . . . . . . . . . . . . . . . . . . 707
29.2
The Binary Field F2 and the Vector Space Fκ
2
. . . . . . . . . . . . . 708
29.3
Binary Linear Encoders and Codes
. . . . . . . . . . . . . . . . . . . 711
29.4
Binary Encoders with Antipodal Signaling
. . . . . . . . . . . . . . . 714
29.5
Power and Operational Power Spectral Density
. . . . . . . . . . . . 715
29.6
Performance Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . 719
29.7
Minimizing the Block Error Rate . . . . . . . . . . . . . . . . . . . . 720
29.8
Minimizing the Bit Error Rate . . . . . . . . . . . . . . . . . . . . . . 725
29.9
Assuming the All-Zero Codeword . . . . . . . . . . . . . . . . . . . . 729
29.10 System Parameters
. . . . . . . . . . . . . . . . . . . . . . . . . . . 734
29.11 Hard vs. Soft Decisions . . . . . . . . . . . . . . . . . . . . . . . . . 735
29.12 The Varshamov and Singleton Bounds . . . . . . . . . . . . . . . . . 735
xiv
Contents
29.13 Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . 736
29.14 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 737
30 The Radar Problem
740
30.1
The Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 740
30.2
The Radar and the Knapsack Problems . . . . . . . . . . . . . . . . . 746
30.3
Pareto-Optimality and Linear Functionals . . . . . . . . . . . . . . . . 747
30.4
One Type of Error Is Not Allowed . . . . . . . . . . . . . . . . . . . . 748
30.5
Likelihood-Ratio Tests . . . . . . . . . . . . . . . . . . . . . . . . . . 751
30.6
A Gaussian Example . . . . . . . . . . . . . . . . . . . . . . . . . . . 759
30.7
Detecting a Signal in White Gaussian Noise
. . . . . . . . . . . . . . 760
30.8
Suﬃcient Statistics
. . . . . . . . . . . . . . . . . . . . . . . . . . . 762
30.9
A Noncoherent Detection Problem . . . . . . . . . . . . . . . . . . . 763
30.10 Randomization Is Not Needed . . . . . . . . . . . . . . . . . . . . . . 768
30.11 The Big Picture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 772
30.12 Relative Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776
30.13 Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . 781
30.14 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 781
31 A Glimpse at Discrete-Time Signal Processing
786
31.1
Discrete-Time Filters
. . . . . . . . . . . . . . . . . . . . . . . . . . 786
31.2
Processing Discrete-Time Stochastic Processes . . . . . . . . . . . . . 789
31.3
Discrete-Time Whitening Filters
. . . . . . . . . . . . . . . . . . . . 794
31.4
Processing Discrete-Time Complex Processes
. . . . . . . . . . . . . 797
31.5
Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . 801
31.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 801
32 Intersymbol Interference
803
32.1
The Linearly-Dispersive Channel
. . . . . . . . . . . . . . . . . . . . 803
32.2
PAM on the ISI Channel . . . . . . . . . . . . . . . . . . . . . . . . . 803
32.3
Guessing the Data Bits
. . . . . . . . . . . . . . . . . . . . . . . . . 807
32.4
QAM on the ISI Channel
. . . . . . . . . . . . . . . . . . . . . . . . 818
32.5
From Passband to Baseband
. . . . . . . . . . . . . . . . . . . . . . 823
32.6
Additional Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . 827
32.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 828
A On the Fourier Series
830
A.1
Introduction and Preliminaries
. . . . . . . . . . . . . . . . . . . . . 830
A.2
Reconstruction in L1
. . . . . . . . . . . . . . . . . . . . . . . . . . 832
A.3
Geometric Considerations . . . . . . . . . . . . . . . . . . . . . . . . 835
A.4
Pointwise Reconstruction
. . . . . . . . . . . . . . . . . . . . . . . . 839
B On the Discrete-Time Fourier Transform
841
C Positive Deﬁnite Functions
845
D The Baseband Representation of Passband Stochastic Processes
848
Bibliography
858
Contents
xv
Theorems Referenced by Name
864
Abbreviations
865
List of Symbols
866
Index
875
Preface to the Second Edition
Without conceding a blemish in the ﬁrst edition, I think I had best come clean
and admit that I embarked on a second edition largely to adopt a more geometric
approach to the detection of signals in white Gaussian noise. Equally rigorous, yet
more intuitive, this approach is not only student-friendly, but also extends more
easily to the detection problem with random parameters and to the radar problem.
The new approach is based on the projection of white Gaussian noise onto a ﬁnite-
dimensional subspace (Section 25.15.2) and on the independence of this projec-
tion and the diﬀerence between noise and projection; see Theorem 25.15.6 and
Theorem 25.15.7. The latter theorem allows for a simple proof of the suﬃciency
of the matched-ﬁlters’ outputs without the need to deﬁne suﬃcient statistics for
continuous-time observables. The key idea is that—while the receiver cannot re-
cover the observable from its projection onto the subspace spanned by the mean
signals—it can mimic the performance of any receiver that bases its decision on
the observable using three steps (Figure 26.1 on Page 623): use local randomness
to generate an independent stochastic process whose law is equal to that of the
diﬀerence between the noise and its projection; add this stochastic process to the
projection; and feed the result to the original receiver.
But the new geometric approach was not the only impetus for a second edition.
I also wanted to increase the book’s scope. This edition contains new chapters
on the radar problem (Chapter 30), the intersymbol interference (ISI) channel
(Chapter 32), and on the mathematical preliminaries needed for its study (Chap-
ter 31). The treatment of the radar problem is fairly standard with two twists: we
characterize all achievable pairs of false-alarm and missed-detection probabilities
(pFA, pMD) and not just those that are Pareto-optimal. Moreover, we show that
when the observable has a density under both hypotheses, all achievable pairs can
be achieved using deterministic decision rules.
As to ISI channels, I adopted the classic approach of matched ﬁltering, discrete-
time noise whitening, and running the Viterbi Algorithm. I only allow (bounded-
input/bounded-output) stable whitening ﬁlters, i.e., ﬁlters whose impulse response
is absolutely summable; others often only require that the impulse response be
square summable. While my approach makes it more diﬃcult to prove the exis-
tence of whitening ﬁlters (and I do recommend that the proof be skipped), it is
conceptually much cleaner because the convolution of the noise sequence with the
impulse response exists with probability one. This results in the convolution being
associative, and therefore greatly simpliﬁes the proof that no information is lost in
the whitening process. It is also in line with the book’s philosophy of obtaining all
xvi
Preface to the Second Edition
xvii
results sample-wise.
The chapter on ISI channels also includes Section 32.5, which treats the detec-
tion of QAM signals where—as in most practical receivers—matched ﬁltering is
not performed at the carrier frequency but after conversion to baseband (or to
an intermediate frequency). An analysis of the complex stochastic process that
results when a (real) stationary Gaussian passband stochastic process is converted
to baseband can be found in Appendix D.
In addition, some original chapters were expanded. New sections on the opera-
tional power spectral density (in Chapter 15) and a new section on conditionally
independent Gaussians and the zeros of their precision matrix (Section 23.10) are
now included.
Last but not least, I have added over a hundred new exercises. Most reinforce and
test, but some present additional results. Enjoy!
Preface to the First Edition
Claude Shannon, the father of Information Theory, described the fundamental
problem of point-to-point communications in his classic 1948 paper as “that of
reproducing at one point either exactly or approximately a message selected at
another point.”
How engineers solve this problem is the subject of this book.
But unlike Shannon’s general problem, where the message can be an image, a
sound clip, or a movie, here we restrict ourselves to bits. We thus envision that
the original message is either a binary sequence to start with, or else that it was
described using bits by a device outside our control and that our job is to reproduce
the describing bits with high reliability. The issue of how images or text ﬁles are
converted eﬃciently into bits is the subject of lossy and lossless data compression
and is addressed in texts on information theory and on quantization.
The engineering solutions to the point-to-point communication problem greatly
depend on the available resources and on the channel between the points. They
typically bring together beautiful techniques from Fourier Analysis, Hilbert Spaces,
Probability Theory, and Decision Theory. The purpose of this book is to introduce
the reader to these techniques and to their interplay.
The book is intended for advanced undergraduates and beginning graduate stu-
dents. The key prerequisites are basic courses in Calculus, Linear Algebra, and
Probability Theory. A course in Linear Systems is a plus but not a must, because
all the results from Linear Systems that are needed for this book are summarized
in Chapters 5 and 6. But more importantly, the book requires a certain mathemat-
ical maturity and patience, because we begin with ﬁrst principles and develop the
theory before discussing its engineering applications. The book is for those who
appreciate the views along the way as much as getting to the destination; who like
to “stop and smell the roses;” and who prefer fundamentals to acronyms. I ﬁrmly
believe that those with a sound foundation can easily pick up the acronyms and
learn the jargon on the job, but that once one leaves the academic environment,
one rarely has the time or peace of mind to study fundamentals.
In the early stages of the planning of this book I took a decision that greatly
inﬂuenced the project. I decided that every key concept should be unambiguously
deﬁned; that every key result should be stated as a mathematical theorem; and that
every mathematical theorem should be correct. This, I believe, makes for a solid
foundation on which one can build with conﬁdence. But it is also a tall order. It
required that I scrutinize each “classical” result before I used it in order to be sure
that I knew what the needed qualiﬁers were, and it forced me to include background
material to which the reader may have already been exposed, because I needed the
xviii
Preface to the First Edition
xix
results with all the ﬁne print. Hence Chapters 5 and 6 on Linear Systems and
Fourier Analysis. This is also partly the reason why the book is so long. When I
started out my intention was to write a much shorter book. But I found that to do
justice to the beautiful mathematics on which Digital Communications is based I
had to expand the book.
Most physical-layer communication problems are at their core of a continuous-
time nature. The transmitted physical waveforms are functions of time and not
sequences synchronized to a clock. But most solutions ﬁrst reduce the problem to a
discrete-time setting and then solve the problem in the discrete-time domain. The
reduction to discrete-time often requires great ingenuity, which I try to describe.
It is often taken for granted in courses that open with a discrete-time model from
Lecture 1. I emphasize that most communication problems are of a continuous-
time nature, and that the reduction to discrete-time is not always trivial or even
possible. For example, it is extremely diﬃcult to translate a peak-power constraint
(stating that at no epoch is the magnitude of the transmitted waveform allowed to
exceed a given constant) to a statement about the sequence that is used to represent
the waveform. Similarly, in Wireless Communications it is often very diﬃcult to
reduce the received waveform to a sequence without any loss in performance.
The quest for mathematical precision can be demanding. I have therefore tried to
precede the statement of every key theorem with its gist in plain English. Instruc-
tors may well choose to present the material in class with less rigor and direct the
students to the book for a more mathematical approach. I would rather have text-
books be more mathematical than the lectures than the other way round. Having
a rigorous textbook allows the instructor in class to discuss the intuition knowing
that the students can obtain the technical details from the book at home.
The communication problem comes with a beautiful geometric picture that I try
to emphasize.
To appreciate this picture one needs the deﬁnition of the inner
product between energy-limited signals and some of the geometry of the space of
energy-limited signals. These are therefore introduced early on in Chapters 3 and 4.
Chapters 5 and 6 cover standard material from Linear Systems. But note the early
introduction of the matched ﬁlter as a mechanism for computing inner products
in Section 5.8. Also key is Parseval’s Theorem in Section 6.2.2 which relates the
geometric pictures in the time domain and in the frequency domain.
Chapter 7 deals with passband signals and their baseband representation. We em-
phasize how the inner product between passband signals is related to the inner
product between their baseband representations. This elegant geometric relation-
ship is often lost in the haze of various trigonometric identities. While this topic is
important in wireless applications, it is not always taught in a ﬁrst course in Digital
Communications. Instructors who prefer to discuss baseband communication only
can skip Chapters 7, 9, 16, 17, 18, 24, 27, and Sections 26.8, 28.5, 30.9, 31.4, 32.4,
32.5. But it would be a shame.
Chapter 8 presents the celebrated Sampling Theorem from a geometric perspective.
It is inessential to the rest of the book but is a striking example of the geometric
approach. Chapter 9 discusses the Sampling Theorem for passband signals.
Chapter 10 discusses modulation.
I have tried to motivate Linear Modulation
xx
Preface to the First Edition
and Pulse Amplitude Modulation and to minimize the use of the “that’s just how
it is done” argument. The use of the Matched Filter for detecting (here in the
absence of noise) is emphasized. This also motivates the Nyquist Theory, which is
treated in Chapter 11. I stress that the motivation for the Nyquist Theory is not
to avoid intersymbol interference at the sampling points but rather to guarantee
the orthogonality of the time shifts of the pulse shape by integer multiples of the
baud period. This ultimately makes more engineering sense and leads to cleaner
mathematics: compare Theorem 11.3.2 with its corollary, Corollary 11.3.4.
The result of modulating random bits is a stochastic process, a concept which is
ﬁrst encountered in Chapter 10; formally deﬁned in Chapter 12; and revisited in
Chapters 13, 17, 25, and 31. It is an important concept in Digital Communica-
tions, and I ﬁnd it best to ﬁrst introduce man-made synthesized stochastic processes
(as the waveforms produced by an encoder when fed random bits) and only later
to introduce the nature-made stochastic processes that model noise. Stationary
discrete-time stochastic processes are introduced in Chapter 13 and their complex
counterparts in Chapter 17. These are needed for the analysis in Chapter 14 of the
power in Pulse Amplitude Modulation and for the analysis in Chapter 17 of the
power in Quadrature Amplitude Modulation. They are revisited in Chapter 31,
which presents additional results that are needed in the study of intersymbol in-
terference channels (Chapter 32).
I emphasize that power is a physical quantity that is related to the time-averaged
energy in the continuous-time transmitted power. Its relation to the power in the
discrete-time modulating sequence is a nontrivial result. In deriving this relation
I refrain from adding random timing jitters that are often poorly motivated and
that turn out to be unnecessary. (The transmitted power does not depend on the
realization of the ﬁctitious jitter.) The Power Spectral Density in Pulse Amplitude
Modulation and Quadrature Amplitude Modulation is discussed in Chapters 15
and 18. The discussion requires a deﬁnition for Power Spectral Density for non-
stationary processes (Deﬁnitions 15.3.1 and 18.4.1) and a proof that this deﬁnition
coincides with the classical deﬁnition when the process is wide-sense stationary
(Theorem 25.14.3).
Chapter 19 opens the second part of the book, which deals with noise and detection.
It introduces the univariate Gaussian distribution and some related distributions.
The principles of Detection Theory are presented in Chapters 20–22. I emphasize
the notion of Suﬃcient Statistics, which is central to Detection Theory. Building
on Chapter 19, Chapter 23 introduces the all-important multivariate Gaussian
distribution. Chapter 24 treats the complex case.
Chapter 25 deals with continuous-time stochastic processes with an emphasis on
stationary Gaussian processes, which are often used to model the noise in Digital
Communications. This chapter also introduces white Gaussian noise. My approach
to this topic is perhaps new and is probably where this text diﬀers the most from
other textbooks on the subject.
I deﬁne white Gaussian noise of double-sided power spectral density N0/2
with respect to the bandwidth W as any measurable,1 stationary, Gaussian
1This book does not assume any Measure Theory and does not teach any Measure Theory.
Preface to the First Edition
xxi
−W
W
N0/2
f
SNN(f)
Figure 1: The power spectral density of a white Gaussian noise process of double-
sided power spectral density N0/2 with respect to the bandwidth W.
stochastic process whose power spectral density is a nonnegative, symmetric, inte-
grable function of frequency that is equal to N0/2 at all frequencies f satisfying
|f| ≤W. The power spectral density at other frequencies can be arbitrary. An
example of the power spectral density of such a process is depicted in Figure 1.
Adopting this deﬁnition has a number of advantages. The ﬁrst is, of course, that
such processes exist. One need not discuss “generalized processes,” Gaussian pro-
cesses with inﬁnite variances (that, by deﬁnition, do not exist), or introduce the
Itˆo calculus to study stochastic integrals. (Stochastic integrals with respect to the
Brownian motion are mathematically intricate and physically unappealing. The
idea of the noise having inﬁnite power is ludicrous.) The above deﬁnition also frees
me from discussing Dirac’s Delta, and, in fact, Dirac’s Delta is never used in this
book. (A rigorous treatment of Generalized Functions is beyond the engineering
curriculum in most schools, so using Dirac’s Delta always gives the reader the
unsettling feeling of being on unsure footing.)
The detection problem in white Gaussian noise is treated in Chapter 26. No course
in Digital Communications should end without Theorem 26.3.1. Roughly speaking,
this theorem states that if the mean-signals are bandlimited to W Hz and if the
noise is white Gaussian noise with respect to the bandwidth W, then there is no
loss of optimality in basing our guess on the projection of the received waveform
onto the subspace spanned by the mean-signals. Numerous examples as well as a
treatment of colored noise are also discussed in this chapter. Extensions to nonco-
herent detection are addressed in Chapter 27 and implications for Pulse Amplitude
Modulation and for Quadrature Amplitude Modulation in Chapter 28.
Coding is introduced in Chapter 29. It emphasizes how the code design inﬂuences
the transmitted power, the transmitted power spectral density, the required band-
(I do deﬁne sets of Lebesgue measure zero in order to be able to state uniqueness theorems.) I
use Measure Theory only in stating theorems that require measurability assumptions. This is
in line with my attempt to state theorems together with all the assumptions that are required
for their validity. I recommend that students ignore measurability issues and just make a mental
note that whenever measurability is mentioned there is a minor technical condition lurking in the
background.
xxii
Preface to the First Edition
width, and the probability of error. The construction of good codes is left to texts
on Coding Theory.
Motivated by the radar problem, Chapter 30 introduces the Neyman-Pearson the-
ory of hypothesis testing as well as the Kullback-Leibler divergence. And after some
mathematical preliminaries in Chapter 31, the book concludes with Chapter 32,
which introduces the intersymbol interference channel and the Viterbi Algorithm.
Basic Latin
Mathematics sometimes reads like a foreign language. I therefore include here a
short glossary for such terms as “i.e.,” “that is,” “in particular,” “a fortiori,” “for
example,” and “e.g.,” whose meaning in Mathematics is slightly diﬀerent from the
deﬁnition you will ﬁnd in your English dictionary. In mathematical contexts these
terms are actually logical statements that the reader should verify. Verifying these
statements is an important way to make sure that you understand the math.
What are these logical statements? First note the synonym “i.e.” = “that is” and
the synonym “e.g.” = “for example.” Next note that the term “that is” often
indicates that the statement following the term is equivalent to the one preceding
it: “We next show that p is a prime, i.e., that p is a positive integer larger than
one that is not divisible by any positive integer other than one and itself.” The
terms “in particular” or “a fortiori” indicate that the statement following them
is implied by the one preceding them: “Since g(·) is diﬀerentiable and, a fortiori,
continuous, it follows from the Mean Value Theorem that the integral of g(·) over
the interval [0, 1] is equal to g(ξ) for some ξ ∈[0, 1].” The term “for example” can
have its regular day-to-day meaning but in mathematical writing it also sometimes
indicates that the statement following it implies the one preceding it: “Suppose
that the function g(·) is monotonically nondecreasing, e.g., that it is diﬀerentiable
with a nonnegative derivative.”
Another important word to look out for is “indeed,” which in this book typically
signiﬁes that the statement just made is about to be expanded upon and explained.
So when you read something that is unclear to you, be sure to check whether the
next sentence begins with the word “indeed” before you panic.
The Latin phrases “a priori” and “a posteriori” show up in Probability Theory.
The former is usually associated with the unconditional probability of an event and
the latter with the conditional. Thus, the “a priori” probability that the sun will
shine this Sunday in Zurich is 25%, but now that I know that it is raining today,
my outlook on life changes and I assign this event the a posteriori probability of
15%.
The phrase “prima facie” is roughly equivalent to the phrase “before any further
mathematical arguments have been presented.” For example, the deﬁnition of the
projection of a signal v onto the signal u as the vector w that is collinear with u and
for which v−w is orthogonal to u, may be followed by the sentence: “Prima facie,
it is not clear that the projection always exists and that it is unique. Nevertheless,
as we next show, this is the case.”
Preface to the First Edition
xxiii
Syllabuses or Syllabi
The book can be used as a textbook for a number of diﬀerent courses. For a course
that focuses on deterministic signals one could use Chapters 1–9 and Chapter 11.
A course that covers Stochastic Processes and Detection Theory could be based
on Chapter 12, Chapters 19–26, and Chapter 30 with or without discrete-time
stochastic processes (Chapters 13 and 31) and with or without complex random
variables and processes (Chapters 17 and 24).
For a course on Digital Communications one could use the entire book or, if time
does not permit it, discuss only baseband communication. In the latter case one
could omit Chapters 7, 9, 16, 17, 18, 24, 27, and Sections 26.8, 28.5, 30.9, 31.4,
32.4, 32.5.
The dependencies between the chapters are depicted on Page xxiv. A simpler chart
pertaining only to baseband communication can be found on Page xxv.
The book’s web page is
www.afidc.ethz.ch
xxiv
Preface to the First Edition
1,2
3
4
5
6
10
11
7
8
12
13
17
14
16
9
15
18
19
23
24
20
25
21
22
26
27
28.
1–4
28.
5
30
29
31.
1–3
31.
4
32.
1–3
32.
4–5
A Dependency Diagram.
Preface to the First Edition
xxv
1,2
3
4
5
6
10
11
8
12
13
14
19
23
15
20
25
21
22
26
28.
1–4
30
29
31.
1–3
32.
1–3
A Dependency Diagram for Baseband Communications.
Acknowledgments for the Second Edition
I received help from numerous people in numerous ways. Some pointed out typos
in the ﬁrst edition, some oﬀered comments on the new material in the second
edition, and some suggested topics that ended up as new exercises. My thanks go
to all of them: the anonymous readers who reported typos through the book’s web
page, C´eline Aubel, Annina Bracher, Helmut B¨olcskei, Christoph Bunte, Paul Cuﬀ,
Samuel Gaehwiler, Johannes Huber, Tobias Koch, Gernot Kubin, Hans-Andrea
Loeliger, Mehdi Molkaraie, Stefan Moser, G¨otz Pfander, Christoph Pﬁster, Qiuting
Huang, Bixio Rimoldi, Igal Sason, Alain-Sol Sznitman, Emre Telatar, and Markos
Troulis. Finally, I thank my wife, Danielle Lapidoth-Berger, for her encouragement
and willingness to go through this all over again.
xxvi
Acknowledgments for the First Edition
This book has a long history. Its origins are in a course entitled “Introduction to
Digital Communication” that Bob Gallager and I developed at the Massachusetts
Institute of Technology (MIT) in the years 1997 (course number 6.917) and 1998
(course number 6.401). Assisting us in these courses were Emre Koksal and Poom-
pat Saengudomlert (Tengo) respectively.
The course was ﬁrst conceived as an
advanced undergraduate course, but at MIT it has since evolved into a ﬁrst-year
graduate course leading to the publication of the textbook (Gallager, 2008). At
ETH the course is still an advanced undergraduate course, and the lecture notes
evolved into the present book. Assisting me at ETH were my former and current
Ph.D. students Stefan Moser, Daniel H¨osli, Natalia Miliou, Stephan Tinguely, To-
bias Koch, Mich`ele Wigger, and Ligong Wang. I thank them all for their enormous
help. Marion Br¨andle was also a great help.
I also thank Bixio Rimoldi for his comments on an earlier draft of this book, from
which he taught at ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL) and Thomas
Mittelholzer, who used a draft of this book to teach a course at ETH during my
sabbatical.
Extremely helpful were discussions with Amir Dembo, Sanjoy Mitter, Alain-Sol
Sznitman, and Ofer Zeitouni about some of the more mathematical aspects of this
book. Discussions with Ezio Biglieri, Holger Boche, Stephen Boyd, Young-Han
Kim, and Sergio Verd´u are also gratefully acknowledged.
Special thanks are due to Bob Gallager and Dave Forney with whom I had endless
discussions about the material in this book both while at MIT and afterwards at
ETH. Their ideas have greatly inﬂuenced my thinking about how this course should
be taught.
I thank Helmut B¨olcskei, Andi Loeliger, and Nikolai Nefedov for having tolerated
my endless ramblings regarding Digital Communications during our daily lunches.
Jim Massey was a huge help in patiently answering my questions regarding English
usage. I should have asked him much more!
A number of dear colleagues read parts of this manuscript.
Their comments
were extremely useful. These include Helmut B¨olcskei, Moritz Borgmann, Samuel
Braendle, Shraga Bross, Giuseppe Durisi, Yariv Ephraim, Minnie Ho, Young-
Han Kim, Yiannis Kontoyiannis, Nick Laneman, Venya Morgenshtern, Prakash
Narayan, Igal Sason, Brooke Shrader, Aslan Tchamkerten, Sergio Verd´u, Pascal
Vontobel, and Ofer Zeitouni. I am especially indebted to Emre Telatar for his
enormous help in all aspects of this project.
xxvii
xxviii
Acknowledgments for the First Edition
I would like to express my sincere gratitude to the Rockefeller Foundation at whose
Study and Conference Center in Bellagio, Italy, this all began.
Finally, I thank my wife, Danielle, for her encouragement, her tireless editing, and
for making it possible for me to complete this project.
Chapter 1
Some Essential Notation
Reading a whole chapter about notation can be boring. We have thus chosen to
collect here only the essentials and to introduce the rest when it is ﬁrst used. The
“List of Symbols” on Page 866 is more comprehensive.
We denote the set of complex numbers by C, the set of real numbers by R, the set
of integers by Z, and the set of natural numbers (positive integers) by N. Thus,
N = {n ∈Z : n ≥1}.
The above equation is not meant to belabor the point. We use it to introduce the
notation
{x ∈A : statement}
for the set consisting of all those elements of the set A for which “statement” holds.
In treating real numbers, we use the notation (a, b), [a, b), [a, b], (a, b] to denote
open, half open on the right, closed, and half open on the left intervals of the real
line. Thus, for example,
[a, b) = {x ∈R : a ≤x < b}.
A statement followed by a comma and a condition indicates that the statement
holds whenever the condition is satisﬁed. For example,
|an −a| < ϵ,
n ≥n0
means that |an −a| < ϵ whenever n ≥n0.
We use I{statement} to denote the indicator of the statement. It is equal to 1, if
the statement is true, and it is equal to 0, if the statement is false. Thus
I{statement} =

1
if statement is true,
0
if statement is false.
In dealing with complex numbers we use i to denote the purely imaginary complex
number whose imaginary part is one
i =
√
−1.
1
2
Some Essential Notation
We use z∗to denote the complex conjugate of z, we use Re(z) to denote the real
part of z, we use Im(z) to denote the imaginary part of z, and we use |z| to denote
the absolute value (or “modulus”, or “complex magnitude”) of z. Thus, if z = a+ib,
where a, b ∈R, then z∗= a −ib, Re(z) = a, Im(z) = b, and |z| =
√
a2 + b2.
The notation used to deﬁne functions is extremely important and is, alas, some-
times confusing to students, so please pay attention. A function or a mapping
associates with each element in its domain a unique element in its codomain. If
a function has a name, the name is often written in bold as in u.1 Alternatively,
we sometimes denote a function u by u(·). The notation
u: A →B
indicates that u is a function of domain A and codomain B. The rule specifying
for each element of the domain the element in the codomain to which it is mapped
is often written to the right or underneath. Thus, for example,
u: R →(−5, ∞),
t 	→t2
indicates that the domain of the function u is the reals, that its codomain is the
set of real numbers that exceed −5, and that u associates with t the nonnegative
number t2. We write u(t) for the result of applying the mapping u to t. The range
of a mapping u: A →B is the set of all elements of the codomain B to which at
least one element in the domain is mapped by u:
range of

u: A →B

=

u(x) : x ∈A

.
(1.1)
The range of a mapping is a subset of its codomain. In the above example, the
range of the mapping is the set of nonnegative reals [0, ∞). A mapping u: A →B
is said to be onto (or surjective) if its range is equal to its codomain. Thus,
u: A →B is onto if, and only if, for every y ∈B there corresponds some x ∈A
(not necessarily unique) such that u(x) = y. If the range of g(·) is a subset of the
domain of h(·), then the composition of g(·) and h(·) is the mapping x 	→h

g(x)

,
which is denoted by h ◦g. A function u: A →B is said to be one-to-one (or
injective) if diﬀerent elements of A are mapped to diﬀerent elements of B, i.e., if
x1 ̸= x2 implies that u(x1) ̸= u(x2) (whenever x1, x2 ∈A).
Sometimes we do not specify the domain and codomain of a function if they are
clear from the context.
Thus, we might write u: t 	→v(t) cos(2πfct) without
making explicit what the domain and codomain of u are. In fact, if there is no
need to give a function a name, then we will not. For example, we might write t 	→
v(t) cos(2πfct) to designate the unnamed function that maps t to v(t) cos(2πfct).
(Here v(·) is some other function, which was presumably deﬁned before.)
If the domain of a function u is R and if the codomain is R, then we sometimes say
that u is a real-valued signal or a real signal, especially if the argument of u
stands for time. Similarly we shall sometimes refer to a function u: R →C as a
complex-valued signal or a complex signal. If we refer to u as a signal, then
1But some special functions such as the self-similarity function Rgg, the autocovariance func-
tion KXX, and the power spectral density SXX, which will be introduced in later chapters, are
not in boldface.
Some Essential Notation
3
the question whether it is complex-valued or real-valued should be clear from the
context, or else immaterial to the claim.
We caution the reader that, while u and u(·) denote functions, u(t) denotes the
result of applying u to t. If u is a real-valued signal then u(t) is a real number!
Given two signals u and v we deﬁne their superposition or sum as the signal
t 	→u(t) + v(t). We denote this signal by u + v. Also, if α ∈C and u is any signal,
then we deﬁne the ampliﬁcation of u by α as the signal t 	→α u(t). We denote
this signal by α u. Thus,
α u + β v
is the signal
t 	→α u(t) + β v(t).
We refer to the function that maps every element in its domain to zero as the all-
zero function and we denote it by 0. The all-zero signal 0 maps every t ∈R
to zero. If x: R →C is a signal that maps every t ∈R to x(t), then its reﬂection
or mirror image is denoted by ~x and is the signal that is deﬁned by
~x: t 	→x(−t).
Dirac’s Delta (which will hardly be mentioned in this book) is not a function.
A probability space is deﬁned as a triple (Ω, F, P), where the set Ω is the set of
experiment outcomes, the elements of the set F are subsets of Ω and are called
events, and where P : F →[0, 1] assigns probabilities to the various events. It is
assumed that F forms a σ-algebra, i.e., that Ω ∈F; that if a set is in F then so
is its complement (with respect to Ω); and that every ﬁnite or countable union of
elements of F is also an element of F. A random variable (RV) X is a mapping
from Ω to R that satisﬁes the technical condition that
{ω ∈Ω : X(ω) ≤ξ} ∈F,
ξ ∈R.
(1.2)
This condition guarantees that it is always meaningful to evaluate the probability
that the value of X is smaller or equal to ξ.
Chapter 2
Signals, Integrals, and Sets of Measure Zero
2.1
Introduction
The purpose of this chapter is not to develop the Lebesgue theory of integration.
Mastering this theory is not essential to understanding Digital Communications.
But some concepts from this theory are needed in order to state the main results
of Digital Communications in a mathematically rigorous way.
In this chapter
we introduce these required concepts and provide references to the mathematical
literature that develops them.
The less mathematically-inclined may gloss over most of this chapter. Readers
who interpret the integrals in this book as Riemann integrals; who interpret “mea-
surable” as “satisfying a minor mathematical restriction”; who interpret “a set of
Lebesgue measure zero” as “a set that is so small that integrals of functions are
insensitive to the values the integrand takes in this set”; and who swap orders of
summations, expectations and integrations fearlessly will not miss any engineering
insights.
But all readers should pay attention to the way the integral of complex-valued
signals is deﬁned (Section 2.3); to the basic inequality (2.13); and to the notation
introduced in (2.6).
2.2
Integrals
Recall that a real-valued signal u is a function u: R →R. The integral of u is
denoted by
 ∞
−∞
u(t) dt.
(2.1)
For (2.1) to be meaningful some technical conditions must be met. (You may re-
call from your calculus studies, for example, that not every function is Riemann
integrable.) In this book all integrals will be understood to be Lebesgue integrals,
but nothing essential will be lost on readers who interpret them as Riemann inte-
grals. For the Lebesgue integral to be deﬁned the integrand u must be a Lebesgue
measurable function. Again, do not worry if you have not studied the Lebesgue
integral or the notion of measurable functions. We point this out merely to cover
ourselves when we state various theorems. Also, for the integral in (2.1) to be
4
2.3 Integrating Complex-Valued Signals
5
deﬁned we insist that
 ∞
−∞
|u(t)| dt < ∞.
(2.2)
(There are ways of deﬁning the integral in (2.1) also when (2.2) is violated, but
they lead to fragile expressions that are diﬃcult to manipulate.)
A function u: R →R which is Lebesgue measurable and which satisﬁes (2.2) is
said to be integrable, and we denote the set of all such functions by L1. We shall
refrain from integrating functions that are not elements of L1.
2.3
Integrating Complex-Valued Signals
This section should assuage your fear of integrating complex-valued signals. (Some
of you may have a trauma from your Complex Analysis courses where you dealt
with integrals of functions from the complex plane to the complex plane. Here
things are much simpler because we are dealing only with integrals of functions
from the real line to the complex plane.)
We formally deﬁne the integral of a
complex-valued function u: R →C by
 ∞
−∞
u(t) dt ≜
 ∞
−∞
Re

u(t)

dt + i
 ∞
−∞
Im

u(t)

dt.
(2.3)
For this to be meaningful, we require that the real functions t 	→Re

u(t)

and
t 	→Im

u(t)

both be integrable real functions.
That is, they should both be
Lebesgue measurable and we should have
 ∞
−∞
Re

u(t)
 dt < ∞
and
 ∞
−∞
Im

u(t)
 dt < ∞.
(2.4)
It is not diﬃcult to show that (2.4) is equivalent to the more compact condition
 ∞
−∞
u(t)
 dt < ∞.
(2.5)
We say that a complex signal u: R →C is Lebesgue measurable if the mappings
t 	→Re

u(t)

and t 	→Im

u(t)

are Lebesgue measurable real signals. We say that
a function u: R →C is integrable if it is Lebesgue measurable and (2.4) holds.
The set of all Lebesgue measurable integrable complex signals is denoted by L1.
Note that we use the same symbol L1 to denote both the set of integrable real
signals and the set of integrable complex signals. To which of these two sets we
refer should be clear from the context, or else immaterial.
For u ∈L1 we deﬁne ∥u∥1 as
∥u∥1 ≜
 ∞
−∞
u(t)
 dt.
(2.6)
6
Signals, Integrals, and Sets of Measure Zero
Before summarizing the key properties of the integral of complex signals we remind
the reader that if u and v are complex signals and if α, β are complex numbers, then
the complex signal α u+β v is deﬁned as the complex signal t 	→α u(t)+β v(t). The
intuition for the following proposition comes from thinking about the integrals as
Riemann integrals, which can be approximated by ﬁnite sums and by then invoking
the analogous results about ﬁnite sums.
Proposition 2.3.1 (Properties of Complex Integrals). Let the complex signals u, v
be in L1, and let α, β be arbitrary complex numbers.
(i) Integration is linear in the sense that α u + β v ∈L1 and
 ∞
−∞

α u(t) + β v(t)

dt = α
 ∞
−∞
u(t) dt + β
 ∞
−∞
v(t) dt.
(2.7)
(ii) Integration commutes with complex conjugation
 ∞
−∞
u∗(t) dt =
	 ∞
−∞
u(t) dt

∗
.
(2.8)
(iii) Integration commutes with the operation of taking the real part
Re
	 ∞
−∞
u(t) dt

=
 ∞
−∞
Re

u(t)

dt.
(2.9)
(iv) Integration commutes with the operation of taking the imaginary part
Im
	 ∞
−∞
u(t) dt

=
 ∞
−∞
Im

u(t)

dt.
(2.10)
Proof. For a proof of (i) see, for example, (Rudin, 1987, Theorem 1.32). The rest
of the claims follow easily from the deﬁnition of the integral of a complex-valued
signal (2.3).
2.4
An Inequality for Integrals
Probably the most important inequality for complex numbers is the Triangle
Inequality for Complex Numbers
|w + z| ≤|w| + |z|,
w, z ∈C.
(2.11)
This inequality extends by induction to ﬁnite sums:

n

j=1
zj
 ≤
n

j=1
|zj| ,
z1, . . . , zn ∈C.
(2.12)
The extension to integrals is the most important inequality for integrals:
2.5 Sets of Lebesgue Measure Zero
7
Proposition 2.4.1. For every complex-valued or real-valued signal u in L1

 ∞
−∞
u(t) dt
 ≤
 ∞
−∞
u(t)
 dt.
(2.13)
Proof. See, for example, (Rudin, 1987, Theorem 1.33).
Note that in (2.13) we should interpret | · | as the absolute-value function if u is a
real signal, and as the modulus function if u is a complex signal.
Another simple but useful inequality is
∥u + v∥1 ≤∥u∥1 + ∥v∥1 ,
u, v ∈L1,
(2.14)
which can be proved using the calculation
∥u + v∥1 =
 ∞
−∞
|u(t) + v(t)| dt
≤
 ∞
−∞

|u(t)| + |v(t)|

dt
=
 ∞
−∞
|u(t)| dt +
 ∞
−∞
|v(t)| dt
= ∥u∥1 + ∥v∥1 ,
where the inequality follows by applying the Triangle Inequality for Complex Num-
bers (2.11) with the substitution of u(t) for w and v(t) for z.
2.5
Sets of Lebesgue Measure Zero
It is one of life’s minor grievances that the integral of a nonnegative function can
be zero even if the function is not identically zero. For example, t 	→I{t = 17} is a
nonnegative function whose integral is zero and which is nonetheless not identically
zero (it maps 17 to one). In this section we shall derive a necessary and suﬃcient
condition for the integral of a nonzero function to be zero. This condition will
allow us later to state conditions under which various integral inequalities hold
with equality. It will give mathematical meaning to the physical intuition that if
the waveform describing some physical phenomenon (such as voltage over a resistor)
is nonnegative and integrates to zero then “for all practical purposes” the waveform
is zero.
We shall deﬁne sets of Lebesgue measure zero and then show that a nonnegative
function u: R →[0, ∞) integrates to zero if, and only if, the set {t ∈R : u(t) > 0} is
of Lebesgue measure zero. Thus, whether or not a nonnegative function integrates
to zero depends on the set over which it takes on positive values and not on the
actual values. We shall then introduce the notation u ≡v to indicate that the set
{t ∈R : u(t) ̸= v(t)} is of Lebesgue measure zero.
It should be noted that since the integral is unaltered when the integrand is changed
at a ﬁnite (or countable) number of points, it follows that any nonnegative function
8
Signals, Integrals, and Sets of Measure Zero
that is zero except at a countable number of points integrates to zero. The reverse,
however, is not true. One can ﬁnd nonnegative functions that integrate to zero
and that are nonzero on an uncountable set of points.
The less mathematically inclined readers may skip the mathematical deﬁnition of
sets of measure zero and just think of a subset of the real line as being of Lebesgue
measure zero if it is so “small” that the integral of any function is unaltered when
the values it takes in the subset are altered. Such readers should then think of the
statement u ≡v as indicating that u −v is just the result of altering the all-zero
signal 0 on a set of Lebesgue measure zero and that, consequently,
 ∞
−∞
|u(t) −v(t)| dt = 0.
Deﬁnition 2.5.1 (Sets of Lebesgue Measure Zero). We say that a subset N of
the real line R is a set of Lebesgue measure zero (or a Lebesgue null set)
if for every ϵ > 0 we can ﬁnd a sequence of intervals [a1, b1], [a2, b2], . . . such that
the total length of the intervals is smaller than or equal to ϵ
∞

j=1
(bj −aj) ≤ϵ
(2.15a)
and such that the union of the intervals covers N
N ⊆[a1, b1] ∪[a2, b2] ∪· · · .
(2.15b)
As an example, note that the set {1} is of Lebesgue measure zero. Indeed, it is
covered by the single interval [1 −ϵ/2, 1 + ϵ/2] whose length is ϵ. Similarly, any
ﬁnite set is of Lebesgue measure zero. Indeed, the set {α1, . . . , αn} can be covered
by n intervals of total length not exceeding ϵ as follows:
{α1, . . . , αn} ⊂

α1 −ϵ/(2n), α1 + ϵ/(2n)

∪· · · ∪

αn −ϵ/(2n), αn + ϵ/(2n)

.
This argument can be also extended to show that any countable set is of Lebesgue
measure zero. Indeed the countable set {α1, α2, . . .} can be covered as
{α1, α2, . . .} ⊆
∞

j=1

αj −2−j−1ϵ, αj + 2−j−1ϵ

where we note that the length of the interval

αj −2−j−1ϵ, αj + 2−j−1ϵ

is 2−jϵ,
which when summed over j yields ϵ.
With a similar argument one can show that the union of a countable number of
sets of Lebesgue measure zero is of Lebesgue measure zero.
The above examples notwithstanding, it should be emphasized that there exist sets
of Lebesgue measure zero that are not countable.1 Thus, the concept of a set of
Lebesgue measure zero is diﬀerent from the concept of a countable set.
1For example, the Cantor set is of Lebesgue measure zero and uncountable; see (Rudin, 1976,
Section 11.11, Remark (f), p. 309).
2.5 Sets of Lebesgue Measure Zero
9
Loosely speaking, we say that two signals are indistinguishable if they agree except
possibly on a set of Lebesgue measure zero. We warn the reader, however, that
this terminology is not standard.
Deﬁnition 2.5.2 (Indistinguishable Functions). We say that the Lebesgue measur-
able functions u, v from R to C (or to R) are indistinguishable and write
u ≡v
if the set {t ∈R : u(t) ̸= v(t)} is of Lebesgue measure zero.
Note that u ≡v if, and only if, the signal u −v is indistinguishable from the
all-zero signal 0

u ≡v

⇐⇒

u −v ≡0

.
(2.16)
The main result of this section is the following:
Proposition 2.5.3.
(i) A nonnegative Lebesgue measurable signal integrates to zero if, and only if,
it is indistinguishable from the all-zero signal 0.
(ii) If u, v are Lebesgue measurable functions from R to C (or to R), then
  ∞
−∞
|u(t) −v(t)| dt = 0

⇐⇒

u ≡v

(2.17)
and
  ∞
−∞
|u(t) −v(t)|2 dt = 0

⇐⇒

u ≡v

.
(2.18)
(iii) If u and v are integrable and indistinguishable, then their integrals are equal:

u ≡v

=⇒
  ∞
−∞
u(t) dt =
 ∞
−∞
v(t) dt

,
u, v ∈L1.
(2.19)
Proof. The proof of (i) is not very diﬃcult, but it requires more familiarity with
Measure Theory than we are willing to assume.
The interested reader is thus
referred to (Rudin, 1987, Theorem 1.39).
The equivalence in (2.17) follows by applying Part (i) to the nonnegative function
t 	→|u(t) −v(t)|. Similarly, (2.18) follows by applying Part (i) to the nonnegative
function t 	→|u(t)−v(t)|2 and by noting that the set of t’s for which |u(t)−v(t)|2 ̸= 0
is the same as the set of t’s for which u(t) ̸= v(t).
Part (iii) follows from (2.17) by noting that

 ∞
−∞
u(t) dt −
 ∞
−∞
v(t) dt
 =

 ∞
−∞

u(t) −v(t)

dt

≤
 ∞
−∞
u(t) −v(t)
 dt,
where the ﬁrst equality follows by the linearity of integration, and where the sub-
sequent inequality follows from Proposition 2.4.1.
10
Signals, Integrals, and Sets of Measure Zero
2.6
Swapping Integration, Summation, and Expectation
In numerous places in this text we shall swap the order of integration as in
 ∞
−∞
	 ∞
−∞
u(α, β) dα

dβ =
 ∞
−∞
	 ∞
−∞
u(α, β) dβ

dα
(2.20)
or the order of summation as in
∞

ν=1
	 ∞

η=1
aν,η

=
∞

η=1
	 ∞

ν=1
aν,η

(2.21)
or the order of summation and integration as in
 ∞
−∞
	 ∞

ν=1
aν uν(t)

dt =
∞

ν=1
	
aν
 ∞
−∞
uν(t) dt

(2.22)
or the order of integration and expectation as in
E
 ∞
−∞
X u(t) dt

=
 ∞
−∞
E[X u(t)] dt = E[X]
 ∞
−∞
u(t) dt.
These changes of order are usually justiﬁed using Fubini’s Theorem, which states
that these changes of order are permissible provided that a very technical measura-
bility condition is satisﬁed and that, in addition, either the integrand is nonnegative
or that in some order (and hence in all orders) the integrals/summation/expectation
of the absolute value of the integrand is ﬁnite.
For example, to justify (2.20) it suﬃces to verify that the function u: R2 →R in
(2.20) is Lebesgue measurable and that, in addition, it is either nonnegative or
 ∞
−∞
	 ∞
−∞
|u(α, β)| dα

dβ < ∞
or
 ∞
−∞
	 ∞
−∞
|u(α, β)| dβ

dα < ∞.
Similarly, to justify (2.21) it suﬃces to show that aν,η ≥0 or that
∞

η=1
	 ∞

ν=1
|aν,η|

< ∞
or that
∞

ν=1
	 ∞

η=1
|aν,η|

< ∞.
(No need to worry about measurability which is automatic in this setup.)
As a ﬁnal example, to justify (2.22) it suﬃces that the functions {uν} are all
measurable and that either aν uν(t) is nonnegative for all ν ∈N and t ∈R or
 ∞
−∞
	 ∞

ν=1
|aν| |uν(t)|

dt < ∞
2.7 Additional Reading
11
or
∞

ν=1
|aν|
	 ∞
−∞
|uν(t)| dt

< ∞.
A precise statement of Fubini’s Theorem requires some Measure Theory that is
beyond the scope of this book. The reader is referred to (Rudin, 1987, Theorem
8.8) and (Billingsley, 1995, Chapter 3, Section 18) for such a statement and for a
proof.
We shall frequently use the swapping-of-order argument to manipulate the square
of a sum or the square of an integral.
Proposition 2.6.1.
(i) If 
ν |aν| < ∞then
	 ∞

ν=1
aν

2
=
∞

ν=1
∞

ν′=1
aνaν′.
(2.23)
(ii) If u is an integrable real-valued or complex-valued signal, then
	 ∞
−∞
u(α) dα

2
=
 ∞
−∞
 ∞
−∞
u(α) u(α′) dα dα′.
(2.24)
Proof. The proof is a direct application of Fubini’s Theorem. But ignoring the
technicalities, the intuition is quite clear: it all boils down to the fact that (a + b)2
can be written as (a+b)(a+b), which can in turn be written as aa+ab+ba+bb.
2.7
Additional Reading
Numerous books cover the basics of Lebesgue integration. Classic examples are
(Riesz and Sz.-Nagy, 1990), (Rudin, 1987), and (Royden and Fitzpatrick, 2010).
These texts also cover the notion of sets of Lebesgue measure zero, e.g., (Riesz
and Sz.-Nagy, 1990, Chapter 1, Section 2). For the changing of order of Riemann
integration see (K¨orner, 1988, Chapters 47 & 48).
2.8
Exercises
Exercise 2.1 (Scaling the Integrand). Starting from the deﬁnition of the integral of a
complex signal (2.3), prove that if u is integrable and a is a complex number, then
 ∞
−∞
a u(t) dt = a
 ∞
−∞
u(t) dt.
Exercise 2.2 (A Useful Change-of-Variable). Let x be an integrable signal, and let ~x be
its mirror image. Prove that
 ∞
−∞
~x(t) dt =
 ∞
−∞
x(t) dt.
12
Signals, Integrals, and Sets of Measure Zero
Exercise 2.3 (Integrating an Exponential). Show that
 ∞
0
e−zt dt = 1
z ,
Re(z) > 0.
Exercise 2.4 (The Sinc Is not Integrable). Show that the mapping t →sin(πt)/(πt) is
not integrable irrespective of how we deﬁne it at zero.
Hint: Lower-bound the integral of the absolute value of this function from n to n + 1 by
the integral from n + 1/4 to n + 3/4. Lower-bound the integrand in this region, and use
the fact that the harmonic sum 
k≥1 1/k diverges.
Exercise 2.5 (Triangle Inequality for Complex Numbers). Prove the Triangle Inequality
for complex numbers (2.11). Under what conditions does it hold with equality?
Exercise 2.6 (When Are Complex Numbers Equal?). Prove that if the complex numbers
w and z are such that Re(βz) = Re(βw) for all β ∈C, then w = z.
Exercise 2.7 (Bounding Complex Exponentials). Show that
eiθ −1
 ≤|θ|,
θ ∈R.
Exercise 2.8 (An Integral Inequality). Show that if u, v, and w are integrable signals,
then
 ∞
−∞
u(t) −w(t)
 dt ≤
 ∞
−∞
u(t) −v(t)
 dt +
 ∞
−∞
v(t) −w(t)
 dt.
Exercise 2.9 (An Integral to Note). Given some f ∈R, compute the integral
 ∞
−∞
I{t = 17} e−i2πft dt.
Exercise 2.10 (Subsets of Sets of Lebesgue Measure Zero). Show that a subset of a set
of Lebesgue measure zero must also be of Lebesgue measure zero.
Exercise 2.11 (The Union of Sets of Lebesgue Measure Zero). Show that the union of
two sets that are each of Lebesgue measure zero is also of Lebesgue measure zero.
Exercise 2.12 (Nonuniqueness of the Probability Density Function). We say that the
random variable X is of density fX(·) if fX(·) is a (Lebesgue measurable) nonnegative
function such that
Pr[X ≤x] =
 x
−∞
fX(ξ) dξ,
x ∈R.
Show that if X is of density fX(·) and if g(·) is a nonnegative function that is indistin-
guishable from fX(·), then X is also of density g(·). (The reverse is also true: if X is of
density g1(·) and also of density g2(·), then g1(·) and g2(·) must be indistinguishable.)
Exercise 2.13 (Indistinguishability). Let ψ : R2 →R satisfy ψ(α, β) ≥0, for all α, β ∈R
with equality only if α = β. Let u and v be Lebesgue measurable signals. Show that
 ∞
−∞
ψ

u(t), v(t)

dt = 0

=⇒
	
v ≡u

.
2.8 Exercises
13
Exercise 2.14 (Indistinguishable Signals). Show that if the Lebesgue measurable signals g
and h are indistinguishable, then the set of epochs t ∈R where the sums ∞
j=−∞g(t + j)
and ∞
j=−∞h(t + j) are diﬀerent (in the sense that they both converge but to diﬀerent
limits or that one converges but the other does not) is of Lebesgue measure zero.
Exercise 2.15 (Continuous Nonnegative Functions). A subset of R containing a nonempty
open interval cannot be of Lebesgue measure zero. Use this fact to show that if a con-
tinuous function g: R →R is nonnegative except perhaps on a set of Lebesgue measure
zero, then the exception set is empty and the function is nonnegative.
Exercise 2.16 (Order of Summation Sometimes Matters). For every ν, η ∈N deﬁne
aν,η =
⎧
⎪
⎨
⎪
⎩
2 −2−ν
if ν = η
−2 + 2−ν
if ν = η + 1
0
otherwise.
Show that (2.21) is not satisﬁed. See (Royden and Fitzpatrick, 2010, Section 20.1, Exer-
cise 5).
Exercise 2.17 (Using Fubini’s Theorem). Using the relation
1
x =
 ∞
0
e−xt dt,
x > 0
and Fubini’s Theorem, show that
lim
α→∞
 α
0
sin x
x
dx = π
2 .
See (Rudin, 1987, Chapter 8, Exercise 12).
Hint: See also Problem 2.3.
Chapter 3
The Inner Product
3.1
The Inner Product
The inner product is central to Digital Communications, so it is best to introduce
it early. The motivation will have to wait.
Recall that u: A →B indicates that u (sometimes denoted u(·)) is a function
(or mapping) that maps each element in its domain A to an element in its
codomain B.
If both the domain and the codomain of u are the set of real
numbers R, then we sometimes refer to u as being a real signal, especially if the
argument of u(·) stands for time. Similarly, if u: R →C where C denotes the set
of complex numbers and the argument of u(·) stands for time, then we sometimes
refer to u as a complex signal.
The inner product between two real functions u: R →R and v: R →R is
denoted ⟨u, v⟩and is deﬁned as
⟨u, v⟩≜
 ∞
−∞
u(t) v(t) dt,
(3.1)
whenever the integral is deﬁned.1(In Section 3.2 we shall study conditions under
which the integral is deﬁned, i.e., conditions on the functions u and v that guar-
antee that the product function t 	→u(t) v(t) is an integrable function.)
The signals that arise in our study of Digital Communications often represent
electric ﬁelds or voltages over resistors. The energy required to generate them is
thus proportional to the integral of their squared magnitude. This motivates us to
deﬁne the energy of a Lebesgue measurable real-valued function u: R →R as
 ∞
−∞
u2(t) dt.
(If this integral is not ﬁnite, then we say that u is of inﬁnite energy.) We say that
u: R →R is of ﬁnite energy if it is Lebesgue measurable and if
 ∞
−∞
u2(t) dt < ∞.
1We use the term inner product much more freely than some mathematical texts that reserve
this term for inner product spaces.
14
3.1 The Inner Product
15
The class of all ﬁnite-energy real-valued functions u: R →R is denoted by L2.
Since the energy of u: R →R is nonnegative, we can discuss its nonnegative square
root, which we denote2 by ∥u∥2:
∥u∥2 ≜
 ∞
−∞
u2(t) dt.
(3.2)
(Throughout this book we denote by √ξ the nonnegative square root of ξ for every
ξ ≥0.) We can now express the energy in u using the inner product as
∥u∥2
2 =
 ∞
−∞
u2(t) dt
= ⟨u, u⟩.
(3.3)
In writing ∥u∥2
2 above we used diﬀerent fonts for the subscript and the superscript.
The subscript is just a graphical character which is part of the notation ∥·∥2. We
could have replaced it with ♦and designated the energy by ∥u∥2
♦without any
change in mathematical meaning.3 The superscript, however, indicates that the
quantity ∥u∥2 is being squared.
For complex-valued functions u: R →C and v: R →C we deﬁne the inner product
⟨u, v⟩as
⟨u, v⟩≜
 ∞
−∞
u(t) v∗(t) dt,
(3.4)
whenever the integral is deﬁned. Here v∗(t) denotes the complex conjugate of v(t).
The above integral in (3.4) is a complex integral, but that should not worry you:
it can also be written as
⟨u, v⟩=
 ∞
−∞
Re

u(t) v∗(t)

dt + i
 ∞
−∞
Im

u(t) v∗(t)

dt,
(3.5)
where i = √−1 and where Re(·) and Im(·) denote the functions that map a complex
number to its real and imaginary parts: Re(a+ib) = a and Im(a+ib) = b whenever
a, b ∈R. Each of the two integrals appearing in (3.5) is the integral of a real signal.
See Section 2.3.
Note that (3.1) and (3.4) are in agreement in the sense that if u and v happen
to take on only real values (i.e., satisfy that u(t), v(t) ∈R for every t ∈R), then
viewing them as real functions and thus using (3.1) would yield the same inner
product as viewing them as (degenerate) complex functions and using (3.4). Note
also that for complex functions u, v: R →C the inner product ⟨u, v⟩is in general
not the same as ⟨v, u⟩. One is the complex conjugate of the other.
2The subscript 2 is here to distinguish ∥u∥2 from ∥u∥1 , where the latter was deﬁned in (2.6)
as ∥u∥1 =
 ∞
−∞|u(t)| dt.
3We prefer ∥·∥2 to ∥·∥♦because it reminds us that in the deﬁnition (3.2) the integrand is
raised to the second power. This should be contrasted with the symbol ∥·∥1 where the magnitude
of the integrand is raised to the ﬁrst power (and where no square root is taken of the result); see
(2.6).
16
The Inner Product
Some of the properties of the inner product between complex-valued functions
u, v: R →C are given below.
⟨u, v⟩= ⟨v, u⟩∗
(3.6)
⟨α u, v⟩= α ⟨u, v⟩,
α ∈C
(3.7)
⟨u, α v⟩= α∗⟨u, v⟩,
α ∈C
(3.8)
⟨u1 + u2, v⟩= ⟨u1, v⟩+ ⟨u2, v⟩
(3.9)
⟨u, v1 + v2⟩= ⟨u, v1⟩+ ⟨u, v2⟩.
(3.10)
The above equalities hold whenever the inner products appearing on the right-
hand side (RHS) are deﬁned. The reader is encouraged to produce a similar list of
properties for the inner product between real-valued functions u, v: R →R.
The energy in a Lebesgue measurable complex-valued function u: R →C is de-
ﬁned as
 ∞
−∞
u(t)
2 dt,
where |·| denotes absolute value so |a + ib| =
√
a2 + b2 whenever a, b ∈R. This
deﬁnition of energy might seem a bit contrived because there is no such thing
as complex voltage, so prima facie it seems meaningless to deﬁne the energy of
a complex signal. But this is not the case. Complex signals are used to repre-
sent real passband signals, and the representation is such that the energy in the
real passband signal is proportional to the integral of the squared modulus of the
complex-valued signal representing it; see Section 7.6 ahead.
Deﬁnition 3.1.1 (Energy-Limited Signal). We say that u: R →C is energy-
limited or of ﬁnite energy if u is Lebesgue measurable and
 ∞
−∞
u(t)
2 dt < ∞.
The set of all energy-limited complex-valued functions u: R →C is denoted L2.
Note that whether L2 stands for the class of energy-limited complex-valued or
real-valued functions should be clear from the context, or else immaterial.
For every u ∈L2 we deﬁne ∥u∥2 as the nonnegative square root of its energy
∥u∥2 ≜

⟨u, u⟩,
(3.11)
so
∥u∥2 =
 ∞
−∞
|u(t)|2 dt.
(3.12)
Again (3.12) and (3.2) are in agreement in the sense that for every u: R →R,
computing ∥u∥2 via (3.2) yields the same result as if we viewed u as mapping
from R to C and computed ∥u∥2 via (3.12).
3.2 When Is the Inner Product Deﬁned?
17
3.2
When Is the Inner Product Deﬁned?
As noted in Section 2.2, in this book we shall only discuss the integral of integrable
functions, where a function u: R →R is integrable if it is Lebesgue measurable
and if
 ∞
−∞|u(t)| dt < ∞. (We shall sometimes make an exception for functions
that take on only nonnegative values. If u: R →[0, ∞) is Lebesgue measurable
and if

u(t) dt is not ﬁnite, then we shall say that

u(t) dt = +∞.)
Similarly, as in Section 2.3, in integrating complex signals u: R →C we limit
ourselves to signals that are integrable in the sense that both t 	→Re

u(t)

and
t 	→Im

u(t)

are Lebesgue measurable real-valued signals and
 ∞
−∞|u(t)| dt < ∞.
Consequently, we shall say that the inner product between u: R →C and v: R →C
is well-deﬁned only when they are both Lebesgue measurable (thus implying that
t 	→u(t) v∗(t) is Lebesgue measurable) and when
 ∞
−∞
u(t) v(t)
 dt < ∞.
(3.13)
We next discuss conditions on the Lebesgue measurable complex signals u and v
that guarantee that (3.13) holds. The simplest case is when one of the functions,
say u, is bounded and the other, say v, is integrable. Indeed, if σ∞∈R is such
that |u(t)| ≤σ∞for all t ∈R, then |u(t) v(t)| ≤σ∞|v(t)| and
 ∞
−∞
u(t) v(t)
 dt ≤σ∞
 ∞
−∞
v(t)
 dt = σ∞∥v∥1 ,
where the RHS is ﬁnite by our assumption that v is integrable.
Another case where the inner product is well-deﬁned is when both u and v are of
ﬁnite energy. To prove that in this case too the mapping t 	→u(t) v(t) is integrable
we need the inequality
α β ≤1
2(α2 + β2),
α, β ∈R,
(3.14)
which follows directly from the inequality (α −β)2 ≥0 by simple algebra:
0 ≤(α −β)2
= α2 + β2 −2αβ.
By substituting |u(t)| for α and |v(t)| for β in (3.14) we obtain the inequality
|u(t) v(t)| ≤(|u(t)|2 + |v(t)|2)/2 and hence
 ∞
−∞
u(t) v(t)
 dt ≤1
2
 ∞
−∞
u(t)
2 dt + 1
2
 ∞
−∞
v(t)
2 dt,
(3.15)
thus demonstrating that if both u and v are of ﬁnite energy (so the RHS is ﬁnite),
then the inner product is well-deﬁned, i.e., t 	→u(t) v(t) is integrable.
As a by-product of this proof we can obtain an upper bound on the magnitude of
the inner product in terms of the energies of u and v. All we need is the inequality

 ∞
−∞
f(ξ) dξ
 ≤
 ∞
−∞
f(ξ)
 dξ
18
The Inner Product
(see Proposition 2.4.1) to conclude from (3.15) that
|⟨u, v⟩| =

 ∞
−∞
u(t) v∗(t) dt

≤
 ∞
−∞
u(t)
 v(t)
 dt
≤1
2
 ∞
−∞
u(t)
2 dt + 1
2
 ∞
−∞
v(t)
2 dt
= 1
2

∥u∥2
2 + ∥v∥2
2

.
(3.16)
This inequality will be improved in Theorem 3.3.1, which introduces the Cauchy-
Schwarz Inequality.
We ﬁnally mention here, without proof, a third case where the inner product
between the Lebesgue measurable signals u, v is deﬁned. The result here is that if
for some numbers 1 < p, q < ∞satisfying 1/p + 1/q = 1 we have that
 ∞
−∞
u(t)
p dt < ∞
and
 ∞
−∞
v(t)
q dt < ∞,
then t 	→u(t) v(t) is integrable. The proof of this result follows from H¨older’s
Inequality (Theorem 3.3.2 ahead). Notice that the second case we addressed (where
u and v are both of ﬁnite energy) follows from this case by considering p = q = 2.
3.3
The Cauchy-Schwarz Inequality
The Cauchy-Schwarz Inequality is probably the most important inequality on the
inner product. Its discrete version is attributed to Augustin-Louis Cauchy (1789–
1857) and its integral form to Victor Yacovlevich Bunyakovsky (1804–1889) who
studied with him in Paris. Its (double) integral form was derived independently by
Hermann Amandus Schwarz (1843–1921). See (Steele, 2004, pp. 10–12) for more
on the history of this inequality and on how inequalities get their names.
Theorem 3.3.1 (Cauchy-Schwarz Inequality). If the functions u, v: R →C are
of ﬁnite energy, then the mapping t 	→u(t) v∗(t) is integrable and
⟨u, v⟩
 ≤∥u∥2 ∥v∥2 .
(3.17)
That is,

 ∞
−∞
u(t) v∗(t) dt
 ≤
 ∞
−∞
u(t)
2 dt
 ∞
−∞
v(t)
2 dt.
Equality in the Cauchy-Schwarz Inequality is possible, e.g., if u is a scaled version
of v, i.e., if for some constant α
u(t) = α v(t),
t ∈R.
3.3 The Cauchy-Schwarz Inequality
19
In fact, the Cauchy-Schwarz Inequality holds with equality if, and only if, either v(t)
is zero for all t outside a set of Lebesgue measure zero or for some constant α we
have u(t) = α v(t) for all t outside a set of Lebesgue measure zero.
There are a number of diﬀerent proofs of this important inequality. We shall focus
here on one that is based on (3.16) because it demonstrates a general technique for
improving inequalities. The idea is that once one obtains a certain inequality—in
our case (3.16)—one can try to improve it by taking advantage of one’s under-
standing of how the quantity in question is aﬀected by various transformations.
This technique is beautifully illustrated in (Steele, 2004).
Proof. The quantity in question is |⟨u, v⟩|. We shall take advantage of our under-
standing of how this quantity behaves when we replace u with its scaled version
α u and when we replace v with its scaled version β v. Here α, β ∈C are arbitrary.
The quantity in question transforms as
|⟨α u, β v⟩| = |α| |β| |⟨u, v⟩|.
(3.18)
We now use (3.16) to upper-bound the left-hand side (LHS) of the above by sub-
stituting α u and β v for u and v in (3.16) to obtain
|α| |β| |⟨u, v⟩| = |⟨α u, β v⟩|
≤1
2|α|2 ∥u∥2
2 + 1
2|β|2 ∥v∥2
2 ,
α, β ∈C.
(3.19)
If both ∥u∥2 and ∥v∥2 are positive, then (3.17) follows from (3.19) by choosing
α = 1/ ∥u∥2 and β = 1/ ∥v∥2. To conclude the proof it thus remains to show that
(3.17) also holds when either ∥u∥2 or ∥v∥2 is zero so the RHS of (3.17) is zero.
That is, we need to show that if either ∥u∥2 or ∥v∥2 is zero, then ⟨u, v⟩must also
be zero. To show this, suppose ﬁrst that ∥u∥2 is zero. By substituting α = 1 in
(3.19) we obtain in this case that
|β| |⟨u, v⟩| ≤1
2|β|2 ∥v∥2
2 ,
which, upon dividing by |β|, yields
|⟨u, v⟩| ≤1
2|β| ∥v∥2
2 ,
β ̸= 0.
Upon letting |β| tend to zero from above this demonstrates that ⟨u, v⟩must be zero
as we set out to prove. (As an alternative proof of this case one notes that ∥u∥2 = 0
implies, by Proposition 2.5.3, that the set {t ∈R : u(t) ̸= 0} is of Lebesgue measure
zero. Consequently, since every zero of t 	→u(t) is also a zero of t 	→u(t) v∗(t),
it follows that {t ∈R : u(t) v∗(t) ̸= 0} is included in {t ∈R : u(t) ̸= 0}, and
must therefore also be of Lebesgue measure zero (Exercise 2.10). Consequently,
by Proposition 2.5.3,
 ∞
−∞|u(t) v∗(t)| dt must be zero, which, by Proposition 2.4.1,
implies that |⟨u, v⟩| must be zero.)
The case where ∥v∥2 = 0 is very similar: by substituting β = 1 in (3.19) we obtain
that (in this case)
|⟨u, v⟩| ≤1
2|α| ∥u∥2
2 ,
α ̸= 0
20
The Inner Product
and the result follows upon letting |α| tend to zero from above.
While we shall not use the following inequality in this book, it is suﬃciently im-
portant that we mention it in passing.
Theorem 3.3.2 (H¨older’s Inequality). If u: R →C and v: R →C are Lebesgue
measurable functions satisfying
 ∞
−∞
u(t)
p dt < ∞
and
 ∞
−∞
v(t)
q dt < ∞
for some 1 < p, q < ∞satisfying 1/p + 1/q = 1, then the function t 	→u(t) v∗(t) is
integrable and

 ∞
−∞
u(t) v∗(t) dt
 ≤
	 ∞
−∞
u(t)
p dt

1/p 	 ∞
−∞
v(t)
q dt

1/q
.
(3.20)
Note that the Cauchy-Schwarz Inequality corresponds to the case where p = q = 2.
Proof. See, for example, (Rudin, 1987, Theorem 3.5) or (Royden and Fitzpatrick,
2010, Section 7.2, Theorem 1).
3.4
Applications
There are numerous applications of the Cauchy-Schwarz Inequality. Here we only
mention a few. The ﬁrst relates the energy in the superposition of two signals to
the energies of the individual signals. The result holds for both complex-valued and
real-valued functions, and—as is our custom—we shall thus not make the codomain
explicit.
Proposition 3.4.1 (Triangle Inequality for L2). If u and v are in L2, then
∥u + v∥2 ≤∥u∥2 + ∥v∥2 .
(3.21)
Proof. The proof is a straightforward application of the Cauchy-Schwarz Inequality
and the basic properties of the inner product (3.6)–(3.10):
∥u + v∥2
2 = ⟨u + v, u + v⟩
= ⟨u, u⟩+ ⟨v, v⟩+ ⟨u, v⟩+ ⟨v, u⟩
= ∥u∥2
2 + ∥v∥2
2 + 2 Re(⟨u, v⟩)
≤∥u∥2
2 + ∥v∥2
2 + 2 |⟨u, v⟩|
≤∥u∥2
2 + ∥v∥2
2 + 2 ∥u∥2 ∥v∥2
=

∥u∥2 + ∥v∥2
2,
from which the result follows by taking square roots. Here the ﬁrst line follows from
the deﬁnition of ∥·∥2 (3.11); the second from (3.9) & (3.10); the third because ⟨v, u⟩
is the complex conjugate of ⟨u, v⟩(3.6); the fourth from the inequality |Re(z)| ≤|z|,
which holds for every z ∈C; the ﬁfth by the Cauchy-Schwarz Inequality; and the
sixth by simple algebra.
3.4 Applications
21
Another important mathematical consequence of the Cauchy-Schwarz Inequality is
the continuity of the inner product. To state the result we use the notation an →a
to indicate that the sequence a1, a2, . . . converges to a, i.e., that limn→∞an = a.
Proposition 3.4.2 (Continuity of the Inner Product). Let u and v be in L2. If
the sequence u1, u2, . . . of elements of L2 satisﬁes
∥un −u∥2 →0,
and if the sequence v1, v2, . . . of elements of L2 satisﬁes
∥vn −v∥2 →0,
then
⟨un, vn⟩→⟨u, v⟩.
Proof.
|⟨un, vn⟩−⟨u, v⟩|
= |⟨un −u, v⟩+ ⟨un −u, vn −v⟩+ ⟨u, vn −v⟩|
≤|⟨un −u, v⟩| + |⟨un −u, vn −v⟩| + |⟨u, vn −v⟩|
≤∥un −u∥2 ∥v∥2 + ∥un −u∥2 ∥vn −v∥2 + ∥u∥2 ∥vn −v∥2
→0,
where the ﬁrst equality follows from the basic properties of the inner product (3.6)–
(3.10); the subsequent inequality by the Triangle Inequality for Complex Numbers
(2.12); the subsequent inequality from the Cauchy-Schwarz Inequality; and where
the ﬁnal limit follows from the proposition’s hypotheses.
Another useful consequence of the Cauchy-Schwarz Inequality is that if a signal is
energy-limited and is zero outside an interval, then it is also integrable.
Proposition 3.4.3 (Finite-Energy Functions over Finite Intervals Are Integrable).
If for some real numbers a and b satisfying a ≤b we have
 b
a
x(ξ)
2 dξ < ∞,
then
 b
a
x(ξ)
 dξ ≤
√
b −a
 b
a
x(ξ)
2 dξ ,
and, in particular,
 b
a
x(ξ)
 dξ < ∞.
22
The Inner Product
Proof.
 b
a
x(ξ)
 dt =
 ∞
−∞
I{a ≤ξ ≤b}
x(ξ)
 dξ
=
 ∞
−∞
I{a ≤ξ ≤b}



u(ξ)
I{a ≤ξ ≤b}
x(ξ)




v(ξ)
dξ
≤
√
b −a
 b
a
x(ξ)
2 dξ,
where the inequality is just an application of the Cauchy-Schwarz Inequality to the
function ξ 	→I{a ≤ξ ≤b} |x(ξ)| and the indicator function ξ 	→I{a ≤ξ ≤b}.
Note that, in general, an energy-limited signal need not be integrable. For example,
the real signal
t 	→

0
if t ≤1,
1/t
otherwise,
(3.22)
is of ﬁnite energy but is not integrable.
The Cauchy-Schwarz Inequality demonstrates that if both u and v are of ﬁnite
energy, then their inner product ⟨u, v⟩is well-deﬁned, i.e., the integrand in (3.4) is
integrable. It can also be used in slightly more sophisticated ways. For example, it
can be used to treat cases where one of the functions, say u, is not of ﬁnite energy
but where the second function decays to zero suﬃciently quickly to compensate for
that. For example:
Proposition 3.4.4. If the Lebesgue measurable functions x: R →C and y: R →C
satisfy
 ∞
−∞
|x(t)|2
t2 + 1 dt < ∞
and
 ∞
−∞
|y(t)|2 (t2 + 1) dt < ∞,
then the function t 	→x(t) y∗(t) is integrable and

 ∞
−∞
x(t) y∗(t) dt
 ≤
 ∞
−∞
|x(t)|2
t2 + 1 dt
 ∞
−∞
|y(t)|2 (t2 + 1) dt.
Proof. This is a simple application of the Cauchy-Schwarz Inequality to the func-
tions t 	→x(t)/
√
t2 + 1 and t 	→y(t)
√
t2 + 1. Simply write
 ∞
−∞
x(t) y∗(t) dt =
 ∞
−∞
x(t)
√
t2 + 1



u(t)

t2 + 1 y∗(t)



v∗(t)
dt
and apply the Cauchy-Schwarz Inequality to the functions u(·) and v(·).
3.5 The Cauchy-Schwarz Inequality for Random Variables
23
3.5
The Cauchy-Schwarz Inequality for Random Variables
There is also a version of the Cauchy-Schwarz Inequality for random variables. It is
very similar to Theorem 3.3.1 but with time integrals replaced by expectations. We
denote the expectation of the random variable X by E[X] and remind the reader
that the variance Var[X] of the random variable X is deﬁned as
Var[X] = E

(X −E[X])2
.
(3.23)
Theorem 3.5.1 (Cauchy-Schwarz Inequality for Random Variables). Let the ran-
dom variables U and V be of ﬁnite variance. Then
E[U V ]
 ≤

E[U 2]

E[V 2],
(3.24)
with equality if, and only if, Pr[α U = β V ] = 1 for some real α and β that are not
both equal to zero.
Proof. Use the proof of Theorem 3.3.1 with all time integrals replaced with ex-
pectations. For a diﬀerent proof and for the conditions for equality see (Grimmett
and Stirzaker, 2001, Chapter 3, Section 3.5, Theorem 9).
For the next corollary we need to recall that the covariance Cov[U, V ] between the
ﬁnite-variance random variables U, V is deﬁned as
Cov[U, V ] = E

U −E[U]

V −E[V ]

.
(3.25)
Corollary 3.5.2 (Covariance Inequality). If the random variables U and V are of
ﬁnite variance Var[U] and Var[V ], then
Cov[U, V ]
 ≤

Var[U]

Var[V ].
(3.26)
Proof. Apply Theorem 3.5.1 to the random variables U −E[U] and V −E[V ].
Corollary 3.5.2 shows that the correlation coeﬃcient, which is deﬁned for ran-
dom variables U and V having strictly positive variances as
ρ =
Cov[U, V ]

Var[U]

Var[V ]
,
(3.27)
satisﬁes
−1 ≤ρ ≤+1.
(3.28)
3.6
Mathematical Comments
(i) Mathematicians typically consider ⟨u, v⟩only when both u and v are of ﬁnite
energy. We are more forgiving and simply require that the integral deﬁning
the inner product be well-deﬁned, i.e., that the integrand be integrable.
24
The Inner Product
(ii) Some refer to ∥u∥2 as the “norm of u” or the “L2 norm of u.” We shall
refrain from this usage because mathematicians use the term “norm” very
selectively. They require that no function other than the all-zero function be
of zero norm, and this is not the case for ∥·∥2. Indeed, any function u that
is indistinguishable from the all-zero function satisﬁes ∥u∥2 = 0, and there
are many such functions (e.g., the function t 	→I{t is a rational number}
that is equal to one at rational times and that is equal to zero at all other
times). This diﬃculty can be overcome by deﬁning two functions to be the
same if their diﬀerence is of zero energy. In this case ∥·∥2 is a norm in the
mathematical sense and is, in fact, what mathematicians call the L2 norm.
This issue is discussed in greater detail in Section 4.7. To stay out of trouble
we shall refrain from giving ∥·∥2 a name.
3.7
Exercises
Exercise 3.1 (Truncated Polynomials). Consider the signals u: t →(t + 2) I{0 ≤t ≤1}
and v: t →(t2 −2t −3) I{0 ≤t ≤1}. Compute the energies ∥u∥2
2 and ∥v∥2
2 as well as
the inner product ⟨u, v⟩.
Exercise 3.2 (Inner Products of Mirror Images). Express the inner product ⟨~x, ~y⟩in
terms of the inner product ⟨x, y⟩.
Exercise 3.3 (Finite-Energy Signals). Let x be an energy-limited signal.
(i) Show that, for every t0 ∈R, the signal t →x(t −t0) must also be energy-limited.
(ii) Show that the reﬂection of x is also energy-limited. I.e., show that the signal ~x
that maps t to x(−t) is energy-limited.
(iii) How are the energies in t →x(t), t →x(t −t0), and t →x(−t) related?
Exercise 3.4 (Manipulating Inner Products). Show that if u, v, and w are energy-limited
complex signals, then
⟨u + v, 3u + v + iw⟩= 3 ∥u∥2
2 + ∥v∥2
2 + ⟨u, v⟩+ 3 ⟨u, v⟩∗−i ⟨u, w⟩−i ⟨v, w⟩.
Exercise 3.5 (A Useful Identity). A classic result in algebra states that a2 −b2 factorizes
as (a −b)(a + b) for all a, b ∈R.
(i) Prove its integral counterpart for real energy-limited signals:
∥u∥2
2 −∥v∥2
2 = ⟨u −v, u + v⟩.
(ii) Does this identity also hold for complex signals?
Exercise 3.6 (Orthogonality to All Signals). Let u be an energy-limited signal. Show
that
	
u ≡0

⇐⇒
	
⟨u, v⟩= 0,
v ∈L2

.
Exercise 3.7 (On the Tightness of the Cauchy-Schwarz Inequality). Show that the bound
obtained from the Cauchy-Schwarz Inequality is at least as tight as (3.16).
3.7 Exercises
25
Exercise 3.8 (A Generalized Cauchy-Schwarz Inequality). Let h: R →R be a nonnegative
function. Prove that if the complex signals u, v are such that the integrals

|u(t)|2 h(t) dt
and

|v(t)|2 h(t) dt are ﬁnite, then

 ∞
−∞
u(t) v∗(t) h(t) dt

2
≤
 ∞
−∞
u(t)
2 h(t) dt
 ∞
−∞
v(t)
2 h(t) dt

.
Exercise 3.9 (Indistinguishability and Inner Products). Let u ∈L2 be indistinguishable
from u′ ∈L2, and let v ∈L2 be indistinguishable from v′ ∈L2. Show that the inner
product ⟨u′, v′⟩is equal to the inner product ⟨u, v⟩.
Exercise 3.10 (Finite Energy and Integrability). Let x: R →C be Lebesgue measurable.
(i) Show that the conditions that x is of ﬁnite energy and that the mapping t →t x(t)
is of ﬁnite energy are simultaneously met if, and only if,
 ∞
−∞
|x(t)|2 
1 + t2
dt < ∞.
(3.29)
(ii) Show that (3.29) implies that x is integrable.
(iii) Give an example of an integrable signal that does not satisfy (3.29).
Exercise 3.11 (The Variance of a Sum). Show that if the random variables X and Y are
of ﬁnite variance, then
Var[X + Y ] ≤

Var[X] +

Var[Y ]
2.
Exercise 3.12 (The Cauchy-Schwarz Inequality for Sequences).
(i) Let the complex sequences a1, a2, . . . and b1, b2, . . . satisfy
∞

ν=1
|aν|2,
∞

ν=1
|bν|2 < ∞.
Show that

∞

ν=1
aν b∗
ν

2
≤
 ∞

ν=1
|aν|2
 ∞

ν=1
|bν|2

.
(ii) Derive the Cauchy-Schwarz Inequality for d-tuples:

d

ν=1
aν b∗
ν

2
≤

d

ν=1
|aν|2

d

ν=1
|bν|2

.
Exercise 3.13 (Cauchy-Schwarz and Matrices). Prove that if A and B are real n × m
matrices then
	
tr

ABT
2
≤

n

k=1
m

ℓ=1

a(k,ℓ)2

n

k=1
m

ℓ=1

b(k,ℓ)2

.
Hint: Exercise 3.12 (ii) may be helpful.
26
The Inner Product
Exercise 3.14 (Summability and Square Summability). Let a1, a2, . . . be a sequence of
complex numbers. Show that
 ∞

ν=1
|aν| < ∞

=⇒
 ∞

ν=1
|aν|2 < ∞

.
Exercise 3.15 (A Friendlier GPA). Use the Cauchy-Schwarz Inequality for d-tuples (Prob-
lem 3.12) to show that for any positive integer d,
a1 + · · · + ad
d
≤

a2
1 + · · · + a2
d
d
,
a1, . . . , ad ∈R.
Chapter 4
The Space L2 of Energy-Limited Signals
4.1
Introduction
In this chapter we shall study the space L2 of energy-limited signals in greater
detail. We shall show that its elements can be viewed as vectors in a vector space
and begin developing a geometric intuition for understanding its structure. We
shall focus on the case of complex-valued signals, but with some minor changes the
results are also applicable to real-valued signals. (The main changes that are needed
for translating the results to real-valued signals are replacing C with R, ignoring
the conjugation operation, and interpreting |·| as the absolute value function for
real arguments as opposed to the modulus function.)
We remind the reader that the space L2 was deﬁned in Deﬁnition 3.1.1 as the set
of all Lebesgue measurable complex-valued signals u: R →C satisfying
 ∞
−∞
u(t)
2 dt < ∞,
(4.1)
and that in (3.12) we deﬁned for every u ∈L2 the quantity ∥u∥2 as
∥u∥2 =
 ∞
−∞
u(t)
2 dt.
(4.2)
We refer to L2 as the space of energy-limited signals and to its elements as energy-
limited signals or signals of ﬁnite energy.
4.2
L2 as a Vector Space
In this section we shall explain how to view the space L2 as a vector space over
the complex ﬁeld by thinking about signals in L2 as vectors, by interpreting the
superposition u + v of two signals as vector-addition, and by interpreting the
ampliﬁcation of u by α as the operation of multiplying the vector u by the scalar
α ∈C.
We begin by reminding the reader that the superposition of the two signals u
and v is denoted by u + v and is the signal that maps every t ∈R to u(t) + v(t).
The ampliﬁcation of u by α is denoted by α u and is the signal that maps every
27
28
The Space L2 of Energy-Limited Signals
t ∈R to α u(t). More generally, if u and v are signals and if α and β are complex
numbers, then α u + β v is the signal t 	→α u(t) + β v(t).
If u ∈L2 and α ∈C, then α u is also in L2. Indeed, the measurability of u implies
the measurability of α u, and if u is of ﬁnite energy, then α u is also of ﬁnite energy,
because the energy in α u is the product of |α|2 by the energy in u. We thus see
that the operation of ampliﬁcation of u by α results in an element of L2 whenever
u ∈L2 and α ∈C.
We next show that if the signals u and v are in L2, then their superposition
u+v must also be in L2. This holds because a standard result in Measure Theory
guarantees that the superposition of two Lebesgue measurable signals is a Lebesgue
measurable signal and because Proposition 3.4.1 guarantees that if both u and v
are of ﬁnite energy, then so is their superposition. Thus the superposition that
maps u and v to u + v results in an element of L2 whenever u, v ∈L2.
It can be readily veriﬁed that the following properties hold:
(i) commutativity:
u + v = v + u,
u, v ∈L2;
(ii) associativity:
(u + v) + w = u + (v + w),
u, v, w ∈L2,
(α β) u = α (β u),

α, β ∈C,
u ∈L2

;
(iii) additive identity: the all-zero signal 0: t 	→0 satisﬁes
0 + u = u,
u ∈L2;
(iv) additive inverse:
to every u ∈L2 there corresponds a signal w ∈L2
(namely, the signal t 	→−u(t)) such that
u + w = 0;
(v) multiplicative identity:
1 u = u,
u ∈L2;
(vi) distributive properties:
α (u + v) = α u + α v,

α ∈C,
u, v ∈L2

,
(α + β) u = α u + β u,

α, β ∈C,
u ∈L2

.
We conclude that with the operations of superposition and ampliﬁcation the set L2
forms a vector space over the complex ﬁeld (Axler, 2015, Chapter 1). This justiﬁes
referring to the elements of L2 as “vectors,” to the operation of signal superposition
as “vector addition,” and to the operation of ampliﬁcation of an element of L2 by
a complex scalar as “scalar multiplication.”
4.3 Subspace, Dimension, and Basis
29
4.3
Subspace, Dimension, and Basis
Once we have noted that L2 together with the operations of superposition and
ampliﬁcation forms a vector space, we can borrow numerous deﬁnitions and results
from the theory of vector spaces. Here we shall focus on the very basic ones.
A linear subspace (or just subspace) of L2 is a nonempty subset U of L2 that
is closed under superposition
u1 + u2 ∈U,
u1, u2 ∈U
(4.3)
and under ampliﬁcation
α u ∈U,

α ∈C,
u ∈U

.
(4.4)
Example 4.3.1. Consider the set of all functions of the form
t 	→p(t) e−|t|,
where p(t) is any polynomial of degree no larger than 3. Thus, the set is the set of
all functions of the form
t 	→

α0 + α1 t + α2 t2 + α3 t3
e−|t|,
(4.5)
where α0, α1, α2, α3 are arbitrary complex numbers.
In spite of the polynomial growth of the function in parentheses in (4.5), all the
functions in this set are in L2 because the exponential decay more than compen-
sates for the polynomial growth. The above set is thus a subset of L2. Moreover,
as we show next, this is a linear subspace of L2.
If u is of the form (4.5), then so is α u, because α u is the mapping
t 	→

α α0 + α α1 t + α α2 t2 + α α3 t3
e−|t|,
which is of the same form.
Similarly, if u is as given in (4.5) and
v: t 	→

β0 + β1 t + β2 t2 + β3 t3
e−|t|,
then u + v is the mapping
t 	→

(α0 + β0) + (α1 + β1) t + (α2 + β2) t2 + (α3 + β3) t3
e−|t|,
which is again of this form.
An n-tuple of vectors from L2 is a (possibly empty) ordered list of n vectors
from L2 separated by commas and enclosed in parentheses, e.g., (v1, . . . , vn). Here
n ≥0 can be any nonnegative integer, where the case n = 0 corresponds to the
empty list.
A vector v ∈L2 is said to be a linear combination of the n-tuple (v1, . . . , vn) if
it is equal to
α1 v1 + · · · + αn vn,
(4.6)
30
The Space L2 of Energy-Limited Signals
which is written more succinctly as
n

ν=1
αν vν,
(4.7)
for some scalars α1, . . . , αn ∈C. The all-zero signal is a linear combination of any
n-tuple including the empty tuple.
The span of an n-tuple (v1, . . . , vn) of vectors in L2 is denoted by
span(v1, . . . , vn)
and is the set of all vectors in L2 that are linear combinations of (v1, . . . , vn):
span(v1, . . . , vn) ≜{α1 v1 + · · · + αn vn : α1, . . . , αn ∈C}.
(4.8)
(The span of the empty tuple is given by the one-element set {0} containing the
all-zero signal only.)
Note that for any n-tuple of vectors (v1, . . . , vn) in L2 we have that span(v1, . . . , vn)
is a linear subspace of L2. Also, if U is a linear subspace of L2 and if the vectors
u1, . . . , un are in U, then span(u1, . . . , un) is a linear subspace which is contained
in U. A subspace U of L2 is said to be ﬁnite-dimensional if there exists an
n-tuple (u1, . . . , un) of vectors in U such that span(u1, . . . , un) = U. Otherwise,
we say that U is inﬁnite-dimensional. For example, the space of all mappings
of the form t 	→p(t) e−|t| for some polynomial p(·) can be shown to be inﬁnite-
dimensional, but under the restriction that p(·) be of degree smaller than 5, it is
ﬁnite-dimensional. If U is a ﬁnite-dimensional subspace and if U′ is a subspace
contained in U, then U′ must also be ﬁnite-dimensional.
An n-tuple of signals (v1, . . . , vn) in L2 is said to be linearly independent if
whenever the scalars α1, . . . , αn ∈C are such that α1 v1 +· · ·+αn vn = 0, we have
α1 = · · · = αn = 0. I.e., if
	
n

ν=1
αν vν = 0

=⇒

αν = 0,
ν = 1, . . . , n

.
(4.9)
(By convention, the empty tuple is linearly independent.)
For example, the 3-
tuple consisting of the signals t 	→e−|t|, t 	→t e−|t|, and t 	→t2 e−|t| is linearly
independent. If (v1, . . . , vn) is not linearly independent, then we say that it is
linearly dependent. For example, the 3-tuple consisting of the signals t 	→e−|t|,
t 	→t e−|t|, and t 	→

2t + 1

e−|t| is linearly dependent. The n-tuple (v1, . . . , vn)
is linearly dependent if, and only if, (at least) one of the signals in the tuple can
be written as a linear combination of the others.
The d-tuple (u1, . . . , ud) is said to form a basis for the linear subspace U if it is
linearly independent and if span(u1, . . . , ud) = U. The latter condition is equivalent
to the requirement that every u ∈U can be represented as
u = α1 u1 + · · · + αd ud
(4.10)
for some α1, . . . , αd ∈C.
The former condition that the tuple (u1, . . . , ud) be
linearly independent guarantees that if such a representation exists, then it is
4.4 ∥u∥2 as the “length” of the Signal u(·)
31
unique. Thus, (u1, . . . , ud) forms a basis for U if u1, . . . , ud ∈U (thus guaranteeing
that span(u1, . . . , ud) ⊆U) and if every u ∈U can be written uniquely as in (4.10).
Every ﬁnite-dimensional linear subspace U has a basis, and all bases for U have the
same number of elements. This number is called the dimension of U. Thus, if U
is a ﬁnite-dimensional subspace and if both (u1, . . . , ud) and (u′
1, . . . , u′
d′) form a
basis for U, then d = d′ and both are equal to the dimension of U. The dimension
of the subspace {0} is zero. If U is a ﬁnite-dimensional subspace and if U′ is a
subspace contained in U, then the dimension of U′ cannot exceed that of U.
4.4
∥u∥2 as the “length” of the Signal u(·)
Having presented the elements of L2 as vectors, we next propose to view ∥u∥2 as
the “length” of the vector u ∈L2. To motivate this view, we ﬁrst present the key
properties of ∥·∥2.
Proposition 4.4.1 (Properties of ∥·∥2). Let u and v be elements of L2, and let α
be some complex number. Then
∥α u∥2 = |α| ∥u∥2 ,
(4.11)
∥u + v∥2 ≤∥u∥2 + ∥v∥2 ,
(4.12)
and

∥u∥2 = 0

⇐⇒

u ≡0

.
(4.13)
Proof. Identity (4.11) follows directly from the deﬁnition of ∥·∥2; see (4.2). In-
equality (4.12) is a restatement of Proposition 3.4.1. The equivalence of the con-
dition ∥u∥2 = 0 and the condition that u is indistinguishable from the all-zero
signal 0 follows from Proposition 2.5.3.
Identity (4.11) is in agreement with our intuition that stretching a vector merely
scales its length. Inequality (4.12) is sometimes called the Triangle Inequality
because it is reminiscent of the theorem from planar geometry that states that the
length of no side of a triangle can exceed the sum of the lengths of the others; see
Figure 4.1.
Substituting −y for u and x + y for v in (4.12) yields ∥x∥2 ≤∥y∥2 + ∥x + y∥2,
i.e., the inequality ∥x + y∥2 ≥∥x∥2 −∥y∥2. And substituting −x for u and x + y
for v in (4.12) yields the inequality ∥y∥2 ≤∥x∥2 + ∥x + y∥2, i.e., the inequality
∥x + y∥2 ≥∥y∥2 −∥x∥2. Combining the two inequalities we obtain the inequality
∥x + y∥2 ≥
∥x∥2 −∥y∥2
. This inequality can be combined with the inequality
∥x + y∥2 ≤∥x∥2 + ∥y∥2 in the compact form of a double-sided inequality
∥x∥2 −∥y∥2
 ≤∥x + y∥2 ≤∥x∥2 + ∥y∥2 ,
x, y ∈L2.
(4.14)
Finally, (4.13) “almost” supports the intuition that the only vector of length zero
is the zero-vector. In our case, alas, we can only claim that if a vector is of zero
length, then it is indistinguishable from the all-zero signal, i.e., that all t’s outside
a set of Lebesgue measure zero are mapped by the signal to zero.
32
The Space L2 of Energy-Limited Signals
u
v
u + v
Figure 4.1: A geometric interpretation of the Triangle Inequality for energy-limited
signals: ∥u + v∥2 ≤∥u∥2 + ∥v∥2.
A
B
C
v
u
w
w −v
u −w
u −v
Figure 4.2: Illustration of the shortest path property in L2. The shortest path
from A to B is no longer than the sum of the shortest path from A to C and the
shortest path from C to B.
The Triangle Inequality (4.12) can also be stated slightly diﬀerently. In planar
geometry the sum of the lengths of two sides of a triangle can never be smaller
than the length of the remaining side. Thus, the shortest path from Point A to
Point B cannot exceed the sum of the lengths of the shortest paths from Point A to
Point C, and from Point C to Point B. By applying Inequality (4.12) to the signal
u −w and w −v we obtain
∥u −v∥2 ≤∥u −w∥2 + ∥w −v∥2 ,
u, v, w ∈L2,
(4.15)
i.e., that the distance from u to v cannot exceed the sum of distances from u to w
and from w to v. See Figure 4.2.
4.5 Orthogonality and Inner Products
33
4.5
Orthogonality and Inner Products
To further develop our geometric view of L2 we next discuss orthogonality. We
shall motivate its deﬁnition with an attempt to generalize Pythagoras’s Theorem
to L2. As an initial attempt at deﬁning orthogonality we might deﬁne two func-
tions u, v ∈L2 to be orthogonal if ∥u + v∥2
2 = ∥u∥2
2 + ∥v∥2
2.
Recalling the
deﬁnition of ∥·∥2 (4.2) we obtain that this condition is equivalent to the condition
Re

u(t) v∗(t) dt

= 0, because
∥u + v∥2
2 =
 ∞
−∞
|u(t) + v(t)|2 dt
=
 ∞
−∞

u(t) + v(t)

u(t) + v(t)
∗dt
=
 ∞
−∞

|u(t)|2 + |v(t)|2 + 2 Re

u(t) v∗(t)

dt
= ∥u∥2
2 + ∥v∥2
2 + 2 Re
	 ∞
−∞
u(t) v∗(t) dt

,
u, v ∈L2,
(4.16)
where we have used the fact that integration commutes with the operation of taking
the real part; see Proposition 2.3.1.
While this approach would work well for real-valued functions, it has some embar-
rassing consequences when it comes to complex-valued functions. It allows for the
possibility that u is orthogonal to v, but that its scaled version αu is not. For exam-
ple, with this deﬁnition, the function t 	→i I{|t| ≤5} is orthogonal to the function
t 	→I{|t| ≤17} but its scaled (by α = i) version t 	→i i I{|t| ≤5} = −I{|t| ≤5} is
not. To avoid this embarrassment, we deﬁne u to be orthogonal to v if
∥α u + v∥2
2 = ∥α u∥2
2 + ∥v∥2
2 ,
α ∈C.
This, by (4.16), is equivalent to
Re
	
α
 ∞
−∞
u(t) v∗(t) dt

= 0,
α ∈C,
i.e., to the condition
 ∞
−∞
u(t) v∗(t) dt = 0
(4.17)
(because if z ∈C is such that Re(α z) = 0 for all α ∈C, then z = 0). Recalling
the deﬁnition of the inner product ⟨u, v⟩from (3.4)
⟨u, v⟩=
 ∞
−∞
u(t) v∗(t) dt,
(4.18)
we conclude that (4.17) is equivalent to the condition ⟨u, v⟩= 0 or, equivalently
(because by (3.6) ⟨u, v⟩= ⟨v, u⟩∗) to the condition ⟨v, u⟩= 0.
Deﬁnition 4.5.1 (Orthogonal Signals in L2). The signals u, v ∈L2 are said to
be orthogonal if
⟨u, v⟩= 0.
(4.19)
34
The Space L2 of Energy-Limited Signals
The n-tuple (u1, . . . , un) of signals in L2 is said to be orthogonal if the signals in
it are pairwise orthogonal
⟨uℓ, uℓ′⟩= 0,

ℓ̸= ℓ′,
ℓ, ℓ′ ∈{1, . . . , n}

.
(4.20)
The reader is encouraged to verify that if u is orthogonal to v then so is αu. Also,
u is orthogonal to v if, and only if, v is orthogonal to u. Finally every function is
orthogonal to the all-zero function 0.
Having judiciously deﬁned orthogonality in L2, we can now extend Pythagoras’s
Theorem.
Theorem 4.5.2 (A Pythagorean Theorem). If the n-tuple of vectors (u1, . . . , un)
in L2 is orthogonal, then
∥u1 + · · · + un∥2
2 = ∥u1∥2
2 + · · · + ∥un∥2
2 .
Proof. This theorem can be proved by induction on n. The case n = 2 follows
from (4.16) using Deﬁnition 4.5.1 and (4.18).
Assume now that the theorem holds for n = ν, for some ν ≥2, i.e.,
∥u1 + · · · + uν∥2
2 = ∥u1∥2
2 + · · · + ∥uν∥2
2 ,
and let us show that this implies that it also holds for n = ν + 1, i.e., that
∥u1 + · · · + uν+1∥2
2 = ∥u1∥2
2 + · · · + ∥uν+1∥2
2 .
To that end, let
v = u1 + · · · + uν.
(4.21)
Since the ν-tuple (u1, . . . , uν) is orthogonal, our induction hypothesis guarantees
that
∥v∥2
2 = ∥u1∥2
2 + · · · + ∥uν∥2
2 .
(4.22)
Now v is orthogonal to uν+1 because
⟨v, uν+1⟩= ⟨u1 + · · · + uν, uν+1⟩
= ⟨u1, uν+1⟩+ · · · + ⟨uν, uν+1⟩
= 0,
so by the n = 2 case
∥v + uν+1∥2
2 = ∥v∥2
2 + ∥uν+1∥2
2 .
(4.23)
Combining (4.21), (4.22), and (4.23) we obtain
∥u1 + · · · + uν+1∥2
2 = ∥v + uν+1∥2
2
= ∥v∥2
2 + ∥uν+1∥2
2
= ∥u1∥2
2 + · · · + ∥uν+1∥2
2 .
4.5 Orthogonality and Inner Products
35
v
u
w
Figure 4.3: The projection w of the vector v onto u.
To derive a geometric interpretation for the inner product ⟨u, v⟩we next extend
to L2 the notion of the projection of a vector onto another. We ﬁrst recall the
deﬁnition for vectors in R2. Consider two nonzero vectors u and v in the real
plane R2. The projection w of the vector v onto u is a scaled version of u. More
speciﬁcally, it is a scaled version of u and its length is equal to the product of the
length of v multiplied by the cosine of the angle between v and u (see Figure 4.3).
More explicitly,
w = (length of v) cos(angle between v and u)
u
length of u.
(4.24)
This deﬁnition does not seem to have a natural extension to L2 because we have not
deﬁned the angle between two signals. An alternative deﬁnition of the projection,
and one that is more amenable to extensions to L2, is the following. The vector w
is the projection of the vector v onto u, if w is a scaled version of u, and if v −w
is orthogonal to u.
This deﬁnition makes perfect sense also in L2, because we have already deﬁned
what we mean by “scaled version” (i.e., “ampliﬁcation” or “scalar multiplication”)
and “orthogonality.” We thus have:
Deﬁnition 4.5.3 (Projection of a Signal in L2 onto another). Let u ∈L2 have
positive energy. The projection of the signal v ∈L2 onto the signal u ∈L2
is the signal w that satisﬁes both of the following conditions:
1) w = α u for some α ∈C and
2) v −w is orthogonal to u.
Note that since L2 is closed with respect to scalar multiplication, Condition 1)
guarantees that the projection w is in L2.
Prima facie it is not clear that a projection always exists and that it is unique.
Nevertheless, this is the case.
We prove this by ﬁnding an explicit expression
for w. We need to ﬁnd some α ∈C so that α u will satisfy the requirements of
36
The Space L2 of Energy-Limited Signals
the projection. The scalar α is chosen so as to guarantee that v −w is orthogonal
to u. That is, we seek to solve for α ∈C satisfying
⟨v −α u, u⟩= 0,
i.e.,
⟨v, u⟩−α ∥u∥2
2 = 0.
Recalling our hypothesis that ∥u∥2 > 0 (strictly), we conclude that α is uniquely
given by
α = ⟨v, u⟩
∥u∥2
2
,
and the projection w is thus unique and is given by
w = ⟨v, u⟩
∥u∥2
2
u.
(4.25)
Comparing (4.24) and (4.25) we can interpret
⟨v, u⟩
∥u∥2 ∥v∥2
(4.26)
as the cosine of the angle between the function v and the function u (provided
that neither u nor v is of zero energy). If the inner product is zero, then we have
said that v and u are orthogonal, which is consistent with the cosine of the angle
between them being zero. Note, however, that this interpretation should be taken
with a grain of salt because in the complex case the inner product in (4.26) is
typically a complex number.
The interpretation of (4.26) as the cosine of the angle between v and u is further
supported by noting that the magnitude of (4.26) is always in the range [0, 1]. This
follows directly from the Cauchy-Schwarz Inequality (Theorem 3.3.1) to which we
next give another (geometric) proof. Let w be the projection of v onto u. Then
starting from (4.25)
|⟨v, u⟩|2
∥u∥2
2
= ∥w∥2
2
≤∥w∥2
2 + ∥v −w∥2
2
= ∥w + (v −w)∥2
2
= ∥v∥2
2 ,
(4.27)
where the ﬁrst equality follows from (4.25); the subsequent inequality from the
nonnegativity of ∥·∥2; and the subsequent equality by the Pythagorean Theorem
because, by its deﬁnition, the projection w of v onto u must satisfy that v −w is
orthogonal to u and hence also to w, which is a scaled version of u. The Cauchy-
Schwarz Inequality now follows from (4.27) by multiplying each of its sides by ∥u∥2
2
and then taking the square root of the result.
4.6 Orthonormal Bases
37
4.6
Orthonormal Bases
We next consider orthonormal bases for ﬁnite-dimensional linear subspaces. These
are special bases that are particularly useful for the calculation of projections and
inner products.
4.6.1
Deﬁnition
Deﬁnition 4.6.1 (Orthonormal Tuple). An n-tuple of signals in L2 is said to be
orthonormal if it is orthogonal and if each of the signals in the tuple is of unit
energy.
Thus, the n-tuple (φ1, . . . , φn) of signals in L2 is orthonormal, if
⟨φℓ, φℓ′⟩=

0
if ℓ̸= ℓ′,
1
if ℓ= ℓ′,
ℓ, ℓ′ ∈{1, . . . , n}.
(4.28)
Linearly independent tuples need not be orthonormal, but orthonormal tuples must
be linearly independent:
Proposition 4.6.2 (Orthonormal Tuples Are Linearly Independent). If a tuple of
signals in L2 is orthonormal, then it must be linearly independent.
Proof. Let the n-tuple (φ1, . . . , φn) of signals in L2 be orthonormal, i.e., satisfy
(4.28). We need to show that if
n

ℓ=1
αℓφℓ= 0,
(4.29)
then all the coeﬃcients α1, . . . , αn must be zero. To that end, assume (4.29). It
then follows that for every ℓ′ ∈{1, . . . , n}
0 = ⟨0, φℓ′⟩
=

n

ℓ=1
αℓφℓ, φℓ′

=
n

ℓ=1
αℓ⟨φℓ, φℓ′⟩
=
n

ℓ=1
αℓI{ℓ= ℓ′}
= αℓ′,
thus demonstrating that (4.29) implies that αℓ′ = 0 for every ℓ′ ∈{1, . . . , n}. Here
the ﬁrst equality follows because 0 is orthogonal to every energy-limited signal
and, a fortiori, to φℓ′; the second by (4.29); the third by the linearity of the inner
product in its left argument (3.7) & (3.9); and the fourth by (4.28).
38
The Space L2 of Energy-Limited Signals
Deﬁnition 4.6.3 (Orthonormal Basis). A d-tuple of signals in L2 is said to form
an orthonormal basis for the linear subspace U ⊂L2 if it is orthonormal and
its span is U.
By convention the empty tuple forms an orthonormal basis for the 0-dimensional
subspace {0} whose sole element is the all-zero signal.
4.6.2
Representing a Signal Using an Orthonormal Basis
Suppose that (φ1, . . . , φd) is an orthonormal basis for U ⊂L2.
The fact that
(φ1, . . . , φd) spans U guarantees that every u ∈U can be written as u = 
ℓαℓφℓ
for some coeﬃcients α1, . . . , αd ∈C. The fact that (φ1, . . . , φd) is orthonormal
implies, by Proposition 4.6.2, that it is also linearly independent and hence that
the coeﬃcients {αℓ} are unique. How does one go about ﬁnding these coeﬃcients?
We next show that the orthonormality of (φ1, . . . , φd) also implies a very simple
expression for αℓabove. Indeed, as the next proposition demonstrates, αℓis given
explicitly as ⟨u, φℓ⟩.
Proposition 4.6.4 (Representing a Signal Using an Orthonormal Basis).
(i) If (φ1, . . . , φd) is an orthonormal tuple of functions in L2 and if u ∈L2 can
be written as u = d
ℓ=1 αℓφℓfor some complex numbers α1, . . . , αd, then
αℓ= ⟨u, φℓ⟩for every ℓ∈{1, . . . , d}:
	
u =
d

ℓ=1
αℓφℓ

=⇒
	
αℓ= ⟨u, φℓ⟩,
ℓ∈{1, . . . , d}

,

(φ1, . . . , φd) orthonormal

.
(4.30)
(ii) If (φ1, . . . , φd) is an orthonormal basis for the subspace U ⊂L2, then
u =
d

ℓ=1
⟨u, φℓ⟩φℓ,
u ∈U.
(4.31)
Proof. We begin by proving Part (i).
If u = d
ℓ=1 αℓφℓ, then for every ℓ′ ∈
{1, . . . , d}
⟨u, φℓ′⟩=

d

ℓ=1
αℓφℓ, φℓ′

=
d

ℓ=1
αℓ⟨φℓ, φℓ′⟩
=
d

ℓ=1
αℓI{ℓ= ℓ′}
= αℓ′,
4.6 Orthonormal Bases
39
thus proving Part (i).
We next prove Part (ii). Let u ∈U be arbitrary. Since, by assumption, the tuple
(φ1, . . . , φd) forms an orthonormal basis for U it follows a fortiori that its span
is U and, consequently, that there exist coeﬃcients α1, . . . , αd ∈C such that
u =
d

ℓ=1
αℓφℓ.
(4.32)
It now follows from Part (i) that for each ℓ∈{1, . . . , d} the coeﬃcient αℓin (4.32)
must be equal to ⟨u, φℓ⟩, thus establishing (4.31).
This proposition shows that if (φ1, . . . , φd) is an orthonormal basis for the sub-
space U and if u ∈U, then u is fully determined by the complex constants ⟨u, φ1⟩,
. . . , ⟨u, φd⟩. Thus, any calculation involving u can be computed from these con-
stants by ﬁrst reconstructing u using the proposition. As we shall see in Proposi-
tion 4.6.9, calculations involving inner products and norms are, however, simpler
than that.
4.6.3
Projection
We next discuss the projection of a signal v ∈L2 onto a ﬁnite-dimensional linear
subspace U that has an orthonormal basis (φ1, . . . , φd).1 To deﬁne the projection
we shall extend the approach we adopted in Section 4.5 for the projection of the
vector v onto the vector u. Recall that in that section we deﬁned the projection
as the vector w that is a scaled version of u and that satisﬁes that (v −w) is
orthogonal to u. Of course, if (v −w) is orthogonal to u, then it is orthogonal to
any scaled version of u, i.e., it is orthogonal to every signal in the space span(u).
We would like to adopt this approach and to deﬁne the projection of v ∈L2 onto U
as the element w of U for which (v −w) is orthogonal to every signal in U. Before
we can adopt this deﬁnition, we must show that such an element of U always exists
and that it is unique.
Lemma 4.6.5. Let (φ1, . . . , φd) be an orthonormal basis for the linear subspace
U ⊂L2. Let v ∈L2 be arbitrary.
(i) The signal v −d
ℓ=1 ⟨v, φℓ⟩φℓis orthogonal to every signal in U:

v −
d

ℓ=1
⟨v, φℓ⟩φℓ, u

= 0,

v ∈L2,
u ∈U

.
(4.33)
(ii) If w ∈U is such that v −w is orthogonal to every signal in U, then
w =
d

ℓ=1
⟨v, φℓ⟩φℓ.
(4.34)
1As we shall see in Section 4.6.5, not every ﬁnite-dimensional linear subspace of L2 has an
orthonormal basis. Here we shall only discuss projections onto subspaces that do.
40
The Space L2 of Energy-Limited Signals
Proof. To prove (4.33) we ﬁrst verify that it holds when u = φℓ′, for some ℓ′ in
the set {1, . . . , d}:

v −
d

ℓ=1
⟨v, φℓ⟩φℓ, φℓ′

= ⟨v, φℓ′⟩−

d

ℓ=1
⟨v, φℓ⟩φℓ, φℓ′

= ⟨v, φℓ′⟩−
d

ℓ=1
⟨v, φℓ⟩⟨φℓ, φℓ′⟩
= ⟨v, φℓ′⟩−
d

ℓ=1
⟨v, φℓ⟩I{ℓ= ℓ′}
= ⟨v, φℓ′⟩−⟨v, φℓ′⟩
= 0,
ℓ′ ∈{1, . . . , d}.
(4.35)
Having veriﬁed (4.33) for u = φℓ′ we next verify that this implies that it holds
for all u ∈U. By Proposition 4.6.4 we obtain that any u ∈U can be written as
u = d
ℓ′=1 βℓ′φℓ′, where βℓ′ = ⟨u, φℓ′⟩. Consequently,

v −
d

ℓ=1
⟨v, φℓ⟩φℓ, u

=

v −
d

ℓ=1
⟨v, φℓ⟩φℓ,
d

ℓ′=1
βℓ′φℓ′

=
d

ℓ′=1
β∗
ℓ′

v −
d

ℓ=1
⟨v, φℓ⟩φℓ, φℓ′

=
d

ℓ′=1
β∗
ℓ′ 0
= 0,
u ∈U,
where the third equality follows from (4.35) and the basic properties of the inner
product (3.6)–(3.10).
We next prove Part (ii) by showing that if w, w′ ∈U satisfy
⟨v −w, u⟩= 0,
u ∈U
(4.36)
and
⟨v −w′, u⟩= 0,
u ∈U,
(4.37)
then w = w′.
This follows from the calculation:
w −w′ =
d

ℓ=1
⟨w, φℓ⟩φℓ−
d

ℓ=1
⟨w′, φℓ⟩φℓ
=
d

ℓ=1
⟨w −w′, φℓ⟩φℓ
=
d

ℓ=1

(v −w′) −(v −w), φℓ

φℓ
4.6 Orthonormal Bases
41
=
d

ℓ=1

(v −w′), φℓ

−

(v −w), φℓ

φℓ
=
d

ℓ=1

0 −0

φℓ
= 0,
where the ﬁrst equality follows from Proposition 4.6.4; the second by the linearity of
the inner product in its left argument (3.9); the third by adding and subtracting v;
the fourth by the linearity of the inner product in its left argument (3.9); and the
ﬁfth equality from (4.36) & (4.37) applied by substituting φℓfor u.
With the aid of the above lemma we can now deﬁne the projection of a signal onto
a ﬁnite-dimensional subspace that has an orthonormal basis.2
Deﬁnition 4.6.6 (Projection of v ∈L2 onto U). Let U ⊂L2 be a ﬁnite-
dimensional linear subspace of L2 having an orthonormal basis. Let v ∈L2 be an
arbitrary energy-limited signal. Then the projection of v onto U is the unique
element w of U such that
⟨v −w, u⟩= 0,
u ∈U.
(4.38)
Note 4.6.7. By Lemma 4.6.5 it follows that if (φ1, . . . , φd) is an orthonormal basis
for U, then the projection of v ∈L2 onto U is given by
d

ℓ=1
⟨v, φℓ⟩φℓ.
(4.39)
To further develop the geometric picture of L2, we next show that, loosely speaking,
the projection of v ∈L2 onto U is the element in U that is closest to v. This result
can also be viewed as an optimal approximation result: if we wish to approximate v
by an element of U, then the optimal approximation is the projection of v onto U,
provided that we measure the quality of our approximation using the energy in the
error signal.
Proposition 4.6.8 (Projection as Best Approximation). Let U ⊂L2 be a ﬁnite-
dimensional subspace of L2 having an orthonormal basis. Let v ∈L2 be arbitrary.
Then the projection of v onto U is the element w ∈U that, among all the elements
of U, is closest to v in the sense that
∥v −u∥2 ≥∥v −w∥2 ,
u ∈U.
(4.40)
Proof. Let w be the projection of v onto U and let u be an arbitrary signal in U.
Since, by the deﬁnition of projection, w is in U and since U is a linear subspace,
it follows that w −u ∈U. Consequently, since by the deﬁnition of the projection
2A projection can also be deﬁned if the subspace does not have an orthonormal basis, but in
this case there is a uniqueness issue. There may be numerous vectors w ∈U such that v −w is
orthogonal to all vectors in U. Fortunately, they are all indistinguishable.
42
The Space L2 of Energy-Limited Signals
v −w is orthogonal to every element of U, it follows that v −w is a fortiori
orthogonal to w −u. Thus
∥v −u∥2
2 = ∥(v −w) + (w −u)∥2
2
= ∥v −w∥2
2 + ∥w −u∥2
2
(4.41)
≥∥v −w∥2
2 ,
(4.42)
where the ﬁrst equality follows by subtracting and adding w, the second equality
from the orthogonality of (v −w) and (w −u), and the ﬁnal inequality by the
nonnegativity of ∥·∥2. It follows from (4.42) that no signal in U is closer to v
than w is.
And it follows from (4.41) that if u ∈U is as close to v as w is,
then u −w must be an element of U that is of zero energy.
We shall see in
Proposition 4.6.10 that the hypothesis that U has an orthonormal basis implies
that the only zero-energy element of U is 0. Thus u and w must be identical, and
no other element of U is as close to v as w is.
4.6.4
Energy, Inner Products, and Orthonormal Bases
As demonstrated by Proposition 4.6.4, if (φ1, . . . , φd) forms an orthonormal basis
for the subspace U ⊂L2, then any signal u ∈U can be reconstructed from the d
numbers ⟨u, φ1⟩, . . . , ⟨u, φd⟩. Any quantity that can be computed from u can thus
be computed from ⟨u, φ1⟩, . . . , ⟨u, φd⟩by ﬁrst reconstructing u and by then per-
forming the calculation on u. But some calculations involving u can be performed
based on ⟨u, φ1⟩, . . . , ⟨u, φd⟩much more easily.
Proposition 4.6.9. Let (φ1, . . . , φd) be an orthonormal basis for the linear subspace
U ⊂L2.
(i) The energy ∥u∥2
2 of every u ∈U can be expressed in terms of the d inner
products ⟨u, φ1⟩, . . . , ⟨u, φd⟩as
∥u∥2
2 =
d

ℓ=1
⟨u, φℓ⟩
2.
(4.43)
(ii) More generally, if v ∈L2 (not necessarily in U), then
∥v∥2
2 ≥
d

ℓ=1
⟨v, φℓ⟩
2
(4.44)
with equality if, and only if, v is indistinguishable from some signal in U.
(iii) The inner product between any v ∈L2 and any u ∈U can be expressed in
terms of the inner products {⟨v, φℓ⟩} and {⟨u, φℓ⟩} as
⟨v, u⟩=
d

ℓ=1
⟨v, φℓ⟩⟨u, φℓ⟩∗.
(4.45)
4.6 Orthonormal Bases
43
Proof. Part (i) follows directly from the Pythagorean Theorem (Theorem 4.5.2)
applied to the d-tuple

⟨u, φ1⟩φ1, . . . , ⟨u, φd⟩φd

.
To prove Part (ii) we expand the energy in v as
∥v∥2
2 =


v −
d

ℓ=1
⟨v, φℓ⟩φℓ

+
d

ℓ=1
⟨v, φℓ⟩φℓ

2
2
=
v −
d

ℓ=1
⟨v, φℓ⟩φℓ

2
2 +

d

ℓ=1
⟨v, φℓ⟩φℓ

2
2
=
v −
d

ℓ=1
⟨v, φℓ⟩φℓ

2
2 +
d

ℓ=1
⟨v, φℓ⟩
2
≥
d

ℓ=1
⟨v, φℓ⟩
2,
(4.46)
where the ﬁrst equality follows by subtracting and adding the projection of v
onto U; the second from the Pythagorean Theorem and by Lemma 4.6.5, which
guarantees that the diﬀerence between v and its projection is orthogonal to any
signal in U and hence a fortiori also to the projection itself; the third by Part (i)
applied to the projection of v onto U; and the ﬁnal inequality by the nonnegativity
of energy.
If Inequality (4.46) holds with equality, then the last inequality in its derivation
must hold with equality, so
v −d
ℓ=1 ⟨v, φℓ⟩φℓ

2 = 0 and hence v must be
indistinguishable from the signal d
ℓ=1 ⟨v, φℓ⟩φℓ, which is in U.
Conversely, if v is indistinguishable from some u′ ∈U, then
∥v∥2
2 = ∥(v −u′) + u′∥2
2
= ∥v −u′∥2
2 + ∥u′∥2
2
= ∥u′∥2
2
=
d

ℓ=1
|⟨u′, φℓ⟩|2
=
d

ℓ=1
|⟨v, φℓ⟩+ ⟨u′ −v, φℓ⟩|2
=
d

ℓ=1
|⟨v, φℓ⟩|2,
where the ﬁrst equality follows by subtracting and adding u′; the second follows
from the Pythagorean Theorem because the fact that ∥v −u′∥2 = 0 implies that
⟨v −u′, u′⟩= 0 (as can be readily veriﬁed using the Cauchy-Schwarz Inequality
|⟨v −u′, u′⟩| ≤∥v −u′∥2 ∥u′∥2); the third from our assumption that v and u′ are
indistinguishable; the fourth from Part (i) applied to the function u′ (which is in U);
the ﬁfth by adding and subtracting v; and where the ﬁnal equality follows because
44
The Space L2 of Energy-Limited Signals
⟨u′ −v, φℓ⟩= 0 (as can be readily veriﬁed from the Cauchy Schwarz Inequality
|⟨u′ −v, φℓ⟩| ≤∥u′ −v∥2 ∥φℓ∥2).
To prove Part (iii) we compute ⟨v, u⟩as
⟨v, u⟩=

v,
d

ℓ=1
⟨u, φℓ⟩φℓ

=
d

ℓ=1
⟨u, φℓ⟩∗⟨v, φℓ⟩,
where the ﬁrst equality holds because u is in U and can therefore be expressed as
in (4.31) of Proposition 4.6.4 (ii); and where the second equality follows from the
sesquilinearity of the inner product ((3.10) and (3.8)).
Proposition 4.6.9 has interesting consequences. It shows that if one thinks of ⟨u, φℓ⟩
as the ℓ-th coordinate of u (with respect to the orthonormal basis (φ1, . . . , φd)),
then the energy in u is simply the sum of the squares of the coordinates, and the
inner product between two functions is the sum of the products of each coordinate
of u and the conjugate of the corresponding coordinate of v.
We hope that the properties of orthonormal bases that we presented above have
convinced the reader by now that there are certain advantages to describing func-
tions using an orthonormal basis. A crucial question arises as to whether orthonor-
mal bases always exist. This question is addressed next.
4.6.5
Does an Orthonormal Basis Exist?
Word on the street has it that every ﬁnite-dimensional subspace of L2 has an
orthonormal basis, but this is not true. (It is true for the space L2 that we shall
encounter later.) For example, the set

u ∈L2 : u(t) = 0
whenever t ̸= 17

of all energy-limited signals that map t to zero whenever t ̸= 17 (with the value
to which t = 17 is mapped being unspeciﬁed) is a one dimensional subspace of L2
that does not have an orthonormal basis. (All the signals in this subspace are of
zero energy, so there are no unit-energy signals in it.)
Proposition 4.6.10. If U is a ﬁnite-dimensional subspace of L2, then the following
two statements are equivalent:
(a) U has an orthonormal basis.
(b) The only element of U of zero energy is the all-zero signal 0.
Proof. The proof has two parts. The ﬁrst consists of showing that (a) ⇒(b), i.e.,
that if U has an orthonormal basis and if u ∈U is of zero energy, then u must
be the all-zero signal 0. The second part consists of showing that (b) ⇒(a), i.e.,
that if the only element of zero energy in U is the all-zero signal 0, then U has an
orthonormal basis.
4.6 Orthonormal Bases
45
We begin with the ﬁrst part, namely, (a) ⇒(b). We thus assume that (φ1, . . . , φd)
is an orthonormal basis for U and that u ∈U satisﬁes ∥u∥2 = 0 and proceed
to prove that u = 0. We simply note that, by the Cauchy-Schwarz Inequality,
|⟨u, φℓ⟩| ≤∥u∥2 ∥φℓ∥2 so the condition ∥u∥2 = 0 implies
⟨u, φℓ⟩= 0,
ℓ∈{1, . . . , d},
(4.47)
and hence, by Proposition 4.6.4, that u = 0.
To show (b) ⇒(a) we need to show that if no signal in U other than 0 has zero
energy, then U has an orthonormal basis. The proof is based on the Gram-Schmidt
Procedure, which is presented next. As we shall prove, if the input to this procedure
is a basis for U and if no element of U other than 0 is of energy zero, then the
procedure produces an orthonormal basis for U. The procedure is actually even
more powerful. If it is fed a basis for a subspace that does contain an element other
than 0 of zero-energy, then the procedure produces such an element and halts.
It should be emphasized that the Gram-Schmidt Procedure is not only useful for
proving theorems; it can be quite useful for ﬁnding orthonormal bases for practical
problems.3
4.6.6
The Gram-Schmidt Procedure
The Gram-Schmidt Procedure is named after the mathematicians Jørgen Pedersen
Gram (1850–1916) and Erhard Schmidt (1876–1959). However, as pointed out in
(Farebrother, 1988), this procedure was apparently already presented by Pierre-
Simon Laplace (1749–1827) and was used by Augustin Louis Cauchy (1789–1857).
The input to the Gram-Schmidt Procedure is a basis (u1, . . . , ud) for a d-dimensional
subspace U ⊂L2. We assume that d ≥1. (The only 0-dimensional subspace of L2
is the subspace {0} containing the all-zero signal only, and for this subspace the
empty tuple is an orthonormal basis; there is not much else to say here.) If U
does not contain a signal of zero energy other than the all-zero signal 0, then the
procedure runs in d steps and produces an orthonormal basis for U (and thus also
proves that U does not contain a zero-energy signal other than 0). Otherwise, the
procedure stops after d or fewer steps and produces an element of U of zero energy
other than 0.
The Gram-Schmidt Procedure:
Step 1: If ∥u1∥2 = 0, then the procedure declares that there exists a
zero-energy element of U other than 0, it produces u1 as proof, and it
halts. Otherwise, it deﬁnes
φ1 =
u1
∥u1∥2
and halts with the output (φ1) (if d = 1) or proceeds to Step 2 (if
d > 1).
3Numerically, however, it is unstable; see (Golub and van Loan, 1996).
46
The Space L2 of Energy-Limited Signals
Assuming that the procedure has run for ν −1 steps without halting
and has deﬁned the vectors φ1, . . . , φν−1, we next describe Step ν.
Step ν: Consider the signal
˜uν = uν −
ν−1

ℓ=1
⟨uν, φℓ⟩φℓ.
(4.48)
If ∥˜uν∥2 = 0, then the procedure declares that there exists a zero-
energy element of U other than 0, it produces ˜uν as proof, and it halts.
Otherwise, the procedure deﬁnes
φν =
˜uν
∥˜uν∥2
(4.49)
and halts with the output (φ1, . . . , φd) (if ν is equal to d) or proceeds
to Step ν + 1 (if ν < d).
We next prove that the procedure behaves as we claim.
Proof. To prove that the procedure behaves as we claim, we shall assume that the
procedure performs Step ν (i.e., that it has not halted in the steps preceding ν)
and prove the following: if at Step ν the procedure declares that U contains a
nonzero signal of zero-energy and produces ˜uν as proof, then this is indeed the
case; otherwise, if it deﬁnes φν as in (4.49), then (φ1, . . . , φν) is an orthonormal
basis for span(u1, . . . , uν).
We prove this by induction on ν. For ν = 1 this can be veriﬁed as follows. If
∥u1∥2 = 0, then we need to show that u1 ∈U and that it is not equal to 0. This
follows from the assumption that the procedure’s input (u1, . . . , ud) forms a basis
for U, so a fortiori the signals u1, . . . , ud must all be elements of U and neither
of them can be the all-zero signal. If ∥u1∥2 > 0, then φ1 is a unit-energy scaled
version of u1 and thus (φ1) is an orthonormal basis for span(u1).
We now assume that our claim is true for ν −1 and proceed to prove that it is also
true for ν. We thus assume that Step ν is executed and that (φ1, . . . , φν−1) is an
orthonormal basis for span(u1, . . . , uν−1):
φ1, . . . , φν−1 ∈U;
(4.50)
span(φ1, . . . , φν−1) = span(u1, . . . , uν−1);
(4.51)
and
⟨φℓ, φℓ′⟩= I{ℓ= ℓ′},
ℓ, ℓ′ ∈{1, . . . , ν −1}.
(4.52)
We need to prove that if ˜uν is of zero energy, then it is a nonzero element of U of
zero energy, and that otherwise the ν-tuple (φ1, . . . , φν) is an orthonormal basis
for span(u1, . . . , uν). To that end we ﬁrst prove that
˜uν ∈U
(4.53)
and that
˜uν ̸= 0.
(4.54)
4.6 Orthonormal Bases
47
We begin with a proof of (4.53). Since (4.48) expresses ˜uν as a linear combination
of (φ1, . . . , φν−1, uν), and since U is by assumption a linear subspace, it suﬃces to
show that φ1, . . . , φν−1 ∈U and that uν ∈U. The former follows from (4.50) and
the latter from our assumption that (u1, . . . , ud) forms a basis for U.
We next prove (4.54). By (4.48) it suﬃces to show that uν /∈span(φ1, . . . , φν−1).
By (4.51) this is equivalent to showing that uν /∈span(u1, . . . , uν−1), which fol-
lows from our assumption that (u1, . . . , ud) is a basis for U and a fortiori linearly
independent.
Having established (4.53) and (4.54) it follows that if ∥˜uν∥2 = 0, then ˜uν is a
nonzero element of U which is of zero-energy as we had claimed.
To conclude the proof we now assume ∥˜uν∥2 > 0 and prove that (φ1, . . . , φν) is
an orthonormal basis for span(u1, . . . , uν). That (φ1, . . . , φν) is orthonormal fol-
lows because (4.52) guarantees that (φ1, . . . , φν−1) is orthonormal; because (4.49)
guarantees that φν is of unit energy; and because Lemma 4.6.5 (applied to the lin-
ear subspace span(φ1, . . . , φν−1)) guarantees that ˜uν—and hence also its scaled
version φν—is orthogonal to every element of span(φ1, . . . , φν−1) and in par-
ticular to φ1, . . . , φν−1. It thus only remains to show that span(φ1, . . . , φν) =
span(u1, . . . , uν). We ﬁrst show that span(φ1, . . . , φν) ⊆span(u1, . . . , uν). This
follows because (4.51) implies that
φ1, . . . , φν−1 ∈span(u1, . . . , uν−1);
(4.55)
because (4.55), (4.48) and (4.49) imply that
φν ∈span(u1, . . . , uν);
(4.56)
and because (4.55) and (4.56) imply that φ1, . . . , φν ∈span(u1, . . . , uν) and hence
that span(φ1, . . . , φν) ⊆span(u1, . . . , uν). The reverse inclusion can be argued
very similarly: by (4.51)
u1, . . . , uν−1 ∈span(φ1, . . . , φν−1);
(4.57)
by (4.48) and (4.49) we can express uν as a linear combination of (φ1, . . . , φν)
uν = ∥˜uν∥2 φν +
ν−1

ℓ=1
⟨uν, φℓ⟩φℓ;
(4.58)
and (4.57) & (4.58) combine to prove that u1, . . . , uν ∈span(φ1, . . . , φν) and hence
that span(u1, . . . , uν) ⊆span(φ1, . . . , φν).
By far the more important scenario for us is when U does not contain a nonzero
element of zero energy. This is because we shall mostly focus on signals that are
bandlimited (see Chapter 6), and the only energy-limited signal that is bandlimited
to W Hz and that has zero-energy is the all-zero signal (Note 6.4.2). For subspaces
not containing zero-energy signals other than 0 the key properties to note about
the signals φ1, . . . , φd produced by the Gram-Schmidt procedure are that they
satisfy for each ν ∈{1, . . . , d}
span(u1, . . . , uν) = span(φ1, . . . , φν)
(4.59a)
48
The Space L2 of Energy-Limited Signals
and

φ1, . . . , φν

is an orthonormal basis for span(u1, . . . , uν).
(4.59b)
These properties are, of course, of greatest importance when ν = d.
We next provide an example of the Gram-Schmidt procedure.
Example 4.6.11. Consider the following three signals: u1 : t 	→I{0 ≤t ≤1},
u2 : t 	→t I{0 ≤t ≤1}, and u3 : t 	→t2 I{0 ≤t ≤1}. The tuple (u1, u2, u3) forms
a basis for the subspace of all signals of the form t 	→p(t) I{0 ≤t ≤1}, where p(·)
is a polynomial of degree smaller than 3. To construct an orthonormal basis for
this subspace with the Gram-Schmidt Procedure, we begin by normalizing u1. To
that end, we compute
∥u1∥2
2 =
 ∞
−∞
|I{0 ≤t ≤1}|2 dt = 1
and set φ1 = u1/ ∥u1∥2, so
φ1 : t 	→I{0 ≤t ≤1}.
(4.60a)
The second function φ2 is now obtained by normalizing u2 −⟨u2, φ1⟩φ1. We ﬁrst
compute the inner product ⟨u2, φ1⟩
⟨u2, φ1⟩=
 ∞
−∞
I{0 ≤t ≤1} t I{0 ≤t ≤1} dt =
 1
0
t dt = 1
2
to obtain that u2 −⟨u2, φ1⟩φ1 : t 	→(t −1/2) I{0 ≤t ≤1}, which is of energy
∥u2 −⟨u2, φ1⟩φ1∥2
2 =
 1
0

t −1
2
2
dt = 1
12.
Hence,
φ2 : t 	→
√
12

t −1
2

I{0 ≤t ≤1}.
(4.60b)
The third function φ3 is the normalized version of u3 −⟨u3, φ1⟩φ1 −⟨u3, φ2⟩φ2.
The inner products ⟨u3, φ1⟩and ⟨u3, φ2⟩are respectively
⟨u3, φ1⟩=
 1
0
t2 dt = 1
3,
⟨u3, φ2⟩=
 1
0
t2 √
12
	
t −1
2

dt =
1
√
12.
Consequently
u3 −⟨u3, φ1⟩φ1 −⟨u3, φ2⟩φ2 : t 	→
	
t2 −1
3 −

t −1
2

I{0 ≤t ≤1}
with corresponding energy
∥u3 −⟨u3, φ1⟩φ1 −⟨u3, φ2⟩φ2∥2
2 =
 1
0

t2 −t + 1
6
2
dt =
1
180.
Hence, the orthonormal basis is completed by the third function
φ3 : t 	→
√
180

t2 −t + 1
6

I{0 ≤t ≤1}.
(4.60c)
4.7 The Space L2
49
4.7
The Space L2
Very informally one can describe the space L2 as the space of all energy-limited
complex-valued signals, where we think of two signals as being diﬀerent only if they
are distinguishable. This section deﬁnes L2 more precisely. It can be skipped be-
cause we shall have only little to do with L2. Understanding this space is, however,
important for readers who wish to fully understand how the Fourier Transform is
deﬁned for energy-limited signals that are not integrable (Section 6.2.3). Readers
who continue should recall from Section 2.5 that two energy-limited signals u and v
are said to be indistinguishable if the set {t ∈R : u(t) ̸= v(t)} is of Lebesgue
measure zero. We write u ≡v to indicate that u and v are indistinguishable. By
Proposition 2.5.3, the condition u ≡v is equivalent to the condition ∥u −v∥2 = 0.
To motivate the deﬁnition of the space L2, we begin by noting that the space L2
of energy-limited signals is “almost” an example of what mathematicians call an
“inner product space,” but it is not. The problem is that mathematicians insist
that in an inner product space the only vector whose inner product with itself is
zero be the zero vector. This is not the case in L2: it is possible that u ∈L2
satisfy ⟨u, u⟩= 0 (i.e., ∥u∥2 = 0) and yet not be the all-zero signal 0. From the
condition ∥u∥2 = 0 we can only infer that u is indistinguishable from 0.
The fact that L2 is not an inner product space is an annoyance because it pre-
cludes us from borrowing from the vast literature on inner product spaces (and
Hilbert spaces, which are special kinds of inner product spaces), and because it
does not allow us to view some of the results about L2 as instances of more gen-
eral principles. For this reason mathematicians prefer to study the space L2, which
is an inner product space (and which is, in fact, a Hilbert space) rather than L2.
Unfortunately, for this luxury they pay a certain price that I am loath to pay.
Consequently, in most of this book I have decided to stick to L2 even though this
precludes me from using the standard results on inner product spaces. The price
one pays for using L2 will become apparent once we deﬁne it.
To understand how L2 is constructed it is useful to note that the relation “u ≡v”,
i.e., “u is indistinguishable from v” is an equivalence relation on L2, i.e., it
satisﬁes
u ≡u,
u ∈L2;
(reﬂexive)

u ≡v

⇐⇒

v ≡u

,
u, v ∈L2;
(symmetric)
and

u ≡v and v ≡w

=⇒

u ≡w

,
u, v, w ∈L2.
(transitive)
Using these properties one can verify that if for every u ∈L2 we deﬁne its equiv-
alence class [u] as
[u] ≜
˜u ∈L2 : ˜u ≡u},
(4.61)
then two equivalence classes [u] and [v] must be either identical or disjoint. In
fact, the sets [u] ⊂L2 and [v] ⊂L2 are identical if, and only if, u and v are
indistinguishable

[u] = [v]

⇐⇒

∥u −v∥2 = 0

,
u, v ∈L2,
50
The Space L2 of Energy-Limited Signals
and they are disjoint if, and only if, u and v are distinguishable

[u] ∩[v] = ∅

⇐⇒

∥u −v∥2 > 0

,
u, v ∈L2.
We deﬁne L2 as the set of all such equivalence classes
L2 ≜

[u] : u ∈L2}.
(4.62)
Thus, the elements of L2 are not functions, but sets of functions. Each element
of L2 is an equivalence class, i.e., a set of the form [u] for some u ∈L2. And for
each u ∈L2 the equivalence class [u] is an element of L2.
As we next show, the space L2 can also be viewed as a vector space. To this end
we need to ﬁrst deﬁne “ampliﬁcation of an equivalence class by a scalar α ∈C” and
“superposition of two equivalence classes.” How do we deﬁne the scaling-by-α of
an equivalence class S ∈L2? A natural approach is to ﬁnd some function u ∈L2
such that S is its equivalence class (i.e., satisfying S = [u]), and to deﬁne the
scaling-by-α of S as the equivalence class of α u, i.e., as [α u]. Thus we would
deﬁne α S as the equivalence class of the signal t 	→α u(t). While this turns out to
be a good approach, the careful reader might be concerned by something. Suppose
that S = [u] but that also S = [˜u]. Should α S be deﬁned as the equivalence class
of t 	→α u(t) or of t 	→α ˜u(t)? Fortunately, it does not matter because the two
equivalence classes are the same! Indeed, if [u] = [˜u], then the equivalence class of
t 	→α u(t) is equal to the equivalence class of t 	→α ˜u(t) (because [u] = [˜u] implies
that u and ˜u agree except on a set of measure zero so α u and α ˜u also agree except
on a set of measure zero, which in turn implies that [α u] = [α ˜u]).
Similarly, one can show that if S1 ∈L2 and S2 ∈L2 are two equivalence classes,
then we can deﬁne their sum (or superposition) S1 + S2 as [u1 + u2] where u1
is any function in L2 such that S1 = [u1] and where u2 is any function in L2
such that S2 = [u2]. Again, to make sure that the result of the superposition of
S1 and S2 does not depend on the choice of u1 and u2 we need to verify that if
S1 = [u1] = [˜u1] and if S2 = [u2] = [˜u2] then [u1 + u2] = [˜u1 + ˜u2]. This is not
diﬃcult but is omitted.
Using these deﬁnitions and by deﬁning the zero vector to be the equivalence
class [0], it is not diﬃcult to show that L2 forms a linear space over the com-
plex ﬁeld. To make it into an inner product space we need to deﬁne the inner
product ⟨S1, S2⟩between two equivalence classes. If S1 = [u1] and if S2 = [u2]
we deﬁne the inner product ⟨S1, S2⟩as the complex number ⟨u1, u2⟩. Again, we
have to show that our deﬁnition is good in the sense that it does not depend on
the particular choice of u1 and u2. More speciﬁcally, we need to verify that if
S1 = [u1] = [˜u1] and if S2 = [u2] = [˜u2] then ⟨u1, u2⟩= ⟨˜u1, ˜u2⟩. This can be
proved as follows:
⟨u1, u2⟩= ⟨˜u1 + (u1 −˜u1), u2⟩
= ⟨˜u1, u2⟩+ ⟨u1 −˜u1, u2⟩
= ⟨˜u1, u2⟩
= ⟨˜u1, ˜u2 + (u2 −˜u2)⟩
4.8 Additional Reading
51
= ⟨˜u1, ˜u2⟩+ ⟨˜u1, u2 −˜u2⟩
= ⟨˜u1, ˜u2⟩,
where the third equality follows because [u1] = [˜u1] implies that ∥u1 −˜u1∥2 = 0
and hence that ⟨u1 −˜u1, u2⟩= 0 (Cauchy-Schwarz Inequality), and where the
last equality follows by a similar reasoning about u2 and ˜u2.
Using the above
deﬁnition of the inner product between equivalence classes one can show that if for
some equivalence class S we have ⟨S, S⟩= 0, then S is the zero vector, i.e., the
equivalence class [0].
With these deﬁnitions of the scaling of an equivalence class by a scalar, the super-
position of two equivalence classes, and the inner product between two equivalence
classes, the space of equivalence classes L2 becomes an inner product space in the
sense that mathematicians like. In fact, it is a Hilbert space.
What is the price we have to pay for working in an inner product space?
It
is that the elements of L2 are not functions but equivalence classes and that it
is meaningless to talk about the value they take at a given time. For example,
it is meaningless to discuss the supremum (or maximum) of an element of L2.4
To add to the confusion, mathematicians refer to elements of L2 as “functions”
(even though they are equivalence classes of functions), and they drop the square
brackets. Things get even trickier when one deals with signals contaminated by
noise. If one views the signals as elements of L2, then the result of adding noise to
them is not a stochastic process (Deﬁnition 12.2.1 ahead). We ﬁnd this price too
high, and in this book we shall mostly deal with L2.
4.8
Additional Reading
Most of the results of this chapter follow from basic results on inner product spaces
and can be found, for example, in (Axler, 2015). However, since L2 is not an inner-
product space, we had to introduce some slight modiﬁcations.
More on the deﬁnition of the space L2 can be found in most texts on analysis. See,
for example, (Rudin, 1987, Chapter 3, Remark 3.10) and (Royden and Fitzpatrick,
2010, Section 7.1).
4.9
Exercises
Exercise 4.1 (Subsets that Are not Subspaces).
(i) Provide an example of a set of real energy-limited signals that is closed with respect
to superposition but is not a linear subspace of L2.
(ii) Provide an example of a set of real energy-limited signals that is closed with respect
to ampliﬁcation but is not a linear subspace of L2.
Exercise 4.2 (Linear Subspace). Consider the set of signals u of the form u: t →e−t2p(t),
where p(·) is a polynomial whose degree does not exceed d. Is this a linear subspace of L2?
If yes, ﬁnd a basis for this subspace.
4To deal with this, mathematicians deﬁne the essential supremum.
52
The Space L2 of Energy-Limited Signals
Exercise 4.3 (Characterizing Inﬁnite-Dimensional Subspaces). Recall that we say that a
linear subspace is inﬁnite dimensional if it is not of ﬁnite dimension. Show that a linear
subspace U is inﬁnite dimensional if, and only if, there exists a sequence u1, u2, . . . of
elements of U such that for every n ∈N the tuple (u1, . . . , un) is linearly independent.
Exercise 4.4 (L2 Is Inﬁnite Dimensional). Show that L2 is inﬁnite dimensional.
Hint: Exercises 4.2 and 4.3 may be useful.
Exercise 4.5 (Separation between Signals). Given u1, u2 ∈L2, let V be the set of all
complex signals v that are equidistant to u1 and u2:
V =

v ∈L2 : ∥v −u1∥2 = ∥v −u2∥2

.
(i) Show that
V =

v ∈L2 : Re
	
v, u2 −u1

= ∥u2∥2
2 −∥u1∥2
2
2

.
(ii) Is V a linear subspace of L2?
(iii) Show that (u1 + u2)/2 ∈V.
Exercise 4.6 (Orthogonal Subspace). Given signals v1, . . . , vn ∈L2, deﬁne the set
U =

u ∈L2 : ⟨u, v1⟩= ⟨u, v2⟩= · · · = ⟨u, vn⟩= 0

.
Show that U is a linear subspace of L2.
Exercise 4.7 (Projecting a Signal). Let u ∈L2 be of positive energy, and let v ∈L2 be
arbitrary.
(i) Show that Deﬁnitions 4.6.6 and 4.5.3 agree in the sense that the projection of v
onto span(u) (according to Deﬁnition 4.6.6) is the same as the projection of v onto
the signal u (according to Deﬁnition 4.5.3).
(ii) Show that if the signal u is an element of a ﬁnite-dimensional subspace U having
an orthonormal basis, then the projection of u onto U is u.
Exercise 4.8 (On Projections and Approximations). Let (u1, . . . , un) be an n-tuple in L2,
and let U be its span. Assume that U contains no zero-energy signals other than the all-
zero signal 0. Prove that if v is any signal in L2 and if u is its projection onto U, then
∥v −u∥2 ≤∥v −u1∥2. Under what conditions does this hold with equality?
Exercise 4.9 (Computing the Projection). Let (u1, . . . , un) be an n-tuple in L2, and let
U be its span. Assume that U contains no zero-energy signals other than 0.
(i) Prove that the projection of any signal v ∈L2 onto U can be computed from
(u1, . . . , un) and the n inner products ⟨v, u1⟩, . . . , ⟨v, un⟩.
(ii) Prove that if n
i=1 αi ui is the projection of v onto U then
⎛
⎜
⎜
⎜
⎝
⟨u1, u1⟩
⟨u2, u1⟩
· · ·
⟨un, u1⟩
⟨u1, u2⟩
⟨u2, u2⟩
· · ·
⟨un, u2⟩
...
...
...
...
⟨u1, un⟩
⟨u2, un⟩
· · ·
⟨un, un⟩
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
α1
α2
...
αn
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
⟨v, u1⟩
⟨v, u2⟩
...
⟨v, un⟩
⎞
⎟
⎟
⎟
⎠.
Does this set of n linear equations in α1, . . . , αn always have a solution? Can it
have more than one solution?
4.9 Exercises
53
Exercise 4.10 (Projecting in Two Steps). Let U1 and U2 be ﬁnite-dimensional linear
subspaces of L2 that do not contain signals of zero energy other than the all-zero signal.
Assume that U1 ⊆U2. Show that if v is any energy-limited signal, then its projection
onto U1 can be computed by ﬁrst projecting v onto U2 and by then projecting the result
onto U1. Is the assumption U1 ⊆U2 essential?
Exercise 4.11 (Constructing an Orthonormal Basis). Let Ts be a positive constant. Con-
sider the signals s1 : t →I{0 ≤t ≤Ts/2} −I{Ts/2 < t ≤Ts}; s2 : t →I{0 ≤t ≤Ts};
s3 : t →I{0 ≤t ≤Ts/4} + I{3Ts/4 ≤t ≤Ts}; and s4 : t →I{0 ≤t ≤Ts/4} −I{3Ts/4 ≤
t ≤Ts}.
(i) Plot s1, s2, s3, and s4.
(ii) Find an orthonormal basis for span(s1, s2, s3, s4).
(iii) Express each of the signals s1, s2, s3, and s4 as a linear combination of the basis
vectors found in Part (ii).
Exercise 4.12 (Is the L2-Limit Unique?). Show that for signals ζ, x1, x2, . . . in L2 the
statement
lim
n→∞∥xn −ζ∥2 = 0
is equivalent to the statement
	
lim
n→∞
xn −˜ζ

2 = 0

⇐⇒
	
˜ζ ∈[ζ]

.
Exercise 4.13 (Signals of Zero Energy). Given v1, . . . , vn ∈L2, show that there exist
integers 1 ≤ν1 < ν2 < · · · < νd ≤n such that the following three conditions hold:
the d-tuple

vν1, . . . , vνd

is linearly independent; span(vν1, . . . , vνd) contains no signal
of zero energy other than the all-zero signal 0; and each element of span(v1, . . . , vn) is
indistinguishable from some element of span(vν1, . . . , vνd).
Exercise 4.14 (Orthogonal Subspace). Given v1, . . . , vn ∈L2, deﬁne the set
U =

u ∈L2 : ⟨u, v1⟩= ⟨u, v2⟩= · · · = ⟨u, vn⟩= 0

,
and the set of all energy-limited signals that are orthogonal to all the signals in U:
U⊥=
 
w ∈L2 :

⟨w, u⟩= 0, u ∈U
!
.
(i) Show that U ⊥is a linear subspace of L2.
(ii) Show that an energy-limited signal is in U ⊥if, and only if, it is indistinguishable
from some element of span(v1, . . . , vn).
Hint: For Part (ii) you may ﬁnd Exercise 4.13 useful.
Exercise 4.15 (More on Indistinguishability). Given v1, . . . , vn ∈L2 and some w ∈L2,
propose an algorithm to check whether there exists an element of span(v1, . . . , vn) that
is indistinguishable from w.
Hint: Exercise 4.13 may be useful.
Exercise 4.16 (Delaying an Equivalence Class). Given some τ ∈R and some u ∈L2,
the delay-by-τ of the equivalence class [u] ∈L2 is deﬁned as the equivalence class of
the mapping t →u(t −τ). Show that the delay-by-τ operator is well-deﬁned in the sense
that if S = [u] and also S = [˜u], then the equivalence class of the mapping t →u(t −τ)
is equal to the equivalence class of the mapping t →˜u(t −τ).
Chapter 5
Convolutions and Filters
5.1
Introduction
Convolutions play a central role in the analysis of linear systems, and it is thus
not surprising that they will appear repeatedly in this book. Most of the readers
have probably seen the deﬁnition and key properties in an earlier course on linear
systems, so this chapter can be viewed as a very short review. New perhaps is
the following section on notation and the all-important Section 5.8 on the matched
ﬁlter and its use in calculating inner products.
5.2
Time Shifts and Reﬂections
Suppose that x: R →R is a real signal, where we think of the argument as being
time. Such functions are typically plotted on paper with the time arrow pointing
to the right. Take a moment to plot an example of such a function, and on the
same coordinates plot the function
t 	→x(t −t0),
which maps every t ∈R to x(t −t0) for some positive t0. Repeat with t0 being
negative. This may seem like a mindless exercise but there is a point to it. It
will help you understand convolutions graphically and help you visualize mappings
such as t 	→
ℓαℓg(t −ℓTs), which we will encounter later in our study of Pulse
Amplitude Modulation (PAM). It will also help you visualize the matched ﬁlter.
Given a complex signal x: R →C, we denote its reﬂection or mirror image
by ~x, so
~x: t 	→x(−t).
(5.1)
When x is real, the plot of ~x is the mirror image of the plot of x about the vertical
axis. The mirror image of the mirror image of x is always x.
54
5.3 The Convolution Expression
55
5.3
The Convolution Expression
The convolution x ⋆h between two complex signals x: R →C and h: R →C is
formally deﬁned as the complex signal whose time-t value (x ⋆h)(t) is given by
(x ⋆h)(t) =
 ∞
−∞
x(τ) h(t −τ) dτ.
(5.2)
Note that the integrand in the above is complex. (Integrals of complex functions
are discussed in Section 2.3.) This deﬁnition also holds for real signals.
We used the term “formally deﬁned” because certain conditions need to be met
for this integral to be deﬁned. It is conceivable that for some t ∈R the integrand
τ 	→x(τ) h(t −τ) will not be integrable, so the integral will be undeﬁned. (Recall
that in this book we only allow integrals of the form
 ∞
−∞g(t) dt if the integrand
g(·) is in L1 so
 ∞
−∞|g(t)| dt < ∞.1) We thus say that x ⋆h is deﬁned at t ∈R if
τ 	→x(τ) h(t −τ) is integrable.
While (5.2) does not make it apparent, the convolution is in fact symmetric in x
and h. Thus, the integral in (5.2) is deﬁned for a given t if, and only if, the integral
 ∞
−∞
h(σ) x(t −σ) dσ
(5.3)
is deﬁned. And if both are deﬁned, then their values are identical. This follows
directly by the change of variable σ ≜t −τ.
5.4
Thinking About the Convolution
Depending on the application, we can think about the convolution operation in a
number of diﬀerent ways.
(i) Especially when h(·) is nonnegative and integrates to one, one can think of
the convolution as an averaging, or smoothing, operation. Thus, when x is
convolved with h the result at time t0 is not x(t0) but rather a smoothed
version thereof, namely,
 ∞
−∞x(t0 −τ) h(τ) dτ. For example, if h is the map-
ping t 	→I{|t| ≤T/2}/T for some T > 0, then the convolution x ⋆h at time
t0 is not x(t0) but rather
1
T
 t0+T/2
t0−T/2
x(τ) dτ.
Thus, in this example, we can think of x ⋆h as being a “moving average,” or
a “sliding-window average” of x.
(ii) For energy-limited signals it is sometimes beneﬁcial to think about (x⋆h)(t0)
as the inner product between the functions τ 	→x(τ) and τ 	→h∗(t0 −τ):
(x ⋆h)(t0) =

τ 	→x(τ), τ 	→h∗(t0 −τ)

.
(5.4)
1On rare occasions we make exceptions for nonnegative integrands whose integral can be +∞.
56
Convolutions and Filters
(iii) Another useful informal way is to think about x ⋆h as a limit of expressions
of the form

j
(tj+1 −tj) h(tj) x(t −tj),
(5.5)
i.e., as a limit of linear combinations of the time shifts of x where the coeﬃ-
cients are determined by h.
5.5
When Is the Convolution Deﬁned?
There are a number of useful theorems providing suﬃcient conditions for the con-
volution’s existence. These theorems can be classiﬁed into two kinds: those that
guarantee that the convolution x ⋆h is deﬁned at every epoch t ∈R and those
that only guarantee that the convolution is deﬁned for all epochs t outside a set of
Lebesgue measure zero. Both types are useful. We begin with the former.
Convolution deﬁned for every t ∈R:
(i) A particularly simple case where the convolution is deﬁned at every time
instant t is when both x and h are energy-limited:
x, h ∈L2.
(5.6a)
In this case we can use (5.4) and the Cauchy-Schwarz Inequality (Theo-
rem 3.3.1) to conclude that the integral in (5.2) is deﬁned for every t ∈R
and that x ⋆h is a bounded function with
(x ⋆h)(t)
 ≤∥x∥2 ∥h∥2 ,
t ∈R.
(5.6b)
Indeed,
(x ⋆h)(t)
 =

τ 	→x(τ), τ 	→h∗(t −τ)

≤∥τ 	→x(τ)∥2 ∥τ 	→h∗(t −τ)∥2
= ∥x∥2 ∥h∥2 .
In fact, it can be shown that the result of convolving two energy-limited
signals is not only bounded but also uniformly continuous.2 (See, for example,
(Adams and Fournier, 2003, Paragraph 2.23).)
Note that even if both x and h are of ﬁnite energy, the convolution x ⋆h
need not be. However, if x, h are both of ﬁnite energy and if one of them
is additionally also integrable, then the convolution x ⋆h is a ﬁnite-energy
signal. Indeed,
∥x ⋆h∥2 ≤∥h∥1 ∥x∥2 ,
h ∈L1 ∩L2,
x ∈L2.
(5.7)
For a proof see, for example, (Rudin, 1987, Chapter 8, Exercise 4) or (Stein
and Weiss, 1971, Chapter 1, Section 1, Theorem 1.3).
2A function s: R →C is said to be uniformly continuous if for every ϵ > 0 there corresponds
some positive δ(ϵ) such that |s(ξ′) −s(ξ′′)| < ϵ whenever ξ′, ξ′′ ∈R are such that |ξ′ −ξ′′| < δ(ϵ).
5.5 When Is the Convolution Deﬁned?
57
(ii) Another simple case where the convolution is deﬁned at every epoch t ∈R is
when one of the functions is measurable and bounded and when the other is
integrable. For example, if
h ∈L1
(5.8a)
and if x is a Lebesgue measurable function that is bounded in the sense that
|x(t)| ≤σ∞,
t ∈R
(5.8b)
for some constant σ∞, then for every t ∈R the integrand in (5.3) is integrable
because |h(σ) x(t −σ)| ≤|h(σ)| σ∞, with the latter being integrable by our
assumption that h is integrable. The result of the convolution is a bounded
function because
|(x ⋆h)(t)| =

 ∞
−∞
h(τ) x(t −τ) dτ

≤
 ∞
−∞
h(τ) x(t −τ)
 dτ
≤σ∞∥h∥1 ,
t ∈R,
(5.8c)
where the ﬁrst inequality follows from Proposition 2.4.1, and where the second
inequality follows from (5.8b).
For this case too one can show that the result of the convolution is not only
bounded but also uniformly continuous.
(iii) Using H¨older’s Inequality (Theorem 3.3.2), we can generalize the above two
cases to show that whenever x and h satisfy the assumptions of H¨older’s
Inequality, their convolution is deﬁned at every epoch t ∈R and is, in fact,
a bounded uniformly continuous function.
See, for example, (Adams and
Fournier, 2003, Paragraph 2.23).
(iv) Another important case where the convolution is deﬁned at every time instant
will be discussed in Proposition 6.2.5. There it is shown that the convolution
between an integrable function (of time) with the Inverse Fourier Transform
of an integrable function (of frequency) is deﬁned at every time instant and
has a simple representation. This scenario is not as contrived as the reader
might suspect. It arises quite naturally, for example, when discussing the
lowpass ﬁltering of an integrable signal (Section 6.4.2). The impulse response
of an ideal lowpass ﬁlter (LPF) is not integrable, but it can be represented
as the Inverse Fourier Transform of an integrable function; see (6.35).
Regarding theorems that guarantee that the convolution be deﬁned for every t
outside a set of Lebesgue measure zero, we mention two.
Convolution deﬁned for t outside a set of Lebesgue measure zero:
(i) If both x and h are integrable, then one can show (see, for example, (Rudin,
1987, Theorem 8.14), (Katznelson, 2004, Section VI.1), or (Stein and Weiss,
1971, Chapter 1, Section 1, Theorem 1.3)) that, for all t outside a set of
58
Convolutions and Filters
Lebesgue measure zero, the mapping τ 	→x(τ) h(t −τ) is integrable, and
(x ⋆h)(t) is hence deﬁned. Moreover, irrespective of how we deﬁne (x ⋆h)(t)
for t inside the set of Lebesgue measure zero
∥x ⋆h∥1 ≤∥x∥1 ∥h∥1 ,
x, h ∈L1.
(5.9)
What is nice about this case is that the result of the convolution stays in
the same class of integrable functions. This makes it meaningful to discuss
associativity and other important properties of the convolution.
(ii) Another case where the convolution is deﬁned for all t outside a set of
Lebesgue measure zero is when h is integrable and when x is a measur-
able function for which τ 	→|x(τ)|p is integrable for some 1 ≤p < ∞. In this
case we have (see, for example, (Rudin, 1987, Chapter 8 Exercise 4) or (Stein
and Weiss, 1971, Chapter 1, Section 1, Theorem 1.3)) that for all t outside a
set of Lebesgue measure zero the mapping τ 	→x(τ) h(t −τ) is integrable so
for such t the convolution (x ⋆h)(t) is well-deﬁned.3 Moreover, irrespective
of how we deﬁne (x ⋆h)(t) for t inside the set of Lebesgue measure zero
	 ∞
−∞
(x ⋆h)(t)
p dt

1/p
≤∥h∥1
	 ∞
−∞
|x(t)|p dt

1/p
.
(5.10)
This inequality is a special case of Young’s Inequality of Theorem 5.11.1
ahead. It can be written more compactly as
∥x ⋆h∥p ≤∥h∥1 ∥x∥p ,
p ≥1,
(5.11)
where we use the notation that for any measurable function g and p > 0
∥g∥p ≜
	 ∞
−∞
|g(t)|p dt

1/p
.
(5.12)
5.6
Basic Properties of the Convolution
The main properties of the convolution are summarized in the following theorem.
Theorem 5.6.1 (Properties of the Convolution). The convolution is
x ⋆h ≡h ⋆x,
(commutative)

x ⋆g

⋆h ≡x ⋆

g ⋆h

,
(associative)
x ⋆

g + h

≡x ⋆g + x ⋆h,
(distributive)
and linear in each of its arguments
x ⋆

α g + β h

≡α

x ⋆g

+ β

x ⋆h


α g + β h

⋆x ≡α

g ⋆x

+ β

h ⋆x

,
where the above hold for all g, h, x ∈L1, and α, β ∈C.
Some of these properties hold under more general or diﬀerent sets of assumptions,
so the reader should focus here on the properties rather than on the restrictions.
3When p = 1 we recover the previous result, namely (i).
5.7 Filters
59
5.7
Filters
A ﬁlter of impulse response h
is a physical device that when fed the input
waveform x produces the output waveform h⋆x. The impulse response h is assumed
to be a real or complex signal, and it is tacitly assumed that we only feed the device
with inputs x for which the convolution x ⋆h is deﬁned.4
Deﬁnition 5.7.1 (Stable Filter). A ﬁlter is said to be stable if its impulse response
is integrable.
Stable ﬁlters are also called bounded-input/bounded-output stable or BIBO
stable, because, as the next proposition shows, if such ﬁlters are fed a bounded
signal, then their output is also a bounded signal.
Proposition 5.7.2 (BIBO Stability). If h is integrable and if x is a bounded
Lebesgue measurable signal, then the signal x ⋆h is also bounded.
Proof. If the impulse response h is integrable, and if the input x is bounded by
some constant σ∞, then (5.8a) and (5.8b) are both satisﬁed, and the boundedness
of the output then follows from (5.8c).
Deﬁnition 5.7.3 (Causal Filter). A ﬁlter of impulse response h is said to be causal
or nonanticipative if h is zero at negative times, i.e., if
h(t) = 0,
t < 0.
(5.13)
Causal ﬁlters play an important role in engineering because (5.13) guarantees that
the present ﬁlter output be computable from the past ﬁlter inputs. Indeed, the
time-t ﬁlter output can be expressed in the form
(x ⋆h)(t) =
 ∞
−∞
x(τ) h(t −τ) dτ
=
 t
−∞
x(τ) h(t −τ) dτ,
h causal,
where the calculation of the latter integral only requires knowledge of x(τ) for
τ < t. Here the ﬁrst equality follows from the deﬁnition of the convolution (5.2),
and the second equality follows from (5.13).
5.8
The Matched Filter
In Digital Communications inner products are often computed using a matched
ﬁlter. In its deﬁnition we shall use the notation (5.1).
4This deﬁnition of a ﬁlter is reminiscent of the concept of a “linear time invariant system.”
Note, however, that since we do not deal with Dirac’s Delta in this book, our deﬁnition is more
restrictive. For example, a device that produces at its output a waveform that is identical to its
input is excluded from our discussion here because we do not allow h to be Dirac’s Delta.
60
Convolutions and Filters
Deﬁnition 5.8.1 (The Matched Filter). The matched ﬁlter for the signal φ is
a ﬁlter whose impulse response is ~φ∗, i.e., the mapping
t 	→φ∗(−t).
(5.14)
The main use of the matched ﬁlter is for computing inner products:
Theorem 5.8.2 (Computing Inner Products with a Matched Filter). The inner
product ⟨u, φ⟩between the energy-limited signals u and φ is given by the output at
time t = 0 of a matched ﬁlter for φ that is fed u:
⟨u, φ⟩=

u ⋆~φ∗
(0),
u, φ ∈L2.
(5.15)
More generally, if g: t 	→φ(t−t0), then ⟨u, g⟩is the time-t0 output of the matched
ﬁlter for φ that is fed u:
 ∞
−∞
u(t) φ∗(t −t0) dt =

u ⋆~φ∗
(t0).
(5.16)
Proof. We shall prove the second part of the theorem, i.e., (5.16): the ﬁrst follows
from the second by setting t0 = 0. We express the time-t0 output of the matched
ﬁlter as

u ⋆~φ∗
(t0) =
 ∞
−∞
u(τ) ~φ∗(t0 −τ) dτ
=
 ∞
−∞
u(τ) φ∗(τ −t0) dτ,
where the ﬁrst equality follows from the deﬁnition of convolution (5.2) and the
second from the deﬁnition of ~φ∗as the conjugated mirror image of φ.
From the above theorem we see that if we wish to compute, say, the three inner
products ⟨u, g1⟩, ⟨u, g2⟩, and ⟨u, g3⟩in the very special case where the functions
g1, g2, g3 are all time shifts of the same waveform φ, i.e., when g1 : t 	→φ(t −t1),
g2 : t 	→φ(t −t2), and g3 : t 	→φ(t −t3), then we need only one ﬁlter, namely, the
matched ﬁlter for φ. Indeed, we can feed u to the matched ﬁlter for φ and the
inner products ⟨u, g1⟩, ⟨u, g2⟩, and ⟨u, g3⟩simply correspond to the ﬁlter’s outputs
at times t1, t2, and t3. One circuit computes all three inner products. This is so
exciting that it is worth repeating:
Corollary 5.8.3 (Computing Many Inner Products Using One Filter). If the
energy-limited signals {gj}J
j=1 are all time shifts of the same signal φ in the sense
that
gj : t 	→φ(t −tj),
j = 1, . . . , J,
and if u is any energy-limited signal, then all J inner products
⟨u, gj⟩,
j = 1, . . . , J
5.9 The Ideal Unit-Gain Lowpass Filter
61
can be computed using one ﬁlter by feeding u to a matched ﬁlter for φ and sampling
the output at the appropriate times t1, . . . , tJ:
⟨u, gj⟩=

u ⋆~φ∗
(tj),
j = 1, . . . , J.
(5.17)
5.9
The Ideal Unit-Gain Lowpass Filter
The impulse response of the ideal unit-gain lowpass ﬁlter of cutoﬀfrequency Wc
is denoted by LPFWc(·) and is given for every Wc > 0 by5
LPFWc(t) ≜

2Wc
sin(2πWct)
2πWct
if t ̸= 0,
2Wc
if t = 0,
t ∈R.
(5.18)
This can be alternatively written as
LPFWc(t) = 2Wc sinc(2Wct),
t ∈R,
(5.19)
where the function sinc(·) is deﬁned by6
sinc(ξ) ≜
 sin(πξ)
πξ
if ξ ̸= 0,
1
if ξ = 0,
ξ ∈R.
(5.20)
Notice that the deﬁnition of sinc(0) as being 1 makes sense because, for very small
(but nonzero) values of ξ the value of sin(ξ)/ξ is approximately 1. In fact, with
this deﬁnition at zero the function is not only continuous at zero but also inﬁnitely
diﬀerentiable there. Indeed, the function from C to C
z 	→
 sin(πz)
πz
if z ̸= 0,
1
otherwise,
is an entire function, i.e., an analytic function throughout the complex plane.
The importance of the ideal unit-gain lowpass ﬁlter will become clearer when we
discuss the ﬁlter’s frequency response in Section 6.3. It is thus named because
the Fourier Transform of LPFWc(·) is equal to 1 (hence “unit gain”), whenever
|f| ≤Wc, and is equal to zero, whenever |f| > Wc. See (6.38) ahead.
From a mathematical point of view, working with the ideal unit-gain lowpass ﬁlter
is tricky because the impulse response (5.18) is not an integrable function. (It
decays like 1/t, which does not have a ﬁnite integral from t = 1 to t = ∞.) This
ﬁlter is thus not a stable ﬁlter. We shall revisit this issue in Section 6.4. Note,
however, that the impulse response (5.18) is of ﬁnite energy. (The square of the
impulse response decays like 1/t2 which does have a ﬁnite integral from one to
inﬁnity.) Consequently, the result of feeding an energy-limited signal to the ideal
unit-gain lowpass ﬁlter is always well-deﬁned.
Note also that the ideal unit-gain lowpass ﬁlter is not causal.
5For convenience we deﬁne the impulse response of the ideal unit-gain lowpass ﬁlter of cutoﬀ
frequency zero as the all zero signal. This is in agreement with (5.19).
6Some texts omit the π’s in (5.20) and deﬁne the sinc(·) function as sin(ξ)/ξ for ξ ̸= 0.
62
Convolutions and Filters
5.10
The Ideal Unit-Gain Bandpass Filter
The ideal unit-gain bandpass ﬁlter (BPF) of bandwidth W around the carrier
frequency fc, where fc > W/2 > 0 is a ﬁlter of impulse response BPFW,fc(·),
where
BPFW,fc(t) ≜2W cos(2πfct) sinc(Wt),
t ∈R.
(5.21)
This ﬁlter too is nonstable and noncausal. It derives its name from its frequency
response (discussed in Section 6.3 ahead), which is equal to one at frequencies f
satisfying
|f| −fc
 ≤W/2 and which is equal to zero at all other frequencies.
5.11
Young’s Inequality
Many of the inequalities regarding convolutions are special cases of a result known
as Young’s Inequality. Recalling (5.12), we can state Young’s Inequality as follows.
Theorem 5.11.1 (Young’s Inequality). Let x and h be Lebesgue measurable func-
tions such that ∥x∥p , ∥h∥q < ∞for some 1 ≤p, q < ∞satisfying 1/p + 1/q > 1.
Deﬁne r through 1/p + 1/q = 1 + 1/r. Then the convolution integral (5.2) is de-
ﬁned for all t outside a set of Lebesgue measure zero; it is a Lebesgue measurable
function; and
∥x ⋆h∥r ≤K ∥x∥p ∥h∥q ,
(5.22)
where K < 1 is some constant that depends only on p and q.
Proof. See (Adams and Fournier, 2003, Corollary 2.25). Alternatively, see (Stein
and Weiss, 1971, Chapter 5, Section 1) where it is derived from the M. Riesz
Convexity Theorem.
5.12
Additional Reading
For some of the properties of the convolution and its use in the analysis of linear
systems see (Oppenheim and Willsky, 1997) and (Kwakernaak and Sivan, 1991).
5.13
Exercises
Exercise 5.1 (Convolving Brickwall Functions). For a given a > 0, compute the convolu-
tion of the signal t →I{|t| ≤a} with itself.
Exercise 5.2 (Convolution of Delayed Signals). Let x and h be energy-limited signals.
Let xd : t →x(t −td) be the result of delaying x by some td ∈R. Show that

xd ⋆h

(t) =

x ⋆h

(t −td),
t ∈R.
Exercise 5.3 (The Convolution of Reﬂections). Let the signals x, y be such that their
convolution (x ⋆y)(t) is deﬁned at every t ∈R.
Show that the convolution of their
reﬂections is also deﬁned at every t ∈R and that it is equal to the reﬂection of their
convolution:
~x ⋆~y

(t) =

x ⋆y

(−t),
t ∈R.
5.13 Exercises
63
Exercise 5.4 (The Convolution and Inner Products). Let y and φ be energy-limited
complex signals, and let h be an integrable complex signal. Argue that

y, h ⋆φ

=

y ⋆~h∗, φ

.
Exercise 5.5 (The Real and Imaginary Parts of a Convolution). Let the signals x and h
be energy-limited, and assume that h is real. Prove that
Re

x ⋆h

= Re(x) ⋆h and Im

x ⋆h

= Im(x) ⋆h,
h is real-valued.
Is the assumption that h is real essential?
Exercise 5.6 (The Convolution in Probability Theory).
(i) Let X and Y be independent random variables of probability density functions
fX(·) and fY (·).
Show that if Z = X + Y then the density fZ(·) of Z is the
convolution of fX(·) and fY (·).
(ii) Convolve the mean-2 exponential density x →1/2 e−x/2 I{x ≥0} with itself to
obtain the density of the central χ2 distribution with four degrees of freedom. (See
Section 19.8.1 for more on the central χ2 distribution.)
Exercise 5.7 (The Convolution’s Derivative). Let the signal g: R →C be bounded,
diﬀerentiable, and with a bounded derivative g′. Let h: R →C be integrable. Show that
g ⋆h is diﬀerentiable and that its derivative (g ⋆h)′ is g′ ⋆h.
Exercise 5.8 (Continuity of the Convolution). Show that if the signals x and y are both
in L2 then their convolution is a continuous function.
Hint: Use the Cauchy-Schwarz Inequality and the fact that if x ∈L2 and if we deﬁne
xδ : t →x(t −δ), then lim
δ→0 ∥x −xδ∥2 = 0.
Exercise 5.9 (More on the Continuity of the Convolution). Let x and y be in L2. Let the
sequence of energy-limited signals x1, x2, . . . converge to x in the sense that ∥x −xn∥2
tends to zero as n tends to inﬁnity. Show that at every epoch t ∈R,
lim
n→∞

xn ⋆y

(t) =

x ⋆y

(t).
Hint: Use the Cauchy-Schwarz Inequality
Exercise 5.10 (Convolving Bi-Inﬁnite Sequences). The convolution of the bi-inﬁnite se-
quence . . . , a−1, a0, a1 . . . with the bi-inﬁnite sequence . . . , b−1, b0, b1 . . . is the bi-inﬁnite
sequence . . . , c−1, c0, c1 . . . formally deﬁned by
cm =
∞

ν=−∞
aνbm−ν,
m ∈Z.
(5.23)
Show that if
∞

ν=−∞
|aν| ,
∞

ν=−∞
|bν| < ∞,
then the sum on the RHS of (5.23) converges for every integer m, and
∞

m=−∞
|cm| ≤

∞

ν=−∞
|aν|

∞

ν=−∞
|bν|

.
Hint: Recall Problems 3.14 & 3.12 and the Triangle Inequality for Complex Numbers.
64
Convolutions and Filters
Exercise 5.11 (Periodic Signals through Stable Filters). Prove that the output of a stable
ﬁlter that is fed a bounded periodic signal is bounded and periodic.
Exercise 5.12 (Stability of the Matched Filter). Let g be an energy-limited signal. Under
what conditions is the matched ﬁlter for g stable?
Exercise 5.13 (Causality of the Matched Filter). Let g be an energy-limited signal.
(i) Under what conditions is the matched ﬁlter for g causal?
(ii) Under what conditions can you ﬁnd a causal ﬁlter of impulse response h and a
sampling time t0 such that

r ⋆h

(t0) = ⟨r, g⟩,
r ∈L2?
(iii) Show that for every δ > 0 we can ﬁnd a stable causal ﬁlter of impulse response h
and a sampling epoch t0 such that for every r ∈L2

r ⋆h

(t0) −⟨r, g⟩
 ≤δ ∥r∥2 .
Exercise 5.14 (The Output of the Matched Filter). Compute and plot the output of the
matched ﬁlter for the signal t →e−t I{t ≥0} when it is fed the input t →I{|t| ≤1/2}.
Chapter 6
The Frequency Response of Filters and
Bandlimited Signals
6.1
Introduction
We begin this chapter with a review of the Fourier Transform and its key properties.
We then use these properties to deﬁne the frequency response of ﬁlters, to discuss
the ideal unit-gain lowpass ﬁlter, and to deﬁne bandlimited signals.
6.2
Review of the Fourier Transform
6.2.1
On Hats, 2π’s, ω’s, and f’s
We denote the Fourier Transform (FT) of a (possibly complex) signal x(·) by
ˆx(·). Some other books denote it by X(·), but we prefer our notation because,
where possible, we use lowercase letters for deterministic quantities and reserve
uppercase letters for random quantities. In places where convention forces us to
use uppercase letters for deterministic quantities, we try to use a special font, e.g.,
P for power, W for bandwidth, or A for a deterministic matrix.
More importantly, our deﬁnition of the Fourier Transform may be diﬀerent from
the one you are used to.
Deﬁnition 6.2.1 (Fourier Transform). The Fourier Transform (or the L1-
Fourier Transform) of an integrable signal x: R →C is the mapping ˆx: R →C
deﬁned by
ˆx: f 	→
 ∞
−∞
x(t) e−i2πft dt.
(6.1)
(The FT can also be deﬁned in more general settings. For example, in Section 6.2.3
it will be deﬁned via a limiting argument for ﬁnite-energy signals that are not
integrable.)
This deﬁnition should be contrasted with the deﬁnition
X(iω) =
 ∞
−∞
x(t) e−iωt dt,
(6.2)
65
66
The Frequency Response of Filters and Bandlimited Signals
which you may have seen before. Note the 2π, which appears in the exponent in
our deﬁnition (6.1) and not in (6.2). We apologize to readers who are used to (6.2)
for forcing a new deﬁnition, but we have some good reasons:
(i) With our deﬁnition, the transform and its inverse are very similar; see (6.1)
and (6.4) below. If one uses the deﬁnition of (6.2), then the expression for
the Inverse Fourier Transform requires scaling the integral by 1/(2π).
(ii) With our deﬁnition, the Fourier Transform and the Inverse Fourier Transform
of a symmetric function are the same; see (6.6).
This makes it easier to
remember some Fourier pairs.
(iii) As we shall state more precisely in Section 6.2.2 and Section 6.2.3, with our
deﬁnition the Fourier Transform possesses an extremely important property:
it preserves inner products
⟨u, v⟩= ⟨ˆu, ˆv⟩
(certain restrictions apply).
Again, no 2π’s.
(iv) If x(·) models a function of time, then ˆx(·) becomes a function of frequency.
Thus, it is natural to use the generic argument t for such signals x(·) and the
generic argument f for their transforms. It is more common these days to
describe tones in terms of their frequencies (i.e., in Hz) and not in terms of
their radial frequency (in radians per second).
(v) It seems that all books on communications use our deﬁnition, perhaps because
people are used to setting their radios in Hz, kHz, or MHz.
Plotting the FT of a signal is tricky, because it is a complex-valued function. This
is generally true even for real signals.
However, for any integrable real signal
x: R →R the Fourier Transform ˆx(·) is conjugate-symmetric, i.e.,

ˆx(−f) = ˆx∗(f),
f ∈R

,
x ∈L1 is real-valued.
(6.3)
Equivalently, the magnitude of the FT of an integrable real signal is symmetric, and
the argument is anti-symmetric.1 (The reverse statement is “essentially” correct.
If ˆx is conjugate-symmetric then the set of epochs t for which x(t) is not real is
of Lebesgue measure zero.) Consequently, when plotting the FT of a “generic”
real signal we shall plot a symmetric function, but with solid lines for the positive
frequencies and dashed lines for the negative frequencies. This is to remind the
reader that the FT of a real signal is not symmetric but conjugate-symmetric. See,
for example, Figures 7.1 and 7.2 for plots of the Fourier Transforms of real signals.
When plotting the FT of a complex-valued signal, we shall use a generic plot that
is “highly asymmetric,” using solid lines. See, for example, Figure 7.4 for the FT
of a complex signal.
1The argument of a nonzero complex number z is deﬁned as the element θ of [−π, π) such
that z = |z| eiθ.
6.2 Review of the Fourier Transform
67
Deﬁnition 6.2.2 (Inverse Fourier Transform). The Inverse Fourier Transform
(IFT) of an integrable function g: R →C is denoted by ˇg and is deﬁned by
ˇg: t 	→
 ∞
−∞
g(f) ei2πft df.
(6.4)
We emphasize that the word “inverse” here is just part of the name of the transform.
Applying the IFT to the FT of a signal does not always recover the signal.2 (Condi-
tions under which the IFT does recover the signal are explored in Theorem 6.2.13.)
However, if one does not insist on using the IFT, then every integrable signal can
be reconstructed to within indistinguishability from its FT; see Theorem 6.2.12.
Proposition 6.2.3 (Some Properties of the Inverse Fourier Transform).
(i) If g is integrable, then its IFT is the FT of its mirror image
ˇg = ˆ~g,
g ∈L1.
(6.5)
(ii) If g is integrable and also symmetric in the sense that ~g = g, then the IFT
of g is equal to its FT
ˆg = ˇg,

g ∈L1 and ~g = g

.
(6.6)
(iii) If g is integrable and ˇg is also integrable, then
ˆˇg = ˇˆg.
(6.7)
Proof. Part (i) follows by a simple change of integration variable:
ˇg(ξ) =
 ∞
−∞
g(α) ei2παξ dα = −
 −∞
∞
g(−β) e−i2πβξ dβ
=
 ∞
−∞
~g(β) e−i2πβξ dβ
= ˆ~g(ξ),
ξ ∈R,
where we have changed the integration variable to β ≜−α.
Part (ii) is a special case of Part (i). To prove Part (iii) we compute
ˆˇg(ξ) =
 ∞
−∞
	 ∞
−∞
g(f) ei2πft df

e−i2πξt dt
=
 ∞
−∞
ˆg(−t) e−i2πξt dt
=
 ∞
−∞
ˆg(τ) ei2πξτ dτ
= ˇˆg(ξ),
ξ ∈R,
2This can be seen by considering the signal t →I{t = 17}, which is zero everywhere except
at 17 where it takes on the value 1. Its FT is zero at all frequencies, but if one applies the IFT to
the all-zero function one obtains the all-zero function, which is not the function we started with.
Things could be much worse. The FT of some integrable signals (such as the signal t →I{|t| ≤1})
is not integrable, so the IFT of their FT is not even deﬁned.
68
The Frequency Response of Filters and Bandlimited Signals
where we have changed the integration variable to τ ≜−t.
Identity (6.6) will be useful in Section 6.2.5 when we memorize the FT of the
Brickwall function ξ 	→β I{|ξ| ≤γ}, which is symmetric. Once we succeed we will
also know its IFT.
Table 6.1 summarizes some of the properties of the FT. Note that some of these
properties require additional technical assumptions.
Property
Function
Fourier Transform
linearity
α x + β y
α ˆx + β ˆy
time shifting
t 	→x(t −t0)
f 	→e−i2πft0 ˆx(f)
frequency shifting
t 	→ei2πf0t x(t)
f 	→ˆx(f −f0)
conjugation
t 	→x∗(t)
f 	→ˆx∗(−f)
stretching (α ∈R, α ̸= 0)
t 	→x(αt)
f 	→
1
|α| ˆx( f
α)
convolution in time
x ⋆y
f 	→ˆx(f) ˆy(f)
multiplication in time
t 	→x(t) y(t)
ˆx ⋆ˆy
real part
t 	→Re

x(t)

f 	→1
2 ˆx(f) + 1
2 ˆx∗(−f)
time reﬂection
~x
ˇx
transforming twice
ˆx
~x
FT of IFT
ˇx
x
Table 6.1: Basic properties of the Fourier Transform. Some restrictions apply!
6.2.2
Parseval-like Theorems
A key result on the Fourier Transform is that, subject to some restrictions, it pre-
serves inner products. Thus, if ˆx1 and ˆx2 are the Fourier Transforms of x1 and x2,
then the inner product ⟨x1, x2⟩between x1 and x2 is typically equal to the inner
product ⟨ˆx1, ˆx2⟩between their transforms. In this section we shall describe two
scenarios where this holds. A third scenario, which is described in Theorem 6.2.9,
will have to wait until we discuss the FT of signals that are energy-limited but not
integrable.
To see how the next proposition is related to the preservation of the inner product
under the Fourier Transform, think about g as being a function of frequency and
of its IFT ˇg as a function of time.
Proposition 6.2.4. If g: f 	→g(f) and x: t 	→x(t) are integrable mappings from R
to C, then
 ∞
−∞
x(t) ˇg∗(t) dt =
 ∞
−∞
ˆx(f) g∗(f) df,
(6.8)
i.e.,
⟨x, ˇg⟩= ⟨ˆx, g⟩,
g, x ∈L1.
(6.9)
6.2 Review of the Fourier Transform
69
Proof. The key to the proof is to use Fubini’s Theorem to justify changing the
order of integration in the following calculation:
 ∞
−∞
x(t) ˇg∗(t) dt =
 ∞
−∞
x(t)
	 ∞
−∞
g(f) ei2πft df

∗
dt
=
 ∞
−∞
x(t)
 ∞
−∞
g∗(f) e−i2πft df dt
=
 ∞
−∞
g∗(f)
 ∞
−∞
x(t) e−i2πft dt df
=
 ∞
−∞
g∗(f) ˆx(f) df,
where the ﬁrst equality follows from the deﬁnition of ˇg; the second because the
conjugation of an integral is accomplished by conjugating the integrand (Proposi-
tion 2.3.1); the third by changing the order of integration; and the ﬁnal equality
by the deﬁnition of the FT of x.
A related result is that the convolution of an integrable function with the IFT of
an integrable function is always deﬁned:
Proposition 6.2.5. If the mappings x: t 	→x(t) and g: f 	→g(f) from R to C are
both integrable, then the convolution x ⋆ˇg is deﬁned at every epoch t ∈R and

x ⋆ˇg

(t) =
 ∞
−∞
g(f) ˆx(f) ei2πft df,
t ∈R.
(6.10)
Proof. Here too the key is in changing the order of integration:

x ⋆ˇg

(t) =
 ∞
−∞
x(τ) ˇg(t −τ) dτ
=
 ∞
−∞
x(τ)
 ∞
−∞
ei2πf(t−τ)g(f) df dτ
=
 ∞
−∞
g(f) ei2πft
 ∞
−∞
x(τ) e−i2πfτ dτ df
=
 ∞
−∞
g(f) ˆx(f) ei2πft df,
where the ﬁrst equality follows from the deﬁnition of the convolution; the second
from the deﬁnition of the IFT; the third by changing the order of integration; and
the ﬁnal equality by the deﬁnition of the FT. The justiﬁcation of the changing of the
order of integration can be argued using Fubini’s Theorem because, by assumption,
both g and x are integrable.
We next present another useful version of the preservation of inner products under
the FT. It is useful for functions (of time) that are zero outside some interval
[−T, T ] or for the IFT of functions (of frequency) that are zero outside an interval
[−W, W ].
70
The Frequency Response of Filters and Bandlimited Signals
Proposition 6.2.6 (A Mini Parseval Theorem).
(i) Let the signals x1 and x2 be given by
xν(t) =
 ∞
−∞
gν(f) ei2πft df,

t ∈R, ν = 1, 2

,
(6.11a)
where the functions gν : f 	→gν(f) satisfy
gν(f) = 0,

|f| > W, ν = 1, 2

,
(6.11b)
for some W ≥0, and
 ∞
−∞
|gν(f)|2 df < ∞,
ν = 1, 2.
(6.11c)
Then
⟨x1, x2⟩= ⟨g1, g2⟩.
(6.11d)
(ii) Let g1 and g2 be given by
gν(f) =
 ∞
−∞
xν(t) e−i2πft dt,

f ∈R, ν = 1, 2

,
(6.12a)
where the signals x1, x2 ∈L2 are such that for some T ≥0
xν(t) = 0,

|t| > T, ν = 1, 2

.
(6.12b)
Then
⟨x1, x2⟩= ⟨g1, g2⟩.
(6.12c)
Proof. See the proof of Lemma A.3.6 on Page 837 and its corollary in the appendix.
6.2.3
The L2-Fourier Transform
To appreciate some of the mathematical subtleties of this section, the reader is
encouraged to review Section 4.7 in order to recall the diﬀerence between the
space L2 and the space L2 and in order to recall the diﬀerence between an energy-
limited signal x ∈L2 and the equivalence class [x] ∈L2 to which it belongs. In this
section we shall sketch how the Fourier Transform is deﬁned for elements of L2.
This section can be skipped provided that you are willing to take on faith that
such a transform exists and that, very roughly speaking, it has some of the same
properties of the Fourier Transform of Deﬁnition 6.2.1. To diﬀerentiate between
the transform of Deﬁnition 6.2.1 and the transform that we are about to deﬁne
for elements of L2, we shall refer in this section to the former as the L1-Fourier
Transform and to the latter as the L2-Fourier Transform. Both will be denoted
by a “hat.” In subsequent sections the Fourier Transform will be understood to be
the L1-Fourier Transform unless explicitly otherwise speciﬁed.
6.2 Review of the Fourier Transform
71
Some readers may have already encountered the L2-Fourier Transform without
even being aware of it. For example, the sinc(·) function, which is deﬁned in (5.20),
is an energy-limited signal that is not integrable.
Consequently, its L1-Fourier
Transform is undeﬁned. Nevertheless, you may have seen its Fourier Transform
being given as the Brickwall function. As we shall see, this is somewhat in line
with how the L2-Fourier Transform of the sinc(·) is deﬁned.3
For more on the
Fourier Transform of the sinc(·) see Section 6.2.5. Another example of an energy-
limited signal that is not integrable is t 	→1/(1 + |t|).
We next sketch how the L2-Fourier Transform is deﬁned and explore some of its
key properties. We begin with the bad news.
(i) There is no explicit simple expression for the L2-Fourier Transform.
(ii) The result of applying the transform is not a function but an equivalence
class of functions.
The L2-Fourier Transform is a mapping
ˆ: L2 →L2
that maps elements of L2 to elements of L2. It thus maps equivalence classes
to equivalence classes, not functions.
As long as the operation we perform on
the result of the L2-Fourier Transform does not depend on which member of the
equivalence class it is performed on, there is no need to worry about this issue.
Otherwise, we can end up performing operations that are ill-deﬁned. For example,
an operation that is ill-deﬁned is evaluating the result of the transform at a given
frequency, say at f = 17.
An operation you cannot go wrong with is integration, because the integrals of
two functions that diﬀer on a set of measure zero are equal; see Proposition 2.5.3.
Consequently, inner products, which are deﬁned via integration, are ﬁne too. In
this book we shall therefore refrain from applying to the result of the L2-Fourier
Transform any operation other than integration (or related operations such as the
computation of energy or inner product).
In fact, since we ﬁnd the notion of
equivalence classes somewhat abstract we shall try to minimize its use.
Suppose that x ∈L2 is an energy-limited signal and that [x] ∈L2 is its equivalence
class. How do we deﬁne the L2-Fourier Transform of [x]? We ﬁrst deﬁne for every
positive integer n the time-truncated function
xn : t 	→x(t) I{|t| ≤n}
and note that, by Proposition 3.4.3, xn is integrable. Consequently, its L1-Fourier
Transform ˆxn is well-deﬁned and is given by
ˆxn(f) =
 n
−n
x(t) e−i2πft dt,
f ∈R.
3However, as we shall see, the result of the L2 -Fourier Transform is an element of L2 , i.e., an
equivalence class, and not a function.
72
The Frequency Response of Filters and Bandlimited Signals
We then note that ∥x −xn∥2 tends to zero as n tends to inﬁnity, so for every ϵ > 0
there exists some L(ϵ) suﬃciently large so that
∥xn −xm∥2 < ϵ,
n, m > L(ϵ).
(6.13)
Applying Proposition 6.2.6 (ii) with the substitution of max{n, m} for T and of
xn −xm for both x1 and x2, we obtain that (6.13) implies
∥ˆxn −ˆxm∥2 < ϵ,
n, m > L(ϵ).
(6.14)
Because the space of energy-limited signals is complete in the sense of Theo-
rem 8.6.1 ahead, we may infer from (6.14) that there exists some function ζ ∈L2
such that ∥ˆxn −ζ∥2 converges to zero.4 We then deﬁne the L2-Fourier Transform
of the equivalence class [x] to be the equivalence class [ζ]. In view of Footnote 4
we can deﬁne the L2-Fourier Transform as follows.
Deﬁnition 6.2.7 (L2-Fourier Transform). The L2-Fourier Transform of the
equivalence class [x] ∈L2 is denoted by  
[x] and is given by
 
[x] ≜

g ∈L2 : lim
n→∞
 ∞
−∞
g(f) −
 n
−n
x(t) e−i2πft dt

2
df = 0
!
.
The main properties of the L2-Fourier Transform are summarized in the following
theorem.
Theorem 6.2.8 (Properties of the L2-Fourier Transform). The L2-Fourier Trans-
form is a mapping from L2 onto L2 with the following properties:
(i) If x ∈L2 ∩L1, then the L2-Fourier Transform of [x] is the equivalence class
of the mapping
f 	→
 ∞
−∞
x(t) e−i2πft dt.
(ii) The L2-Fourier Transform is linear in the sense that

α[x1] + β[x2] = α "
[x1] + β "
[x2],

x1, x2 ∈L2,
α, β ∈C

.
(iii) The L2-Fourier Transform is invertible in the sense that to each [g] ∈L2
there corresponds a unique equivalence class in L2 whose L2-Fourier Trans-
form is [g]. This equivalence class can be obtained by reﬂecting each of the el-
ements of [g] to obtain the equivalence class [~g] of ~g, and by then applying the
L2-Fourier Transform to it. The result  
[~g]—which is called the L2-Inverse
Fourier Transform of [g]—then satisﬁes
 
 
~g

= [g],
g ∈L2.
(6.15)
4The function ζ is not unique. If ∥xn −ζ∥2 →0, then also
xn −˜ζ

2 →0 whenever ˜ζ ∈[ζ].
And conversely, if ∥xn −ζ∥2 →0 and
xn −˜ζ

2 →0, then ˜ζ must be in [ζ].
6.2 Review of the Fourier Transform
73
(iv) Applying the L2-Fourier Transform twice is equivalent to reﬂecting the ele-
ments of the equivalence class
 
 
[x] = [~x],
x ∈L2.
(6.16)
(v) The L2-Fourier Transform preserves energies:5
 
[x]

2 =
[x]

2,
x ∈L2.
(6.17)
(vi) The L2-Fourier Transform preserves inner products:6

[x], [y]

=
# 
[x],  
[y]
$
,
x, y ∈L2.
(6.18)
Proof. This theorem is a restatement of (Rudin, 1987, Chapter 9, Theorem 9.13).
Identity (6.16) appears in this form in (Stein and Weiss, 1971, Chapter 1, Section 2,
Theorem 2.4).
The result that the L2-Fourier Transform preserves energies is sometimes called
Plancherel’s Theorem and the result that it preserves inner products Parseval’s
Theorem. We shall use “Parseval’s Theorem” for both. It is so important that
we repeat it here in the form of a theorem. Following mathematical practice, we
drop the square brackets in the theorem’s statement.
Theorem 6.2.9 (Parseval’s Theorem). For any x, y ∈L2
⟨x, y⟩= ⟨ˆx, ˆy⟩
(6.19)
and
∥x∥2 = ∥ˆx∥2 .
(6.20)
As we mentioned earlier, there is no simple explicit expression for the L2-Fourier
Transform. The following proposition simpliﬁes its calculation under certain as-
sumptions that are, for example, satisﬁed by the sinc(·) function.
Proposition 6.2.10. If x = ˇg for some g ∈L1 ∩L2, then:
(i) x ∈L2.
(ii) ∥x∥2 = ∥g∥2.
(iii) The L2-Fourier Transform of [x] is the equivalence class [g].
5The energy of an equivalence class was deﬁned in Section 4.7.
6The inner product between equivalence classes was deﬁned in Section 4.7.
74
The Frequency Response of Filters and Bandlimited Signals
Proof. It suﬃces to prove Part (iii) because Parts (i) and (ii) will then follow from
the preservation of energy under the L2-Fourier Transform (Theorem 6.2.8 (v)).
To prove Part (iii) we compute
[g] =  
 
~g

= "
%
ˆ~g
&
=  
[x],
where the ﬁrst equality follows from (6.15); the second from Theorem 6.2.8 (i)
(because the hypothesis g ∈L1 ∩L2 implies that ~g ∈L1 ∩L2); and the ﬁnal
equality from Proposition 6.2.3 (i) and from the hypothesis that x = ˇg.
6.2.4
More on the Fourier Transform
In this section we present additional results that shed some light on the problem of
reconstructing a signal from its FT. The ﬁrst is a continuity result, which may seem
technical but which has some useful consequences. It can be used to show that the
IFT (of an integrable function) always yields a continuous signal. Consequently,
if one starts with a discontinuous function, takes its FT, and then the IFT, one
does not obtain the original function. It can also be used—once we deﬁne the
frequency response of a ﬁlter in Section 6.3—to show that no stable ﬁlter can have
a discontinuous frequency response.
Theorem 6.2.11 (Continuity and Boundedness of the Fourier Transform).
(i) If x is integrable, then its FT ˆx is a uniformly continuous function satisfying
ˆx(f)
 ≤
 ∞
−∞
|x(t)| dt,
f ∈R,
(6.21)
and
lim
|f|→∞ˆx(f) = 0.
(6.22)
(ii) If g is integrable, then its IFT ˇg is a uniformly continuous function satisfying
ˇg(t)
 ≤
 ∞
−∞
|g(f)| df,
t ∈R.
(6.23)
Proof. We begin with Part (i). Inequality (6.21) follows directly from the deﬁnition
of the FT and from Proposition 2.4.1. The proof of the uniform continuity of ˆx is
not very diﬃcult but is omitted. See (Katznelson, 2004, Section VI.1, Theorem 1.2).
A proof of (6.22) can be found in (Katznelson, 2004, Section VI.1, Theorem 1.7).
Part (ii) follows by substituting ~g for x in Part (i) because the IFT of g is the FT
of its mirror image (6.5).
6.2 Review of the Fourier Transform
75
The second result we present is that every integrable signal can be reconstructed
from its FT, but not necessarily via the IFT. The reconstruction formula in (6.25)
ahead works even when the IFT does not do the job.
Theorem 6.2.12 (Reconstructing a Signal from Its Fourier Transform).
(i) If two integrable signals have the same FT, then they are indistinguishable:

ˆx1(f) = ˆx2(f),
f ∈R

=⇒

x1 ≡x2

,
x1, x2 ∈L1.
(6.24)
(ii) Every integrable function x can be reconstructed from its FT in the sense that
lim
λ→∞
 ∞
−∞
x(t) −
 λ
−λ

1 −|f|
λ

ˆx(f) ei2πft df
 dt = 0.
(6.25)
Proof. See (Katznelson, 2004, Chapter VI, Section 1.10).
Conditions under which the IFT of the FT of a signal recovers the signal are given
in the following theorem.
Theorem 6.2.13 (The Inversion Theorem).
(i) Suppose that x is integrable and that its FT ˆx is also integrable. Deﬁne
˜x = ˇˆx.
(6.26)
Then ˜x is a continuous function with
lim
|t|→∞˜x(t) = 0,
(6.27)
and the functions x and ˜x agree except on a set of Lebesgue measure zero.
(ii) Suppose that g is integrable and that its IFT ˇg is also integrable. Deﬁne
˜g = ˆˇg.
(6.28)
Then ˜g is a continuous function with
lim
|f|→∞˜g(f) = 0
(6.29)
and the functions g and ˜g agree except on a set of Lebesgue measure zero.
Proof. For a proof of Part (i) see (Rudin, 1987, Theorem 9.11). Part (ii) follows
by substituting g for x in Part (i) and using Proposition 6.2.3 (iii).
Corollary 6.2.14.
(i) If x is a continuous integrable signal whose FT is integrable, then
ˇˆx = x.
(6.30)
76
The Frequency Response of Filters and Bandlimited Signals
(ii) If g is continuous and integrable, and if ˇg is also integrable, then
ˆˇg = g.
(6.31)
Proof. Part (i) follows from Theorem 6.2.13 (i) by noting that if two continuous
functions are equal outside a set of Lebesgue measure zero, then they are identical.
Part (ii) follows similarly from Theorem 6.2.13 (ii).
6.2.5
On the Brickwall and the sinc(·) Functions
We next discuss the FT and the IFT of the Brickwall function
ξ 	→I{|ξ| ≤1},
(6.32)
which derives its name from the shape of its plot. Since it is a symmetric function,
it follows from (6.6) that its FT and IFT are identical. Both are equal to a properly
stretched and scaled sinc(·) function (5.20).
More generally, we oﬀer the reader advice on how to remember that for α, γ > 0,
t 	→δ sinc(αt) is the IFT of f 	→β I{|f| ≤γ}
(6.33)
if, and only if,
δ = 2γβ
(6.34a)
and
γ 1
α = 1
2.
(6.34b)
Condition (6.34a) is easily remembered because its LHS is the value at t = 0 of
δ sinc(αt) and its RHS is the value at t = 0 of the IFT of f 	→β I{|f| ≤γ}:
 ∞
−∞
β I{|f| ≤γ} ei2πft df

t=0
=
 ∞
−∞
β I{|f| ≤γ} df = 2γβ.
Condition (6.34b) is intimately related to the Sampling Theorem that you may
have already seen and that we shall discuss in Chapter 8. Indeed, in the Sam-
pling Theorem (Theorem 8.4.3) the time between consecutive samples T and the
bandwidth W satisfy
T W = 1
2.
(In this application α corresponds to 1/T and γ corresponds to the bandwidth W.)
It is tempting to say that Conditions (6.34) also imply that the FT of the func-
tion t 	→δ sinc(αt) is the function f 	→β I{|f| ≤γ}, but there is a caveat. The
signal t 	→δ sinc(αt) is not integrable. Consequently, its L1-Fourier Transform
(Deﬁnition 6.2.1) is undeﬁned. However, since it is energy-limited, its L2-Fourier
Transform is deﬁned (Deﬁnition 6.2.7). Using Proposition 6.2.10 with the substitu-
tion of f 	→β I{|f| ≤γ} for g, we obtain that, indeed, Conditions (6.34) imply that
the L2-Fourier Transform of the (equivalence class of the) function t 	→δ sinc(αt)
is the (equivalence class of the) function f 	→β I{|f| ≤γ}.
6.3 The Frequency Response of a Filter
77
δ
ﬁrst zero at
1
α
β
cutoﬀγ
Figure 6.1: The stretched & scaled sinc(·) function and the stretched & scaled
Brickwall function above are an L2-Fourier pair if the value of the former at zero
(i.e., δ) is the integral of the latter (i.e., 2 × β × cutoﬀ) and if the product of the
location of the ﬁrst zero of the former by the cutoﬀof the latter is 1/2.
The relation between the sinc(·) and the Brickwall functions is summarized in
Figure 6.1.
The derivation of the result is straightforward: the IFT of the Brickwall function
can be computed for every t ̸= 0 as
 ∞
−∞
β I{|f| ≤γ} ei2πft df = β
 γ
−γ
ei2πft df
=
β
i2πt ei2πft
γ
−γ
=
β
i2πt

ei2πγt −e−i2πγt
= β
πt sin(2πγt)
= 2βγ sinc(2γt).
(6.35)
And since sinc(0) is deﬁned as 1, (6.35) also holds (by inspection) when t is zero.
6.3
The Frequency Response of a Filter
Recall that in Section 5.7 we deﬁned a ﬁlter of impulse response h to be a physical
device that when fed the input x produces the output x ⋆h. Of course, this is only
meaningful if the convolution is deﬁned. Subject to some technical assumptions
that are made precise in Theorem 6.3.2, the FT of the output waveform x⋆h is the
product of the FT of the input waveform x by the FT of the impulse response h.
78
The Frequency Response of Filters and Bandlimited Signals
Consequently, we can think of a ﬁlter of impulse response h as a physical device
that produces an output signal whose FT is the product of the FT of the input
signal and the FT of the impulse response.
The FT of the impulse response is called the frequency response of the ﬁlter. If
the ﬁlter is stable and its impulse response therefore integrable, then we deﬁne the
ﬁlter’s frequency response as the Fourier Transform of the impulse response using
Deﬁnition 6.2.1 (the L1-Fourier Transform). If the impulse response is energy-
limited but not integrable, then we deﬁne the frequency response as the Fourier
Transform of the impulse response using the deﬁnition of the Fourier Transform for
energy-limited signals that are not integrable as in Section 6.2.3 (the L2-Fourier
Transform).
Deﬁnition 6.3.1 (Frequency Response).
(i) The frequency response of a stable ﬁlter is the Fourier Transform of its
impulse response as deﬁned in Deﬁnition 6.2.1.
(ii) The frequency response of an unstable ﬁlter whose impulse response is
energy-limited is the L2-Fourier Transform of its impulse response as deﬁned
in Section 6.2.3 (Deﬁnition 6.2.7).
As discussed in Section 5.5, if x, h are both integrable, then x ⋆h is deﬁned at
all epochs t outside a set of Lebesgue measure zero, and x ⋆h is integrable. In
this case the FT of x ⋆h is the mapping f 	→ˆx(f) ˆh(f). If x is integrable and
h is of ﬁnite energy, then x ⋆h is also deﬁned at all epochs t outside a set of
Lebesgue measure zero. But in this case the convolution is only guaranteed to be
of ﬁnite energy; it need not be integrable. We can discuss its Fourier Transform
using the deﬁnition of the L2-Fourier Transform for energy-limited signals that are
not integrable as in Section 6.2.3. In this case, again, the L2-Fourier Transform of
x ⋆h is the (equivalence class of the) mapping f 	→ˆx(f) ˆh(f):7
Theorem 6.3.2 (The Fourier Transform of a Convolution).
(i) If the signals h and x are both integrable, then the convolution x⋆h is deﬁned
for all t outside a set of Lebesgue measure zero; it is integrable; and its
L1-Fourier Transform 
x ⋆h is given by

x ⋆h(f) = ˆx(f) ˆh(f),
f ∈R,
(6.36)
where ˆx and ˆh are the L1-Fourier Transforms of x and h.
(ii) If the signal x is integrable and if h is of ﬁnite energy, then the convolution
x ⋆h is deﬁned for all t outside a set of Lebesgue measure zero; it is energy-
limited; and its L2-Fourier Transform 
x ⋆h is also given by (6.36) with ˆx,
7To be precise we should say that the L2 -Fourier Transform of x⋆h is the equivalence class of
the product of the L1 -Fourier Transform of x by any element in the equivalence class consisting
of the L2 -Fourier Transform of [h].
6.3 The Frequency Response of a Filter
79
f
"
LPFWc(f)
Wc
−Wc
Wc
1
Figure 6.2: The frequency response of the ideal unit-gain lowpass ﬁlter of cutoﬀ
frequency Wc. Notice that Wc is the length of the interval of positive frequencies
where the gain is one.
as before, being the L1-Fourier Transform of x but with ˆh now being the
L2-Fourier Transform of h.
Proof. For a proof of Part (i) see, for example, (Stein and Weiss, 1971, Chapter 1,
Section 1, Theorem 1.4).
For Part (ii) see (Stein and Weiss, 1971, Chapter 1,
Section 2, Theorem 2.6).
As an example, recall from Section 5.9 that the unit-gain ideal lowpass ﬁlter of
cutoﬀfrequency Wc is a ﬁlter of impulse response
h(t) = 2Wc sinc(2Wct),
t ∈R.
(6.37)
This ﬁlter is not causal and not stable, but its impulse response is energy-limited.
The ﬁlter’s frequency response is the L2-Fourier Transform of the impulse response
(6.37), which, using the results from Section 6.2.5, is given by (the equivalence class
of) the mapping
f 	→I{|f| ≤Wc},
f ∈R.
(6.38)
This mapping maps all frequencies f satisfying |f| > Wc to 0 and all frequencies
satisfying |f| ≤Wc to one. It is for this reason that we use the adjective “unit-gain”
in describing this ﬁlter. We denote the mapping in (6.38) by "
LPFWc(·) so
"
LPFWc(f) ≜I{|f| ≤Wc},
f ∈R.
(6.39)
This mapping is depicted in Figure 6.2. Note that Wc is the length of the interval
of positive frequencies where the response is one.
Turning to the ideal unit-gain bandpass ﬁlter of bandwidth W around the carrier
frequency fc satisfying fc ≥W/2, we note that, by (5.21), its time-t impulse
response BPFW,fc(t) is given by
BPFW,fc(t) = 2W cos(2πfct) sinc(Wt)
= 2 Re

LPFW/2(t) ei2πfct
.
(6.40)
80
The Frequency Response of Filters and Bandlimited Signals
f

BPFW,fc(f)
fc
−fc
W
1
Figure 6.3: The frequency response of the ideal unit-gain bandpass ﬁlter of band-
width W around the carrier frequency fc. Notice that, as for the lowpass ﬁlter, W
is the length of the interval of positive frequencies where the gain is one.
This ﬁlter too is noncausal and nonstable. From (6.40) and (6.39) we obtain using
Table 6.1 that its frequency response is (the equivalence class of) the mapping
f 	→I
'|f| −fc
 ≤W
2
(
.
We denote this mapping by 
BPFW,fc(·) so

BPFW,fc(f) ≜I
'|f| −fc
 ≤W
2
(
,
f ∈R.
(6.41)
This mapping is depicted in Figure 6.3. Note that, as for the lowpass ﬁlter, W is
the length of the interval of positive frequencies where the response is one.
6.4
Bandlimited Signals and Lowpass Filtering
In this section we deﬁne bandlimited signals and discuss lowpass ﬁltering.
We
treat energy-limited signals and integrable signals separately. As we shall see, any
integrable signal that is bandlimited to W Hz is also an energy-limited signal that
is bandlimited to W Hz (Note 6.4.12).
6.4.1
Energy-Limited Signals
The main result of this section is that the following three statements are equivalent:
(a) The signal x is an energy-limited signal satisfying
(x ⋆LPFW)(t) = x(t),
t ∈R.
(6.42)
(b) The signal x can be expressed in the form
x(t) =
 W
−W
g(f) ei2πft df,
t ∈R,
(6.43a)
6.4 Bandlimited Signals and Lowpass Filtering
81
for some measurable function g: f 	→g(f) satisfying
 W
−W
|g(f)|2 df < ∞.
(6.43b)
(If (b) holds, then the LHS of (6.43b) must equal the energy in x.)
(c) The signal x is a continuous energy-limited signal whose L2-Fourier Trans-
form ˆx satisﬁes
 ∞
−∞
|ˆx(f)|2 df =
 W
−W
|ˆx(f)|2 df.
(6.44)
We can thus deﬁne x to be an energy-limited signal that is bandlimited to W Hz
if one (and hence all) of the above conditions hold.
In deriving this result we shall take (a) as the deﬁnition. We shall then establish
the equivalence (a) ⇔(b) in Proposition 6.4.5, which also establishes that the
function g in (6.43a) can be taken as any element in the equivalence class of the
L2-Fourier Transform of x, and that the LHS of (6.43b) is then ∥x∥2
2. Finally, we
shall establish the equivalence (a) ⇔(c) in Proposition 6.4.6.
We conclude the section with a summary of the key properties of the result of
passing an energy-limited signal through an ideal unit-gain lowpass ﬁlter.
We begin by deﬁning an energy-limited signal to be bandlimited to W Hz if it is
unaltered when it is lowpass ﬁltered by an ideal unit-gain lowpass ﬁlter of cutoﬀ
frequency W.
Recalling that we are denoting by LPFW(t) the time-t impulse
response of an ideal unit-gain lowpass ﬁlter of cutoﬀfrequency W (see (5.19)), we
have the following deﬁnition.8
Deﬁnition 6.4.1 (Energy-Limited Bandlimited Signals). We say that the signal x
is an energy-limited signal that is bandlimited to W Hz if x is in L2 and
(x ⋆LPFW)(t) = x(t),
t ∈R.
(6.45)
Note 6.4.2. If an energy-limited signal that is bandlimited to W Hz is of zero
energy, then it is the all-zero signal 0.
Proof. Let x be an energy-limited signal that is bandlimited to W Hz and that
has zero energy. Then
|x(t)| =
(x ⋆LPFW)(t)

≤∥x∥2 ∥LPFW∥2
= ∥x∥2
√
2W
= 0,
t ∈R,
8Even though the ideal unit-gain lowpass ﬁlter of cutoﬀfrequency W is not stable, its impulse
response LPFW(·) is of ﬁnite energy (because it decays like 1/t and the integral of 1/t2 from
one to inﬁnity is ﬁnite). Consequently, we can use the Cauchy-Schwarz Inequality to prove that
if x ∈L2 then the mapping τ →x(τ) LPFW(t −τ) is integrable for every time instant t ∈R.
Consequently, the convolution x ⋆LPFW is deﬁned at every time instant t; see Section 5.5.
82
The Frequency Response of Filters and Bandlimited Signals
where the ﬁrst equality follows because x is an energy-limited signal that is band-
limited to W Hz and is thus unaltered when it is lowpass ﬁltered; the subsequent
inequality follows from (5.6b); the subsequent equality by computing ∥LPFW∥2
using Parseval’s Theorem and the explicit form of the frequency response of the
ideal unit-gain lowpass ﬁlter of bandwidth W (6.38); and where the ﬁnal equality
follows from the hypothesis that x is of zero energy.
Having deﬁned what it means for an energy-limited signal to be bandlimited to W
Hz, we can now deﬁne its bandwidth.9
Deﬁnition 6.4.3 (Bandwidth). The bandwidth of an energy-limited signal x is
the smallest frequency W to which x is bandlimited.
The next lemma shows that the result of passing an energy-limited signal through
an ideal unit-gain lowpass ﬁlter of cutoﬀfrequency W is an energy-limited signal
that is bandlimited to W Hz.
Lemma 6.4.4.
(i) Let y = x ⋆LPFW be the output of an ideal unit-gain lowpass ﬁlter of cutoﬀ
frequency W that is fed the energy-limited input x ∈L2. Then y ∈L2;
y(t) =
 W
−W
ˆx(f) ei2πft df,
t ∈R;
(6.46)
and the L2-Fourier Transform of y is the (equivalence class of the) mapping
f 	→ˆx(f) I{|f| ≤W}.
(6.47)
(ii) If g: f 	→g(f) is a bounded integrable function and if x is energy-limited,
then x ⋆ˇg is in L2; it can be expressed as

x ⋆ˇg

(t) =
 ∞
−∞
ˆx(f) g(f) ei2πft df,
t ∈R;
(6.48)
and its L2-Fourier Transform is given by (the equivalence class of) the map-
ping f 	→ˆx(f) g(f).
Proof. Even though Part (i) is a special case of Part (ii) corresponding to g being
the mapping f 	→I{|f| ≤W}, we shall prove the two parts separately. We begin
with a proof of Part (i). The idea of the proof is to express for each t ∈R the
time-t output y(t) as an inner product and to then use Parseval’s Theorem. Thus,
9To be more rigorous we should use in this deﬁnition the term “inﬁmum” instead of “smallest,”
but it turns out that the inﬁmum here is also a minimum.
6.4 Bandlimited Signals and Lowpass Filtering
83
(6.46) follows from the calculation
y(t) =

x ⋆LPFW

(t)
=
 ∞
−∞
x(τ) LPFW(t −τ) dτ
=

x, τ 	→LPFW(t −τ)

=

x, τ 	→LPFW(τ −t)

=
#
ˆx, f 	→e−i2πft "
LPFW(f)
$
=
ˆx, f 	→e−i2πft I{|f| ≤W}

=
 W
−W
ˆx(f) ei2πft df,
where the fourth equality follows from the symmetry of the function LPFW(·), and
where the ﬁfth equality follows from Parseval’s Theorem and the fact that delaying
a function multiplies its FT by a complex exponential. Having established (6.46),
Part (i) now follows from Proposition 6.2.10, because, by Parseval’s Theorem, the
mapping f 	→ˆx(f) I{|f| ≤W} is of ﬁnite energy and hence, by Proposition 3.4.3,
also integrable.
We next turn to Part (ii). We ﬁrst note that the assumption that g is bounded
and integrable implies that it is also energy-limited, because if |g(f)| ≤σ∞for all
f ∈R, then |g(f)|2 ≤σ∞|g(f)| and

|g(f)|2 df ≤σ∞

|g(f)| df. Thus,
g ∈L1 ∩L2.
(6.49)
We next prove (6.48). To that end we express the convolution x ⋆ˇg at time t as
an inner product and then use Parseval’s Theorem to obtain

x ⋆ˇg

(t) =
 ∞
−∞
x(τ) ˇg(t −τ) dτ
= ⟨x, τ 	→ˇg∗(t −τ)⟩
=
ˆx, f 	→e−i2πftg∗(f)

=
 ∞
−∞
ˆx(f) g(f) ei2πft df,
t ∈R,
(6.50)
where the third equality follows from Parseval’s Theorem and by noting that the
L2-Fourier Transform of the mapping τ 	→ˇg∗(t −τ) is the equivalence class of
the mapping f 	→e−i2πftg∗(f), as can be veriﬁed by expressing the mapping
τ 	→ˇg∗(t −τ) as the IFT of the mapping f 	→e−i2πftg∗(f)
ˇg∗(t −τ) =
	 ∞
−∞
g(f) ei2πf(t−τ) df

∗
=
 ∞
−∞
g∗(f) ei2πf(τ−t) df
=
 ∞
−∞

g∗(f) e−i2πft
ei2πfτ df,
t, τ ∈R,
84
The Frequency Response of Filters and Bandlimited Signals
and by then applying Proposition 6.2.10 to the mapping f 	→g∗(f) e−i2πft, which
is in L1 ∩L2 by (6.49).
Having established (6.48), we next examine the integrand in (6.48) and note that
if |g(f)| is upper-bounded by σ∞, then the modulus of the integrand is upper-
bounded by σ∞|ˆx(f)|, so the assumption that x ∈L2 (and hence that ˆx is of
ﬁnite energy) guarantees that the integrand is square integrable.
Also, by the
Cauchy-Schwarz Inequality, the square integrability of g and of ˆx implies that the
integrand is integrable. Thus, the integrand in (6.48) is both square integrable and
integrable so, by Proposition 6.2.10, the signal x ⋆ˇg is square integrable and its
Fourier Transform is the (equivalence class of the) mapping f 	→ˆx(f) g(f).
With the aid of the above lemma we can now give an equivalent deﬁnition for
energy-limited signals that are bandlimited to W Hz. This deﬁnition is popular
among mathematicians, because it does not involve the L2-Fourier Transform and
because the continuity of the signal is implied.
Proposition 6.4.5 (On the Deﬁnition of Bandlimited Functions in L2).
(i) If x is an energy-limited signal that is bandlimited to W Hz, then it can be
expressed in the form
x(t) =
 W
−W
g(f) ei2πft df,
t ∈R,
(6.51)
where g(·) satisﬁes
 W
−W
|g(f)|2 df < ∞
(6.52)
and can be taken as (any function in the equivalence class of) ˆx.
(ii) If a signal x can be expressed as in (6.51) for some function g(·) satisfying
(6.52), then x is an energy-limited signal that is bandlimited to W Hz and ˆx
is (the equivalence class of) the mapping f 	→g(f) I{|f| ≤W}.
Proof. We ﬁrst prove Part (i). Let x be an energy-limited signal that is band-
limited to W Hz. Then
x(t) = (x ⋆LPFW)(t)
=
 W
−W
ˆx(f) ei2πft df,
t ∈R,
where the ﬁrst equality follows from Deﬁnition 6.4.1, and where the second equality
follows from Lemma 6.4.4 (i). Consequently, if we pick g as (any element of the
equivalence class of) f 	→ˆx(f) I{|f| ≤W}, then (6.51) will be satisﬁed and (6.52)
will follow from Parseval’s Theorem.
To prove Part (ii) deﬁne ˜g: f 	→g(f) I{|f| ≤W}. From the assumption (6.52) and
from Proposition 3.4.3 it then follows that ˜g ∈L1 ∩L2. This and (6.51) imply that
x ∈L2 and that the L2-Fourier Transform of (the equivalence class of) x is (the
6.4 Bandlimited Signals and Lowpass Filtering
85
equivalence class of) ˜g; see Proposition 6.2.10. To complete the proof of Part (ii)
it thus remains to show that x ⋆LPFW = x. This follows from the calculation:

x ⋆LPFW

(t) =
 W
−W
ˆx(f) ei2πft df
=
 W
−W
g(f) ei2πft df
= x(t),
t ∈R,
where the ﬁrst equality follows from Lemma 6.4.4 (i); the second because we have
already established that the L2-Fourier Transform of (the equivalence class of) x is
(the equivalence class of) f 	→g(f) I{|f| ≤W}; and where the last equality follows
from (6.51).
In the engineering literature a function is often deﬁned as bandlimited to W Hz
if its FT is zero for frequencies f outside the interval [−W, W ]. This deﬁnition
is imprecise because the L2-Fourier Transform of a signal is an equivalence class
and its value at a given frequency is technically undeﬁned. It would be better to
deﬁne an energy-limited signal as bandlimited to W Hz if ∥x∥2
2 =
 W
−W
ˆx(f)
2 df
so “all its energy is contained in the frequency band [−W, W ].” However, this is
not quite equivalent to our deﬁnition. For example, the L2-Fourier Transform of
the discontinuous signal
x(t) =

17
if t = 0,
sinc 2Wt
otherwise,
is (the equivalence class of) the Brickwall (frequency domain) function
1
2W I{|f| ≤W},
f ∈R
(because the discontinuity at t = 0 does not inﬂuence the Fourier integral), but
the signal is altered by the lowpass ﬁlter, which smooths it out to produce the
continuous waveform t 	→sinc(2Wt). Readers who have already seen the Sampling
Theorem will note that the above signal x(·) provides a counterexample to the
Sampling Theorem as it is often imprecisely stated.
The following proposition clariﬁes the relationship between this deﬁnition and ours.
Proposition 6.4.6 (More on the Deﬁnition of Bandlimited Functions in L2).
(i) If x is an energy-limited signal that is bandlimited to W Hz, then x is a
continuous function and all its energy is contained in the frequency interval
[−W, W ] in the sense that its L2-Fourier Transform ˆx satisﬁes
 ∞
−∞
|ˆx(f)|2 df =
 W
−W
|ˆx(f)|2 df.
(6.53)
86
The Frequency Response of Filters and Bandlimited Signals
(ii) If the signal x ∈L2 satisﬁes (6.53), then x is indistinguishable from the
signal x ⋆LPFW, which is an energy-limited signal that is bandlimited to W
Hz. If in addition to satisfying (6.53) the signal x is continuous, then x is
an energy-limited signal that is bandlimited to W Hz.
Proof. This proposition’s claims are a subset of those of Proposition 6.4.7, which
summarizes some of the results relating to lowpass ﬁltering. The proof is therefore
omitted.
Proposition 6.4.7. Let y = x ⋆LPFW be the result of feeding the signal x ∈L2 to
an ideal unit-gain lowpass ﬁlter of cutoﬀfrequency W. Then:
(i) y is energy-limited with
∥y∥2 ≤∥x∥2 .
(6.54)
(ii) y is an energy-limited signal that is bandlimited to W Hz.
(iii) Its L2-Fourier Transform ˆy is given by (the equivalence class of) the mapping
f 	→ˆx(f) I{|f| ≤W}.
(iv) All the energy in y is concentrated in the frequency band [−W, W ] in the
sense that:
 ∞
−∞
|ˆy(f)|2 df =
 W
−W
|ˆy(f)|2 df.
(v) y can be represented as
y(t) =
 ∞
−∞
ˆy(f) ei2πft df,
t ∈R
(6.55)
=
 W
−W
ˆx(f) ei2πft df,
t ∈R.
(6.56)
(vi) y is uniformly continuous.
(vii) If x ∈L2 has all its energy concentrated in the frequency band [−W, W ] in
the sense that
 ∞
−∞
|ˆx(f)|2 df =
 W
−W
|ˆx(f)|2 df,
(6.57)
then x is indistinguishable from the bandlimited signal x ⋆LPFW.
(viii) x is an energy-limited signal that is bandlimited to W if, and only if, it
satisﬁes all three of the following conditions: it is in L2; it is continuous;
and it satisﬁes (6.57).
6.4 Bandlimited Signals and Lowpass Filtering
87
Proof. Part (i) follows from Lemma 6.4.4 (i), which demonstrates that ˆy is (the
equivalence class of) the mapping f 	→ˆx(f) I{|f| ≤W} so, by Parseval’s Theorem,
∥y∥2
2 =
 ∞
−∞
|ˆy(f)|2 df
=
 W
−W
|ˆx(f)|2 df
≤
 ∞
−∞
|ˆx(f)|2 df
= ∥x∥2
2 .
Part (ii) follows because, by Lemma 6.4.4 (i), the signal y satisﬁes
y(t) =
 W
−W
ˆx(f) ei2πft df
where
 W
−W
|ˆx(f)|2 df ≤
 ∞
−∞
|ˆx(f)|2 df = ∥x∥2
2 < ∞,
so, by Proposition 6.4.5, y is an energy-limited signal that is bandlimited to W Hz.
Part (iii) follows directly from Lemma 6.4.4 (i). Part (iv) follows from Part (iii).
Part (v) follows, again, directly from Lemma 6.4.4.
Part (vi) follows from the representation (6.56); from the fact that the IFT of
integrable functions is uniformly continuous (Theorem 6.2.11); and because the
condition ∥x∥2 < ∞implies, by Proposition 3.4.3, that f 	→ˆx(f) I{|f| ≤W} is
integrable.
To prove Part (vii) we note that by Part (ii) x ⋆LPFW is an energy-limited signal
that is bandlimited to W Hz, and we note that (6.57) implies that x is indistin-
guishable from x ⋆LPFW because
∥x −x ⋆LPFW∥2
2 =
 ∞
−∞
ˆx(f) −

x ⋆LPFW(f)

2
df
=
 ∞
−∞
ˆx(f) −ˆx(f) I{|f| ≤W}
2 df
=

|f|>W
ˆx(f)
2 df
= 0,
where the ﬁrst equality follows from Parseval’s Theorem; the second equality from
Lemma 6.4.4 (i); the third equality because the integrand is zero for |f| ≤W; and
the ﬁnal equality from (6.57).
To prove Part (viii) deﬁne y = x ⋆LPFW and note that if x is an energy-limited
signal that is bandlimited to W Hz then, by Deﬁnition 6.4.1, y = x so the continuity
of x and the fact that its energy is concentrated in the interval [−W, W ] follow from
Parts (iv) and (vi). In the other direction, if x satisﬁes (6.57) then by Part (vii)
88
The Frequency Response of Filters and Bandlimited Signals
it is indistinguishable from the signal y, which is continuous by Part (vi).
If,
additionally, x is continuous, then x must be identical to y because two continuous
functions that are indistinguishable must be identical.
6.4.2
Integrable Signals
We next discuss what we mean when we say that x is an integrable signal that is
bandlimited to W Hz. Also important will be Note 6.4.11, which establishes that
if x is such a signal, then x is equal to the IFT of its FT.
Even though the ideal unit-gain lowpass ﬁlter is unstable, its convolution with any
integrable signal is well-deﬁned. Denoting the cutoﬀfrequency by Wc we have:
Proposition 6.4.8. For any x ∈L1 the convolution integral
 ∞
−∞
x(τ) LPFWc(t −τ) dτ
is deﬁned at every epoch t ∈R and is given by
 ∞
−∞
x(τ) LPFWc(t −τ) dτ =
 Wc
−Wc
ˆx(f) ei2πft df,
t ∈R.
(6.58)
Moreover, x ⋆LPFWc is an energy-limited signal that is bandlimited to Wc Hz.10
Its L2-Fourier Transform is (the equivalence class of) the mapping
f 	→ˆx(f) I{|f| ≤Wc}.
Proof. The key to the proof is to note that, although the sinc(·) function is not
integrable, it follows from (6.35) that it can be represented as the Inverse Fourier
Transform of an integrable function (of frequency). Consequently, the existence
of the convolution and its representation as (6.58) follow directly from Proposi-
tion 6.2.5 and (6.35).
To prove the remaining assertions of the proposition we note that, since x is inte-
grable, it follows from Theorem 6.2.11 that |ˆx(f)| ≤∥x∥1 and hence
 Wc
−Wc
|ˆx(f)|2 df < ∞.
(6.59)
The result now follows from (6.58), (6.59), and Proposition 6.4.5.
With the aid of Proposition 6.4.8 we can now deﬁne bandlimited integrable signals:
Deﬁnition 6.4.9 (Bandlimited Integrable Signals). We say that the signal x is
an integrable signal that is bandlimited to W Hz if x is integrable and if it
is unaltered when it is lowpass ﬁltered by an ideal unit-gain lowpass ﬁlter of cutoﬀ
frequency W:
x(t) = (x ⋆LPFW)(t),
t ∈R.
10Lowpass-ﬁltering an integrable signal need not produce an integrable signal (Exercise 6.20).
6.4 Bandlimited Signals and Lowpass Filtering
89
Proposition 6.4.10 (Characterizing Integrable Signals that Are Bandlimited to
W Hz). If x is an integrable signal, then each of the following statements is equiv-
alent to the statement that x is an integrable signal that is bandlimited to W Hz:
(a) The signal x is unaltered when it is lowpass ﬁltered:
x(t) = (x ⋆LPFW)(t),
t ∈R.
(6.60)
(b) The signal x can be expressed as
x(t) =
 W
−W
ˆx(f) ei2πft df,
t ∈R.
(6.61)
(c) The signal x is continuous and
ˆx(f) = 0,
|f| > W.
(6.62)
(d) There exists an integrable function g such that
x(t) =
 W
−W
g(f) ei2πft df,
t ∈R.
(6.63)
Proof. Condition (a) is the condition given in Deﬁnition 6.4.9, so it only remains
to show that the four conditions are equivalent. We proceed to do so by proving
that (a) ⇔(b); that (b) ⇒(d); that (d) ⇒(c); and that (c) ⇒(b).
That (a) ⇔(b) follows directly from Proposition 6.4.8 and, more speciﬁcally, from
the representation (6.58). The implication (b) ⇒(d) is obvious because nothing
precludes us from picking g to be the mapping f 	→ˆx(f) I{|f| ≤W}, which is
integrable because ˆx is bounded by ∥x∥1 (Theorem 6.2.11).
We next prove that (d) ⇒(c). We thus assume that there exists an integrable
function g such that (6.63) holds and proceed to prove that x is continuous and
that (6.62) holds. To that end we ﬁrst note that the integrability of g implies,
by Theorem 6.2.11, that x (= ˇg) is continuous. It thus remains to prove that ˆx
satisﬁes (6.62). Deﬁne g0 as the mapping f 	→g(f) I{|f| ≤W}. By (6.63) it then
follows that x = ˇg0. Consequently,
ˆx = ˆˇg0.
(6.64)
Employing Theorem 6.2.13 (ii) we conclude that the RHS of (6.64) is equal to g0
outside a set of Lebesgue measure zero, so (6.64) implies that ˆx is indistinguishable
from g0.
Since both ˆx and g0 are continuous for |f| > W, this implies that
ˆx(f) = g0(f) for all frequencies |f| > W.
Since, by its deﬁnition, g0(f) = 0
whenever |f| > W we can conclude that (6.62) holds.
Finally (c) ⇒(b) follows directly from Theorem 6.2.13 (i).
From Proposition 6.4.10 (cf. (b) and (c)) we obtain:
90
The Frequency Response of Filters and Bandlimited Signals
Note 6.4.11. If x is an integrable signal that is bandlimited to W Hz, then it is
equal to the IFT of its FT.
By Proposition 6.4.10 it also follows that if x is an integrable signal that is
bandlimited to W Hz, then (6.61) is satisﬁed.
Since the integrand in (6.61) is
bounded (by ∥x∥1) it follows that the integrand is square integrable over the in-
terval [−W, W ]. Consequently, by Proposition 6.4.5, x must be an energy-limited
signal that is bandlimited to W Hz. We have thus proved:
Note 6.4.12. An integrable signal that is bandlimited to W Hz is also an energy-
limited signal that is bandlimited to W Hz.
The reverse statement is not true: the sinc(·) is an energy-limited signal that is
bandlimited to 1/2 Hz, but it is not integrable.
The deﬁnition of bandwidth for integrable signals is similar to Deﬁnition 6.4.3.11
Deﬁnition 6.4.13 (Bandwidth). The bandwidth of an integrable signal is the
smallest frequency W to which it is bandlimited.
6.5
Bandlimited Signals Through Stable Filters
In this section we discuss the result of feeding bandlimited signals to stable ﬁlters.
We begin with energy-limited signals. In Theorem 6.3.2 we saw that the convo-
lution of an integrable signal with an energy-limited signal is deﬁned at all times
outside a set of Lebesgue measure zero. The next proposition shows that if the
energy-limited signal is bandlimited to W Hz, then the convolution is deﬁned at
every time, and the result is an energy-limited signal that is bandlimited to W Hz.
Proposition 6.5.1. Let x be an energy-limited signal that is bandlimited to W Hz
and let h be integrable. Then x⋆h is deﬁned for every t ∈R; it is an energy-limited
signal that is bandlimited to W Hz; and it can be represented as

x ⋆h

(t) =
 W
−W
ˆx(f) ˆh(f) ei2πft df,
t ∈R.
(6.65)
Proof. Since x is an energy-limited signal that is bandlimited to W Hz, it follows
from Proposition 6.4.5 that
x(t) =
 W
−W
ˆx(f) ei2πft df,
t ∈R,
(6.66)
with the mapping f 	→ˆx(f) I{|f| ≤W} being square integrable and hence, by
Proposition 3.4.3, also integrable. Thus the convolution x ⋆h is the convolution
between the IFT of the integrable mapping f 	→ˆx(f) I{|f| ≤W} and the integrable
function h. By Proposition 6.2.5 we thus obtain that the convolution x⋆h is deﬁned
11Again, we omit the proof that the inﬁmum is a minimum.
6.6 The Bandwidth of a Product of Two Signals
91
at every time t and has the representation (6.65). The proposition will now follow
from (6.65) and Proposition 6.4.5 once we demonstrate that
 W
−W
ˆx(f) ˆh(f)
2 df < ∞.
This can be proved by upper-bounding |ˆh(f)| by ∥h∥1 (Theorem 6.2.11) and by
then using Parseval’s Theorem.
We next turn to integrable signals passed through stable ﬁlters.
Proposition 6.5.2 (Integrable Bandlimited Signals through Stable Filters). Let x
be an integrable signal that is bandlimited to W Hz, and let h be integrable. Then
the convolution x ⋆h is deﬁned for every t ∈R; it is an integrable signal that is
bandlimited to W Hz; and it can be represented as

x ⋆h

(t) =
 W
−W
ˆx(f) ˆh(f) ei2πft df,
t ∈R.
(6.67)
Proof. Since every integrable signal that is bandlimited to W Hz is also an energy-
limited signal that is bandlimited to W Hz, it follows from Proposition 6.5.1 that the
convolution x⋆h is deﬁned at every epoch and that it can be represented as (6.65).
Alternatively, one can derive this representation from (6.61) and Proposition 6.2.5
by substituting in that proposition h for x and f 	→ˆx(f) I{|f| ≤W} (which is
integrable because ˆx is bounded by Theorem 6.2.11 (i)) for g. It only remains
to show that x ⋆h is integrable, but this follows because the convolution of two
integrable functions is integrable (5.9).
6.6
The Bandwidth of a Product of Two Signals
In this section we discuss the bandwidth of the product of two bandlimited signals.
The result is a straightforward consequence of the fact that the FT of a product
of two signals is the convolution of their FTs. We begin with the following result
on the FT of a product of signals.
Proposition 6.6.1 (The FT of a Product Is the Convolution of the FTs). If x1
and x2 are energy-limited signals, then their product
t 	→x1(t) x2(t)
is an integrable function whose FT is the mapping
f 	→
ˆx1 ⋆ˆx2

(f).
Proof. Let x1 and x2 be energy-limited signals, and denote their product by y:
y(t) = x1(t) x2(t),
t ∈R.
92
The Frequency Response of Filters and Bandlimited Signals
Since both x1 and x2 are square integrable, it follows from the Cauchy-Schwarz
Inequality that their product y is integrable and that
∥y∥1 ≤∥x1∥2 ∥x2∥2 .
(6.68)
Having established that the product is integrable, we next derive its FT and show
that
ˆy(f) =
ˆx1 ⋆ˆx2

(f),
f ∈R.
(6.69)
This is done by expressing ˆy(f) as an inner product between two ﬁnite-energy
functions and by then using Parseval’s Theorem:
ˆy(f) =
 ∞
−∞
y(t) e−i2πft dt
=
 ∞
−∞
x1(t) x2(t) e−i2πft dt
=

t 	→x1(t), t 	→x∗
2(t) ei2πft
=
#
˜f 	→ˆx1( ˜f), ˜f 	→ˆx∗
2(f −˜f)
$
=
 ∞
−∞
ˆx1( ˜f) ˆx2(f −˜f) d ˜f
=
ˆx1 ⋆ˆx2

(f),
f ∈R.
Proposition 6.6.2. Let x1 and x2 be energy-limited signals that are bandlimited to
W1 Hz and W2 Hz respectively. Then their product is an integrable signal that is
bandlimited to W1 + W2 Hz.
Proof. Since both x1 and x2 are of ﬁnite energy, it follows from the Cauchy-
Schwarz Inequality that their product is integrable. We will show that their product
is an integrable signal that is bandlimited to W1+W2 Hz using Theorem 6.4.10 (d)
by showing that
x1(t) x2(t) =
 W1+W2
−(W1+W2)
g(f) ei2πft df,
t ∈R,
(6.70)
where the function g(·) is bounded and hence integrable
 W1+W2
−(W1+W2)
|g(f)| df < ∞.
(6.71)
To establish (6.70) we begin by noting that since x1 is of ﬁnite energy and band-
limited to W1 Hz we have by Proposition 6.4.5
x1(t) =
 W1
−W1
ˆx1(f1) ei2πf1t df1,
t ∈R.
Similarly,
x2(t) =
 W2
−W2
ˆx2(f2) ei2πf2t df2,
t ∈R.
6.6 The Bandwidth of a Product of Two Signals
93
Consequently,
x1(t) x2(t) =
 W1
−W1
ˆx1(f1) ei2πf1t df1
 W2
−W2
ˆx2(f2) ei2πf2t df2
=
 W1
−W1
 W2
−W2
ˆx1(f1) ˆx2(f2) ei2π(f1+f2)t df1 df2
=
 ∞
−∞
 ∞
−∞
ˆx1(f1) ˆx2(f2) ei2π(f1+f2)t df1 df2
=
 ∞
−∞
 ∞
−∞
ˆx1( ˜f) ˆx2(f −˜f) ei2πft d ˜f df
=
 ∞
−∞
ei2πft ˆx1 ⋆ˆx2

(f) df
=
 ∞
−∞
ei2πftg(f) df,
t ∈R,
(6.72)
where
g(f) =
 ∞
−∞
ˆx1( ˜f) ˆx2(f −˜f) d ˜f,
f ∈R.
(6.73)
Here the second equality follows from Fubini’s Theorem;12 the third because x1
and x2 are bandlimited to W1 and W2 Hz respectively; and the fourth by intro-
ducing the variables f ≜f1 + f2 and ˜f ≜f1.
To establish (6.70) we now need to show that because x1 and x2 are bandlimited
to W1 and W2 Hz respectively, it follows that
g(f) = 0,
|f| > W1 + W2.
(6.74)
To prove this we note that because x1 and x2 are bandlimited to W1 Hz and W2
Hz respectively, we can rewrite (6.73) as
g(f) =
 ∞
−∞
ˆx1( ˜f) I

| ˜f| ≤W1

ˆx2(f −˜f) I

|f −˜f| ≤W2

d ˜f,
f ∈R,
(6.75)
and the product I

| ˜f| ≤W1

I

|f −˜f| ≤W2

is zero for all frequencies ˜f whenever
|f| > W1 + W2.
Having established (6.70) using (6.72) and (6.74), we now proceed to prove (6.71)
by showing that the integrand in (6.71) is bounded.
We do so by noting that
the integrand in (6.71) is the convolution of two square-integrable functions (ˆx1
and ˆx2) so by (5.6b) (with the dummy variable now being f) we have
|g(f)| ≤∥ˆx1∥2 ∥ˆx2∥2 = ∥x1∥2 ∥x2∥2 < ∞,
f ∈R.
12The fact that
 W1
−W1 |ˆx1(f)| df is ﬁnite follows from the ﬁniteness of
 W1
−W1 |ˆx1(f)|2 df (which
follows from Parseval’s Theorem) and from Proposition 3.4.3. The same argument applies to x2.
94
The Frequency Response of Filters and Bandlimited Signals
6.7
Bernstein’s Inequality
Bernstein’s Inequality captures the engineering intuition that the rate at which
a bandlimited signal can change is proportional to its bandwidth. The way the
theorem is phrased makes it clear that it is applicable both to integrable signals
that are bandlimited to W Hz and to energy-limited signals that are bandlimited
to W Hz.
Theorem 6.7.1 (Bernstein’s Inequality). If x can be written as
x(t) =
 W
−W
g(f) ei2πft df,
t ∈R
for some integrable function g, then

dx(t)
dt
 ≤4πW sup
τ∈R
|x(τ)|,
t ∈R.
(6.76)
Proof. A proof of a slightly more general version of this theorem can be found in
(Pinsky, 2002, Chapter 2, Section 2.3.8).
6.8
Time-Limited and Bandlimited Signals
In this section we prove that no nonzero signal can be both time-limited and
bandlimited. We shall present two proofs. The ﬁrst is based on Theorem 6.8.1,
which establishes a connection between bandlimited signals and entire functions.
The second is based on the Fourier Series.
We remind the reader that a function ξ: C →C is an entire function if it is
analytic throughout the complex plane.
Theorem 6.8.1. If x is an energy-limited signal that is bandlimited to W Hz, then
there exists an entire function ξ: C →C that agrees with x on the real axis
ξ(t + i0) = x(t),
t ∈R
(6.77)
and that satisﬁes
|ξ(z)| ≤γ e2πW|z|,
z ∈C,
(6.78)
where γ is some constant that can be taken as
√
2W ∥x∥2.
Proof. Let x be an energy-limited signal that is bandlimited to W Hz. By Propo-
sition 6.4.5 we can express x as
x(t) =
 W
−W
g(f) ei2πft df,
t ∈R
(6.79)
for some square-integrable function g satisfying
 W
−W
|g(f)|2 df = ∥x∥2
2 .
(6.80)
6.8 Time-Limited and Bandlimited Signals
95
Consider now the function ξ: C →C deﬁned by
ξ(z) =
 W
−W
g(f) ei2πfz df,
z ∈C.
(6.81)
This function is well-deﬁned for every z ∈C because in the region of integration
the integrand can be bounded by
g(f) ei2πfz = |g(f)| e−2πf Im(z)
≤|g(f)| e2π|f| |Im(z)|
≤|g(f)| e2π W |z|,
|f| ≤W,
(6.82)
and the RHS of (6.82) is integrable over the interval [−W, W ] by (6.80) and Propo-
sition 3.4.3.
By (6.79) and (6.81) it follows that ξ is an extension of the function x in the sense
of (6.77). It is but a technical matter to prove that ξ is analytic. One approach is
to prove that it is diﬀerentiable at every z ∈C by verifying that the swapping of
diﬀerentiation and integration, which leads to
dξ
dz (z) =
 W
−W
g(f) (i2πf) ei2πfz df,
z ∈C
is justiﬁed. See (Rudin, 1987, Section 19.1) for a diﬀerent approach.
To prove (6.78) we compute
|ξ(z)| =

 W
−W
g(f) ei2πfz df

≤
 W
−W
g(f) ei2πfz df
≤e2π W |z|
 W
−W
|g(f)| df
≤e2πW|z| √
2W
 W
−W
|g(f)|2 df
=
√
2W ∥x∥2 e2πW|z|,
where the inequality in the second line follows from Proposition 2.4.1; the inequality
in the third line from (6.82); the inequality in the fourth line from Proposition 3.4.3;
and the ﬁnal equality from (6.80).
Using Theorem 6.8.1 we can now easily prove the main result of this section.
Theorem 6.8.2. Let W and T be ﬁxed nonnegative real numbers. If x is an energy-
limited signal that is bandlimited to W Hz and that is time-limited in the sense that
it is zero for all t /∈[−T/2, T/2], then x(t) = 0 for all t ∈R.
By Note 6.4.12 this theorem also holds for integrable bandlimited signals.
96
The Frequency Response of Filters and Bandlimited Signals
Proof. By Theorem 6.8.1 x can be extended to an entire function ξ. Since x has
inﬁnitely many zeros in a bounded interval (e.g., for all t ∈[T, 2T ]) and since ξ
agrees with x on the real line, it follows that ξ also has inﬁnitely many zeros
in a bounded set (e.g., whenever z ∈{w ∈C : Im(w) = 0, Re(w) ∈[T, 2T] }).
Consequently, ξ is an entire function that has inﬁnitely many zeros in a bounded
subset of the complex plane and is thus the all-zero function (Rudin, 1987, Theo-
rem 10.18). But since x and ξ agree on the real line, it follows that x is also the
all-zero function.
Another proof can be based on the Fourier Series, which is discussed in the ap-
pendix. Starting from (6.79) we obtain that the time-η/(2W) sample of x(·) satisﬁes
1
√
2W
x
 η
2W

=
 W
−W
g(f)
1
√
2W
ei2πfη/(2W) df,
η ∈Z,
where we recognize the RHS of the above as the η-th Fourier Series Coeﬃcient of
the function f 	→g(f) I{|f| ≤W} with respect to the interval [−W, W) (Note A.3.5
on Page 837). But since x(t) = 0 whenever |t| > T/2, it follows that all but a ﬁnite
number of these samples can be nonzero, thus leading us to conclude that all but a
ﬁnite number of the Fourier Series Coeﬃcients of g(·) are zero. By the uniqueness
theorem for the Fourier Series (Theorem A.2.3) it follows that g(·) is equal to a
trigonometric polynomial (except possibly on a set of measure zero). Thus,
g(f) =
n

η=−n
aη ei2πηf/(2W),
f ∈[−W, W ] \ N,
(6.83)
for some n ∈N; for some 2n + 1 complex numbers a−n, . . . , an; and for some set
N ⊂[−W, W ] of Lebesgue measure zero. Since the integral in (6.79) is insensitive
to the behavior of g on the set N, it follows from (6.79) and (6.83) that
x(t) =
 W
−W
ei2πft
n

η=−n
aη ei2πηf/(2W) df
=
n

η=−n
aη
 ∞
−∞
ei2πf(t+
η
2W) I

|f| ≤W

df
= 2W
n

η=−n
aη sinc(2Wt + η),
t ∈R,
i.e., that x is a linear combination of a ﬁnite number of time-shifted sinc(·) func-
tions. It now remains to show that no linear combination of a ﬁnite number of
time-shifted sinc(·) functions can be zero for all t ∈[T, 2T ] unless it is zero for
all t ∈R. This can be established by extending the sincs to entire functions so
that the linear combination of the time-shifted sinc(·) functions is also an entire
function and by then calling again on the theorem that an entire function that has
inﬁnitely many zeros in a bounded subset of the complex plane must be the all-zero
function.
6.9 A Theorem by Paley and Wiener
97
6.9
A Theorem by Paley and Wiener
The theorem of Paley and Wiener that we discuss next is important in the study
of bandlimited functions, but it will not be used in this book.
Theorem 6.8.1 showed that every energy-limited signal x that is bandlimited to W
Hz can be extended to an entire function ξ satisfying (6.78) for some constant γ
by deﬁning ξ(z) as
ξ(z) =
 W
−W
ˆx(f) ei2πfz df,
z ∈C.
(6.84)
The theorem of Paley and Wiener that we present next can be viewed as the
reverse statement. It demonstrates that if ξ: C →C is an entire function that
satisﬁes (6.78) and whose restriction to the real axis is square integrable, then its
restriction to the real axis is an energy-limited signal that is bandlimited to W Hz
and, moreover, if we denote this restriction by x so x(t) = ξ(t + i0) for all t ∈R,
then ξ is given by (6.84). This theorem demonstrates the close connection between
entire functions satisfying (6.78)—functions that are called entire functions of
exponential type—and energy-limited signals that are bandlimited to W Hz.
Theorem 6.9.1 (Paley-Wiener). If for some positive constants W and γ the entire
function ξ: C →C satisﬁes
|ξ(z)| ≤γ e2πW|z|,
z ∈C
(6.85)
and if
 ∞
−∞
|ξ(t + i0)|2 dt < ∞,
(6.86)
then there exists an energy-limited function g: R →C such that
ξ(z) =
 W
−W
g(f) ei2πfz df,
z ∈C.
(6.87)
Proof. See for example, (Rudin, 1987, Theorem 19.3) or (Katznelson, 2004, Sec-
tion VI.7) or (Dym and McKean, 1972, Section 3.3).
6.10
Picket Fences and Poisson Summation
Engineering textbooks often contain a useful expression for the FT of an inﬁnite
series of equally-spaced Dirac’s Deltas. Very roughly, the result is that the FT of
the mapping
t 	→
∞

j=−∞
δ

t + jTs

is the mapping
f 	→1
Ts
∞

η=−∞
δ

f + η
Ts

,
98
The Frequency Response of Filters and Bandlimited Signals
where δ(·) denotes Dirac’s Delta. Needless to say, we are being extremely informal
because we said nothing about convergence. This result is sometimes called the
picket-fence miracle, because if we envision the plot of Dirac’s Delta as an
upward pointing bold arrow stemming from the origin, then the plot of a sum of
shifted Delta’s resembles a picket fence. The picket-fence miracle is that the FT
of a picket fence is yet another scaled picket fence; see (Oppenheim and Willsky,
1997, Chapter 4, Example 4.8 and also Chapter 7, Section 7.1.1.) or (Kwakernaak
and Sivan, 1991, Chapter 7, Example 7.4.19(c)).
In the mathematical literature, this result is called “the Poisson summation for-
mula.” It states that under certain conditions on the function ψ ∈L1,
∞

j=−∞
ψ

jTs

= 1
Ts
∞

η=−∞
ˆψ
 η
Ts

.
(6.88)
To identify the roots of (6.88) deﬁne the mapping
φ(t) =
∞

j=−∞
ψ

t + jTs

,
(6.89)
and note that this function is periodic in the sense that φ(t + Ts) = φ(t) for every
t ∈R. Consequently, it is instructive to study its Fourier Series on the interval
[−Ts/2, Ts/2] (Note A.3.5 in the appendix). Its η-th Fourier Series Coeﬃcient with
respect to the interval [−Ts/2, Ts/2] is given by
 Ts/2
−Ts/2
φ(t)
1
√Ts
e−i2πηt/Ts dt =
1
√Ts
 Ts/2
−Ts/2
∞

j=−∞
ψ(t + jTs) e−i2πηt/Ts dt
=
1
√Ts
∞

j=−∞
 Ts/2+jTs
−Ts/2+jTs
ψ(τ) e−i2πη(τ−jTs)/Ts dτ
=
1
√Ts
∞

j=−∞
 Ts/2+jTs
−Ts/2+jTs
ψ(τ) e−i2πητ/Ts dτ
=
1
√Ts
 ∞
−∞
ψ(τ) e−i2πητ/Ts dτ
=
1
√Ts
ˆψ
 η
Ts

,
η ∈Z,
where the ﬁrst equality follows from the deﬁnition of φ(·) (6.89); the second by
swapping the summation and the integration and by deﬁning τ ≜t+jTs; the third
by the periodicity of the complex exponential; the fourth because summing the
integrals over disjoint intervals whose union is R is just the integral over R; and
the ﬁnal equality from the deﬁnition of the FT.
We can thus interpret the RHS of (6.88) as the evaluation13 at t = 0 of the Fourier
Series of φ(·) and the LHS as the evaluation of φ(·) at t = 0. Having established
13At t = 0 the complex exponentials are all equal to one, and the Fourier Series is thus just
the sum of the Fourier Series Coeﬃcients.
6.11 Additional Reading
99
the origin of the Poisson summation formula, we can now readily state conditions
that guarantee that it holds. An example of a set of conditions that guarantees
(6.88) is the following:
1) The function ψ(·) is integrable.
2) The RHS of (6.89) converges at t = 0.
3) The Fourier Series of φ(·) converges at t = 0 to the value of φ(·) at t = 0.
We draw the reader’s attention to the fact that it is not enough that both sides of
(6.88) converge absolutely and that both ψ(·) and ˆψ(·) be continuous; see (Katznel-
son, 2004, Chapter VI, Section 1, Exercise 15).
A setting where the above conditions are satisﬁed and where (6.88) thus holds is
given in the following proposition.
Proposition 6.10.1. Let ψ(·) be a continuous function satisfying
ψ(t) =

0
if |t| ≥T,
 t
−T ξ(τ) dτ
otherwise,
(6.90a)
where
 T
−T
|ξ(τ)|2 dτ < ∞,
(6.90b)
and where T > 0 is some constant. Then for any Ts > 0
∞

j=−∞
ψ

jTs

= 1
Ts
∞

η=−∞
ˆψ
 η
Ts

.
(6.90c)
Proof. The integrability of ψ(·) follows because ψ(·) is continuous and zero outside
a ﬁnite interval. That the RHS of (6.89) converges at t = 0 follows because the
fact that ψ(·) is zero outside the interval [−T, +T ] implies that only a ﬁnite number
of terms contribute to the sum at t = 0. That the Fourier Series of φ(·) converges
at t = 0 to the value of φ(·) at t = 0 follows from (Katznelson, 2004, Chapter I,
Section 6.2, Equation (6.2)) and from the corollary in (Katznelson, 2004, Chapter I,
Section 3.1).
6.11
Additional Reading
There are a number of excellent books on Fourier Analysis.
We mention here
(Katznelson, 2004), (Dym and McKean, 1972), (Pinsky, 2002), and (K¨orner, 1988).
In particular, readers who would like to better understand how the FT is deﬁned for
energy-limited functions that are not integrable may wish to consult (Katznelson,
2004, Chapter VI, Section 3.1) or (Dym and McKean, 1972, Sections 2.3–2.5).
Numerous surprising applications of the FT can be found in (K¨orner, 1988).
100
The Frequency Response of Filters and Bandlimited Signals
Engineers often speak of the 2WT degrees of freedom that signals that are band-
limited and time-limited have. A good starting point for the literature on this is
(Slepian, 1976).
Bandlimited functions are intimately related to “entire functions of exponential
type.” For an accessible introduction to this concept see (Requicha, 1980); for a
more mathematical approach see (Boas, 1954).
6.12
Exercises
Exercise 6.1 (Symmetries of the FT). Let x: R →C be integrable, and let ˆx be its FT.
(i) Show that if x is a real signal, then ˆx is conjugate-symmetric, i.e., ˆx(−f) = ˆx∗(f),
for every f ∈R.
(ii) Show that if x is purely imaginary (i.e., takes on only purely imaginary values),
then ˆx is conjugate-antisymmetric, i.e., ˆx(−f) = −ˆx∗(f), for every f ∈R.
(iii) Show that ˆx can be written uniquely as the sum of a conjugate-symmetric function
gcs and a conjugate-antisymmetric function gcas. Express gcs & gcas in terms of ˆx.
Exercise 6.2 (Diﬀerentiating the FT). Show that if both x and t →t x(t) are integrable,
then ˆx is diﬀerentiable and the FT of t →t x(t) is
f →−1
i2π
dˆx(f)
df
.
(See, for example, (Katznelson, 2004, Chapter VI, Section 1, Theorem 1.6).)
Exercise 6.3 (The L2-Fourier Transform and Its Inverse). Prove that the Inverse L2-
Fourier Transform is the reﬂection of the L2-Fourier Transform:
|
[x] = ~#
[x].
Hint: Use Theorem 6.2.8.
Exercise 6.4 (The L2-Fourier Transform, Delays, and Frequency Shifts). Let x: t →x(t)
and g: f →g(f) be energy-limited with the L2-Fourier Transform of [x] being [g].
(i) Let f0 ∈R be ﬁxed. Prove that the L2-Fourier Transform of the equivalence class of
the mapping t →x(t) ei2πf0t is the equivalence class of the mapping f →g(f −f0).
(ii) Let τ ∈R be ﬁxed. Prove that the L2-Fourier Transform of the equivalence class of
the mapping t →x(t −τ) is the equivalence class of the mapping f →g(f) e−i2πfτ.
Hint: For Part (ii) use Part (i) and Exercise 6.3.
Exercise 6.5 (Reconstructing a Function from Its IFT). Formulate and prove a result
analogous to Theorem 6.2.12 for the Inverse Fourier Transform.
Exercise 6.6 (Eigenfunctions of the FT). Show that if the energy-limited signal x satisﬁes
ˆx = λx for some λ ∈C, then λ can only be ±1 or ±i. (The Hermite functions are such
signals.)
6.12 Exercises
101
Exercise 6.7 (Delaying a Sinc). For W, δ > 0 compute
 ∞
−∞
	
sinc

2Wt

−sinc

2W(t −δ)

2
dt
as a function of W and δ. Study the result for the cases where δ W ≪1 and δ W ≫1.
Exercise 6.8 (Existence of a Stable Filter (1)). Let W > 0 be given. Does there exist a
stable ﬁlter whose frequency response is zero for |f| ≤W and is one for W < f ≤2W ?
Exercise 6.9 (Existence of a Stable Filter (2)). Let W > 0 be given. Does there exist a
stable ﬁlter whose frequency response equals cos(f) for all |f| ≥W ?
Exercise 6.10 (Existence of an Energy-Limited Signal). Argue that there exists an energy-
limited signal x whose FT is (the equivalence class of) the mapping f →e−f I{f ≥0}.
What is the energy in x? What is the energy in the result of feeding x to an ideal unit-gain
lowpass ﬁlter of cutoﬀfrequency Wc = 1?
Exercise 6.11 (Delaying a Bandlimited Signal). Let x be an energy-limited signal that
is bandlimited to W Hz. For any δ ∈R, let xδ denote the result of delaying x by δ, so
xδ : t →x(t −δ). Prove that
∥x −xδ∥2 ≤min

2, 2πWδ

∥x∥2 .
Hint: First argue that, irrespective of W and δ, we have ∥x −xδ∥2 ≤2 ∥x∥2. Then use
Parseval’s Theorem to treat the case Wδ < 1/2.
Exercise 6.12 (The Derivative of Bandlimited Signals).
(i) Let the signal x be given by
x(t) =
 W
−W
g(f) ei2πft df,
t ∈R,
where g is integrable. Show that x is diﬀerentiable and that
dx(t)
dt
=
 W
−W
i2πf g(f) ei2πft df,
t ∈R.
(ii) Conclude that if x is an energy-limited signal that is bandlimited to W Hz, then so
is its derivative.
Hint: Mathematically inclined readers can prove Part (i) using the Dominated Conver-
gence Theorem and Exercise 2.7.
Exercise 6.13 (The Gram-Schmidt Procedure and Bandwidth). Let U be a d-dimensional
linear subspace of L2 that does not contain any zero-energy signals other than the all-zero
signal. Let (u1, . . . , ud) be a basis for U, where u1, . . . , ud are all energy-limited signals
that are bandlimited to W Hz (with some of the signals possibly being of bandwidth
smaller than W).
The d-tuple (u1, . . . , ud) is fed to the Gram-Schmidt Procedure to
produce the orthonormal basis (φ1, . . . , φd).
(i) Prove that φ1, . . . , φd are energy-limited signals that are bandlimited to W Hz.
102
The Frequency Response of Filters and Bandlimited Signals
(ii) How does the largest of the bandwidths of u1, . . . , ud compare with that of the
bandwidths of φ1, . . . , φd?
Exercise 6.14 (Passive Filters). Let h be the impulse response of a stable ﬁlter. Show
that the condition that “for every x ∈L2 the energy in x ⋆h does not exceed the energy
in x” is equivalent to the condition
ˆh(f)
 ≤1,
f ∈R.
Exercise 6.15 (Real and Imaginary Parts of Bandlimited Signals). Show that if x(·) is an
integrable signal that is bandlimited to W Hz, then so are its real and imaginary parts.
Exercise 6.16 (Inner Products and Filtering). Let x be an energy-limited signal that is
bandlimited to W Hz. Show that
⟨x, y⟩=

x, y ⋆LPFW

,
y ∈L2.
Exercise 6.17 (Squaring a Signal). Show that if x is an energy-limited signal that is
bandlimited to W Hz, then t →x2(t) is an integrable signal that is bandlimited to 2W
Hz.
Exercise 6.18 (Squared sinc(·)). Find the FT and IFT of the mapping t →sinc2(t).
Exercise 6.19 (A Stable Filter). Show that the IFT of the function
g0 : f →
⎧
⎪
⎨
⎪
⎩
1
if |f| ≤a
b−|f|
b−a
if a < |f| < b
0
otherwise
is given by
ˇg0 : t →
1
(πt)2
cos(2πat) −cos(2πbt)
2(b −a)
and that this signal is integrable. Here b > a > 0.
Exercise 6.20 (Filtering Integrable Signals). Prove that feeding an integrable signal to
an ideal unit-gain lowpass ﬁlter need not produce an integrable signal.
Hint: Construct an example using ˇg0 of Exercise 6.19.
Exercise 6.21 (Multiplying Bandlimited Signals by a Carrier). Let x be an integrable
signal that is bandlimited to W Hz.
(i) Show that if fc > W, then
 ∞
−∞
x(t) cos(2πfct) dt =
 ∞
−∞
x(t) sin(2πfct) dt = 0.
(ii) Show that if fc > W/2, then
 ∞
−∞
x(t) cos2(2πfct) dt = 1
2
 ∞
−∞
x(t) dt.
6.12 Exercises
103
Exercise 6.22 (An Identity). Prove that for every W ∈R
sinc(2Wt) cos(2πWt) = sinc(4Wt),
t ∈R.
Illustrate the identity in the frequency domain.
Exercise 6.23 (Filtering a Periodic Signal). Let x be a bounded periodic signal with period
T > 0, and let h be integrable. Show that the η-th Fourier Series Coeﬃcient of x ⋆h with
respect to the interval [−T/2, T/2] (Note A.3.5) is the product of the corresponding η-th
Fourier Series Coeﬃcient of x and ˆh(η/T).
Exercise 6.24 (A Superposition of Time Shifts of a Pulse). Let g be an integrable signal
of bandwidth W Hz. Let cm, . . . , cn be complex numbers that are not all zero, where
m and n are integers satisfying n ≥m.
Prove that, for every positive Ts, the signal
x: t →n
ℓ=m cℓg(t −ℓTs) is also an integrable signal of bandwidth W Hz.
Hint: There can be at most a ﬁnite number of frequencies where ˆx is zero but ˆg is not.
Exercise 6.25 (Picket Fences). If you are familiar with Dirac’s Delta, explain how (6.88)
is related to the heuristic statement that the FT of 
j∈Z δ(t+jTs) is T−1
s

η∈Z δ(f +η/Ts).
Exercise 6.26 (Bounding the Derivative). Show that if x is an energy-limited signal that
is bandlimited to W Hz, then its time-t derivative satisﬁes

dx(t)
dt
 ≤

8
3 π W 3/2 ∥x∥2 ,
t ∈R.
Hint: Use Proposition 6.4.5 and the Cauchy-Schwarz Inequality
Exercise 6.27 (Another Notion of Bandwidth). Let U denote the set of all energy-limited
signals u such that at least 90% of the energy of u is contained in the band [−W, W ].
Is U a linear subspace of L2?
Chapter 7
Passband Signals and Their Representation
7.1
Introduction
The signals encountered in wireless communications are typically real passband
signals. In this chapter we shall deﬁne such signals and deﬁne their bandwidth
around a carrier frequency. We shall then explain how such signals can be rep-
resented using their complex baseband representation. We shall emphasize two
relationships: that between the energy in the passband signal and in its baseband
representation, and that between the bandwidth of the passband signal around the
carrier frequency and the bandwidth of its baseband representation. We ask the
reader to pay special attention to the fact that only real passband signals have a
baseband representation.
Most of the chapter deals with the family of integrable passband signals. As we
shall see in Corollary 7.2.4, an integrable passband signal must have ﬁnite energy,
and this family is thus a subset of the family of energy-limited passband signals.
Restricting ourselves to integrable signals—while reducing the generality of some of
the results—simpliﬁes the exposition because we can discuss the Fourier Transform
without having to resort to the L2-Fourier Transform, which requires all statements
to be phrased in terms of equivalence classes. But most of the derived results will
also hold for the more general family of energy-limited passband signals with only
slight modiﬁcations. The required modiﬁcations are discussed in Section 7.7.
7.2
Baseband and Passband Signals
Integrable signals that are bandlimited to W Hz were deﬁned in Deﬁnition 6.4.9. By
Proposition 6.4.10, an integrable signal x is bandlimited to W Hz if it is continuous
and if its FT is zero for all frequencies outside the band [−W, W ]. The bandwidth
of x is the smallest W to which it is bandlimited (Deﬁnition 6.4.13). As an example,
Figure 7.1 depicts the FT ˆx of a real signal x, which is bandlimited to W Hz.
Since the signal x in this example is real, its FT is conjugate-symmetric, (i.e.,
ˆx(−f) = ˆx∗(f) for all frequencies f ∈R). Thus, the magnitude of ˆx is symmetric
(even), i.e., |ˆx(f)| = |ˆx(−f)|, but its phase is anti-symmetric (odd). In the ﬁgure
dashed lines indicate this conjugate symmetry.
Consider now the real signal y whose FT ˆy is depicted in Figure 7.2. Again, since
104
7.2 Baseband and Passband Signals
105
W
f
W
−W
ˆx(f)
Figure 7.1: The FT ˆx of a real bandwidth-W baseband signal x.
W
f
fc
−fc
fc + W
2
fc −W
2
ˆy(f)
Figure 7.2: The FT ˆy of a real passband signal y that is bandlimited to W Hz
around the carrier frequency fc.
the signal is real, its FT is conjugate-symmetric, and hence the dashed lines. This
signal (if continuous) is bandlimited to fc +W/2 Hz. But note that ˆy(f) = 0 for all
frequencies f in the interval |f| < fc−W/2. Signals such as y are often encountered
in wireless communications, because in a wireless channel the very-low frequencies
often suﬀer severe attenuation and are therefore seldom used.
Another reason
is the concurrent use of the wireless spectrum by many systems. If all systems
transmitted in the same frequency band, they would interfere with each other.
Consequently, diﬀerent systems are often assigned diﬀerent carrier frequencies so
that their transmitted signals will not overlap in frequency. This is why diﬀerent
radio stations transmit around diﬀerent carrier frequencies.
7.2.1
Deﬁnition and Characterization
To describe signals such as y we use the following deﬁnition for passband signals.
We ask the reader to recall the deﬁnition of the impulse response BPFW,fc(·) (see
(5.21)) and of the frequency response 
BPFW,fc(·) (see (6.41)) of the ideal unit-gain
bandpass ﬁlter of bandwidth W around the carrier frequency fc.
106
Passband Signals and Their Representation
Deﬁnition 7.2.1 (A Passband Signal). A signal xPB is said to be an integrable
passband signal that is bandlimited to W Hz around the carrier fre-
quency fc if it is integrable
xPB ∈L1;
(7.1a)
the carrier frequency fc satisﬁes
fc > W
2 > 0;
(7.1b)
and if xPB is unaltered when it is fed to an ideal unit-gain bandpass ﬁlter of band-
width W around the carrier frequency fc
xPB(t) =

xPB ⋆BPFW,fc

(t),
t ∈R.
(7.1c)
An energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc is analogously deﬁned but with (7.1a) replaced by the
condition
xPB ∈L2.
(7.1a’)
(That the convolution in (7.1c) is deﬁned at every t ∈R whenever xPB is integrable
can be shown using Proposition 6.2.5 because BPFW,fc is the Inverse Fourier Trans-
form of the integrable function f 	→I
|f| −fc
 ≤W/2

. That the convolution is
deﬁned at every t ∈R also when xPB is of ﬁnite energy can be shown by noting
that BPFW,fc is of ﬁnite energy, and the convolution of two ﬁnite-energy signals is
deﬁned at every time t ∈R; see Section 5.5.)
In analogy to Proposition 6.4.10 we have the following characterization:
Proposition 7.2.2 (Characterizing Integrable Passband Signals). Let fc and W
satisfy fc > W/2 > 0. If xPB is an integrable signal, then each of the following
statements is equivalent to the statement that xPB is an integrable passband signal
that is bandlimited to W Hz around the carrier frequency fc.
(a) The signal xPB is unaltered when it is bandpass ﬁltered:
xPB(t) =

xPB ⋆BPFW,fc

(t),
t ∈R.
(7.2)
(b) The signal xPB can be expressed as
xPB(t) =

||f|−fc|≤W/2
ˆxPB(f) ei2πft df,
t ∈R.
(7.3)
(c) The signal xPB is continuous and
ˆxPB(f) = 0,
|f| −fc
 > W
2 .
(7.4)
(d) There exists an integrable function g such that
xPB(t) =

||f|−fc|≤W/2
g(f) ei2πft df,
t ∈R.
(7.5)
Proof. The proof is similar to the proof of Proposition 6.4.10 and is omitted.
7.3 Bandwidth around a Carrier Frequency
107
7.2.2
Important Properties
By comparing (7.4) with (6.62) we obtain:
Corollary 7.2.3 (Integrable Passband Signals Are Bandlimited). If xPB is an
integrable passband signal that is bandlimited to W Hz around the carrier frequency
fc, then it is an integrable signal that is bandlimited to fc + W/2 Hz.
Using Corollary 7.2.3 and Note 6.4.12 we obtain:
Corollary 7.2.4 (Integrable Passband Signals Are of Finite Energy). Any inte-
grable passband signal that is bandlimited to W Hz around the carrier frequency fc
is of ﬁnite energy.
Proposition 7.2.5 (Integrable Passband Signals through Stable Filters). If xPB
is an integrable passband signal that is bandlimited to W Hz around the carrier
frequency fc, and if h ∈L1 is the impulse response of a stable ﬁlter, then the
convolution xPB ⋆h is deﬁned at every epoch; it is an integrable passband signal
that is bandlimited to W Hz around the carrier frequency fc; and its FT is the
mapping f 	→ˆxPB(f) ˆh(f).
Proof. The proof is similar to the proof of the analogous result for bandlimited
signals (Proposition 6.5.2) and is omitted.
7.3
Bandwidth around a Carrier Frequency
Deﬁnition 7.3.1 (The Bandwidth around a Carrier Frequency). The bandwidth
around the carrier fc of an integrable or energy-limited passband signal xPB is
the smallest W for which both (7.1b) and (7.1c) hold.
Note 7.3.2 (The Carrier Frequency Is Critical). The bandwidth of xPB around
the carrier frequency fc is determined not only by the FT of xPB but also by fc.
For example, the real passband signal whose FT is depicted in Figure 7.3 is of
bandwidth W around the carrier frequency fc, but its bandwidth is smaller around
a slightly higher carrier frequency.
At ﬁrst it may seem that the deﬁnition of bandwidth for passband signals is incon-
sistent with the deﬁnition for baseband signals. This, however, is not the case. A
good way to remember the deﬁnitions is to focus on real signals. For such signals
the bandwidth for both baseband and passband signals is deﬁned as the length of
an interval of positive frequencies where the FT of the signal may be nonzero. For
baseband signals the bandwidth is the length of the smallest interval of positive
frequencies of the form [0, W] containing all positive frequencies where the FT may
be nonzero. For passband signals it is the length of the smallest interval of positive
frequencies that is symmetric around the carrier frequency fc and that contains
all positive frequencies where the signal may be nonzero. (For complex signals we
have to allow for the fact that the zeros of the FT may not be symmetric sets
around the origin.) See also Figures 6.2 and 6.3.
108
Passband Signals and Their Representation
W
W
W
−W
f
f
W
fc
−fc
fc + W
2
fc −W
2
Figure 7.3: The FT of a complex baseband signal of bandwidth W Hz (above)
and of a real passband signal of bandwidth W Hz around the carrier frequency fc
(below).
We draw the reader’s attention to an important consequence of our deﬁnition of
bandwidth:
Proposition 7.3.3 (Multiplication by a Carrier Doubles the Bandwidth). If x is
an integrable signal of bandwidth W Hz and if fc > W, then t 	→x(t) cos(2πfct) is
an integrable passband signal of bandwidth 2W around the carrier frequency fc.
Proof. Deﬁne y: t 	→x(t) cos(2πfct). The proposition is a straightforward conse-
quence of the deﬁnition of the bandwidth of x (Deﬁnition 6.4.13); the deﬁnition of
the bandwidth of y around the carrier frequency fc (Deﬁnition 7.3.1); and the fact
that if x is an integrable signal of FT ˆx, then y is an integrable signal of FT
ˆy(f) = 1
2

ˆx(f −fc) + ˆx(f + fc)

,
f ∈R,
(7.6)
where (7.6) follows from the calculation
ˆy(f) =
 ∞
−∞
y(t) e−i2πft dt
=
 ∞
−∞
x(t) cos(2πfct) e−i2πft dt
7.3 Bandwidth around a Carrier Frequency
109
W
f
W
−W
1
ˆx(f)
Figure 7.4: The FT of a complex baseband bandwidth-W signal x.
2W
f
fc
fc + W
fc −W
1
2
ˆy(f)
Figure 7.5: The FT of y: t 	→x(t) cos (2πfct), where ˆx is as depicted in Figure 7.4.
Note that x is of bandwidth W and that y is of bandwidth 2W around the carrier
frequency fc.
=
 ∞
−∞
x(t) ei2πfct + e−i2πfct
2
e−i2πft dt
= 1
2
 ∞
−∞
x(t) e−i2π(f−fc)t dt + 1
2
 ∞
−∞
x(t) e−i2π(f+fc)t dt
= 1
2

ˆx(f −fc) + ˆx(f + fc)

,
f ∈R.
As an illustration of the relation (7.6) note that if x is the complex bandwidth-W
signal whose FT is depicted in Figure 7.4, then the signal y: t 	→x(t) cos(2πfct) is
the complex passband signal of bandwidth 2W around fc whose FT is depicted in
Figure 7.5.
Similarly, if x is the real baseband signal of bandwidth W whose FT is depicted
in Figure 7.6, then y: t 	→x(t) cos(2πfct) is the real passband signal of bandwidth
2W around fc whose FT is depicted in Figure 7.7.
In wireless applications the bandwidth W of the signals around the carrier frequency
is typically much smaller than the carrier frequency fc, but for most of our results
110
Passband Signals and Their Representation
W
f
W
−W
1
ˆx(f)
Figure 7.6: The FT of a real baseband bandwidth-W signal x.
2W
f
fc
fc + W
fc −W
1
2
ˆy(f)
Figure 7.7: The FT of y: t 	→x(t) cos (2πfct), where ˆx is as depicted in Figure 7.6.
Here x is of bandwidth W and y is of bandwidth 2W around the carrier frequency
fc.
it suﬃces that (7.1b) hold.
The notion of a passband signal is also applied somewhat loosely in instances where
the signals are not bandlimited. Engineers say that an energy-limited signal is a
passband signal around the carrier frequency fc if most of its energy is contained
in frequencies that are close to fc and −fc. Notice that in this “deﬁnition” we are
relying heavily on Parseval’s theorem. I.e., we think about the energy ∥x∥2
2 of x as
being computed in the frequency domain, i.e., by computing ∥ˆx∥2
2 =

|ˆx(f)|2 df.
By “most of the energy is contained in frequencies that are close to fc and −fc”
we thus mean that most of the contributions to this integral come from small
frequency intervals around fc and −fc. In other words, we say that x is a passband
signal whose energy is mostly concentrated in a bandwidth W around the carrier
frequency fc if
 ∞
−∞
|ˆx(f)|2 df ≈

||f|−fc|≤W/2
|ˆx(f)|2 df.
(7.7)
Similarly, a signal is approximately a baseband signal that is bandlimited to W Hz
7.4 Real Passband Signals
111
if
 ∞
−∞
|ˆx(f)|2 df ≈
 W
−W
|ˆx(f)|2 df.
(7.8)
7.4
Real Passband Signals
Before discussing the baseband representation of real passband signals we empha-
size the following.
(i) The passband signals transmitted and received in Digital Communications
are real.
(ii) Only real passband signals have a baseband representation.
(iii) The baseband representation of a real passband signal is typically a complex
signal.
(iv) While the FT of real signals is conjugate-symmetric (6.3), this does not imply
any symmetry with respect to the carrier frequency. Thus—although not
symmetric around fc—the FT depicted in Figure 7.2 is that of a real passband
signal that is bandlimited to W Hz around fc.
We also note that if x is a real integrable signal, then its FT must be conjugate-
symmetric. But if g ∈L1 is such that its IFT ˇg is real, it does not follow that g
must be conjugate-symmetric.
For example, the conjugate symmetry could be
broken on a set of frequencies of Lebesgue measure zero, a set that does not inﬂu-
ence the IFT. As the next proposition shows, this is the only way the conjugate
symmetry can be broken.
Proposition 7.4.1. If x is a real signal and if x = ˇg for some integrable function
g: f 	→g(f), then:
(i) The signal x can be represented as the IFT of a conjugate-symmetric inte-
grable function.
(ii) The function g and the conjugate-symmetric function f 	→

g(f)+g∗(−f)

/2
agree except on a set of frequencies of Lebesgue measure zero.
Proof. (i) Since x is real and since x = ˇg it follows that
x(t) = Re

x(t)

= 1
2x(t) + 1
2x∗(t)
= 1
2
 ∞
−∞
g(f) ei2πft df + 1
2
	 ∞
−∞
g(f) ei2πft df

∗
= 1
2
 ∞
−∞
g(f) ei2πft df + 1
2
 ∞
−∞
g∗(f) e−i2πft df
= 1
2
 ∞
−∞
g(f) ei2πft df + 1
2
 ∞
−∞
g∗(−˜f) ei2π ˜
ft d ˜f
112
Passband Signals and Their Representation
=
 ∞
−∞
g(f) + g∗(−f)
2
ei2πft df,
t ∈R,
where the ﬁrst equality follows from the hypothesis that x is a real signal; the second
because for any z ∈C we have Re(z) = (z + z∗)/2; the third by the hypothesis
that x = ˇg; the fourth because conjugating a complex integral is tantamount
to conjugating the integrand (Proposition 2.3.1 (ii)); the ﬁfth by changing the
integration variable in the second integral to ˜f ≜−f; and the sixth by combining
the integrals. Thus, x is the IFT of the conjugate-symmetric function deﬁned by
f 	→

g(f) + g∗(−f)

/2, and (i) is established.
As to (ii), since x is the IFT of both g and f 	→

g(f) + g∗(−f)

/2, it follows from
the IFT analog of Theorem 6.2.12 that the two agree outside a set of frequencies
of Lebesgue measure zero.
7.5
The Analytic Signal
In this section we shall deﬁne the analytic representation of a real passband
signal.
This is also sometimes called the analytic signal associated with the
signal. We shall use the two terms interchangeably. The analytic representation
will serve as a steppingstone to the baseband representation, which is extremely
important in Digital Communications. We emphasize that an analytic signal can
only be associated with a real passband signal. The analytic signal itself, however,
is complex-valued.
7.5.1
Deﬁnition and Characterization
Let xPB be a real integrable passband signal that is bandlimited to W Hz around
the carrier frequency fc. We would have liked to deﬁne its analytic representation
as the complex signal xA whose FT is the mapping
f 	→ˆxPB(f) I{f ≥0},
(7.9)
i.e., as the integrable signal whose FT is equal to zero at negative frequencies and to
ˆxPB(f) at nonnegative frequencies. While this is often the way we think about xA,
there are two problems with this deﬁnition: an existence problem and a uniqueness
problem. It is not prima facie clear that there exists an integrable signal whose FT
is the mapping (7.9). (We shall soon see that there does.) And, since two signals
that diﬀer on a set of Lebesgue measure zero have identical Fourier Transforms, the
above deﬁnition would not fully specify xA. This could be remedied by insisting
that xA be continuous, but this would further exacerbate the existence issue. (We
shall see that there does exist a unique integrable continuous signal whose FT is
the mapping (7.9), but this requires proof.) Our approach is to deﬁne xA as the
IFT of the mapping (7.9) and to then explore the properties of xA.
Deﬁnition 7.5.1 (Analytic Representation of a Real Passband Signal). The an-
alytic representation of a real integrable passband signal xPB that is bandlimited
to W Hz around the carrier frequency fc is the complex signal xA deﬁned by
xA(t) ≜
 ∞
0
ˆxPB(f) ei2πft df,
t ∈R.
(7.10)
7.5 The Analytic Signal
113
Note that, by Proposition 7.2.2, ˆxPB(f) vanishes at frequencies f that satisfy
|f| −fc
 > W/2, so we can also write (7.10) as
xA(t) =
 fc+ W
2
fc−W
2
ˆxPB(f) ei2πft df,
t ∈R.
(7.11)
This latter expression has the advantage that it makes it clear that the integral is
well-deﬁned for every t ∈R, because the integrability of xPB implies that the inte-
grand is bounded, i.e., that |ˆxPB(f)| ≤∥xPB∥1 for every f ∈R (Theorem 6.2.11)
and hence that the mapping f 	→ˆxPB(f) I{|f −fc| ≤W/2} is integrable.
Also note that our deﬁnition of the analytic signal may be oﬀby a factor of two
or
√
2 from the one used in some textbooks. (Some textbooks introduce a factor
of
√
2 in order to make the energy in the analytic signal equal that in the passband
signal. We do not do so and hence end up with a factor of two in (7.24) ahead.)
We next show that the analytic signal xA is a continuous and integrable signal and
that its FT is given by the mapping (7.9). In fact, we prove more.
Proposition 7.5.2 (Characterizations of the Analytic Signal). Let xPB be a real
integrable passband signal that is bandlimited to W Hz around the carrier fre-
quency fc. Then each of the following statements is equivalent to the statement
that the complex-valued signal xA is its analytic representation.
(a) The signal xA is given by
xA(t) =
 fc+ W
2
fc−W
2
ˆxPB(f) ei2πft df,
t ∈R.
(7.12)
(b) The signal xA is a continuous integrable signal satisfying
ˆxA(f) =

ˆxPB(f)
if f ≥0,
0
otherwise.
(7.13)
(c) The signal xA is an integrable passband signal that is bandlimited to W Hz
around the carrier frequency fc and that satisﬁes (7.13).
(d) The signal xA is given by
xA = xPB ⋆ˇg
(7.14a)
for every integrable mapping g: f 	→g(f) satisfying
g(f) = 1,
f −fc
 ≤W
2 ,
(7.14b)
and
g(f) = 0,
f + fc
 ≤W
2
(7.14c)
(with g(f) unspeciﬁed at other frequencies).
114
Passband Signals and Their Representation
Proof. That Condition (a) is equivalent to the statement that xA is the analytic
representation of xPB is just a restatement of Deﬁnition 7.5.1. It thus only remains
to show that Conditions (a), (b), (c), and (d) are equivalent. We shall do so by
establishing that (a) ⇔(d); that (b) ⇔(c); that (b) ⇒(a); and that (d) ⇒(c).
To establish (a) ⇔(d), let g: f 	→g(f) be any integrable function satisfying (7.14b)
and (7.14c). Since both xPB and g are integrable, we can compute xPB ⋆ˇg using
Proposition 6.2.5:

xPB ⋆ˇg

(t) =
 ∞
−∞
ˆxPB(f) g(f) ei2πft df
=
 ∞
0
ˆxPB(f) g(f) ei2πft df
=
 fc+ W
2
fc−W
2
ˆxPB(f) g(f) ei2πft df
=
 fc+ W
2
fc−W
2
ˆxPB(f) ei2πft df,
t ∈R,
(7.15)
where the ﬁrst equality follows from Proposition 6.2.5; the second because the
assumption that xPB is a passband signal implies, by Proposition 7.2.2 (cf. (c)),
that the only negative frequencies f < 0 where ˆxPB(f) can be nonzero are those
satisfying | −f −fc| ≤W/2, and at those frequencies g is zero by (7.14c); the
third by Proposition 7.2.2 (cf. (c)); and the fourth equality by (7.14b). The LHS
of (7.15) is thus equal to xA(t) if, and only if, the RHS is equal to xA(t). This
establishes that (a) ⇔(d).
The equivalence (b) ⇔(c) is an immediate consequence of Proposition 7.2.2. That
(b) ⇒(a) can be proved using Corollary 6.2.14 as follows. If (b) holds, then xA
is a continuous integrable signal whose FT is given by the integrable function on
the RHS of (7.13) and therefore, by Corollary 6.2.14, xA is the IFT of the RHS of
(7.13), thus establishing (a).
We now complete the proof by showing that (d) ⇒(c). To this end let g: f 	→g(f)
be a continuous integrable function satisfying (7.14b) & (7.14c) and additionally
satisfying that its IFT ˇg is integrable. For example, g could be the function
g(f) =
⎧
⎪
⎨
⎪
⎩
1
if |f −fc| ≤W/2,
0
if |f −fc| ≥Wc/2,
Wc−2|f−fc|
Wc−W
otherwise,
(7.16)
where Wc can be chosen arbitrarily in the range
W < Wc < 2fc.
(7.17)
This function is depicted in Figure 7.8. By direct calculation, it can be shown that
its IFT is given by1
ˇg(t) = ei2πfct
1
(πt)2
cos(π W t) −cos(π Wc t)
Wc −W
,
t ∈R,
(7.18)
1At t = 0, the RHS of (7.18) should be interpreted as (W + Wc)/2.
7.5 The Analytic Signal
115
Wc
f
W
fc
1
g(f)
Figure 7.8: The function g of (7.16), which is used in the proof of Proposition 7.5.2.
which is integrable. Deﬁne h = ˇg, and note that, by Corollary 6.2.14 (ii), ˆh = g.
If (d) holds, then
xA = xPB ⋆ˇg
= xPB ⋆h,
so xA is the result of feeding an integrable passband signal that is bandlimited
to W Hz around the carrier frequency fc (the signal xPB) through a stable ﬁlter
(of impulse response h). Consequently, by Proposition 7.2.5, xA is an integrable
passband signal that is bandlimited to W Hz around the carrier frequency fc and
its FT is given by f 	→ˆxPB(f) ˆh(f). Hence, as we next justify,
ˆxA(f) = ˆxPB(f) ˆh(f)
= ˆxPB(f) g(f)
= ˆxPB(f) g(f) I{f ≥0}
= ˆxPB(f) I{f ≥0},
f ∈R,
thus establishing (c). Here the third equality is justiﬁed by noting that the as-
sumption that xPB is a passband signal implies, by Proposition 7.2.2 (cf. (c)),
that the only negative frequencies f < 0 where ˆxPB(f) can be nonzero are those
satisfying |−f −fc| ≤W/2, and at those frequencies g is zero by (7.16), (7.17),
and (7.1b). The fourth equality follows by noting that the assumption that xPB
is a passband signal implies, by Proposition 7.2.2 (cf. (c)), that the only positive
frequencies f > 0 where ˆxPB(f) can be nonzero are those satisfying |f −fc| ≤W/2
and at those frequencies g(f) = 1 by (7.16).
7.5.2
From xA back to xPB
Proposition 7.5.2 describes the analytic representation xA in terms of the real
passband signal xPB. This representation would have been useless if we had not
been able to recover xPB from xA. Fortunately, we can. The key is that, because
xPB is real, its FT is conjugate-symmetric
ˆxPB(−f) = ˆx∗
PB(f),
f ∈R.
(7.19)
116
Passband Signals and Their Representation
Consequently, since the FT of xA is equal to that of xPB at the positive frequencies
and to zero at the negative frequencies (7.13), we can add to ˆxA its conjugated
mirror-image to obtain ˆxPB:
ˆxPB(f) = ˆxA(f) + ˆx∗
A(−f),
f ∈R;
(7.20)
see Figure 7.9 on Page 121. From here it is just a technicality to obtain the time-
domain relationship
xPB(t) = 2 Re

xA(t)

,
t ∈R.
(7.21)
These results are summarized in the following proposition.
Proposition 7.5.3 (Recovering xPB from xA). Let xPB be a real integrable pass-
band signal that is bandlimited to W Hz around the carrier frequency fc, and let xA
be its analytic representation. Then,
ˆxPB(f) = ˆxA(f) + ˆx∗
A(−f),
f ∈R,
(7.22a)
and
xPB(t) = 2 Re

xA(t)

,
t ∈R.
(7.22b)
Proof. The frequency relation (7.22a) is just a restatement of (7.20), whose deriva-
tion was rigorous. To prove (7.22b) we note that, by Proposition 7.2.2 (cf. (b) &
(c)),
xPB(t) =
 ∞
−∞
ˆxPB(f) ei2πft df
=
 ∞
0
ˆxPB(f) ei2πft df +
 0
−∞
ˆxPB(f) ei2πft df
= xA(t) +
 0
−∞
ˆxPB(f) ei2πft df
= xA(t) +
 ∞
0
ˆxPB(−˜f) e−i2π ˜
ft d ˜f
= xA(t) +
 ∞
0
ˆx∗
PB( ˜f) e−i2π ˜
ft d ˜f
= xA(t) +
	 ∞
0
ˆxPB( ˜f) ei2π ˜
ft d ˜f

∗
= xA(t) + x∗
A(t)
= 2 Re

xA(t)

,
t ∈R,
where in the second equality we broke the integral into two; in the third we used
Deﬁnition 7.5.1; in the fourth we changed the integration variable to ˜f ≜−f;
in the ﬁfth we used the conjugate symmetry of ˆxPB (7.19); in the sixth we used
the fact that conjugating the integrand results in the conjugation of the integral
(Proposition 2.3.1); in the seventh we used the deﬁnition of the analytic signal;
and in the last equality we used the fact that a complex number and its conjugate
add up to twice its real part.
7.5 The Analytic Signal
117
7.5.3
Relating ⟨xPB, yPB⟩to ⟨xA, yA⟩
We next relate the inner product between two real passband signals to the inner
product between their analytic representations.
Proposition 7.5.4 (⟨xPB, yPB⟩and ⟨xA, yA⟩). Let xPB and yPB be real integrable
passband signals that are bandlimited to W Hz around the carrier frequency fc, and
let xA and yA be their analytic representations. Then
⟨xPB, yPB⟩= 2 Re

⟨xA, yA⟩

,
(7.23)
and
∥xPB∥2
2 = 2 ∥xA∥2
2 .
(7.24)
Note that in (7.23) the inner product appearing on the LHS is the inner product
between real signals whereas the one appearing on the RHS is between complex
signals.
Proof. We ﬁrst note that the inner products and energies are well-deﬁned because
integrable passband signals are also energy-limited (Corollary 7.2.4). Next, even
though (7.24) is a special case of (7.23), we ﬁrst prove (7.24). The proof is a simple
application of Parseval’s Theorem. The intuition is as follows. Since xPB is real,
it follows that its FT is conjugate-symmetric (7.19) so the magnitude of ˆxPB is
symmetric. Consequently, the positive frequencies and the negative frequencies
of ˆxPB contribute an equal share to the total energy in ˆxPB. And since the energy
in the analytic representation is equal to the share corresponding to the positive
frequencies only, its energy must be half the energy of xPB.
This can be argued more formally as follows. Because xPB is real-valued, its FT ˆxPB
is conjugate-symmetric (7.19), so its magnitude is symmetric |ˆxPB(f)| = |ˆxPB(−f)|
for all f ∈R and, a fortiori,
 ∞
0
|ˆxPB(f)|2 df =
 0
−∞
|ˆxPB(f)|2 df.
(7.25)
Also, by Parseval’s Theorem (applied to xPB),
 ∞
0
|ˆxPB(f)|2 df +
 0
−∞
|ˆxPB(f)|2 df = ∥xPB∥2
2 .
(7.26)
Consequently, by combining (7.25) and (7.26), we obtain
 ∞
0
|ˆxPB(f)|2 df = 1
2 ∥xPB∥2
2 .
(7.27)
We can now establish (7.24) from (7.27) by using Parseval’s Theorem (applied
to xA) and (7.13) to obtain
∥xA∥2
2 = ∥ˆxA∥2
2
=
 ∞
−∞
|ˆxA(f)|2 df
118
Passband Signals and Their Representation
=
 ∞
0
|ˆxPB(f)|2 df
= 1
2 ∥xPB∥2
2 ,
where the last equality follows from (7.27).
We next prove (7.23). We oﬀer two proofs. The ﬁrst is very similar to our proof
of (7.24): we use Parseval’s Theorem to express the inner products in the fre-
quency domain, and then argue that the contribution of the negative frequencies
to the inner product is the complex conjugate of the contribution of the positive
frequencies. The second proof uses a trick to relate inner products and energies.
We begin with the ﬁrst proof. Using Proposition 7.5.3 we have
ˆxPB(f) = ˆxA(f) + ˆx∗
A(−f),
f ∈R,
ˆyPB(f) = ˆyA(f) + ˆy∗
A(−f),
f ∈R.
Using Parseval’s Theorem we now have
⟨xPB, yPB⟩= ⟨ˆxPB, ˆyPB⟩
=
 ∞
−∞
ˆxPB(f)ˆy∗
PB(f) df
=
 ∞
−∞

ˆxA(f) + ˆx∗
A(−f)

ˆyA(f) + ˆy∗
A(−f)
∗
df
=
 ∞
−∞

ˆxA(f) + ˆx∗
A(−f)

ˆy∗
A(f) + ˆyA(−f)

df
=
 ∞
−∞
ˆxA(f) ˆy∗
A(f) df +
 ∞
−∞
ˆx∗
A(−f) ˆyA(−f) df
=
 ∞
−∞
ˆxA(f) ˆy∗
A(f) df +
	 ∞
−∞
ˆxA(−f) ˆy∗
A(−f) df

∗
=
 ∞
−∞
ˆxA(f) ˆy∗
A(f) df +
	 ∞
−∞
ˆxA( ˜f) ˆy∗
A( ˜f) d ˜f

∗
= ⟨ˆxA, ˆyA⟩+ ⟨ˆxA, ˆyA⟩∗
= 2 Re

⟨ˆxA, ˆyA⟩

= 2 Re

⟨xA, yA⟩

,
where the ﬁfth equality follows because at all frequencies f ∈R the cross-terms
ˆxA(f) ˆyA(−f) and ˆx∗
A(−f) ˆy∗
A(f) are zero, and where the last equality follows from
Parseval’s Theorem.
The second proof is based on (7.24) and on the identity
2 Re

⟨u, v⟩

= ∥u + v∥2
2 −∥u∥2
2 −∥v∥2
2 ,
u, v ∈L2,
(7.28)
which holds for both complex and real signals and which follows by expressing
∥u + v∥2
2 as
∥u + v∥2
2 = ⟨u + v, u + v⟩
7.6 Baseband Representation of Real Passband Signals
119
= ⟨u, u⟩+ ⟨u, v⟩+ ⟨v, u⟩+ ⟨v, v⟩
= ∥u∥2
2 + ∥v∥2
2 + ⟨u, v⟩+ ⟨u, v⟩∗
= ∥u∥2
2 + ∥v∥2
2 + 2 Re

⟨u, v⟩

.
From Identity (7.28) and from (7.24) we have for the real signals xPB and yPB
2⟨xPB, yPB⟩= 2 Re

⟨xPB, yPB⟩

= ∥xPB + yPB∥2
2 −∥xPB∥2
2 −∥yPB∥2
2
= 2

∥xA + yA∥2
2 −∥xA∥2
2 −∥yA∥2
2

= 4 Re

⟨xA, yA⟩

,
where the ﬁrst equality follows because the passband signals are real; the second
from Identity (7.28) applied to the passband signals xPB and yPB; the third from
the second part of Proposition 7.5.4 and because the analytic representation of
xPB + yPB is xA + yA; and the ﬁnal equality from Identity (7.28) applied to the
analytic signals xA and yA.
7.6
Baseband Representation of Real Passband Signals
Strictly speaking, the baseband representation xBB of a real passband sig-
nal xPB is not a “representation” because one cannot recover xPB from xBB alone;
one also needs to know the carrier frequency fc. This may seem like a disadvantage,
but engineers view this as an advantage. Indeed, in some cases, it may illuminate
the fact that certain operations and results do not depend on the carrier frequency.
This decoupling of various operations from the carrier frequency is very useful in
hardware implementation of communication systems that need to work around
selectable carrier frequencies. It allows for some of the processing to be done us-
ing carrier-independent hardware and for only a small part of the communication
system to be tunable to the carrier frequency. Very loosely speaking, engineers
think of xBB as everything about xPB that is not carrier-dependent. Thus, one
does not usually expect the quantity fc to show up in a formula for the baseband
representation. Philosophical thoughts aside, the baseband representation has a
straightforward deﬁnition.
7.6.1
Deﬁnition and Characterization
Deﬁnition 7.6.1 (Baseband Representation). The baseband representation of
a real integrable passband signal xPB that is bandlimited to W Hz around the carrier
frequency fc is the complex signal
xBB(t) ≜e−i2πfct xA(t),
t ∈R,
(7.29)
where xA is the analytic representation of xPB.
Note that, by (7.29), the magnitudes of xA and xBB are identical
xBB(t)
 =
xA(t)
,
t ∈R.
(7.30)
120
Passband Signals and Their Representation
Consequently, since xA is integrable we also have:
Proposition 7.6.2 (Integrability of xPB Implies Integrability of xBB). The base-
band representation of a real integrable passband signal that is bandlimited to W
Hz around the carrier frequency fc is integrable.
By (7.29) and (7.13) we obtain that if xPB is a real integrable passband signal that
is bandlimited to W Hz around the carrier frequency fc, then
ˆxBB(f) = ˆxA(f + fc) =

ˆxPB(f + fc)
if |f| ≤W/2,
0
otherwise.
(7.31)
Thus, the FT of xBB is the FT of xA but shifted to the left by the carrier fre-
quency fc. The relationship between the Fourier Transforms of xPB, xA, and xBB
is depicted in Figure 7.9.
We have deﬁned the baseband representation of a passband signal in terms of its
analytic representation, but sometimes it is useful to deﬁne the baseband represen-
tation directly in terms of the passband signal. This is not very diﬃcult. Rather
than taking the passband signal and passing it through a ﬁlter of frequency re-
sponse g satisfying (7.14) to obtain xA and then multiplying the result by e−i2πfct
to obtain xBB, we can multiply xPB by t 	→e−i2πfct and then ﬁlter the result to
obtain the baseband representation. This procedure is depicted in the frequency
domain in Figure 7.10 and is made precise in the following proposition.
Proposition 7.6.3 (From xPB to xBB Directly). If xPB is a real integrable passband
signal that is bandlimited to W Hz around the carrier frequency fc, then its baseband
representation xBB is given by
xBB =

t 	→e−i2πfct xPB(t)

⋆ˇg0,
(7.32a)
where g0 : f 	→g0(f) is any integrable function satisfying
g0(f) = 1,
|f| ≤W
2 ,
(7.32b)
and
g0(f) = 0,
|f + 2fc| ≤W
2 .
(7.32c)
Proof. The proof is all in Figure 7.10. For an analytic proof we provide the fol-
lowing details. By Deﬁnition 7.6.1 and by Proposition 7.5.2 (cf. (d)) we have for
any integrable function g: f 	→g(f) satisfying (7.14b) & (7.14c)
xBB(t) = e−i2πfct
xPB ⋆ˇg

(t)
= e−i2πfct
 ∞
−∞
ˆxPB(f) g(f) ei2πft df
=
 ∞
−∞
ˆxPB(f) g(f) ei2π(f−fc)t df
=
 ∞
−∞
ˆxPB( ˜f + fc) g( ˜f + fc) ei2π ˜
ft d ˜f
7.6 Baseband Representation of Real Passband Signals
121
ˆxPB(f)
f
fc
−fc
ˆxA(f)
f
fc
ˆxBB(f)
f
Figure 7.9: The Fourier Transforms of the analytic signal xA and of the baseband
representation xBB of a real passband signal xPB.
=
 ∞
−∞
ˆxPB( ˜f + fc) g0( ˜f) ei2π ˜
ft d ˜f
=

t 	→e−i2πfct xPB(t)

⋆ˇg0

(t),
where we deﬁned
g0(f) = g(f + fc),
f ∈R,
(7.33)
and where we use the following justiﬁcation.
The second equality follows from
Proposition 6.2.5; the third by pulling the complex exponential into the integral;
the fourth by the deﬁning ˜f ≜f −fc; the ﬁfth by deﬁning the function g0 as in
(7.33); and the ﬁnal equality by Proposition 6.2.5 using the fact that
the FT of t 	→e−i2πfct xPB(t) is f 	→ˆxPB(f + fc).
(7.34)
The proposition now follows by noting that g satisﬁes (7.14b) & (7.14c) if, and
only if, the mapping g0 deﬁned in (7.33) satisﬁes (7.32b) & (7.32c).
122
Passband Signals and Their Representation
ˆxPB(f)
ˆxPB(f + fc)
g0(f)
ˆxBB(f)
W
fc
−fc
−fc
−2fc
W
2
W
2
−W
2
−W
2
1
Wc
−Wc
f
f
f
f
Figure 7.10: A frequency-domain description of the process for deriving xBB di-
rectly from xPB.
From top to bottom: ˆxPB; the FT of t 	→e−i2πfct xPB(t); a
function g0 satisfying (7.32b) & (7.32c); and ˆxBB.
7.6 Baseband Representation of Real Passband Signals
123
Corollary 7.6.4. If xPB is a real integrable passband signal that is bandlimited to W
Hz around the carrier frequency fc, then its baseband representation xBB is given
by
xBB =

t 	→e−i2πfct xPB(t)

⋆LPFWc,
(7.35a)
where the cutoﬀfrequency Wc can be chosen arbitrarily in the range
W
2 ≤Wc ≤2fc −W
2 .
(7.35b)
Proof. Let Wc satisfy (7.35b) and deﬁne g0 as follows: if Wc is strictly smaller
than 2fc−W/2, deﬁne g0(f) = I{|f| ≤Wc}; otherwise deﬁne g0(f) = I{|f| < Wc}.
In both cases g0 satisﬁes (7.32b) & (7.32c) and
ˇg0 = LPFWc .
(7.36)
The result now follows by applying Proposition 7.6.3 with this choice of g0.
In analogy to Proposition 7.5.2, we can characterize the baseband representation
of passband signals as follows.
Proposition 7.6.5 (Characterizing the Baseband Representation). Let xPB be
a real integrable passband signal that is bandlimited to W Hz around the carrier
frequency fc. Then each of the following statements is equivalent to the statement
that the complex signal xBB is its baseband representation.
(a) The signal xBB is given by
xBB(t) =
 W/2
−W/2
ˆxPB(f + fc) ei2πft df,
t ∈R.
(7.37)
(b) The signal xBB is a continuous integrable signal satisfying
ˆxBB(f) = ˆxPB(f + fc) I
'
|f| ≤W
2
(
,
f ∈R.
(7.38)
(c) The signal xBB is an integrable signal that is bandlimited to W/2 Hz and that
satisﬁes (7.38).
(d) The signal xBB is given by (7.32a) for any g0 : f 	→g0(f) satisfying (7.32b)
& (7.32c).
Proof. Parts (a), (b), and (c) can be easily deduced from their counterparts in
Proposition 7.5.2 using Deﬁnition 7.6.1 and the fact that (7.30) implies that the
integrability of xBB is equivalent to the integrability of xA. Part (d) is a restatement
of Proposition 7.6.3.
124
Passband Signals and Their Representation
7.6.2
The In-Phase and Quadrature Components
The convolution in (7.35a) is a convolution between a complex signal (the signal
t 	→e−i2πfctxPB(t)) and a real signal (the signal LPFWc). This should not alarm
you. The convolution of two complex signals evaluated at time t is expressed as an
integral (5.2), and in the case of complex signals this is an integral (over the real
line) of a complex-valued integrand. Such integrals were addressed in Section 2.3.
It should, however, be noted that since the deﬁnition of the convolution of two sig-
nals involves their products, the real part of the convolution of two complex-valued
signals is, in general, not equal to the convolution of their real parts. However, as
we next show, if one of the signals is real—as is the case in (7.35a)—then things
become simpler: if x is a complex-valued function of time and if h is a real-valued
function of time, then
Re

x ⋆h

= Re(x) ⋆h and Im

x ⋆h

= Im(x) ⋆h,
h is real-valued.
(7.39)
This follows from the deﬁnition of the convolution,
(x ⋆h)(t) =
 ∞
−∞
x(τ) h(t −τ) dτ
and from the basic properties of complex integrals (Proposition 2.3.1) by noting
that if h(·) is real-valued, then for all t, τ ∈R,
Re

x(τ) h(t −τ)

= Re

x(τ)

h(t −τ),
Im

x(τ) h(t −τ)

= Im

x(τ)

h(t −τ).
We next use (7.39) to express the convolution in (7.32a) using real-number oper-
ations. To that end we ﬁrst note that since xPB is real, it follows from Euler’s
Identity
eiθ = cos θ + i sin θ,
θ ∈R
(7.40)
that
Re

xPB(t) e−i2πfct
= xPB(t) cos(2πfct),
t ∈R,
(7.41a)
Im

xPB(t) e−i2πfct
= −xPB(t) sin(2πfct),
t ∈R,
(7.41b)
so by (7.35a), (7.39), and (7.41)
Re(xBB) =

t 	→xPB(t) cos(2πfct)

⋆LPFWc,
(7.42a)
Im(xBB) = −

t 	→xPB(t) sin(2πfct)

⋆LPFWc .
(7.42b)
It is common in the engineering literature to refer to the real part of xBB as
the in-phase component of xPB and to the imaginary part as the quadrature
component of xPB.
7.6 Baseband Representation of Real Passband Signals
125
cos(2πfct)
90◦
×
×
xPB(t)
xPB(t) cos(2πfc)
−xPB(t) sin(2πfct)
LPFWc
LPFWc
W
2 ≤Wc ≤2fc −W
2
Re

xBB(t)

Im

xBB(t)

Figure 7.11: Obtaining the baseband representation of a real passband signal.
Deﬁnition 7.6.6 (In-Phase and Quadrature Components). The in-phase com-
ponent of a real integrable passband signal xPB that is bandlimited to W Hz around
the carrier frequency fc is the real part of its baseband representation, i.e.,
Re(xBB) =

t 	→xPB(t) cos(2πfct)

⋆LPFWc .
(In-Phase)
The quadrature component is the imaginary part of its baseband representation,
i.e.,
Im(xBB) = −

t 	→xPB(t) sin(2πfct)

⋆LPFWc .
(Quadrature)
Here Wc is any cutoﬀfrequency in the range W/2 ≤Wc ≤2fc −W/2.
Figure 7.11 depicts a block diagram of a circuit that produces the baseband rep-
resentation of a real passband signal.
This circuit will play an important role
in Chapter 9 when we discuss the Sampling Theorem for passband signals and
complex sampling.
7.6.3
Bandwidth Considerations
The following is a simple but exceedingly important observation regarding band-
width. Recall that the bandwidth of xPB around the carrier frequency fc is deﬁned
in Deﬁnition 7.3.1 and that the bandwidth of the baseband signal xBB is deﬁned
in Deﬁnition 6.4.13.
Proposition 7.6.7 (xPB, xBB, and Bandwidth). If the real integrable passband
signal xPB is of bandwidth W Hz around the carrier frequency fc, then its baseband
representation xBB is an integrable signal of bandwidth W/2 Hz.
126
Passband Signals and Their Representation
Proof. This can be seen graphically from Figure 7.9 or from Figure 7.10. It can
be deduced analytically from (7.31).
7.6.4
Recovering xPB from xBB
Recovering a real passband signal xPB from its baseband representation xBB is
conceptually simple. We can recover the analytic representation via (7.29) and
then use Proposition 7.5.3 to recover xPB:
Proposition 7.6.8 (From xBB to xPB). Let xPB be a real integrable passband
signal that is bandlimited to W Hz around the carrier frequency fc, and let xBB be
its baseband representation. Then,
ˆxPB(f) = ˆxBB(f −fc) + ˆx∗
BB(−f −fc),
f ∈R,
(7.43a)
and
xPB(t) = 2 Re

xBB(t) ei2πfct
,
t ∈R.
(7.43b)
The process of recovering xPB from xBB is depicted in the frequency domain in
Figure 7.12. It can, of course, also be carried out using real-number operations
only by rewriting (7.43b) as
xPB(t) = 2 Re

xBB(t)

cos(2πfct) −2 Im

xBB(t)

sin(2πfct),
t ∈R.
(7.44)
It should be emphasized that (7.43b) does not characterize the baseband represen-
tation of xPB; it is possible that xPB(t) = 2 Re

z(t) ei2πfct
hold at every time t and
that z not be the baseband representation of xPB. However, as the next proposition
shows, this cannot happen if z is bandlimited to W/2 Hz.
Proposition 7.6.9. Let xPB be a real integrable passband signal that is bandlimited
to W Hz around the carrier frequency fc. If the complex signal z satisﬁes
xPB(t) = 2 Re

z(t) ei2πfct
,
t ∈R,
(7.45)
and is an integrable signal that is bandlimited to W/2 Hz, then z is the baseband
representation of xPB.
Proof. Since z is bandlimited to W/2 Hz, it follows from Proposition 6.4.10 (cf. (c))
that z must be continuous and that its FT must vanish for |f| > W/2. Conse-
quently, by Proposition 7.6.5 (cf. (b)), all that remains to show in order to establish
that z is the baseband representation of xPB is that
ˆz(f) = ˆxPB(f + fc),
|f| ≤W/2,
(7.46)
and this is what we proceed to do. By taking the FT of both sides of (7.45) we
obtain that
ˆxPB(f) = ˆz(f −fc) + ˆz∗(−f −fc),
f ∈R,
(7.47)
or, upon deﬁning ˜f ≜f −fc,
ˆxPB( ˜f + fc) = ˆz( ˜f) + ˆz∗(−˜f −2fc),
˜f ∈R.
(7.48)
7.6 Baseband Representation of Real Passband Signals
127
ˆxBB(f)
ˆxBB(f −fc)
ˆx∗
BB(−f)
ˆx∗
BB(−f −fc)
ˆxPB(f) = ˆxBB(f −fc) + ˆx∗
BB(−f −fc)
fc
−fc
−fc
−fc
f
f
f
f
f
Figure 7.12: Recovering a passband signal from its baseband representation. Top
plot of ˆxBB is the transform of xBB; next is the transform of t 	→xBB(t) ei2πfct; the
transform of x∗
BB(t); the transform of t 	→x∗
BB(t) e−i2πfct; and ﬁnally the transform
of t 	→xBB(t) ei2πfct + x∗
BB(t) e−i2πfct = 2 Re

xBB(t) ei2πfct
= xPB(t).
128
Passband Signals and Their Representation
By recalling that fc > W/2 and that ˆz is zero for frequencies f satisfying |f| > W/2,
we obtain that ˆz∗(−˜f −2fc) is zero whenever | ˜f| ≤W/2 so
ˆz( ˜f) + ˆz∗(−˜f −2fc) = ˆz( ˜f),
| ˜f| ≤W/2.
(7.49)
Combining (7.48) and (7.49) we obtain
ˆxPB( ˜f + fc) = ˆz( ˜f),
| ˜f| ≤W/2,
thus establishing (7.46) and hence completing the proof.
Proposition 7.6.9 is more useful than its appearance may suggest. It provides an
alternative way of computing the baseband representation of a signal. It demon-
strates that if we can use algebra to express xPB in the form (7.45) for some signal z,
and if we can verify that z is bandlimited to W/2 Hz, then z must be the baseband
representation of xPB.
Note that the proof would also work if we replaced the assumption that z is an
integrable signal that is bandlimited to W/2 Hz with the assumption that z is an
integrable signal that is bandlimited to fc Hz.
7.6.5
Relating ⟨xPB, yPB⟩to ⟨xBB, yBB⟩
If xPB and yPB are integrable real passband signals that are bandlimited to W Hz
around the carrier frequency fc, and if xA, xBB , yA, and yBB are their corre-
sponding analytic and baseband representations, then, by (7.29),
⟨xBB, yBB⟩= ⟨xA, yA⟩,
(7.50)
because
⟨xBB, yBB⟩=
 ∞
−∞
xBB(t) y∗
BB(t) dt
=
 ∞
−∞
e−i2πfct xA(t)

e−i2πfct yA(t)
∗dt
=
 ∞
−∞
e−i2πfct xA(t) ei2πfct y∗
A(t) dt
= ⟨xA, yA⟩.
Combining (7.50) with Proposition 7.5.4 we obtain the following relationship be-
tween the inner product between two real passband signals and the inner product
between their corresponding complex baseband representations.
Theorem 7.6.10 (⟨xPB, yPB⟩and ⟨xBB, yBB⟩). Let xPB and yPB be real integrable
passband signals that are bandlimited to W Hz around the carrier frequency fc, and
let xBB and yBB be their corresponding baseband representations. Then
⟨xPB, yPB⟩= 2 Re

⟨xBB, yBB⟩

,
(7.51)
7.6 Baseband Representation of Real Passband Signals
129
and
∥xPB∥2
2 = 2 ∥xBB∥2
2 .
(7.52)
An extremely important corollary provides a necessary and suﬃcient condition for
the inner product between two real passband signals to be zero, i.e., for two real
passband signals to be orthogonal.
Corollary 7.6.11 (Characterizing Orthogonal Real Passband Signals). Two in-
tegrable real passband signals xPB, yPB that are bandlimited to W Hz around the
carrier frequency fc are orthogonal if, and only if, the inner product between their
baseband representations is purely imaginary (i.e., of zero real part).
Thus, for two such passband signals to be orthogonal their baseband representa-
tions need not be orthogonal: it suﬃces that their inner product be purely imagi-
nary.
7.6.6
The Baseband Representation of xPB ⋆yPB
Proposition 7.6.12 (The Baseband Representation of xPB ⋆yPB Is xBB ⋆yBB).
Let xPB and yPB be real integrable passband signals that are bandlimited to W Hz
around the carrier frequency fc, and let xBB and yBB be their baseband repre-
sentations. Then the convolution xPB ⋆yPB is a real integrable passband signal
that is bandlimited to W Hz around the carrier frequency fc and whose baseband
representation is xBB ⋆yBB.
Proof. The proof is illustrated in Figure 7.13 on Page 130. All that remains is to
add some technical details. We begin by deﬁning
z = xPB ⋆yPB
and by noting that, by Proposition 7.2.5, z is an integrable real passband signal
that is bandlimited to W Hz around the carrier frequency fc and that its FT is
given by
ˆz(f) = ˆxPB(f) ˆyPB(f),
f ∈R.
(7.53)
Thus, it is at least meaningful to discuss the baseband representation of xPB ⋆yPB.
We next note that, by Proposition 7.6.5, both xBB and yBB are integrable signals
that are bandlimited to W/2 Hz. Consequently, by Proposition 6.5.2, the convolu-
tion u = xBB ⋆yBB is deﬁned at every epoch t and is also an integrable signal that
is bandlimited to W/2 Hz. Its FT is
ˆu(f) = ˆxBB(f) ˆyBB(f),
f ∈R.
(7.54)
From Proposition 7.6.5 we infer that to prove that u is the baseband representation
of z it only remains to verify that ˆu is the mapping f 	→ˆz(f + fc) I{|f| ≤W/2},
which, in view of (7.53) and (7.54), is equivalent to showing that
ˆxBB(f) ˆyBB(f) = ˆxPB(f + fc) ˆyPB(f + fc) I{|f| ≤W/2},
f ∈R.
(7.55)
130
Passband Signals and Their Representation
f
f
f
f
f
f
ˆxPB(f)
ˆyPB(f)
ˆxPB(f) ˆyPB(f)
ˆxBB(f)
ˆyBB(f)
ˆxBB(f) ˆyBB(f)
fc
−fc
1
1.5
1.5
Figure 7.13: The convolution of two real passband signals and its baseband rep-
resentation.
7.6 Baseband Representation of Real Passband Signals
131
But this follows because the fact that xBB and yBB are the baseband representa-
tions of xPB and yPB implies that
ˆxBB(f) = ˆxPB(f + fc) I{|f| ≤W/2},
f ∈R,
ˆyBB(f) = ˆyPB(f + fc) I{|f| ≤W/2},
f ∈R,
from which (7.55) follows.
7.6.7
The Baseband Representation of xPB ⋆h
We next study the result of passing a real integrable passband signal xPB that is
bandlimited to W Hz around the carrier frequency fc through a real stable ﬁlter
of impulse response h. Our focus is on the baseband representation of the result.
Proposition 7.6.13 (Baseband Representation of xPB⋆h). Let xPB be a real inte-
grable passband signal that is bandlimited to W Hz around the carrier frequency fc,
and let xBB be its baseband representation. Let h ∈L1 be real. Then xPB ⋆h is
deﬁned at every time instant; it is a real integrable passband signal that is band-
limited to W Hz around the carrier frequency fc; and its baseband representation
is of FT
f 	→ˆxBB(f) ˆh(f + fc),
f ∈R
(7.56)
and is given by

xPB ⋆h

BB(t) =
 ∞
−∞
ˆxBB(f) ˆh(f + fc) ei2πft df,
t ∈R.
(7.57)
Proof. That the convolution xPB ⋆h is deﬁned at every time instant follows from
Proposition 7.2.5. Denote xPB ⋆h by y. By the same proposition, y is a real inte-
grable passband signal that is bandlimited to W Hz around the carrier frequency fc,
and its FT is given by
ˆy(f) = ˆxPB(f) ˆh(f),
f ∈R.
(7.58)
From (7.58) (and Proposition 7.6.5 (cf. (b)) applied to the signal y), we obtain
that the baseband representation of y is of FT
f 	→ˆxPB(f + fc) ˆh(f + fc) I
'
|f| ≤W
2
(
,
f ∈R.
(7.59)
To establish that the mapping in (7.56) is indeed the FT of (xPB ⋆h)BB it thus
remains to establish that the mappings (7.59) and (7.56) are identical. But this
follows because, by Proposition 7.6.5 (cf. (b)) applied to the signal xPB,
ˆxBB(f) = ˆxPB(f + fc) I
'
|f| ≤W
2
(
,
f ∈R.
To justify the inversion formula (7.57), recall that (xPB⋆h)BB—being the baseband
representation of an integrable signal that is bandlimited to W Hz around fc—is an
integrable signal that is bandlimited to W/2 Hz (Proposition 7.6.5 (cf. (c)) applied
to xPB ⋆h), so the inversion is justiﬁed by Proposition 6.4.10 (cf. (b)).
132
Passband Signals and Their Representation
f
f
W
fc
W
2
−W
2
ˆh(f)
Figure 7.14: A real ﬁlter’s frequency response (top) and its frequency response
with respect to the bandwidth W around the carrier frequency fc (bottom).
Motivated by Proposition 7.6.13 we put forth the following deﬁnition.
Deﬁnition 7.6.14 (Frequency Response with Respect to a Band). For a stable
real ﬁlter of impulse response h we deﬁne the frequency response with respect
to the bandwidth W around the carrier frequency fc (satisfying fc > W/2)
as the mapping
f 	→ˆh(f + fc) I
'
|f| ≤W
2
(
.
(7.60)
Figure 7.14 illustrates the relationship between the frequency response of a real
ﬁlter and its response with respect to the carrier frequency fc and bandwidth W.
Heuristically, we can think of the frequency response with respect to the band-
width W around the carrier frequency fc of a ﬁlter of real impulse response h as
the FT of the baseband representation of h ⋆BPFW,fc.2
With the aid of Deﬁnition 7.6.14 we can restate Proposition 7.6.13 as stating that
the baseband representation of the result of passing a real integrable passband
signal that is bandlimited to W Hz around the carrier frequency fc through a
stable real ﬁlter is the product of the FT of the baseband representation of the
signal by the frequency response with respect to the bandwidth W around the
carrier frequency fc of the ﬁlter. This relationship is illustrated in Figures 7.15
2This is mathematically somewhat problematic because h⋆BPFW,fc need not be an integrable
signal.
But this can be remedied because h ⋆BPFW,fc is an energy-limited passband signal
that is bandlimited to W Hz around the carrier frequency, and, as such, also has a baseband
representation; see Section 7.7.
7.7 Energy-Limited Passband Signals
133
f
f
f
fc
fc
fc
−fc
−fc
−fc
W
1
1
1
ˆxPB(f)
ˆh(f)
ˆxPB(f) ˆh(f)
Figure 7.15: The FT of a passband signal (top); the frequency response of a real
ﬁlter (middle); and their product (bottom).
and 7.16. The former depicts the product of the FT of a real passband signal xPB
and the frequency response of a real ﬁlter h. The latter depicts the product of the
baseband representation xBB of xPB by the frequency response of h with respect
to the bandwidth W around the carrier frequency fc.
The relationship between some of the properties of xPB, xA, and xBB are summa-
rized in Table 7.1 on Page 146.
7.7
Energy-Limited Passband Signals
We next repeat the results of this chapter under the weaker assumption that the
passband signal is energy-limited and not necessarily integrable. The key results
require only minor adjustments, and most of the derivations are nearly identical
and hence omitted.
134
Passband Signals and Their Representation
f
f
f
W
2
−W
2
W
2
−W
2
W
2
−W
2
1
1
1
ˆxBB(f)
Figure 7.16: The FT of the baseband representation of the passband signal xPB of
Figure 7.15 (top); the frequency response with respect to the bandwidth W around
the carrier frequency fc of the ﬁlter of Figure 7.15 (middle); and their product
(bottom).
7.7 Energy-Limited Passband Signals
135
7.7.1
Characterization of Energy-Limited Passband Signals
Recall that energy-limited passband signals were deﬁned in Deﬁnition 7.2.1 as
energy-limited signals that are unaltered by bandpass ﬁltering.
In this subsec-
tion we shall describe alternative characterizations. Aiding us in the character-
ization is the following lemma, which can be viewed as the passband analog of
Lemma 6.4.4 (i).
Lemma 7.7.1. Let x be an energy-limited signal, and let fc > W/2 > 0 be given.
Then the signal x ⋆BPFW,fc can be expressed as

x ⋆BPFW,fc

(t) =

||f|−fc|≤W/2
ˆx(f) ei2πft df,
t ∈R;
(7.61)
it is of ﬁnite energy; and its L2-Fourier Transform is (the equivalence class of) the
mapping f 	→ˆx(f) I
|f| −fc
 ≤W/2

.
Proof. The lemma follows from Lemma 6.4.4 (ii) by substituting for g the mapping
f 	→I
|f| −fc
 ≤W/2

, whose IFT is BPFW,fc.
In analogy to Proposition 6.4.5 we can characterize energy-limited passband signals
as follows.
Proposition 7.7.2 (Characterizations of Passband Signals in L2).
(i) If x is an energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc, then it can be expressed in the form
x(t) =

||f|−fc|≤W/2
g(f) ei2πft df,
t ∈R,
(7.62)
for some mapping g: f 	→g(f) satisfying

||f|−fc|≤W/2
|g(f)|2 df < ∞
(7.63)
that can be taken as (any function in the equivalence class of) ˆx.
(ii) If a signal x can be expressed as in (7.62) for some function g satisfying
(7.63) with fc > W/2 > 0, then x is an energy-limited passband signal that
is bandlimited to W Hz around the carrier frequency fc and its FT ˆx is (the
equivalence class of) the mapping f 	→g(f) I
|f| −fc
 ≤W/2

.
Proof. The proof of Part (i) follows from Deﬁnition 7.2.1 and from Lemma 7.7.1 in
very much the same way as Part (i) of Proposition 6.4.5 follows from Deﬁnition 6.4.1
and Lemma 6.4.4 (i).
The proof of Part (ii) is analogous to the proof of Part (ii) of Proposition 6.4.5.
As a corollary we obtain the analog of Corollary 7.2.3:
136
Passband Signals and Their Representation
Corollary 7.7.3 (Energy-Limited Passband Signals Are Bandlimited). If xPB is
an energy-limited passband signal that is bandlimited to W Hz around the carrier
frequency fc, then it is an energy-limited signal that is bandlimited to fc +W/2 Hz.
Proof. If xPB is an energy-limited passband signal that is bandlimited to W Hz
around the carrier frequency fc, then, by Proposition 7.7.2 (i), there exists a func-
tion g: f 	→g(f) satisfying (7.63) such that xPB is given by (7.62). But this implies
that the function f 	→g(f) I
|f| −fc
 ≤W/2

is an energy-limited function such
that
xPB(t) =
 fc+W/2
−fc−W/2
g(f) I
|f| −fc
 ≤W/2

ei2πft df,
t ∈R,
(7.64)
so, by Proposition 6.4.5 (ii), xPB is an energy-limited signal that is bandlimited to
fc + W/2 Hz.
The following is the analog of Proposition 6.4.6.
Proposition 7.7.4.
(i) If xPB is an energy-limited passband signal that is bandlimited to W Hz
around the carrier frequency fc, then xPB is a continuous function and all
its energy is contained in the frequencies f satisfying
|f| −fc
 ≤W/2 in the
sense that
 ∞
−∞
|ˆxPB(f)|2 df =

||f|−fc|≤W/2
|ˆxPB(f)|2 df.
(7.65)
(ii) If xPB ∈L2 satisﬁes (7.65), then xPB is indistinguishable from the signal
xPB⋆BPFW,fc, which is an energy-limited passband signal that is bandlimited
to W Hz around fc.
If in addition to satisfying (7.65) the signal xPB is
continuous, then xPB is an energy-limited passband signal that is bandlimited
to W Hz around the carrier frequency fc.
Proof. This proposition’s claims are a subset of those of Proposition 7.7.5, which
summarizes some of the results related to bandpass ﬁltering.
Proposition 7.7.5. Let y = x⋆BPFW,fc be the result of feeding the signal x ∈L2 to
an ideal unit-gain bandpass ﬁlter of bandwidth W around the carrier frequency fc.
Assume fc > W/2. Then:
(i) y is energy-limited with
∥y∥2 ≤∥x∥2 .
(7.66)
(ii) y is an energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc.
(iii) The L2-Fourier Transform of y is (the equivalence class of) the mapping
f 	→ˆx(f) I
|f| −fc
 ≤W/2

.
7.7 Energy-Limited Passband Signals
137
(iv) All the energy in y is concentrated in the frequencies

f :
|f| −fc
 ≤W/2

in the sense that
 ∞
−∞
|ˆy(f)|2 df =

||f|−fc|≤W/2
|ˆy(f)|2 df.
(v) y can be represented as
y(t) =
 ∞
−∞
ˆy(f) ei2πft df
(7.67)
=

||f|−fc|≤W/2
ˆx(f) ei2πft df,
t ∈R.
(7.68)
(vi) y is uniformly continuous.
(vii) If all the energy of x is concentrated in the frequencies

f :
|f|−fc
 ≤W/2

in the sense that
 ∞
−∞
|ˆx(f)|2 df =

||f|−fc|≤W/2
|ˆx(f)|2 df,
(7.69)
then x is indistinguishable from the passband signal x ⋆BPFW,fc.
(viii) z is an energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc if, and only if, it satisﬁes all three of the following
conditions: it is in L2; it is continuous; and all its energy is concentrated in
the passband frequencies

f :
|f| −fc
 ≤W/2

.
Proof. The proof is very similar to the proof of Proposition 6.4.7 and is thus
omitted.
7.7.2
The Analytic Representation
If xPB is a real energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc, then we deﬁne its analytic representation via (7.11). (Since
xPB ∈L2, it follows from Parseval’s Theorem that ˆxPB is energy-limited so, by
Proposition 3.4.3, the mapping f 	→ˆxPB(f) I{|f −fc| ≤W/2} is integrable and
the integral (7.11) is deﬁned for every t ∈R. Also, the integral does not depend
on which element of the equivalence class consisting of the L2-Fourier Transform
of xPB it is applied to.)
In analogy to Proposition 7.5.2 we can characterize the analytic representation as
follows.
Proposition 7.7.6 (Characterizing the Analytic Representation of xPB ∈L2).
Let xPB be a real energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc. Then each of the following statements is equivalent to the
statement that the complex signal xA is the analytic representation of xPB:
138
Passband Signals and Their Representation
(a) The signal xA is given by
xA(t) =
 fc+ W
2
fc−W
2
ˆxPB(f) ei2πft df,
t ∈R.
(7.70)
(b) The signal xA is a continuous energy-limited signal whose L2-Fourier Trans-
form ˆxA is (the equivalence class of) the mapping
f 	→ˆxPB(f) I{f ≥0}.
(7.71)
(c) The signal xA is an energy-limited passband signal that is bandlimited to W
Hz around the carrier frequency fc and whose L2-Fourier Transform is (the
equivalence class of) the mapping in (7.71).
(d) The signal xA is given by
xA = xPB ⋆ˇg
(7.72)
where g: f 	→g(f) is any function in L1 ∩L2 satisfying
g(f) = 1,
f −fc
 ≤W/2,
(7.73a)
and
g(f) = 0,
f + fc
 ≤W/2.
(7.73b)
Proof. The proof is not very diﬃcult and is omitted.
We note that the reconstruction formula (7.22b) continues to hold also when xPB
is an energy-limited signal that is bandlimited to W Hz around the carrier fre-
quency fc.
7.7.3
The Baseband Representation of xPB ∈L2
Having deﬁned the analytic representation, we now use (7.29) to deﬁne the base-
band representation.
As in Proposition 7.6.3, we can also describe a procedure for obtaining the base-
band representation of a passband signal without having to go via the analytic
representation.
Proposition 7.7.7 (From xPB ∈L2 to xBB Directly). If xPB is a real energy-
limited passband signal that is bandlimited to W Hz around the carrier frequency fc,
then its baseband representation xBB is given by
xBB =

t 	→e−i2πfct xPB(t)

⋆ˇg0,
(7.74)
where g0 : f 	→g0(f) is any function in L1 ∩L2 satisfying
g0(f) = 1,
|f| ≤W/2,
(7.75a)
and
g0(f) = 0,
|f + 2fc| ≤W/2.
(7.75b)
7.7 Energy-Limited Passband Signals
139
Proof. The proof is very similar to the proof of Proposition 7.6.3 and is omitted.
The following proposition, which is the analog of Proposition 7.6.5 characterizes
the baseband representation of energy-limited passband signals.
Proposition 7.7.8 (Characterizing the Baseband Representation of xPB ∈L2).
Let xPB be a real energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc. Then each of the following statements is equivalent to the
statement that the complex signal xBB is the baseband representation of xPB.
(a) The signal xBB is given by
xBB(t) =

W
2
−W
2
ˆxPB(f + fc) ei2πft df,
t ∈R.
(7.76)
(b) The signal xBB is a continuous energy-limited signal whose L2-Fourier Trans-
form is (the equivalence class of) the mapping
f 	→ˆxPB(f + fc) I{|f| ≤W/2}.
(7.77)
(c) The signal xBB is an energy-limited signal that is bandlimited to W/2 Hz
and whose L2-Fourier Transform is (the equivalence class of) the mapping
(7.77).
(d) The signal xBB is given by (7.74) for any mapping g0 : f 	→g0(f) satisfying
(7.75).
The in-phase component and the quadrature component of an energy-limited
passband signal are deﬁned, as in the integrable case, as the real and imaginary
parts of its baseband representation.
Proposition 7.6.7, which asserts that the bandwidth of xBB is half the bandwidth
of xPB continues to hold, as does the reconstruction formula (7.43b). Proposi-
tion 7.6.9 also extends to energy-limited signals. We repeat it (in a slightly more
general way) for future reference.
Proposition 7.7.9.
(i) If z is an energy-limited signal that is bandlimited to W/2 Hz, and if the
signal x is given by
x(t) = 2 Re

z(t) ei2πfct
,
t ∈R,
(7.78)
where fc > W/2, then x is a real energy-limited passband signal that is band-
limited to W Hz around fc, and z is its baseband representation.
(ii) If x is an energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc and if (7.78) holds for some energy-limited signal z
that is bandlimited to fc Hz, then z is the baseband representation of x and
is, in fact, bandlimited to W/2 Hz.
140
Passband Signals and Their Representation
Proof. Omitted.
Identity (7.51) relating the inner products ⟨xPB, yPB⟩and ⟨xBB, yBB⟩continues to
hold for energy-limited passband signals that are not necessarily integrable.
Proposition 7.6.12 does not hold for energy-limited signals, because the convolution
of two energy-limited signals need not be energy-limited. But if we assume that at
least one of the signals is also integrable, then things sail through. Consequently,
using Corollary 7.2.4 we obtain:
Proposition 7.7.10 (The Baseband Representation of xPB ⋆yPB Is xBB ⋆yBB).
Let xPB be a real integrable passband signal that is bandlimited to W Hz around
the carrier frequency fc, and let yPB be a real energy-limited passband signal that
is bandlimited to W Hz around the carrier frequency fc. Let xBB and yBB be their
corresponding baseband representations. Then xPB ⋆yPB is a real energy-limited
signal that is bandlimited to W Hz around the carrier frequency fc and whose
baseband representation is xBB ⋆yBB.
Proposition 7.6.13 too requires only a slight modiﬁcation to address energy-limited
signals.
Proposition 7.7.11 (Baseband Representation of xPB ⋆h). Let xPB be a real
energy-limited passband signal that is bandlimited to W Hz around the carrier fre-
quency fc, and let h be a real integrable signal. Then xPB ⋆h is deﬁned at every
time instant; it is a real energy-limited passband signal that is bandlimited to W
Hz around the carrier frequency fc; and its baseband representation is given by

h ⋆xPB

BB = h′
BB ⋆xBB,
(7.79)
where h′
BB is the baseband representation of the energy-limited signal h⋆BPFW,fc.
The L2-Fourier Transform of the baseband representation of xPB ⋆h is (the equiv-
alence class of) the mapping
f 	→ˆxBB(f) ˆh(f + fc),
f ∈R,
(7.80)
where xBB is the baseband representation of xPB.
The following theorem summarizes some of the properties of the baseband repre-
sentation of energy-limited passband signals.
Theorem 7.7.12 (Properties of the Baseband Representation).
(i) The mapping xPB 	→xBB that maps every real energy-limited passband signal
that is bandlimited to W Hz around the carrier frequency fc to its baseband
representation is a one-to-one mapping onto the space of complex energy-
limited signals that are bandlimited to W/2 Hz.
(ii) The mapping xPB 	→xBB is linear in the sense that if xPB and yPB are
real energy-limited passband signals that are bandlimited to W Hz around
7.8 Shifting to Passband and Convolving
141
the carrier frequency fc, and if xBB and yBB are their corresponding base-
band representations, then for every α, β ∈R, the baseband representation of
αxPB + βyPB is αxBB + βyBB:

αxPB + βyPB

BB= αxBB + βyBB,
α, β ∈R.
(7.81)
(iii) The mapping xPB 	→xBB is—to within a factor of two—energy preserving
in the sense that
∥xPB∥2
2 = 2 ∥xBB∥2
2 .
(7.82)
(iv) Inner products are related via
⟨xPB, yPB⟩= 2 Re

⟨xBB, yBB⟩

,
(7.83)
for xPB and yPB as in (ii) above.
(v) The (baseband) bandwidth of xBB is half the bandwidth of xPB around the
carrier frequency fc.
(vi) The baseband representation xBB can be expressed in terms of xPB as
xBB =

t 	→e−i2πfct xPB(t)

⋆LPFWc
(7.84a)
where Wc is any cutoﬀfrequency satisfying
W/2 ≤Wc ≤2fc −W/2.
(7.84b)
(vii) The real passband signal xPB can be expressed in terms of its baseband rep-
resentation xBB as
xPB(t) = 2 Re

xBB(t) ei2πfct
,
t ∈R.
(7.85)
(viii) If h is a real integrable signal, and if xPB is as above, then h ⋆xPB is a real
energy-limited passband signal that is bandlimited to W Hz around the carrier
frequency fc, and its baseband representation is given by

h ⋆xPB

BB = h′
BB ⋆xBB,
(7.86)
where h′
BB is the baseband representation of the energy-limited real signal
h ⋆BPFW,fc.
7.8
Shifting to Passband and Convolving
The following result is almost trivial if you think about its interpretation in the
frequency domain. To that end, it is good to focus on the case where the signal x
is a bandlimited baseband signal and where fc is positive and large. In this case
we can interpret the LHS of (7.87) as the result of taking the baseband signal x,
up-converting it to passband by forming the signal τ 	→x(τ) ei2πfcτ, and then
convolving the result with h. The RHS corresponds to down-converting h to form
the signal τ 	→e−i2πfcτh(τ), then convolving this signal with x, and then up-
converting the ﬁnal result.
142
Passband Signals and Their Representation
Proposition 7.8.1. Suppose that fc ∈R and that (at least) one of the following
conditions holds:
1) The signal x is a measurable bounded signal and h ∈L1.
2) Both x and h are in L2.
Then, at every epoch t ∈R,

τ 	→x(τ) ei2πfcτ
⋆h

(t) = ei2πfct 
x ⋆

τ 	→e−i2πfcτ h(τ)

(t).
(7.87)
Proof. We evaluate the LHS of (7.87) using the deﬁnition of the convolution:

τ 	→x(τ) ei2πfcτ
⋆h

(t) =
 ∞
−∞
x(τ) ei2πfcτ h(t −τ) dτ
= ei2πfct e−i2πfct
 ∞
−∞
x(τ) ei2πfcτ h(t −τ) dτ
= ei2πfct
 ∞
−∞
x(τ) e−i2πfc(t−τ) h(t −τ) dτ
= ei2πfct 
x ⋆

τ 	→e−i2πfcτ h(τ)

(t).
7.9
Mathematical Comments
The analytic representation is related to the Hilbert Transform; see, for example,
(Pinsky, 2002, Section 3.4). In our proof that xA is integrable whenever xPB is
integrable we implicitly exploited the fact that the strict inequality fc > W/2
implies that for the class of integrable passband signals that are bandlimited to W
Hz around the carrier frequency fc there exist Hilbert Transform kernels that are
integrable. See, for example, (Logan, 1978, Section 2.5).
7.10
Exercises
Exercise 7.1 (Bandwidth around Diﬀerent Carrier Frequencies). Let ˇg be the IFT of
g: f →I{a ≤|f| ≤b},
where b > a > 0. For which positive values of fc and W is ˇg an energy-limited passband
signal that is bandlimited to W Hz around the carrier frequency fc? For those values of
fc and W, express the bandwidth of ˇg around fc in terms of a, b and fc.
Exercise 7.2 (On the Deﬁnition of the Analytic Representation). Let xPB be a real
integrable passband signal that is bandlimited to W Hz around the carrier frequency fc.
Show that the signals xPB and t →2 Re

z(t)

are identical if, and only if, the diﬀerence
z −xA between the signal z and the analytic representation of xPB takes on only purely
imaginary values.
7.10 Exercises
143
Exercise 7.3 (Purely Real and Purely Imaginary Baseband Representations). Let xPB
be a real integrable passband signal that is bandlimited to W Hz around the carrier
frequency fc, and let xBB be its baseband representation.
(i) Show that xBB is real if, and only if, ˆxPB satisﬁes
ˆxPB(fc −δ) = ˆx∗
PB(fc + δ),
|δ| ≤W
2 .
(ii) Show that xBB is imaginary if, and only if,
ˆxPB(fc −δ) = −ˆx∗
PB(fc + δ),
|δ| ≤W
2 .
Exercise 7.4 (Symmetry around the Carrier Frequency). Let xPB be a real integrable
passband signal that is bandlimited to W Hz around the carrier frequency fc.
(i) Show that xPB can be written in the form
xPB(t) = w(t) cos(2πfct)
where w(·) is a real integrable signal that is bandlimited to W/2 Hz if, and only if,
ˆxPB(fc + δ) = ˆx∗
PB(fc −δ),
|δ| ≤W
2 .
(ii) Show that xPB can be written in the form
xPB(t) = w(t) sin(2πfct),
t ∈R
for w(·) as above if, and only if,
ˆxPB(fc + δ) = −ˆx∗
PB(fc −δ),
|δ| ≤W
2 .
Exercise 7.5 (Delaying a Passband Signal). Let xPB be a real integrable passband signal
that is bandlimited to W Hz around the carrier frequency fc, and let xBB be its baseband
representation. Express the baseband representation of the delayed signal t →xPB(t −δ)
in terms of xBB, fc, and δ.
Exercise 7.6 (From Baseband to Passband with Care). Let the complex signal x be the
IFT of g, where
g(f) =
⎧
⎪
⎨
⎪
⎩
1
−B < f ≤0,
1 −f
B
0 < f ≤B,
0
otherwise,
f ∈R,
and B > 0. For α > 0 consider the signal
y(t) = Re

x(t) ei2παt
,
t ∈R.
Under what additional conditions on B, α, fc, and W is y a real energy-limited passband
signal that is bandlimited to W Hz around fc?
Under these conditions, what is the
bandwidth of y around fc, and what is its baseband representation with respect to fc?
144
Passband Signals and Their Representation
Exercise 7.7 (Bandpass Filtering a Passband Signal). Let xPB be a real energy-limited
passband signal of bandwidth W Hz around the carrier frequency fc. Let yPB be the result
of passing xPB through an ideal unit-gain bandpass ﬁlter of bandwidth W/2 around the
frequency fc + W/4
yPB = xPB ⋆BPF W
2 ,fc+ W
4 .
(i) How large can the bandwidth of yPB around fc be?
(ii) How large can the bandwidth of yPB around fc + W/4 be?
(iii) Let xBB be the baseband representation of xPB.
Express the FT of the base-
band representation of yPB in terms of ˆxBB if you view yPB as a passband signal
around fc. Repeat when you view yPB as a passband signal around fc + W/4.
Exercise 7.8 (Viewing a Baseband Signal as a Passband Signal). Let x be a real integrable
signal that is bandlimited to W Hz. Show that if we had informally allowed equality in
(7.1b) and if we had allowed equality between fc and W/2 in (5.21), then we could have
viewed x also as a real integrable passband signal that is bandlimited to W Hz around the
carrier frequency fc = W/2. Viewed as such, what would have been its complex baseband
representation?
Exercise 7.9 (Bandwidth of the Product of Two Signals). Let x be a real energy-limited
signal that is bandlimited to Wx Hz. Let y be a real energy-limited passband signal that
is bandlimited to Wy Hz around the carrier frequency fc. Show that if fc > Wx + Wy/2,
then the signal t →x(t) y(t) is a real integrable passband signal that is bandlimited to
2Wx + Wy Hz around the carrier frequency fc.
Exercise 7.10 (Phase Shift). Let x be a real integrable signal that is bandlimited to
W Hz. Let fc be larger than W.
(i) Express the baseband representation of the real passband signal
zPB(t) = x(t) sin(2πfct + φ),
t ∈R
in terms of x(·) and φ.
(ii) Compute the Fourier Transform of zPB.
Exercise 7.11 (Energy of a Passband Signal). Let x ∈L2 be of energy ∥x∥2
2.
(i) What is the approximate energy in t →x(t) cos(2πfct) if fc is very large?
(ii) Is your answer exact if x is an energy-limited signal that is bandlimited to W Hz,
where W < fc?
Hint: In Part (i) approximate x as being constant over the periods of t →cos (2πfct).
For Part (ii) see also Problem 6.21.
Exercise 7.12 (Diﬀerences in Passband). Let xPB and yPB be real energy-limited pass-
band signals that are bandlimited to W Hz around the carrier frequency fc. Let xBB and
yBB be their baseband representations. Find the relationship between
 ∞
−∞

xPB(t) −yPB(t)
2 dt
and
 ∞
−∞
xBB(t) −yBB(t)
2 dt.
7.10 Exercises
145
Exercise 7.13 (Reﬂection of Passband Signal). Let xPB and yPB be real integrable pass-
band signals that are bandlimited to W Hz around the carrier frequency fc. Let xBB
and yBB be their baseband representations.
(i) Express the baseband representation of ~xPB in terms of xBB.
(ii) Express ⟨xPB, ~yPB⟩in terms of xBB and yBB.
Exercise 7.14 (Deducing xBB). Let xPB be a real integrable passband signal that is
bandlimited to W Hz around the carrier frequency fc.
Show that it is possible that
xPB(t) be given at every epoch t ∈R by 2 Re

z(t) ei2πfct
for some complex signal z and
that z not be the baseband representation of xPB. Does this contradict Proposition 7.6.9?
Exercise 7.15 (Averaging the Instantaneous Power). Let xPB be a real integrable pass-
band signal that is bandlimited to W Hz around the carrier frequency fc, and let xBB be
its baseband representation. The signal xPB is squared, and the result is then fed to a
stable lowpass ﬁlter whose frequency response ˆh satisﬁes
ˆh(f) = 0,
|f| ≥W0
and
ˆh(f) = 1,
|f| ≤W1,
where W < W1 < W0 < 2fc −W. Express the ﬁlter’s output in terms of xBB.
146
Passband Signals and Their Representation
In terms of xPB
In terms of xA
In terms of xBB
xPB
2 Re(xA)
t 	→2 Re

xBB(t) ei2πfct
xPB ⋆

t 	→ei2πfct LPFWc(t)

xA
t 	→ei2πfct xBB(t)

t 	→e−i2πfct xPB(t)

⋆LPFWc
t 	→e−i2πfct xA(t)
xBB
ˆxPB
f 	→ˆxA(f) + ˆx∗
A(−f)
f 	→ˆxBB(f −fc) + ˆx∗
BB(−f −fc)
f 	→ˆxPB(f) I
f −fc
 ≤Wc

ˆxA
f 	→ˆxBB(f −fc)
f 	→ˆxPB(f + fc) I{|f| ≤Wc}
f 	→ˆxA(f + fc)
ˆxBB
BW of xPB around fc
BW of xA around fc
2 × BW of xBB
1
2 × BW of xPB around fc
1
2 × BW of xA around fc
BW of xBB
∥xPB∥2
2
2 ∥xA∥2
2
2 ∥xBB∥2
2
1
2 ∥xPB∥2
2
∥xA∥2
2
∥xBB∥2
2
Table 7.1: Table relating properties of a real integrable passband signal xPB that is bandlimited to W Hz around the carrier
frequency fc to those of its analytic representation xA and its baseband representation xBB. Same-row entries are equal. The cutoﬀ
frequency Wc is assumed to be in the range W/2 ≤Wc ≤2fc −W/2, and BW stands for bandwidth. The transformation from xPB
to xA is based on Proposition 7.5.2 with the function g in (d) being chosen as the mapping f 	→I{|f −fc| ≤Wc}.
Chapter 8
Complete Orthonormal Systems and the
Sampling Theorem
8.1
Introduction
Like Chapter 4, this chapter deals with the geometry of the space L2 of energy-
limited signals. Here, however, our focus is on inﬁnite-dimensional linear subspaces
of L2 and on the notion of a complete orthonormal system (CONS). As an
application of this geometric picture, we shall present the Sampling Theorem as
an orthonormal expansion with respect to a CONS for the space of energy-limited
signals that are bandlimited to W Hz.
8.2
Complete Orthonormal System
Recall that L2 is the space of all Lebesgue measurable signals u: R →C satisfying
 ∞
−∞
|u(t)|2 dt < ∞.
Also recall from Section 4.3 that a subset U of L2 is said to be a linear subspace of
L2 if U is nonempty and if the signal αu1 + βu2 is in U whenever u1, u2 ∈U and
α, β ∈C. A linear subspace is said to be ﬁnite-dimensional if there exists a ﬁnite
number of signals that span it; otherwise, it is said to be inﬁnite-dimensional. The
following are some examples of inﬁnite-dimensional linear subspaces of L2.
(i) The set of all functions of the form t 	→p(t) e−|t|, where p(t) is any polynomial
(of arbitrary degree).
(ii) The set of all energy-limited signals that vanish outside the interval [−1, 1]
(i.e., that map every t outside this interval to zero).
(iii) The set of all energy-limited signals that vanish outside some unspeciﬁed
ﬁnite interval (i.e., the set containing all signals u for which there exist some
a, b ∈R (depending on u) such that u(t) = 0 whenever t /∈[a, b]).
(iv) The set of all energy-limited signals that are bandlimited to W Hz.
147
148
Complete Orthonormal Systems and the Sampling Theorem
While a basis for an inﬁnite-dimensional subspace can be deﬁned,1 this notion does
not turn out to be very useful for our purposes. Much more useful to us is the
notion of a complete orthonormal system, which we shall deﬁne shortly.2
To motivate the deﬁnition, consider a bi-inﬁnite sequence . . . , φ−1, φ0, φ1, φ2, . . .
in L2 satisfying the orthonormality condition
⟨φℓ, φℓ′⟩= I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z,
(8.1)
and let u be an arbitrary element of L2. Deﬁne the signals
uL ≜
L

ℓ=−L
⟨u, φℓ⟩φℓ,
L = 1, 2, . . .
(8.2)
By Note 4.6.7, uL is the projection of the vector u onto the subspace spanned
by (φ−L, . . . , φL).
By the orthonormality (8.1), the tuple (φ−L, . . . , φL) is an
orthonormal basis for this subspace. Consequently, by Proposition 4.6.9,
∥u∥2
2 ≥
L

ℓ=−L
⟨u, φℓ⟩
2,
L = 1, 2, . . . ,
(8.3)
with equality if, and only if, u is indistinguishable from some linear combination
of

φ−L, . . . , φL

. This motivates us to explore the situation where (8.3) holds
with equality when L →∞and to hope that it corresponds to u being—in some
sense that needs to be made precise—indistinguishable from a limit of ﬁnite linear
combinations of . . . , φ−1, φ0, φ1, . . .
Deﬁnition 8.2.1 (Complete Orthonormal System). A bi-inﬁnite sequence of sig-
nals . . . , φ−1, φ0, φ1, . . . is said to form a complete orthonormal system or a
CONS for the linear subspace U of L2 if all three of the following conditions hold:
1) Each element of the sequence is in U
φℓ∈U,
ℓ∈Z.
(8.4)
2) The sequence satisﬁes the orthonormality condition
⟨φℓ, φℓ′⟩= I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z.
(8.5)
3) For every u ∈U we have
∥u∥2
2 =
∞

ℓ=−∞
⟨u, φℓ⟩
2,
u ∈U.
(8.6)
1A (Hamel) basis for a subspace U is a subset of U such that any function in U is equal to a
linear combination of a ﬁnite number of elements of the subset, and such that any ﬁnite number
of elements of the subset are linearly independent. More useful to us is the notion of a complete
orthonormal system. From a complete orthonormal system we require that each function in U be
approximately equal to a linear combination of a ﬁnite number of functions in the system.
2Mathematicians usually deﬁne a CONS only for closed subspaces.
Such subspaces are
discussed in Section 8.6.
8.2 Complete Orthonormal System
149
The following proposition considers equivalent deﬁnitions of a CONS and demon-
strates that if {φℓ} is a CONS for U, then, indeed, every element of U can be
approximated by a ﬁnite linear combination of the functions {φℓ}.
Proposition 8.2.2. Let U be a subspace of L2 and let the bi-inﬁnite sequence
. . . , φ−2, φ−1, φ0, φ1, . . . satisfy (8.4) & (8.5).
Then each of the following con-
ditions on {φℓ} is equivalent to the condition that {φℓ} forms a CONS for U:
(a) For every u ∈U and every ϵ > 0 there exists some positive integer L(ϵ) and
coeﬃcients α−L(ϵ), . . . , αL(ϵ) ∈C such that
u −
L(ϵ)

ℓ=−L(ϵ)
αℓφℓ

2
< ϵ.
(8.7)
(b) For every u ∈U
lim
L→∞
u −
L

ℓ=−L
⟨u, φℓ⟩φℓ

2
= 0.
(8.8)
(c) For every u ∈U
∥u∥2
2 =
∞

ℓ=−∞
⟨u, φℓ⟩
2.
(8.9)
(d) For every u, v ∈U
⟨u, v⟩=
∞

ℓ=−∞
⟨u, φℓ⟩⟨v, φℓ⟩∗.
(8.10)
Proof. Since (8.4) & (8.5) hold (by hypothesis), it follows that the additional
condition (c) is, by Deﬁnition 8.2.1, equivalent to {φℓ} being a CONS. It thus only
remains to show that the four conditions are equivalent. We shall prove this by
showing that (a) ⇔(b); that (b) ⇔(c); and that (c) ⇔(d).
That (b) implies (a) is obvious because nothing precludes us from choosing αℓin
(8.7) to be ⟨u, φℓ⟩. That (a) implies (b) follows because, by Note 4.6.7, the signal
L

ℓ=−L
⟨u, φℓ⟩φℓ,
which we denoted in (8.2) by uL, is the projection of u onto the linear subspace
spanned by (φ−L, . . . , φL) and as such, by Proposition 4.6.8, best approximates u
among all the signals in that subspace. Consequently, replacing αℓby ⟨u, φℓ⟩can
only reduce the LHS of (8.7).
To prove (b) ⇒(c) we ﬁrst note that by letting L tend to inﬁnity in (8.3) it follows
that
∥u∥2
2 ≥
∞

ℓ=−∞
⟨u, φℓ⟩
2,
u ∈L2,
(8.11)
150
Complete Orthonormal Systems and the Sampling Theorem
so to establish (c) we only need to show that if u is in U then ∥u∥2
2 is also upper-
bounded by the RHS of (8.11). To that end we ﬁrst upper-bound ∥u∥2 as
∥u∥2 =

	
u −
L

ℓ=−L
⟨u, φℓ⟩φℓ

+
L

ℓ=−L
⟨u, φℓ⟩φℓ

2
≤
u −
L

ℓ=−L
⟨u, φℓ⟩φℓ

2
+

L

ℓ=−L
⟨u, φℓ⟩φℓ

2
=
u −
L

ℓ=−L
⟨u, φℓ⟩φℓ

2
+
	
L

ℓ=−L
⟨u, φℓ⟩
2

1/2
,
u ∈L2,
(8.12)
where the ﬁrst equality follows by adding and subtracting a term; the subsequent in-
equality by the Triangle Inequality (Proposition 3.4.1); and the ﬁnal equality by the
orthonormality assumption (8.5) and the Pythagorean Theorem (Theorem 4.5.2).
If Condition (b) holds and if u is in U, then the RHS of (8.12) converges to the
square root of the inﬁnite sum 
ℓ∈Z|⟨u, φℓ⟩|2 and thus gives us the desired upper
bound on ∥u∥2.
We next prove (c) ⇒(b). We assume that (c) holds and that u is in U and set out
to prove (8.8). To that end we ﬁrst note that by the basic properties of the inner
product (3.6)–(3.10) and by the orthonormality (8.5) it follows that

u −
L

ℓ=−L
⟨u, φℓ⟩φℓ



u′
, φℓ′

= ⟨u, φℓ′⟩I{|ℓ′| > L},

ℓ′ ∈Z, u ∈L2

.
Consequently, if we apply (c) to the under-braced signal u′ (which for u ∈U is
also in U) we obtain that (c) implies
u −
L

ℓ=−L
⟨u, φℓ⟩φℓ

2
2
=

|ℓ|>L
⟨u, φℓ⟩
2,
u ∈U.
But by applying (c) to u we infer that the RHS of the above tends to zero as L
tends to inﬁnity, thus establishing (8.8) and hence (b).
We next prove (c) ⇔(d). The implication (d) ⇒(c) is obvious because we can
always choose v to be equal to u. We consequently focus on proving (c) ⇒(d).
We do so by assuming that u, v ∈U and calculating for every β ∈C
|β|2 ∥u∥2
2 + 2 Re

β⟨u, v⟩

+ ∥v∥2
2
= ∥β u + v∥2
2
=
∞

ℓ=−∞
⟨β u + v, φℓ⟩
2
=
∞

ℓ=−∞
β⟨u, φℓ⟩+ ⟨v, φℓ⟩
2
8.3 The Fourier Series
151
= |β|2
∞

ℓ=−∞
⟨u, φℓ⟩
2 + 2 Re
	
β
∞

ℓ=−∞
⟨u, φℓ⟩⟨v, φℓ⟩∗

+
∞

ℓ=−∞
⟨v, φℓ⟩
2,

u, v ∈U, β ∈C

,
(8.13)
where the ﬁrst equality follows by writing ∥βu + v∥2
2 as ⟨βu + v, βu + v⟩and using
the basic properties of the inner product (3.6)–(3.10); the second by applying (c)
to βu + v (which for u, v ∈U is also in U); the third by the basic properties of
the inner product; and the ﬁnal equality by writing the squared magnitude of a
complex number as its product by its conjugate. By applying (c) to u and likewise
to v, we now obtain from (8.13) that
2 Re

β⟨u, v⟩

= 2 Re
	
β
∞

ℓ=−∞
⟨u, φℓ⟩⟨v, φℓ⟩∗

,

u, v ∈U, β ∈C

,
which can only hold for all β ∈C (and in particular for both β = 1 and β = i) if
⟨u, v⟩=
∞

ℓ=−∞
⟨u, φℓ⟩⟨v, φℓ⟩∗,
u, v ∈U,
thus establishing (d).
We next describe the two complete orthonormal systems that will be of most in-
terest to us.
8.3
The Fourier Series
A CONS that you have probably already encountered is the one underlying the
Fourier Series representation. You may have encountered the Fourier Series in the
context of periodic functions, but we shall focus on a slightly diﬀerent view.
Proposition 8.3.1. For every T > 0, the functions {φℓ} deﬁned for every integer ℓ
by
φℓ: t 	→
1
√
2T
eiπℓt/T I{|t| ≤T}
(8.14)
form a CONS for the subspace

u ∈L2 : u(t) = 0 whenever |t| > T

of energy-limited signals that vanish outside the interval [−T, T ].
Proof. Follows from Theorem A.3.3 in Appendix A by substituting 2T for S.
Notice that in this case
⟨u, φℓ⟩=
1
√
2T
 T
−T
u(t) e−iπℓt/T dt
(8.15)
is the ℓ-th Fourier Series Coeﬃcient of u with respect to the interval [−T, T); see
Note A.3.5 in Appendix A with 2T substituted for S.
152
Complete Orthonormal Systems and the Sampling Theorem
Note 8.3.2. The dummy argument t is immaterial in Proposition 8.3.1. Indeed, if
we deﬁne for W > 0 the linear subspace
V =

g ∈L2 : g(f) = 0 whenever |f| > W

,
(8.16)
then the functions deﬁned for every integer ℓby
f 	→
1
√
2W
eiπℓf/W I{|f| ≤W}
(8.17)
form a CONS for this subspace.
This note will be crucial when we next discuss a CONS for the space of energy-
limited signals that are bandlimited to W Hz.
8.4
The Sampling Theorem
We next provide a CONS for the space of energy-limited signals that are band-
limited to W Hz. Recall that if x is an energy-limited signal that is bandlimited
to W Hz, then there exists a measurable function3 g: f 	→g(f) satisfying
g(f) = 0,
|f| > W
(8.18)
and
 W
−W
|g(f)|2 df < ∞,
(8.19)
such that
x(t) =
 W
−W
g(f) ei2πft df,
t ∈R.
(8.20)
Conversely, if g is any function satisfying (8.18) & (8.19), and if we deﬁne x via
(8.20) as the Inverse Fourier Transform of g, then x is an energy-limited signal that
is bandlimited to W Hz and its L2-Fourier Transform ˆx is equal to (the equivalence
class of) g.
Thus, if, as in (8.16), we denote by V the set of all functions (of frequency) satisfying
(8.18) & (8.19), then the set of all energy-limited signals that are bandlimited to W
Hz is just the image of V under the IFT, i.e., it is the set ˇV, where
ˇV ≜
ˇg : g ∈V

.
(8.21)
By the Mini Parseval Theorem (Proposition 6.2.6 (i)), if x1 and x2 are given by
ˇg1 and ˇg2, where g1, g2 are in V, then
⟨x1, x2⟩= ⟨g1, g2⟩,
(8.22)
i.e.,
⟨ˇg1, ˇg2⟩= ⟨g1, g2⟩,
g1, g2 ∈V.
(8.23)
The following lemma is a simple but very useful consequence of (8.23).
3Loosely speaking, this function is the Fourier Transform of x. But since x is not necessarily
integrable, its FT ˆx is an equivalence class of signals. Thus, more precisely, the equivalence class
of g is the L2 -Fourier Transform of x. Or, stated diﬀerently, g can be any one of the signals in
the equivalence class of ˆx that is zero outside the interval [−W, W ].
8.4 The Sampling Theorem
153
Lemma 8.4.1. If {ψℓ} is a CONS for the subspace V, which is deﬁned in (8.16),
then { ˇψℓ} is a CONS for the subspace ˇV, which is deﬁned in (8.21).
Proof. Let {ψℓ} be a CONS for the subspace V. By (8.23),
 ˇψℓ, ˇψℓ′
= ⟨ψℓ, ψℓ′⟩,
ℓ, ℓ′ ∈Z,
so our assumption that {ψℓ} is a CONS for V (and hence that, a fortiori, it satisﬁes
⟨ψℓ, ψℓ′⟩= I{ℓ= ℓ′} for all ℓ, ℓ′ ∈Z) implies that
 ˇψℓ, ˇψℓ′
= I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z.
It remains to verify that for every x ∈ˇV
∞

ℓ=−∞

x, ˇψℓ
2 = ∥x∥2
2 .
Equivalently, since every x ∈ˇV can be written as ˇg for some g ∈V, we need to
show that
∞

ℓ=−∞
ˇg, ˇψℓ
2 = ∥ˇg∥2
2 ,
g ∈V.
This follows from (8.23) and from our assumption that {ψℓ} is a CONS for V
because
∞

ℓ=−∞
ˇg, ˇψℓ
2 =
∞

ℓ=−∞
⟨g, ψℓ⟩
2
= ∥g∥2
2
= ∥ˇg∥2
2 ,
g ∈V,
where the ﬁrst equality follows from (8.23) (by substituting g for g1 and by sub-
stituting ψℓfor g2); the second from the assumption that {ψℓ} is a CONS for V;
and the ﬁnal equality from (8.23) (by substituting g for g1 and for g2).
Using this lemma and Note 8.3.2 we now derive a CONS for the subspace ˇV of
energy-limited signals that are bandlimited to W Hz.
Proposition 8.4.2 (A CONS for the Subspace of Energy-Limited Signals that
Are Bandlimited to W Hz).
(i) The sequence of signals that are deﬁned for every integer ℓby
t 	→
√
2W sinc(2Wt + ℓ)
(8.24)
forms a CONS for the space of energy-limited signals that are bandlimited
to W Hz.
154
Complete Orthonormal Systems and the Sampling Theorem
(ii) If x is an energy-limited signal that is bandlimited to W Hz, then its inner
product with the ℓ-th signal is given by its scaled sample at time −ℓ/(2W):
#
x, t 	→
√
2W sinc(2Wt + ℓ)
$
=
1
√
2W
x

−ℓ
2W

,
ℓ∈Z.
(8.25)
Proof. To prove Part (i) we recall that, by Note 8.3.2, the functions deﬁned for
every ℓ∈Z by
ψℓ: f 	→
1
√
2W
eiπℓf/W I{|f| ≤W}
(8.26)
form a CONS for the subspace V. Consequently, by Lemma 8.4.1, their Inverse
Fourier Transforms { ˇψℓ} form a CONS for ˇV.
It just remains to evaluate ˇψℓ
explicitly in order to verify that it is a scaled shifted sinc(·):
ˇψℓ(t) =
 ∞
−∞
ψℓ(f) ei2πft df
=
 W
−W
1
√
2W
eiπℓf/W ei2πft df
(8.27)
=
√
2W sinc(2Wt + ℓ),
(8.28)
where the last calculation can be veriﬁed by direct computation as in (6.35).
We next prove Part (ii). Since x is an energy-limited signal that is bandlimited
to W Hz, it follows that there exists some g ∈V such that
x = ˇg,
(8.29)
i.e.,
x(t) =
 W
−W
g(f) ei2πft df,
t ∈R.
(8.30)
Consequently,
#
x, t 	→
√
2W sinc(2Wt + ℓ)
$
=

x, ˇψℓ

=
ˇg, ˇψℓ

= ⟨g, ψℓ⟩
=
 W
−W
g(f)

1
√
2W
eiπℓf/W∗
df
=
1
√
2W
 W
−W
g(f) e−iπℓf/W df
=
1
√
2W
x

−ℓ
2W

,
ℓ∈Z,
where the ﬁrst equality follows from (8.28); the second by (8.29); the third by (8.23)
(with the substitution of g for g1 and ψℓfor g2); the fourth by the deﬁnition of
the inner product and by (8.26); the ﬁfth by conjugating the complex exponential;
and the ﬁnal equality by substituting −ℓ/(2W) for t in (8.30).
8.4 The Sampling Theorem
155
Using Proposition 8.4.2 and Proposition 8.2.2 we obtain the following L2 version
of the Sampling Theorem.
Theorem 8.4.3 (L2-Sampling Theorem). Let x be an energy-limited signal that
is bandlimited to W Hz, where W > 0, and let
T =
1
2W.
(8.31)
(i) The signal x can be reconstructed from the sequence . . . , x(−T), x(0), x(T), . . .
of its values at integer multiples of T in the sense that
lim
L→∞
 ∞
−∞
x(t) −
L

ℓ=−L
x(−ℓT) sinc
 t
T + ℓ

2
dt = 0.
(ii) The signal’s energy can be reconstructed from its samples via the relation
 ∞
−∞
|x(t)|2 dt = T
∞

ℓ=−∞
|x(ℓT)|2.
(iii) If y is another energy-limited signal that is bandlimited to W Hz, then
⟨x, y⟩= T
∞

ℓ=−∞
x(ℓT) y∗(ℓT).
Note 8.4.4. If T ≤1/(2W), then any energy-limited signal x that is bandlimited
to W Hz is also bandlimited to 1/(2T) Hz. Consequently, Theorem 8.4.3 continues
to hold if we replace (8.31) with the condition
0 < T ≤
1
2W.
(8.32)
Table 8.1 on Page 167 highlights the duality between the Sampling Theorem and
the Fourier Series.
We also mention here without proof a version of the Sampling Theorem that al-
lows one to reconstruct the signal pointwise, i.e., at every epoch t. Thus, while
Theorem 8.4.3 guarantees that, as more and more terms in the sum of the shifted
sinc functions are added, the energy in the error function tends to zero, the follow-
ing theorem demonstrates that at every ﬁxed time t the error tends to zero. The
assumptions are fairly mild and are satisﬁed by any energy-limited signal that is
bandlimited to W Hz. (For such signals a proof is sketched in Exercise 8.12.)
Theorem 8.4.5 (Pointwise Sampling Theorem). If the signal x can be represented
as
x(t) =
 W
−W
g(f) ei2πft df,
t ∈R
(8.33)
for some function g satisfying
 W
−W
|g(f)| df < ∞,
(8.34)
156
Complete Orthonormal Systems and the Sampling Theorem
and if 0 < T ≤1/(2W), then for every t ∈R
x(t) = lim
L→∞
L

ℓ=−L
x(−ℓT) sinc
	 t
T + ℓ

.
(8.35)
Proof. See (Pinsky, 2002, Chapter 4, Section 4.2.3, Theorem 4.2.13).
The Sampling Theorem goes by various names.
It is sometimes attributed to
Claude Elwood Shannon (1916–2001), the founder of Information Theory.
But
it also appears in the works of Vladimir Aleksandrovich Kotelnikov (1908–2005),
Harry Nyquist (1889–1976), and Edmund Taylor Whittaker (1873–1956). For fur-
ther references regarding the history of this result and for a survey of many related
results, see (Unser, 2000).
8.5
The Samples of the Convolution
Digital signal processing owes much of its success to the fact that the samples of a
convolution of bandlimited signals can be expressed in terms of the samples of the
signals. This is made precise in the following theorem.
Theorem 8.5.1 (The Samples of a Convolution). Let x and y be energy-limited
signals that are bandlimited to W Hz. Then

x ⋆y
 ℓ
2W

=
1
2W
∞

ν=−∞
x
 ν
2W

y
ℓ−ν
2W

,
ℓ∈Z.
(8.36)
Proof. For every integer ℓ,

x ⋆y
 ℓ
2W

=
 ∞
−∞
x(τ) y
 ℓ
2W −τ

dτ
=

x, τ 	→y∗ ℓ
2W −τ

=
1
2W
∞

ν=−∞
x
 ν
2W

y
ℓ−ν
2W

,
where the ﬁrst equality follows from the deﬁnition of the convolution; the second
follows by expressing the integral as an inner product; and where in the last step
we have noted that both x and τ 	→y∗
ℓ/(2W) −τ

are energy-limited signals
that are bandlimited to W Hz, so their inner product can be computed from their
samples using Theorem 8.4.3 (iii).
8.6
Closed Subspaces of L2
Our deﬁnition of a CONS for a subspace U is not quite standard, because we
only assumed that U is a linear subspace; we did not assume that U is closed.
In this section we shall deﬁne closed linear subspaces and derive a condition for
8.6 Closed Subspaces of L2
157
a sequence {φℓ} to form a CONS for a closed subspace U. (The set of energy-
limited signals that vanish outside the interval [−T, T ] is closed, as is the class of
energy-limited signals that are bandlimited to W Hz.)
Before proceeding to deﬁne closed linear subspaces, we pause here to recall that
the space L2 is complete.4
Theorem 8.6.1 (L2 Is Complete). If the sequence u1, u2, . . . of signals in L2 is
such that for any ϵ > 0 there exists a positive integer L(ϵ) such that
∥un −um∥2 < ϵ,
n, m > L(ϵ),
then there exists some function u ∈L2 such that
lim
n→∞∥u −un∥2 = 0.
Proof. See, for example, (Rudin, 1987, Chapter 3, Theorem 3.11).
Deﬁnition 8.6.2 (Closed Subspace). A linear subspace U of L2 is said to be
closed if for any sequence of signals u1, u2, . . . in U and any u ∈L2, the condition
∥u −un∥2 →0 implies that u is indistinguishable from some element of U.
Before stating the next theorem we remind the reader that a bi-inﬁnite sequence
of complex numbers . . . , α−1, α0, α1, . . . is said to be square summable if
∞

ℓ=−∞
|αℓ|2 < ∞.
Theorem 8.6.3 (Riesz-Fischer). Let U be a closed linear subspace of L2, and let
the bi-inﬁnite sequence . . . , φ−1, φ0, φ1, . . . satisfy (8.4) & (8.5). Let the bi-inﬁnite
sequence of complex numbers . . . , α−1, α0, α1, . . . be square summable. Then there
exists an element u in U satisfying
lim
L→∞
u −
L

ℓ=−L
αℓφℓ

2
= 0;
(8.37a)
⟨u, φℓ⟩= αℓ,
ℓ∈Z;
(8.37b)
and
∥u∥2
2 =
∞

ℓ=−∞
|αℓ|2.
(8.37c)
Proof. Deﬁne for every positive integer L
uL =
L

ℓ=−L
αℓφℓ,
L ∈N.
(8.38)
4This property is usually stated about L2 but we prefer to work with L2 .
158
Complete Orthonormal Systems and the Sampling Theorem
Since, by hypothesis, U is a linear subspace and the signals {φℓ} are all in U, it fol-
lows that uL ∈U. By the orthonormality assumption (8.5) and by the Pythagorean
Theorem (Theorem 4.5.2), it follows that
∥un −um∥2
2 =

min{m,n}<|ℓ|≤max{m,n}
|αℓ|2
≤

min{m,n}<|ℓ|<∞
|αℓ|2,
n, m ∈N.
From this and from the square summability of {αℓ}, it follows that for any ϵ > 0
we have that ∥un −um∥2 is smaller than ϵ whenever both n and m are suﬃciently
large. By the completeness of L2 it thus follows that there exists some u′ ∈L2
such that
lim
L→∞∥u′ −uL∥2 = 0.
(8.39)
Since U is closed, and since uL is in U for every L ∈N, it follows from (8.39) that u′
is indistinguishable from some element u of U:
∥u −u′∥2 = 0.
(8.40)
It now follows from (8.39) and (8.40) that
lim
L→∞∥u −uL∥2 = 0,
(8.41)
as can be veriﬁed using (4.14) (with the substitution (u′ −uL) for x and (u −u′)
for y). Combining (8.41) with (8.38) establishes (8.37a).
To establish (8.37b) we use (8.41) and the continuity of the inner product (Propo-
sition 3.4.2) to calculate ⟨u, φℓ⟩for every ﬁxed ℓ∈Z as follows:
⟨u, φℓ⟩= lim
L→∞⟨uL, φℓ⟩
= lim
L→∞

L

ℓ′=−L
αℓ′ φℓ′, φℓ

= lim
L→∞αℓI{|ℓ| ≤L}
= αℓ,
ℓ∈Z,
where the ﬁrst equality follows from (8.41) and from the continuity of the inner
product (Proposition 3.4.2); the second by (8.38); the third by the orthonormality
(8.5); and the ﬁnal equality because αℓI{|ℓ| ≤L} is equal to αℓ, whenever L is
large enough (i.e., exceeds |ℓ|).
It remains to prove (8.37c). By the orthonormality of {φℓ} and the Pythagorean
Theorem (Theorem 4.5.2)
∥uL∥2
2 =
L

ℓ=−L
αℓ
2,
L ∈N.
(8.42)
Also, by (4.14) (with the substitution of u for x and of (uL −u) for y) we obtain
∥u∥2 −∥u −uL∥2 ≤∥uL∥2 ≤∥u∥2 + ∥u −uL∥2 .
(8.43)
8.6 Closed Subspaces of L2
159
It now follows from (8.43), (8.41), and the Sandwich Theorem5 that
lim
L→∞∥uL∥2 = ∥u∥2 ,
(8.44)
which combines with (8.42) to prove (8.37c).
By applying Theorem 8.6.3 to the space of energy-limited signals that are band-
limited to W Hz and to the CONS that we derived for that space in Proposi-
tion 8.4.2 we obtain:
Proposition 8.6.4. Any square-summable bi-inﬁnite sequence of complex numbers
corresponds to the samples at integer multiples of T of an energy-limited signal that
is bandlimited to 1/(2T) Hz. Here T > 0 is arbitrary.
Proof. Let . . . , β−1, β0, β1, . . . be a square-summable bi-inﬁnite sequence of com-
plex numbers, and let W = 1/(2T). We seek a signal u that is an energy-limited
signal that is bandlimited to W Hz and whose samples are given by u(ℓT) = βℓ,
for every integer ℓ. Since the set of all energy-limited signals that are bandlimited
to W Hz is a closed linear subspace of L2, and since the sequence { ˇψℓ} (given ex-
plicitly in (8.28) as ˇψℓ: t 	→
√
2W sinc(2Wt+ℓ)) is an orthonormal sequence in that
subspace, it follows from Theorem 8.6.3 (with the substitution of ˇψℓfor φℓand of
β−ℓ/
√
2W for αℓ) that there exists an energy-limited signal u that is bandlimited
to W Hz and for which

u, ˇψℓ

=
1
√
2W
β−ℓ,
ℓ∈Z.
(8.45)
By Proposition 8.4.2 (ii),

u, ˇψℓ

=
1
√
2W
u(−ℓT),
ℓ∈Z,
(8.46)
so by (8.45) and (8.46)
u(−ℓT) = β−ℓ,
ℓ∈Z.
We now give an alternative characterization of a CONS for a closed subspace of L2.
This result will not be used later in the book.
Proposition 8.6.5 (Characterization of a CONS for a Closed Subspace).
(i) If the bi-inﬁnite sequence {φℓ} is a CONS for the linear subspace U ⊆L2,
then an element of U whose inner product with φℓis zero for every integer ℓ
must have zero energy:

⟨u, φℓ⟩= 0,
ℓ∈Z

=⇒

∥u∥2 = 0

,
u ∈U.
(8.47)
5The Sandwich Theorem states that if the sequences of real number {an}, {bn} and {cn} are
such that bn ≤an ≤cn for every n, and if the sequences {bn} and {cn} converge to the same
limit, then {an} also converges to that limit.
160
Complete Orthonormal Systems and the Sampling Theorem
(ii) If U is a closed subspace of L2 and if the bi-inﬁnite sequence {φℓ} satisﬁes
(8.4) & (8.5), then Condition (8.47) is equivalent to the condition that {φℓ}
forms a CONS for U.
Proof. We begin by proving Part (i). By deﬁnition, if {φℓ} is a CONS for U, then
(8.6) must hold for every u ∈U. Consequently, if for some u ∈U we have that
⟨u, φℓ⟩is zero for all ℓ∈Z, then the RHS of (8.6) is zero and hence the LHS must
also be zero, thus showing that u must be of zero energy.
We next turn to Part (ii) and assume that U is closed and that the bi-inﬁnite
sequence {φℓ} satisﬁes (8.4) & (8.5). That the condition that {φℓ} is a CONS
implies Condition (8.47) follows from Part (i). It thus remains to show that if
Condition (8.47) holds, then {φℓ} is a CONS. To prove this we now assume that U
is a closed subspace; that {φℓ} satisﬁes (8.4) & (8.5); and that (8.47) holds and
set out to prove that
∥u∥2
2 =
∞

ℓ=−∞
⟨u, φℓ⟩
2,
u ∈U.
(8.48)
To establish (8.48) ﬁx some arbitrary u ∈U. Since U ⊆L2, the fact that u is
in U implies that it is of ﬁnite energy, which combines with (8.3) to imply that the
bi-inﬁnite sequence . . . , ⟨u, φ−1⟩, ⟨u, φ0⟩, ⟨u, φ1⟩, . . . is square summable. Since,
by hypothesis, U is closed, this implies, by Theorem 8.6.3 (with the substitution
of ⟨u, φℓ⟩for αℓ), that there exists some element ˜u ∈U such that
lim
L→∞
˜u −
L

ℓ=−L
⟨u, φℓ⟩φℓ

2 = 0;
(8.49a)
⟨˜u, φℓ⟩= ⟨u, φℓ⟩,
ℓ∈Z;
(8.49b)
and
∥˜u∥2
2 =
∞

ℓ=−∞
⟨u, φℓ⟩
2.
(8.49c)
By (8.49b) it follows that the element u −˜u of U satisﬁes
⟨u −˜u, φℓ⟩= 0,
ℓ∈Z,
and hence, by Condition (8.47), is of zero energy
∥u −˜u∥2 = 0,
(8.50)
so u and ˜u are indistinguishable and hence
∥u∥2 = ∥˜u∥2 .
This combines with (8.49c) to prove (8.48).
8.7 An Isomorphism
161
8.7
An Isomorphism
In this section we collect the results of Theorem 8.4.3 and Proposition 8.6.4 into a
single theorem about the isomorphism between the space of energy-limited signals
that are bandlimited to W Hz and the space of square-summable sequences. This
theorem is at the heart of quantization schemes for bandlimited signals. It demon-
strates that to describe a bandlimited signal one can use discrete-time processing to
quantize its samples and one can then map the quantized samples to a bandlimited
signal. The energy in the error signal corresponding to the diﬀerence between the
original signal and its description is then proportional to the sum of the squared
diﬀerences between the samples of the original signal and the quantized version.
Theorem 8.7.1 (Bandlimited Signals and Square-Summable Sequences). Let
T = 1/(2W), where W > 0.
(i) If u is an energy-limited signal that is bandlimited to W Hz, then the bi-
inﬁnite sequence
. . . , u(−T), u(0), u(T), u(2T), . . .
consisting of its samples taken at integer multiples of T is square summable
and
T
∞

ℓ=−∞
u(ℓT)
2 = ∥u∥2
2 .
(ii) More generally, if u and v are energy-limited signals that are bandlimited
to W Hz, then
T
∞

ℓ=−∞
u(ℓT) v∗(ℓT) = ⟨u, v⟩.
(iii) If {αℓ} is a bi-inﬁnite square-summable sequence, then there exists an energy-
limited signal u that is bandlimited to W Hz such that its samples are given
by
u(ℓT) = αℓ,
ℓ∈Z.
(iv) The mapping that maps every energy-limited signal that is bandlimited to W
Hz to the square-summable sequence consisting of its samples is linear.
8.8
Prolate Spheroidal Wave Functions
The following result, which is due to Slepian and Pollak, will not be used in this
book; it is included for its sheer beauty.
Theorem 8.8.1. Let the positive constants T > 0 and W > 0 be given.
Then
there exists a sequence of real functions φ1, φ2, . . . and a corresponding sequence
of positive numbers λ1 > λ2 > · · · such that:
162
Complete Orthonormal Systems and the Sampling Theorem
(i) The sequence φ1, φ2, . . . forms a CONS for the space of energy-limited signals
that are bandlimited to W Hz, so, a fortiori,
 ∞
−∞
φℓ(t) φℓ′(t) dt = I{ℓ= ℓ′},
ℓ, ℓ′ ∈N.
(8.51a)
(ii) The sequence of scaled and time-windowed functions ˜φ1,w, ˜φ2,w, . . . deﬁned at
every t ∈R by
˜φℓ,w(t) =
1
√λℓ
φℓ(t) I
'
|t| ≤T
2
(
,
ℓ∈N
(8.51b)
forms a CONS for the subspace of L2 consisting of all energy-limited signals
that vanish outside the interval [−T/2, T/2], so, a fortiori,
 T/2
−T/2
φℓ(t) φℓ′(t) dt = λℓI{ℓ= ℓ′},
ℓ, ℓ′ ∈N.
(8.51c)
(iii) For every t ∈R,
 T/2
−T/2
LPFW(t −τ) φℓ(τ) dτ = λℓφℓ(t),
ℓ∈N.
(8.51d)
The above functions φ1, φ2, . . . are related to Prolate Spheroidal Wave Functions.
For a discussion of this connection, a proof of this theorem, and numerous appli-
cations see (Slepian and Pollak, 1961) and (Slepian, 1976).
8.9
Exercises
Exercise 8.1 (Orthogonality with One Exception). Let . . . , φ−1, φ0, φ1, . . . form a CONS
for the linear subspace U of L2. Show that if u ∈U is orthogonal to every φℓexcept
possibly to φ0, then u is indistinguishable from α φ0 for some α ∈C.
Exercise 8.2 (Expansion of a Function). Expand the function t →sinc2(t/2) as an or-
thonormal expansion in the functions
. . . , t →sinc(t + 2), t →sinc(t + 1), t →sinc(t), t →sinc(t −1), t →sinc(t −2), . . .
Exercise 8.3 (Expanding a Delayed sinc). Prove that for every W > 0 and t, τ ∈R,
sinc

2W(t −τ)

=
∞

ℓ=−∞
sinc

2Wτ + ℓ

sinc

2Wt + ℓ

.
Exercise 8.4 (The Perils of Sub-Nyquist Sampling).
(i) Show that if T exceeds 1/(2W) then there exists a signal x of positive energy whose
samples x(ℓT) are zero for every integer ℓ∈Z and that is nonetheless an energy-
limited signal that is bandlimited to W Hz. This signal and the all-zero signal have
identical samples but are not indistinguishable.
8.9 Exercises
163
(ii) Show that if T exceeds 1/(2W) and y is an energy-limited signal that is bandlimited
to W Hz, then there exists a signal ˜y that is not indistinguishable from y and that
nonetheless is an energy-limited signal that is bandlimited to W Hz and whose
samples satisfy ˜y(ℓT) = y(ℓT) for every ℓ∈Z.
Hint: For Part (i) consider the product of a sinc and a sine wave.
Exercise 8.5 (Inner Product with a Bandlimited Signal). Show that if x is an energy-
limited signal that is bandlimited to W Hz, and if y ∈L2, then
⟨x, y⟩= Ts
∞

ℓ=−∞
x(ℓTs) y∗
LPF(ℓTs),
where yLPF is the result of passing y through an ideal unit-gain lowpass ﬁlter of bandwidth
W Hz, and where Ts = 1/(2W).
Exercise 8.6 (Approximating a Sinc by Sincs). Find the coeﬃcients {αℓ} that minimize
the integral
 ∞
−∞
	
sinc(3t/2) −
∞

ℓ=−∞
αℓsinc(t −ℓ)

2
dt.
What is the value of this integral when the coeﬃcients are chosen as you suggest?
Exercise 8.7 (More on Energy and Samples). Let x be an integrable signal that is band-
limited to W Hz such that the signal t →t x(t) is also integrable. Express the energy in
the signal t →t x(t) in terms of the samples . . . , x(0), x(1/(2W)), . . . of x.
Hint: Recall Exercise 6.2.
Exercise 8.8 (The Derivative as an Inner Product). Let u be an energy-limited signal
that is bandlimited to W Hz. Show that its derivative at zero can be expressed as the
inner product ⟨u, v⟩, where
v(t) = −d
dt2W sinc(2Wt),
t ∈R.
Exercise 8.9 (Integrability and Summability). Show that if x is an integrable signal that
is bandlimited to W Hz and if Ts = 1/(2W), then
∞

ℓ=−∞
x(ℓTs)
 < ∞.
Hint: Let h be the IFT of the mapping in (7.16) when we substitute 0 for fc; 2W for W;
and 2W + Δ for Wc, where Δ > 0.
Express x(ℓTs) as

x ⋆h

(ℓTs); upper-bound the
convolution integral using Proposition 2.4.1; and use Fubini’s Theorem to swap the order
of summation and integration.
Exercise 8.10 (More on Integrability and Summability). Use Exercise 8.9 to show that if
x is an integrable signal that is bandlimited to W Hz and if Ts > 0 is arbitrary, then
∞

ℓ=−∞
|x(ℓTs)| < ∞.
164
Complete Orthonormal Systems and the Sampling Theorem
Exercise 8.11 (Approximating an Integral by a Sum). One often approximates an integral
by a sum, e.g.,
 ∞
−∞
x(t) dt ≈δ
∞

ℓ=−∞
x(ℓδ).
(i) Show that if u is an energy-limited signal that is bandlimited to W Hz, then, for
every 0 < δ ≤1/(2W), the above approximation is exact when we substitute |u(t)|2
for x(t), that is,
 ∞
−∞
|u(t)|2 dt = δ
∞

ℓ=−∞
|u(ℓδ)|2.
(ii) Show that if x is an integrable signal that is bandlimited to W Hz, then, for every
0 < δ ≤1/(2W),
 ∞
−∞
x(t) dt = δ
∞

ℓ=−∞
x(ℓδ).
(iii) Consider the signal u: t →sinc(t). Compute ∥u∥2
2 using Parseval’s Theorem and
use the result and Part (i) to show that
∞

m=0
1
(2m + 1)2 = π2
8 .
For a stronger version of Part (ii), see (Pinsky, 2002, Chapter 4, Proposition 4.2.14).
Exercise 8.12 (On the Pointwise Sampling Theorem).
(i) Let the functions g, g0, g1, . . . be elements of L2 that are zero outside the interval
[−W, W ]. Show that if ∥g −gn∥2 →0, then for every t ∈R
lim
n→∞
 ∞
−∞
gn(f) ei2πft df =
 ∞
−∞
g(f) ei2πft df.
(ii) Use Part (i) to prove the Pointwise Sampling Theorem for energy-limited signals.
Exercise 8.13 (The Error Due to Sub-Nyquist Sampling). Let x be an energy-limited
signal that is bandlimited to W Hz, and let Ts > 0 be possibly larger than 1/(2W). Deﬁne
ˆxσ(f) =
∞

η=−∞
ˆx
	
f + η
Ts

,
f ∈R.
(i) Show that for every integer ℓwe can express T −1/2
s
x(−ℓTs) as
1
√Ts
x(−ℓTs) =

1
2Ts
−
1
2Ts
ˆxσ(f)
1
√Ts
e−i2πfℓTs df,
i.e., as the ℓ-th Fourier Series Coeﬃcient of ˆxσ w.r.t. the interval
$
−1/(2Ts), 1/(2Ts)

.
(ii) Conclude that
∞

ℓ=−∞
x(−ℓTs) sinc
	 t
Ts + ℓ

=

1
2Ts
−
1
2Ts
ˆxσ(f) ei2πft df,
t ∈R.
8.9 Exercises
165
(iii) Show that

1
2Ts
−
1
2Ts
ˆxσ(f) ei2πft df =
∞

η=−∞
e−i2πtη/Ts

η
Ts +
1
2Ts
η
Ts −
1
2Ts
ˆx(f) ei2πft df,
t ∈R,
and
x(t) =
∞

η=−∞

η
Ts +
1
2Ts
η
Ts −
1
2Ts
ˆx(f) ei2πft df,
t ∈R.
(iv) Conclude that
x(t) −
∞

ℓ=−∞
x(−ℓTs) sinc
	 t
Ts + ℓ

 ≤2

|f|>
1
2Ts
ˆx(f)
 df,
t ∈R.
(See (Pinsky, 2002, Theorem 4.2.13).)
Hint: For Part (ii) you may ﬁnd the Pointwise Sampling Theorem useful.
Exercise 8.14 (Inner Product between Passband Signals). Let xPB and yPB be real
energy-limited passband signals that are bandlimited to W Hz around the carrier fre-
quency fc.
Let xBB and yBB be their corresponding baseband representations.
Let
T = 1/W. Show that
⟨xPB, yPB⟩= 2T Re

∞

ℓ=−∞
xBB(ℓT) y∗
BB(ℓT)

.
Exercise 8.15 (Orthogonality to Time Shifts). Let x and y be energy-limited signals that
are bandlimited to W Hz. Show that x is orthogonal to all the time shifts of y by integer
multiples of 1/(2W) if, and only if, f →ˆx(f) ˆy(f) is zero outside a set of frequencies of
Lebesgue measure zero. Conclude that if ˆy(f) is nonzero for all f ∈[−W, W] then x is
orthogonal to all the time shifts of y by integer multiples of 1/(2W) if, and only if, x is
the all-zero signal.
Exercise 8.16 (Closed Subspaces). Let U denote the set of energy-limited signals that
vanish outside some interval. Thus, u is in U if, and only if, there exist a, b ∈R (that may
depend on u) such that u(t) is zero whenever t /∈[a, b]. Show that U is a linear subspace
of L2, but that it is not closed.
Exercise 8.17 (Projection onto an Inﬁnite-Dimensional Subspace).
(i) Let U ⊂L2 be the set of all elements of L2 that are zero outside the interval
[−1, +1]. Given v ∈L2, let w be the signal w: t →v(t) I{|t| ≤1}. Show that w is
in U and that v −w is orthogonal to every signal in U.
(ii) Let U be the subspace of energy-limited signals that are bandlimited to W Hz.
Given v ∈L2, deﬁne w = v ⋆LPFW. Show that w is in U and that v −w is
orthogonal to every signal in U.
Exercise 8.18 (A Maximization Problem). Of all unit-energy real signals that are band-
limited to W Hz, which one has the largest value at t = 0? What is its value at t = 0?
Repeat for t = 17.
166
Complete Orthonormal Systems and the Sampling Theorem
Exercise 8.19 (Deriving a CONS). Let U be a closed linear subspace of L2, and let the
signals . . . , u−1, u0, u1, . . . be in U. Let . . . , φ−1, φ0, φ1, . . . be an orthonormal sequence
in U such that, for every k ∈N,
span (u−k, . . . , uk) = span (φ−k, . . . , φk) .
Show that if no unit-energy element of U is orthogonal to all the signals {uℓ}, then {φℓ}
is a CONS for U.
Hint: Use Proposition 8.6.5.
Exercise 8.20 (A Complete Subspace). A linear subspace of L2 is said to be complete
if the following statement holds: if a sequence u1, u2, . . . ∈U is such that to every ϵ > 0
there corresponds some L(ϵ) for which ∥un −um∥2 < ϵ whenever n, m ≥L(ϵ), then the
sequence converges to some u ∈U in the sense that ∥un −u∥2 tends to 0 as n →∞.
(i) Use the fact that L2 is complete (Theorem 8.6.1) to prove that the space of energy-
limited signals that are zero outside the interval [−T, T ] is complete.
(ii) Show that the space of energy-limited signals that are bandlimited to W Hz is
complete. This combined with Note 6.4.2 shows that this space is a Hilbert Space.
8.9 Exercises
167
ˇV
V
energy-limited signals that
energy-limited functions that
are bandlimited to W Hz
vanish outside the interval [−W, W)
generic element of ˇV
generic element of V
x: t 	→x(t)
g: f 	→g(f)
a CONS
a CONS
. . . , ˇψ−1, ˇψ0, ˇψ1, . . .
. . . , ψ−1, ψ0, ψ1, . . .
ˇψℓ(t) =
√
2W sinc

2Wt + ℓ

ψℓ(f) =
1
√
2W
eiπℓf/W I{−W ≤f < W}
inner product
inner product

x, ˇψℓ

⟨g, ψℓ⟩
 ∞
−∞
x(t)
√
2W sinc

2Wt + ℓ

dt
 W
−W
g(f)
1
√
2W
e−iπℓf/W df
=
1
√
2W
x

−ℓ
2W

= g’s ℓ-th Fourier Series Coeﬃcient (≜cℓ)
Sampling Theorem
Fourier Series
lim
L→∞
x −
L

ℓ=−L

x, ˇψℓ
 ˇψℓ

2
= 0,
lim
L→∞
g −
L

ℓ=−L
⟨g, ψℓ⟩ψℓ

2
= 0,
i.e.,
i.e.,
 ∞
−∞
x(t) −
L

ℓ=−L
x

−ℓ
2W

sinc

2Wt + ℓ

2
dt →0
 W
−W
g(f) −
L

ℓ=−L
cℓ
1
√
2W
eiπℓf/W

2
df →0
Table 8.1: The duality between the Sampling Theorem and the Fourier Series Representation.
Chapter 9
Sampling Real Passband Signals
9.1
Introduction
In this chapter we present a procedure for representing a real energy-limited pass-
band signal that is bandlimited to W Hz around a carrier frequency fc using com-
plex numbers that we accumulate at a rate of W complex numbers per second.
Alternatively, since we can represent every complex number as a pair of real num-
bers (its real and imaginary parts), we can view our procedure as allowing us to
represent the signal using real numbers that we accumulate at a rate of 2W real
numbers per second. Thus we propose to accumulate
2W real samples per second,
or
W complex samples per second.
Note that the carrier frequency fc plays no role here (provided, of course, that
fc > W/2): the rate at which we accumulate real numbers to describe the passband
signal does not depend on fc.1
For real baseband signals this feat is easily accomplished using the Sampling The-
orem as follows. A real energy-limited baseband signal that is bandlimited to W
Hz can be reconstructed from its (real) samples that are taken 1/(2W) seconds
apart (Theorem 8.4.3), so the signal can be reconstructed from real numbers (its
samples) that are being accumulated at the rate of 2W real samples per second.
For passband signals we cannot achieve this feat by invoking the Sampling Theorem
directly. Even though, by Corollary 7.7.3, every energy-limited passband signal xPB
that is bandlimited to W Hz around the center frequency fc is also an energy-limited
bandlimited (baseband) signal, we are only guaranteed that xPB be bandlimited
to fc + W/2 Hz. Consequently, if we were to apply the Sampling Theorem directly
1But the carrier frequency fc does play a role in the reconstruction.
168
9.2 Complex Sampling
169
to xPB we would have to sample xPB every 1/(2fc + W) seconds, i.e., we would
have to accumulate 2fc + W real numbers per second, which can be much higher
than 2W, especially in wireless communications where fc ≫W.
Instead of applying the Sampling Theorem directly to xPB, the idea is to apply it to
xPB’s baseband representation xBB. Suppose that xPB is a real energy-limited pass-
band signal that is bandlimited to W Hz around the carrier frequency fc. By Theo-
rem 7.7.12 (vii), it can be represented using its baseband representation xBB, which
is a complex baseband signal that is bandlimited to W/2 Hz (Theorem 7.7.12 (v)).
Consequently, by the L2-Sampling Theorem (Theorem 8.4.3), xBB can be described
by sampling it at a rate of W samples per second. Since the baseband signal is
complex, its samples are also, in general, complex. Thus, in sampling xBB every
1/W seconds we are accumulating one complex sample every 1/W seconds. Since
we can recover xPB from xBB and fc, it follows that, as we wanted, we have found
a way to describe xPB using complex numbers that are accumulated at a rate of W
complex numbers per second.
9.2
Complex Sampling
Recall from Section 7.7.3 (Theorem 7.7.12) that a real energy-limited passband
signal xPB that is bandlimited to W Hz around a carrier frequency fc can be
represented using its baseband representation xBB as
xPB(t) = 2 Re

ei2πfct xBB(t)

,
t ∈R,
(9.1)
where xBB is given by
xBB =

t 	→e−i2πfct xPB(t)

⋆LPFWc,
(9.2)
and where the cutoﬀfrequency Wc can be chosen arbitrarily in the range
W
2 ≤Wc ≤2fc −W
2 .
(9.3)
The signal xBB is an energy-limited complex baseband signal that is bandlimited
to W/2 Hz. Being bandlimited to W/2 Hz, it follows from the L2-Sampling The-
orem that xBB can be reconstructed from its samples taken 1/(2 (W/2)) = 1/W
seconds apart. We denote these samples by
xBB
 ℓ
W

,
ℓ∈Z
(9.4)
so, by (9.2),
xBB
 ℓ
W

=

t 	→e−i2πfct xPB(t)

⋆LPFWc
 ℓ
W

,
ℓ∈Z.
(9.5)
These samples are, in general, complex. Their real part corresponds to the samples
of the in-phase component Re(xBB), which, by (7.42a), is given by
Re(xBB) =

t 	→xPB(t) cos(2πfct)

⋆LPFWc
(9.6)
170
Sampling Real Passband Signals
cos(2πfct)
90◦
×
×
xPB(t)
xPB(t) cos(2πfct)
−xPB(t) sin(2πfct)
LPFWc
LPFWc
W
2 ≤Wc ≤2fc −W
2
Re

xBB(t)

Im

xBB(t)

Re

xBB(ℓ/W)

Im

xBB(ℓ/W)

ℓ/W
ℓ/W
Figure 9.1: Sampling of a real passband signal xPB.
(for Wc satisfying (9.3)) and their imaginary part corresponds to the samples of
the quadrature-component Im(xBB), which, by (7.42b), is given by
Im(xBB) = −

t 	→xPB(t) sin(2πfct)

⋆LPFWc .
(9.7)
Thus,
xBB
 ℓ
W

=

t 	→xPB(t) cos(2πfct)

⋆LPFWc
 ℓ
W

−i

t 	→xPB(t) sin(2πfct)

⋆LPFWc
 ℓ
W

,
ℓ∈Z.
(9.8)
The procedure of taking a real passband signal xPB and sampling its baseband
representation to obtain the samples (9.8) is called complex sampling.
It is
depicted in Figure 9.1.
The passband signal xPB is ﬁrst separately multiplied
by t 	→cos(2πfct) and by t 	→−sin(2πfct), which are generated using a local
oscillator and a 90◦-phase shifter. Each result is fed to a lowpass ﬁlter with cutoﬀ
frequency Wc to produce the in-phase and quadrature components respectively.
Each component is then sampled at a rate of W real samples per second.
9.3
Reconstructing xPB from its Complex Samples
By the Pointwise Sampling Theorem (Theorem 8.4.5) applied to the energy-limited
signal xBB (which is bandlimited to W/2 Hz) we obtain
xBB(t) =
∞

ℓ=−∞
xBB
 ℓ
W

sinc(Wt −ℓ),
t ∈R.
(9.9)
9.3 Reconstructing xPB from its Complex Samples
171
Consequently, by (9.1), xPB can be reconstructed from its complex samples as
xPB(t) = 2 Re
	
ei2πfct
∞

ℓ=−∞
xBB
 ℓ
W

sinc(Wt −ℓ)

,
t ∈R.
(9.10a)
Since the sinc (·) function is real, this can also be written as
xPB(t) = 2
∞

ℓ=−∞
Re
	
ei2πfct xBB
 ℓ
W

sinc(Wt −ℓ),
t ∈R,
(9.10b)
or, using real operations, as
xPB(t) = 2
∞

ℓ=−∞
Re
	
xBB
 ℓ
W

sinc(Wt −ℓ) cos(2πfct)
−2
∞

ℓ=−∞
Im
	
xBB
 ℓ
W

sinc(Wt −ℓ) sin(2πfct),
t ∈R.
(9.10c)
As we next show, we can obtain another form of convergence using the L2-Sampling
Theorem (Theorem 8.4.3). We ﬁrst note that by that theorem
lim
L→∞
t 	→xBB(t) −
L

ℓ=−L
xBB
 ℓ
W

sinc(Wt −ℓ)

2
2
= 0.
(9.11)
We next note that xBB is the baseband representation of xPB and that—as can be
veriﬁed directly or by using Proposition 7.7.9—the mapping
t 	→xBB(ℓ/W) sinc(Wt −ℓ)
is the baseband representation of the real passband signal
t 	→2 Re
	
ei2πfct xBB
 ℓ
W

sinc(Wt −ℓ)

.
Consequently, by linearity (Theorem 7.7.12 (ii)), the mapping
t 	→xBB(t) −
L

ℓ=−L
xBB
 ℓ
W

sinc(Wt −ℓ)
is the baseband representation of the real passband signal
t 	→xPB(t) −2 Re
	
ei2πfct
L

ℓ=−L
xBB
 ℓ
W

sinc(Wt −ℓ)

and hence, by Theorem 7.7.12 (iii),
t 	→xPB(t) −2 Re
	
ei2πfct
L

ℓ=−L
xBB
 ℓ
W

sinc(Wt −ℓ)


2
2
= 2
t 	→xBB(t) −
L

ℓ=−L
xBB
 ℓ
W

sinc(Wt −ℓ)

2
2
.
(9.12)
172
Sampling Real Passband Signals
Combining (9.11) with (9.12) yields the L2 convergence
lim
L→∞
t 	→xPB(t) −2 Re
	
ei2πfct
L

ℓ=−L
xBB
 ℓ
W

sinc(Wt −ℓ)


2
= 0.
(9.13)
We summarize how a passband signal can be reconstructed from the samples of its
baseband representation in the following theorem.
Theorem 9.3.1 (The Sampling Theorem for Passband Signals). Let xPB be a
real energy-limited passband signal that is bandlimited to W Hz around the carrier
frequency fc. For every integer ℓ, let xBB(ℓ/W) denote the time-(ℓ/W) sample of
the baseband representation xBB of xPB; see (9.5) and (9.8).
(i) xPB can be pointwise reconstructed from the samples using the relation
xPB(t) = 2 Re
	
ei2πfct
∞

ℓ=−∞
xBB
 ℓ
W

sinc(Wt −ℓ)

,
t ∈R.
(ii) xPB can also be reconstructed from the samples in the L2 sense
lim
L→∞
 ∞
−∞
-
xPB(t) −2 Re
	
ei2πfct
L

ℓ=−L
xBB
 ℓ
W

sinc(Wt −ℓ)

.2
dt = 0.
(iii) The energy in xPB can be reconstructed from the sum of the squared magni-
tudes of the samples via
∥xPB∥2
2 = 2
W
∞

ℓ=−∞
xBB
 ℓ
W

2
.
(iv) If yPB is another real energy-limited passband signal that is bandlimited to
W Hz around fc, and if {yBB(ℓ/W)} are the samples of its baseband repre-
sentation, then
⟨xPB, yPB⟩= 2
W Re
	
∞

ℓ=−∞
xBB
 ℓ
W

y∗
BB
 ℓ
W

.
Proof. Part (i) is just a restatement of (9.10b). Part (ii) is a restatement of (9.13).
Part (iii) is a special case of Part (iv) corresponding to yPB being equal to xPB. It
thus only remains to prove Part (iv). This is done by noting that if xBB and yBB
are the baseband representations of xPB and yPB, then, by Theorem 7.7.12 (iv),
⟨xPB, yPB⟩= 2 Re

⟨xBB, yBB⟩

= 2
W Re
	
∞

ℓ=−∞
xBB
 ℓ
W

y∗
BB
 ℓ
W

,
where the second equality follows from Theorem 8.4.3 (iii).
9.4 Exercises
173
Using the isomorphism between the family of complex square-summable sequences
and the family of energy-limited signals that are bandlimited to W Hz (Theo-
rem 8.7.1), and using the relationship between real energy-limited passband signals
and their baseband representation (Theorem 7.7.12), we can readily establish the
following isomorphism between the family of complex square-summable sequences
and the family of real energy-limited passband signals.
Theorem 9.3.2 (Real Passband Signals and Square-Summable Sequences). Let
fc, W, and T be constants satisfying
fc > W/2 > 0,
T = 1/W.
(i) If xPB is a real energy-limited passband signal that is bandlimited to W Hz
around fc, and if xBB is its baseband representation, then the bi-inﬁnite se-
quence consisting of the samples of xBB at integer multiples of T
. . . , xBB(−T), xBB(0), xBB(T), xBB(2T), . . .
is a square-summable sequence of complex numbers and
2T
∞

ℓ=−∞
xBB(ℓT)
2 = ∥xPB∥2
2 .
(ii) More generally, if xPB and yPB are real energy-limited passband signals that
are bandlimited to W Hz around the carrier frequency fc, and if xBB and
yBB are their baseband representations, then
2T Re
	
∞

ℓ=−∞
xBB(ℓT) y∗
BB(ℓT)

= ⟨xPB, yPB⟩.
(iii) If . . . , α−1, α0, α1, . . . is a square-summable bi-inﬁnite sequence of complex
numbers, then there exists a real energy-limited passband signal xPB that is
bandlimited to W Hz around the carrier frequency fc such that the samples
of its baseband representation xBB are given by
xBB(ℓT) = αℓ,
ℓ∈Z.
(iv) The mapping of every real energy-limited passband signal that is bandlimited
to W Hz around fc to the square-summable sequence consisting of the samples
of its baseband representation is linear (over R).
9.4
Exercises
Exercise 9.1 (A Speciﬁc Signal). Let x be a real energy-limited passband signal that
is bandlimited to W Hz around the carrier frequency fc. Suppose that all its complex
samples are zero except for its zeroth complex sample, which is given by 1+i. What is x?
174
Sampling Real Passband Signals
Exercise 9.2 (Real Passband Signals whose Complex Samples Are Real). Characterize
the Fourier Transforms of real energy-limited passband signals that are bandlimited to W
Hz around the carrier frequency fc and whose complex samples are real.
Exercise 9.3 (Multiplying by a Carrier). Let x be a real energy-limited signal that is
bandlimited to W/2 Hz, and let fc be larger than W/2. Express the complex samples of
t →x(t) cos(2πfct) in terms of x. Repeat for t →x(t) sin(2πfct).
Exercise 9.4 (Reﬂecting and Complex Sampling). Let xPB be an integrable signal that
is bandlimited to W Hz around the carrier frequency fc. Relate the complex samples of
~xPB to those of xPB.
Exercise 9.5 (Naively Sampling a Passband Signal).
(i) Consider the signal x: t →m(t) sin(2πfct), where m(·) is an integrable signal that
is bandlimited to 100 Hz and where fc = 100 MHz. Can x be recovered from its
samples . . . , x(−T), x(0), x(T), . . . when 1/T = 100 MHz?
(ii) Consider now the general case where x is an integrable real passband signal that is
bandlimited to W Hz around the carrier frequency fc. Find conditions guaranteeing
that x be reconstructible from its samples . . . , x(−T), x(0), x(T), . . .
Exercise 9.6 (Orthogonal Passband Signals). Let xPB and yPB be real energy-limited
passband signals that are bandlimited to W Hz around the carrier frequency fc. Under
what conditions on their complex samples are they orthogonal?
Exercise 9.7 (Sampling a Baseband Signal as though It Were a Passband Signal). Recall
that, ignoring some technicalities, a real baseband signal x of bandwidth W Hz can be
viewed as a real passband signal of bandwidth W around the carrier frequency fc, where
fc = W/2 (Problem 7.8). Compare the reconstruction formula for x from its samples to
the reconstruction formula for x from its complex samples.
Exercise 9.8 (Multiplying the Complex Samples). Let x be a real energy-limited passband
signal that is bandlimited to W Hz around the carrier frequency fc. Let . . . , x−1, x0, x1, . . .
denote its complex samples taken 1/W seconds apart. Let y be a real energy-limited
passband signal that is bandlimited to W Hz around the carrier frequency fc and whose
complex samples are like those of x but multiplied by i. Relate the FT of y to the FT
of x.
Exercise 9.9 (Delayed Complex Sampling). Let x and y be real energy-limited passband
signals that are bandlimited to W Hz around the carrier frequency fc. Suppose that the
complex samples of y are the same as those of x, but delayed by one:
yBB
	 ℓ
W

= xBB
	ℓ−1
W

,
ℓ∈Z.
How are ˆx and ˆy related? Is y a delayed version of x?
Exercise 9.10 (On the Family of Real Passband Signals). Is the set of all real energy-
limited passband signals that are bandlimited to W Hz around the carrier frequency fc a
linear subspace of the set of all complex energy-limited signals?
9.4 Exercises
175
Exercise 9.11 (Complex Sampling and Inner Products). Show that the ℓ-th complex
sample xBB(ℓ/W) of any real energy-limited passband signal that is bandlimited to W Hz
around the carrier frequency fc can be expressed as an inner product
xBB
	 ℓ
W

= ⟨x, φℓ⟩,
ℓ∈Z,
where . . . , φ−1, φ0, φ1, . . . are orthogonal equi-energy complex signals. Is φℓin general a
delayed version of φ0?
Exercise 9.12 (Absolute Summability of the Complex Samples). Show that the complex
samples of a real integrable passband signal that is bandlimited to W Hz around the
carrier frequency fc must be absolutely summable.
Hint: See Exercise 8.9.
Exercise 9.13 (The Convolution Revisited). Let x and y be real integrable passband
signals that are bandlimited to W Hz around the carrier frequency fc.
Express the
complex samples of x ⋆y in terms of those of x and y.
Exercise 9.14 (Complex Sampling and Filtering). Let x be a real integrable passband
signal that is bandlimited to W Hz around the carrier frequency fc, and let h be the
impulse response of a real stable ﬁlter. Relate the complex samples of x ⋆h to those of x
and h ⋆BPFW,fc.
Exercise 9.15 (Two Bands). Let [a1, b1] and [a2, b2] be disjoint positive frequency inter-
vals. Propose a variation on complex sampling that operates at 2(b1 −a1 + b2 −a2) real
samples per second and that allows for the perfect reconstruction of every real, continuous,
energy-limited signal whose FT is zero at f whenever |f| /∈[a1, b1] ∪[a2, b2].
Chapter 10
Mapping Bits to Waveforms
10.1
What Is Modulation?
Data bits are mathematical entities that have no physical attributes. To send them
over a channel, one needs to ﬁrst map them into some physical signal, which is
then “fed” into a channel to produce a physical signal at the channel’s output. For
example, when we send data over a telephone line, the data bits are ﬁrst converted
to an electrical signal, which then inﬂuences the voltage measured at the other
end of the line. (We use the term “inﬂuences” because the signal measured at the
other end of the line is usually not identical to the channel input: it is typically
attenuated and also corrupted by thermal noise and other distortions introduced
by various conversions in the telephone exchange system.) Similarly, in a wireless
system, the data bits are mapped to an electromagnetic wave that then inﬂuences
the electromagnetic ﬁeld measured at the receiver antenna. In magnetic recording,
data bits are written onto a magnetic medium by a mapping that maps them to
a magnetization pattern, which is then measured (with some distortion and some
noise) by the magnetic head at some later time when the data are read.
In the ﬁrst example the bits are mapped to continuous-time waveforms correspond-
ing to the voltage across an impedance, whereas in the last example the bits are
mapped to a spatial waveform corresponding to diﬀerent magnetizations at dif-
ferent locations across the magnetic medium. While some of the theory we shall
develop holds for both cases, we shall focus here mainly on channels of the former
type, where the channel input signal is some function of time rather than space.
We shall further focus on cases where the channel input corresponds to a time-
varying voltage across a resistor, a time-varying current through a resistor, or a
time-varying electric ﬁeld, so the energy required to transmit the signal is propor-
tional to the time integral of its square. Thus, if x(t) denotes the channel input at
time t, then we shall refer to
 t+Δ
t
x2(τ) dτ as the transmitted energy during the
time interval beginning at time t and ending at time t + Δ.
There are many mappings of bits to waveforms, and our goal is to ﬁnd “good” ones.
We will, of course, have to deﬁne some ﬁgures of merit to compare the quality of
diﬀerent mappings. We shall refer to the mapping of bits to a physical waveform
as modulation and to the part of the system that performs the modulation as the
modulator.
176
10.2 Modulating One Bit
177
Without going into too much detail, we can list a few qualitative requirements of a
modulator. The modulation should be robust with respect to channel impairments,
so that the receiver at the other end of the channel can reliably decode the data bits
from the channel output. Also, the modulator should have reasonable complexity.
Finally, in many applications we require that the transmitted signal be of limited
power so as to preserve the battery. In wireless applications the transmitted signal
may also be subject to spectral restrictions so as to not interfere with other systems.
10.2
Modulating One Bit
One does not typically expect to design a communication system in order to convey
only one data bit. The purpose of the modulator is typically to map an entire bit
stream to a waveform that extends over the entire life of the communication system.
Nevertheless, for pedagogic reasons, it is good to ﬁrst consider the simplest scenario
of modulating a single bit. In this case the modulator is fully characterized by two
functions x0(·) and x1(·) with the understanding that if the data bit D is equal
to zero, then the modulator produces the waveform x0(·) and that otherwise it
produces x1(·). Thus, the signal produced by the modulator is given by
X(t) =

x0(t)
if D = 0,
x1(t)
if D = 1,
t ∈R.
(10.1)
For example, we could choose
x0(t) =

A e−t/T
if t/T ≥0,
0
otherwise,
t ∈R,
and
x1(t) =

A
if 0 ≤t/T ≤1,
0
otherwise,
t ∈R,
where T = 1 sec and where A is a constant such that A2 has units of power.
This may seem like an odd way of writing these waveforms, but we have our
reasons: we typically think of t as having units of time, and we try to avoid
applying transcendental functions (such as the exponential function) to quantities
with units. Also, we think of the squared transmitted waveform as having units
of power, whereas we think of the transcendental functions as returning unit-less
arguments. Hence the introduction of the constant A with the understanding that
A2 has units of power.
We denoted the bit to be sent by an uppercase letter (D) because we like to de-
note random quantities (such as random variables, random vectors, and stochastic
processes) by uppercase letters, and we think of the transmitted bit as a random
quantity.
Indeed, if the transmitted bit were deterministic, there would be no
need to transmit it! This may seem like a statement made in jest, but it is ac-
tually very important. In the ﬁrst half of the twentieth century, engineers often
analyzed the performance of (analog) communication systems by analyzing their
performance in transmitting some particular signal, e.g., a sine wave. Nobody, of
178
Mapping Bits to Waveforms
course, transmitted such “boring” signals, because those could always be produced
at the receiver using a local oscillator. In the second half of the twentieth century,
especially following the work of Claude Shannon, engineers realized that it is only
meaningful to view the data to be transmitted as random, i.e., as quantities that
are unknown at the receiver and also unknown to the system designer prior to the
system’s deployment. We thus view the bit to be sent D as a random variable.
Often we will assume that it takes on the values 0 and 1 equiprobably. This is a
good assumption if prior to transmission a data compression algorithm is used.
By the same token, we view the transmitted signal as a random quantity, and
hence the uppercase X. In fact, if we employ the above signaling scheme, then at
every time instant t′ ∈R the value X(t′) of the transmitted waveform is a random
variable. For example, at time T/2 the value of the transmitted waveform is X(T/2),
which is a random variable that takes on the values A e−1/2 and A equiprobably.
Similarly, at time 2T the value of the transmitted waveform is X(2T), which is a
random variable taking on the values e−2 and 0 equiprobably. Mathematicians call
such a waveform a random process or a stochastic process (SP). This will be
deﬁned formally in Section 12.2.
It is useful to think about a random process as a function of two arguments: time
and “luck” or, more precisely, as a function of time and the result of all the random
experiments in the system. For a ﬁxed instant of time t ∈R, we have that X(t)
is a random variable, i.e., a real-valued function of the randomness in the system
(in this case the realization of D).
Alternatively, for a ﬁxed realization of the
randomness in the system, the random process is a deterministic function of time.
These two views will be used interchangeably in this book.
10.3
From Bits to Real Numbers
Many of the popular modulation schemes can be viewed as operating in two stages.
In the ﬁrst stage the data bits are mapped to real numbers, and in the second stage
the real numbers are mapped to a continuous-time waveform. If we denote by k the
number of data bits that will be transmitted by the system during its lifetime (or
from the moment it is turned on until it is turned oﬀ), and if we denote the data
bits by D1, D2, . . . , Dk, then the ﬁrst stage can be described as the application of
a mapping ϕ(·) that maps length-k sequences of bits to length-n sequences of real
numbers:
ϕ: {0, 1}k →Rn
(d1, . . . , dk) 	→(x1, . . . , xn).
From an engineering point of view, it makes little sense to allow for the encoding
function to map two diﬀerent binary k-tuples to the same real n-tuple, because
this would result in the transmitted waveforms corresponding to the two k-tuples
being identical.
This may cause errors even in the absence of noise.
We shall
therefore assume throughout that the mapping ϕ(·) is one-to-one (injective) so
no two distinct data k-tuples are mapped to the same n-tuple of real numbers.
An example of a mapping that maps bits to real numbers is the mapping that maps
10.4 Block-Mode Mapping of Bits to Real Numbers
179
each data bit Dj to the real number Xj according to the rule
Xj =

+1
if Dj = 0,
−1
if Dj = 1,
j = 1, . . . , k.
(10.2)
In this example one real symbol Xj is produced for every data bit, so n = k. For
this reason we say that this mapping has the rate of one bit per real symbol.
As another example consider the case where k is even and the data bits {Dj} are
broken into pairs
(D1, D2), (D3, D4), . . . , (Dk−1, Dk)
and each pair of data bits is then mapped to a single real number according to the
rule
(D2j−1, D2j) 	→
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
+3
if D2j−1 = D2j = 0,
+1
if D2j−1 = 0 and D2j = 1,
−3
if D2j−1 = D2j = 1,
−1
if D2j−1 = 1 and D2j = 0,
j = 1, . . . , k/2.
(10.3)
In this case n = k/2, and we say that the mapping has the rate of two bits per real
symbol.
Note that the rate of the mapping could also be a fraction. Indeed, if each data
bit Dj produces two real numbers according to the repetition law
Dj 	→

(+1, +1)
if Dj = 0,
(−1, −1)
if Dj = 1,
j = 1, . . . , k,
(10.4)
then n = 2k, and we say that the mapping is of rate half a bit per real symbol.
Since there is a natural correspondence between R2 and C, i.e., between pairs of real
numbers and complex numbers (where a pair of real numbers (x, y) corresponds
to the complex number x + iy), the rate of the above mapping (10.4) can also be
stated as one bit per complex symbol. This may seem like an odd way of stating the
rate, but it has some advantages that will become apparent later when we discuss
the mapping of real (or complex) numbers to waveforms and the Nyquist Criterion.
10.4
Block-Mode Mapping of Bits to Real Numbers
The examples we gave in Section 10.3 of mappings ϕ: {0, 1}k →Rn have something
in common. In each of those examples the mapping can be described as follows: the
data bits D1, . . . , Dk are ﬁrst grouped into binary K-tuples; each K-tuple is then
mapped to a real N-tuple by applying some mapping enc: {0, 1}K →RN; and the
so-produced real N-tuples are then concatenated to form the sequence X1, . . . , Xn,
where n = (k/K)N.
In the ﬁrst example K = N = 1 and the mapping of K-tuples to N-tuples is the
mapping (10.2). In the second example K = 2 and N = 1 with the mapping (10.3).
And in the third example K = 1 and N = 2 with the repetition mapping (10.4).
To describe such mappings ϕ: {0, 1}k →Rn more formally we need the notion of
a binary-to-reals block encoder, which we deﬁne next.
180
Mapping Bits to Waveforms
D1, D2,
. . .
, DK,
enc(·)
X1, X2,
. . .
, XN,
enc(D1, . . . , DK)
DK+1,
. . .
, D2K,
enc(·)
XN+1,
. . .
, X2N,
enc(DK+1, . . . , D2K)
, Dk−K+1, . . . , Dk
enc(·)
, Xn−N+1,
. . .
, Xn
enc(Dk−K+1, . . . , Dk)
Figure 10.1: Block-mode encoding.
Deﬁnition 10.4.1 ((K, N) Binary-to-Reals Block Encoder). A (K, N) binary-to-
reals block encoder is a one-to-one mapping from the set of binary K-tuples to
the set of real N-tuples, where K and N are positive integers. The rate of a (K, N)
binary-to-reals block encoder is deﬁned as
K
N

bit
real symbol

.
Note that we shall sometimes omit the phrase “binary-to-reals” and refer to such
an encoder as a (K, N) block encoder. Also note that “one-to-one” means that
no two distinct binary K-tuples may be mapped to the same real N-tuple.
We say that an encoder ϕ: {0, 1}k →Rn operates in block-mode using the
(K, N) binary-to-reals block encoder enc(·) if
1) k is divisible by K;
2) n is given by (k/K) N; and
3) ϕ(·) maps the binary sequence D1, . . . , Dk to the sequence X1, . . . , Xn by
parsing the sequence D1, . . . , Dk into consecutive length-K binary tuples and
by then concatenating the results of applying enc(·) to each such K-tuple as
in Figure 10.1.
If k is not divisible by K, we often introduce zero padding.
In this case we
choose k′ to be the smallest integer that is no smaller than k and that is divisible
by K, i.e.,
k′ =
/ k
K
0
K,
(where for every ξ ∈R we use ⌈ξ⌉to denote the smallest integer that is no smaller
than ξ, e.g., ⌈1.24⌉= 2) and map D1, . . . , Dk to the sequence X1, . . . , Xn′ where
n′ = k′
K N
by applying the (K, N) encoder in block-mode to the k′-length zero-padded binary
tuple
D1, . . . , Dk, 0, . . . , 0
  
k′ −k zeros
(10.5)
10.5 From Real Numbers to Waveforms with Linear Modulation
181
D1, D2,
. . .
, DK,
enc(·)
X1, X2,
. . .
, XN,
enc(D1, . . . , DK)
DK+1,
. . .
, D2K,
enc(·)
XN+1,
. . .
, X2N,
enc(DK+1, . . . , D2K)
, Dk′−K+1, . . . , Dk, 0, . . . , 0
enc(·)
, Xn′−N+1,
. . .
, Xn′
enc(Dk−K+1, . . . , Dk, 0, . . . , 0)
Figure 10.2: Block-mode encoding with zero padding.
as in Figure 10.2.
10.5
From Real Numbers to Waveforms with Linear Modulation
There are numerous ways to map a sequence of real numbers X1, . . . , Xn to a real-
valued signal. Here we shall focus on mappings that have a linear structure. This
additional structure simpliﬁes the implementation of the modulator and demodu-
lator. It is described next.
Suppose we wish to modulate the k data bits D1, . . . , Dk, and suppose that we have
mapped these bits to the n real numbers X1, . . . , Xn. Here n can be smaller, equal,
or greater than k. In a linear modulation scheme the transmitted waveform X(·),
which we also denote X, is given by
X(t) = A
n

ℓ=1
Xℓgℓ(t),
t ∈R,
(10.6)
where the deterministic real waveforms g1, . . . , gn are speciﬁed in advance, and
where A ≥0 is a scaling factor.1
The waveform X(·) can be thus viewed as
a scaled-by-A linear combination of the tuple

g1, . . . , gn

with the coeﬃcients
X1, . . . , Xn:
X = A
n

ℓ=1
Xℓgℓ.
(10.7)
The transmitted energy is a random variable that is given by
∥X∥2
2 =
 ∞
−∞
X2(t) dt
=
 ∞
−∞
	
A
n

ℓ=1
Xℓgℓ(t)

2
dt
= A2
n

ℓ=1
n

ℓ′=1
XℓXℓ′
 ∞
−∞
gℓ(t) gℓ′(t) dt
1The notation X for the transmitted waveform is discussed in Chapter 12.
182
Mapping Bits to Waveforms
= A2
n

ℓ=1
n

ℓ′=1
XℓXℓ′ ⟨gℓ, gℓ′⟩.
The transmitted energy takes on a particularly simple form if the waveforms gℓ(·)
are orthonormal, i.e., if
⟨gℓ, gℓ′⟩= I{ℓ= ℓ′},
ℓ, ℓ′ ∈{1, . . . , n},
(10.8)
in which case the energy is given by
∥X∥2
2 = A2
n

ℓ=1
X2
ℓ,
{gℓ} orthonormal.
(10.9)
As an exercise, the reader is encouraged to verify that there is no loss in generality
in assuming that the waveforms {gℓ} are orthonormal. More precisely:
Theorem 10.5.1. Suppose that the waveform X(·) is generated from the binary
k-tuple D1, . . . , Dk by applying the mapping ϕ: {0, 1}k →Rn and by then linearly
modulating the resulting n-tuple ϕ(D1, . . . , Dk) using the waveforms {gℓ}n
ℓ=1 as in
(10.6).
Then there exist an integer 1 ≤n′ ≤n; a mapping ϕ′ : {0, 1}k →Rn′; and n′
orthonormal signals {φℓ}n′
ℓ=1 such that if X′(·) is generated from D1, . . . , Dk by
applying linear modulation to ϕ′(D1, . . . , Dk) using the orthonormal waveforms
{φℓ}n′
ℓ=1, then X′(·) and X(·) are indistinguishable for every k-tuple D1, . . . , Dk.
Proof. The proof is based on the Gram-Schmidt procedure (Section 4.6.6) and is
left as an exercise (Exercise 10.7).
Motivated by this theorem, we shall focus on linear modulation with orthonormal
functions. But please note that even if the transmitted waveform satisﬁes (10.8),
the received waveform might not. For example, the channel might consist of a
linear ﬁlter that could destroy the orthogonality.
10.6
Recovering the Signal Coeﬃcients with a Matched Filter
Suppose now that the binary k-tuple (D1, . . . , Dk) is mapped to the real n-tuple
(X1, . . . , Xn) using the mapping
ϕ: {0, 1}k →Rn
(10.10)
and that the n-tuple (X1, . . . , Xn) is then mapped to the waveform
X(t) = A
n

ℓ=1
Xℓφℓ(t),
t ∈R,
(10.11)
where φ1, . . . , φn are orthonormal:
⟨φℓ, φℓ′⟩= I{ℓ= ℓ′},
ℓ, ℓ′ ∈{1, . . . , n}.
(10.12)
10.6 Recovering the Signal Coeﬃcients with a Matched Filter
183
X(·)
AXℓ
~φ
ℓTs
Figure 10.3:
Recovering the symbols from the transmitted waveform using a
matched ﬁlter when (10.14) is satisﬁed.
How can we recover the k-tuple D1, . . . , Dk from X(·)? The decoder’s problem
is, of course, harder, because the decoder usually does not have access to the
transmitted waveform X(·) but only to the received waveform, which may be a
noisy and distorted version of X(·). Nevertheless, it is instructive to consider the
noiseless and distortionless problem ﬁrst.
If we are able to recover the real numbers {Xℓ}n
ℓ=1 from the received signal X(·),
and if the mapping ϕ: {0, 1}k →Rn is one-to-one (as we assume), then the data
bits {Dj}k
j=1 can be reconstructed from X(·). Thus, the question is how to recover
{Xℓ}n
ℓ=1 from X(·). But this is easy if the functions {φℓ}n
ℓ=1 are orthonormal,
because in this case, by Proposition 4.6.4 (i), Xℓis given by the scaled inner
product between X and φℓ:
Xℓ= 1
A ⟨X, φℓ⟩,
ℓ= 1, . . . , n.
(10.13)
Consequently, we can compute Xℓby feeding X to a matched ﬁlter for φℓand
scaling the time-0 output by 1/A (Section 5.8). To recover {Xℓ}n
ℓ=1 we thus need n
matched ﬁlters, one matched to each of the waveforms {φℓ}.
The implementation becomes much simpler if the functions {φℓ} have an additional
structure, namely, if they are all time shifts of some function φ(·):
φℓ(t) = φ(t −ℓTs),

ℓ∈{1, . . . , n}, t ∈R

.
(10.14)
In this case it follows from Corollary 5.8.3 that we can compute all the inner
products {⟨X, φℓ⟩} using one matched ﬁlter of impulse response ~φ by feeding X
to the ﬁlter and sampling its output at the appropriate times:
Xℓ= 1
A
 ∞
−∞
X(τ) φℓ(τ) dτ
= 1
A
 ∞
−∞
X(τ) φ(τ −ℓTs) dτ
= 1
A
 ∞
−∞
X(τ) ~φ(ℓTs −τ) dτ
= 1
A

X ⋆~φ

(ℓTs),
ℓ= 1, . . . , n.
(10.15)
Figure 10.3 demonstrates how the symbols {Xℓ} can be recovered from X(·) using
a single matched ﬁlter if the pulses {φℓ} satisfy (10.14).
184
Mapping Bits to Waveforms
t
t
t
α
α
α
Ts
2
Ts
2
−Ts
2
−Ts
2
−Ts
Ts
φ(t)
φ(t + Ts)
φ(t −Ts)
Figure 10.4:
A function φ(·) whose time shifts by integer multiples of Ts are
orthonormal (with the proper choice of α).
Figure 10.4 depicts a signal φ(·) whose time shifts by integer multiples of Ts are
orthonormal (when α is chosen so that it be of unit energy).
In fact, for this
signal the time shifts do not overlap in the sense that for ℓ̸= 0 the product
φ(t) φ(t −ℓTs) is zero at every epoch t ∈R. But, as we shall see in Chapter 11,
we can achieve orthonormality even if the time shifts do overlap. This is plausible
because t 	→φ(t) φ(t −ℓTs) can integrate to zero even if it is not identically zero.
10.7
Pulse Amplitude Modulation
Under Assumption (10.14), the transmitted signal X(·) in (10.11) is given by
X(t) = A
n

ℓ=1
Xℓφ(t −ℓTs),
t ∈R,
(10.16)
which is a special case of Pulse Amplitude Modulation (PAM), which we
describe next.
10.8 Constellations
185
In PAM, the data bits D1, . . . , Dk are mapped to real numbers X1, . . . , Xn, which
are then mapped to the waveform
X(t) = A
n

ℓ=1
Xℓg(t −ℓTs),
t ∈R,
(10.17)
for some scaling factor A ≥0, some function g: R →R, and some constant Ts > 0.
The function g (always assumed Borel measurable2) is called the pulse shape; the
constant Ts is called the baud period; and its reciprocal 1/Ts is called the baud
rate.3 The units of Ts are seconds, and one often refers to the units of 1/Ts as real
symbols per second. PAM can thus be viewed as a special case of linear modulation
(10.6) with gℓbeing given for every ℓ∈{1, . . . , n} by the mapping t 	→g(t −ℓTs).
The signal (10.16) can be viewed as a PAM signal where the pulse shape φ satisﬁes
the orthonormality condition

t 	→φ(t −ℓTs), t 	→φ(t −ℓ′Ts)

= I{ℓ= ℓ′},
ℓ, ℓ′ ∈{1, . . . , n},
(10.18)
which is implied by (10.12) and (10.14).
In this book we shall typically denote the PAM pulse shape by g. But we shall
use φ if we assume an additional orthonormality condition such as (10.18). In this
case we shall refer to 1/Ts as having units of real dimensions per second:
1
Ts
real dimension
sec

,
φ satisﬁes (10.18).
(10.19)
Note that according to Theorem 10.5.1 there is no loss in generality in assuming
that the pulses {φℓ} are orthonormal. There is, however, a loss in generality in
assuming that they satisfy (10.14).
10.8
Constellations
Recall that in PAM the data bits D1, . . . , Dk are ﬁrst mapped to the real n-tuple
X1, . . . , Xn using a one-to-one mapping ϕ: {0, 1}k →Rn, and that these real
numbers are then mapped to the waveform X(·) via (10.17). Since there are only
2k diﬀerent binary k-tuples, it follows that each symbol Xℓcan take on at most
2k diﬀerent values. The set of values that Xℓcan take on may, in general, depend
on ℓ. The union of all these sets (over ℓ∈{1, . . . , n}) is called the constellation of
the mapping ϕ(·). Denoting the constellation of ϕ(·) by X, we thus have that a real
number x is in X if, and only if, for some choice of the binary k-tuple (d1, . . . , dk)
and for some ℓ∈{1, . . . , n} the ℓ-th component of ϕ

(d1, . . . , dk)

is equal to x.
For example, the constellation corresponding to the mapping (10.2) is the set
{−1, +1}; the constellation corresponding to (10.3) is the set {−3, −1, +1, +3};
2The Borel σ-algebra is the smallest σ-algebra containing the open sets. A function g : R →
R is Borel measurable if for every α ∈R the set {ξ ∈R : g(ξ) < α} is an element of the Borel
σ-algebra.
3These terms honor the French engineer Jean-Maurice-´Emile Baudot (1845–1903) who in-
vented a telegraph printing system. Some authors refer to the baud rate as “baud.” The term
baud period is also not completely standard.
186
Mapping Bits to Waveforms
and the constellation corresponding to (10.4) is the set {−1, +1}.
In all these
examples, the constellation can be viewed as a special case of the constellation
with 2ν symbols

−(2ν −1), . . . , −5, −3, −1, +1, +3, +5, . . . , +(2ν −1)

(10.20)
for some positive integer ν. A less prevalent constellation is the constellation
{−2, −1, +1, +2}.
(10.21)
The number of points in the constellation X is just # X, i.e., the number of
elements (cardinality) of the set X.
The minimum distance δ of a constellation is the Euclidean distance between
the closest distinct elements in the constellation:
δ ≜min
x,x′∈X
x̸=x′
|x −x′|.
(10.22)
The scaling of the constellation is arbitrary because of the scaling factor A in the
signal’s description. Thus, the signal A 
ℓXℓg(t −ℓTs), where Xℓtakes values in
the set {±1} is of constellation {−1, +1}, but it can also be expressed in the form
A′ 
ℓX′
ℓg(t −ℓTs), where A′ = 2A and X′
ℓtakes values in the set {−1/2, +1/2},
i.e., as a PAM signal of constellation {−1/2, +1/2}.
Diﬀerent authors choose to normalize the constellation in diﬀerent ways.
One
common normalization is to express the elements of the constellation as multiples
of the minimum distance. Thus, we would represent the constellation {−1, +1} as
1
−1
2δ , +1
2δ
2
,
and the constellation {−3, −1, +1, +3} as
1
−3
2δ , −1
2δ , +1
2δ , +3
2δ
2
.
The normalized version of the constellation (10.20) is
1
±2ν −1
2
δ, . . . , ±5
2δ, ±3
2δ, ±1
2δ
2
.
(10.23)
We say that a constellation is centered if

x∈X
x = 0.
(10.24)
The second moment of a constellation X is deﬁned as
1
# X

x∈X
x2.
(10.25)
10.9 Uncoded Transmission
187
The second moment of the constellation in (10.23) is given by
1
# X

x∈X
x2 = 1
2ν 2
ν

η=1
(2η −1)2 δ2
4
= 1
12

M2 −1

δ2,
(10.26a)
where
M = 2ν
(10.26b)
is the number of points in the constellation, and where (10.26a)–(10.26b) can be
veriﬁed using the identity
ν

η=1
(2η −1)2 = 1
3 ν (4ν2 −1),
ν = 1, 2, . . .
(10.27)
10.9
Uncoded Transmission
Classifying a coding scheme as uncoded sounds like an oxymoron, but it is not.
The term “uncoded transmission” or “uncoded communication” is important and
will be clariﬁed next. Recall that the mapping ϕ: {0, 1}k →Rn maps the data
bits D1, . . . , Dk to the real symbols X1, . . . , Xn. As we have seen in Section 10.8,
the constellation X is the smallest subset of R for which—irrespective of the data
bits—each of the symbols X1, . . . , Xn is in X. Thus, the range of ϕ is contained in
X n, and X is the smallest set for which this holds. We emphasize that the range
of ϕ can be a strict subset of X n: some sequences in X n may not be producible
by the encoder irrespective of the data bits fed to it. This is the case, for example,
when the encoder is based on the repetition mapping (10.4) and X = {+1, −1},
because in this case X1 must equal X2, and sequences beginning with +1, −1 are
in X n but not in the range of ϕ.
Engineers say that the transmission is uncoded or that the system is uncoded
if the range of ϕ equals X n

ϕ(d) : d ∈{0, 1}k
= X n,
(10.28)
i.e., if every sequence in X n can be produced by the encoder by feeding it the
appropriate data sequence. To see why they use this terminology, it is instructive
to consider some examples.
Examples when (10.28) does hold are when we employ antipodal signaling (10.2)
or the mapping (10.3). More generally, it holds whenever the encoder operates in
block-mode using a (K, 1) block encoder (and k is divisible by K so no zero padding
is required). Such encoders do not rule out any output sequence in X n and they
are therefore error prone (hence “uncoded”). An example when (10.28) does not
hold is when the encoder operates in block-mode using the (1, 2) (repetition) block
encoder of (10.4). More generally, (10.28) does not hold whenever the encoder
operates in block-mode using a (K, N) block encoder that is nontrivial in the sense
that its range is a strict subset of X N. An uncoded system does not therefore
employ any such nontrivial block codes.
188
Mapping Bits to Waveforms
Since we are assuming throughout that ϕ is one-to-one (Section 10.3), the system
is uncoded if, and only if, ϕ: {0, 1}k →X n is one-to-one and onto, i.e., a bijection.
10.10
Bandwidth Considerations
Without knowing the data bits that produced the PAM signal, it is very diﬃcult
to say much about it. A notable exception is the bandwidth, which is essentially
determined by the pulse shape: unless A is zero or the symbols x1, . . . , xn are
all zero, the bandwidth of the PAM signal is equal to the bandwidth of the pulse
shape g. One part of the proof of this statement hinges on elementary properties of
the Fourier Transform. The other hinges on the Fundamental Theorem of Algebra
(Rudin, 1987, Chapter 10, Theorem 10.25), which states that the number of roots
of a nonzero polynomial cannot exceed its degree (and, in fact, equals its degree if
we account for the multiplicity of the roots.)
We present the result in a slightly more general form.
Lemma 10.10.1. Let g: R →C be an integrable signal of bandwidth W.
Let
cm, . . . , cn be complex numbers that are not all zero, where m and n are integers
satisfying m ≤n. Let Ts be positive. Then the signal t 	→n
ℓ=m cℓg(t −ℓTs) is an
integrable signal of the same bandwidth as g, namely, W.
Proof. Let g, m, n, and cm, . . . , cn be as above, and deﬁne the signal
h: t 	→
n

ℓ=m
cℓg(t −ℓTs).
Its Fourier Transform ˆh is
ˆh(f) =
n

ℓ=m
cℓe−i2πfℓTs ˆg(f)
=
	
n

ℓ=m
cℓe−i2πfℓTs

ˆg(f)
= zm
n−m

ℓ=0
cℓ+m zℓ

z=e−i2πfTs
ˆg(f),
f ∈R.
(10.29)
It thus follows that at every frequency at which ˆg is zero so is ˆh, and hence the
bandwidth of h cannot exceed that of g. It remains to show that the bandwidth
of h is not smaller than that of g. We shall establish this by showing that there
can be at most a ﬁnite number of frequencies where ˆh is zero but ˆg is not.
Since ˆg is zero outside the band [−W, W ], we need to show that there are at
most a ﬁnite number of frequencies f ∈[−W, W ] where ˆh(f) is zero but ˆg(f) is
not. For f to be such a frequency, the function z 	→zm n−m
ℓ=0 cℓ+m zℓmust be
zero at e−i2πfTs. Since zm has no roots on the unit circle {z ∈C : |z| = 1}, the
only way a frequency f can satisfy this condition is if e−i2πfTs is a root of the
complex polynomial z 	→n−m
ℓ=0 cℓ+m zℓ, whose degree is at most n −m. Since, by
10.11 Design Considerations
189
hypothesis, the coeﬃcients cm, . . . , cn are not all zero, this polynomial is not the
zero polynomial. Consequently, by the Fundamental Theorem of Algebra, it has at
most n −m roots. Of these roots, only some, say d, are on the unit circle. Denote
these
eiθ1, . . . , eiθd,
where θ1, . . . , θd ∈[−π, π) and d ≤n −m. We conclude that only at frequencies f
for which e−i2πfTs is equal to eiθν for some ν ∈{1, . . . , d} can ˆh(f) be zero and
ˆg(f) not. Since e−i2πfTs can equal eiθν only if f has the form
−θν
2πTs
+ η
Ts
for some integer η, it follows that there are a ﬁnite number of frequencies in the
band [−W, W ] where e−i2πfTs equals eiθν. Consequently, there are at most a ﬁnite
number of frequencies in the band [−W, W ] where ˆh is zero and ˆg is not, and the
bandwidth of h cannot be smaller than that of g.
Corollary 10.10.2 (The Bandwidth of a PAM Is that of the Pulse Shape). If a
PAM signal (with an integrable pulse shape) is not zero, then its bandwidth is equal
to the bandwidth of the pulse shape.
Proof. If the pulse shape is of ﬁnite bandwidth W, then the result follows directly
from the lemma. Otherwise, the result follows by noting that there are a count-
able number of frequencies f where e−i2πTs equals eiθν, and the ﬁnite union of a
countable number of frequencies is countable. Consequently, there are at most a
countable number of frequencies where the FT of the PAM signal is zero whereas
that of the pulse shape is not.
10.11
Design Considerations
Designing a communication system employing PAM with a block encoder entails
making choices. We need to choose the PAM parameters A, Ts, and g, and we
need to choose a (K, N) block encoder enc(·). These choices greatly inﬂuence the
overall system characteristics such as the transmitted power, bandwidth, and the
performance of the system in the presence of noise. To design a system well, we
must understand the eﬀect of the design choices on the overall system at three
levels. At the ﬁrst level we must understand which design parameters inﬂuence
which overall system characteristics.
At the second level we must understand
how the design parameters inﬂuence the system. And at the third level we must
understand how to choose the design parameters so as to optimize the system
characteristics subject to the given constraints.
In this book we focus on the ﬁrst two levels. The third requires tools from Infor-
mation Theory and from Coding Theory that are beyond the scope of this book.
Here we oﬀer a preview of the ﬁrst level. We thus brieﬂy and informally explain
which design choices inﬂuence which overall system properties.
To simplify the preview, we shall assume in this section that the time shifts of the
pulse shape by integer multiples of the baud period are orthonormal. Consequently,
190
Mapping Bits to Waveforms
we shall denote the pulse shape by φ and assume that (10.18) holds. We shall also
assume that k and n tend to inﬁnity as in the bi-inﬁnite block mode discussed in
Section 14.5.2. Roughly speaking this assumption is tantamount to the assumption
that the system has been running since time −∞and that it will continue running
until time +∞.
Our discussion is extremely informal, and we apologize to the reader for discussing
concepts that we have not yet deﬁned. Readers who are aggravated by this practice
may choose to skip this section; the issues will be revisited in Chapter 29 after
everything has been deﬁned and all the claims proved.
The key observation we wish to highlight is that, to a great extent,
the choice of the block encoder enc(·) can be decoupled from the
choice of the pulse shape φ. The bandwidth and power spectral
density depend hardly at all on enc(·) and very much on φ, whereas
the probability of error on the white Gaussian noise channel de-
pends very much on enc(·) and not at all on φ.
This observation greatly simpliﬁes the design problem because it means that, rather
than optimizing over φ and enc(·) jointly, we can choose each of them separately.
We next brieﬂy discuss the diﬀerent overall system characteristics and which design
choices inﬂuence them.
Data Rate:
The data rate Rb that the system supports is determined by the baud
period Ts and by the rate K/N of the encoder. It is given by
Rb = 1
Ts
K
N
 bit
sec

.
Power:
The transmitted power does not depend on the pulse shape φ (Theo-
rem 14.5.2). It is determined by the amplitude A, the baud period Ts, and the
block encoder enc(·). In fact, if the constellation is centered and the transmission
is uncoded, then the transmitted power is determined by A, Ts, and the second
moment of the constellation.
Power Spectral Density:
If the block encoder enc(·) is such that when it is fed
the data bits it produces zero-mean and uncorrelated symbols of equal variance,
then the power spectral density is determined by A, Ts, and φ only; it is unaﬀected
by enc(·) (Section 15.4).
Bandwidth:
The bandwidth of the transmitted waveform is equal to the band-
width of the pulse shape φ (Corollary 10.10.2 and Theorem 15.4.1). We will see in
Chapter 11 that for the orthonormality (10.18) to hold (for all n), the bandwidth W
of the pulse shape must satisfy
W ≥
1
2Ts
.
10.12 Some Implementation Considerations
191
In Chapter 11 we shall also see how to design φ so as to satisfy (10.18) and so as
to have its bandwidth as close as we wish to 1/(2Ts).4
Probability of Error:
It is a remarkable fact that the pulse shape φ does not aﬀect
the performance of the system on the additive white Gaussian noise channel. Per-
formance is determined only by A, Ts, and the block encoder enc(·) (Section 26.4.2).
The preceding discussion focused on PAM, but many of the results also hold for
Quadrature Amplitude Modulation, which is discussed in Chapters 16, 18, and 28.
10.12
Some Implementation Considerations
It is instructive to consider some of the issues related to the generation of a PAM
signal
X(t) = A
n

ℓ=1
Xℓg(t −ℓTs),
t ∈R.
(10.30)
Here we focus on delay, causality, and digital implementation.
10.12.1
Delay
To illustrate the delay issue in PAM, suppose that the pulse shape g(·) is strictly
positive. In this case we note that, irrespective of which epoch t′ ∈R we consider,
the calculation of X(t′) requires knowledge of the entire n-tuple X1, . . . , Xn. Since
the sequence X1, . . . , Xn cannot typically be determined in its entirety unless the
entire sequence D1, . . . , Dk is determined ﬁrst, it follows that, when g(·) is strictly
positive, the modulator cannot produce X(t′) before observing the entire data
sequence D1, . . . , Dk. And this is true for any t′ ∈R! Since in the back of our
minds we think about D1, . . . , Dk as the data bits that will be sent during the
entire life of the system or, at least, from the moment it is turned on until it is
shut oﬀ, it is unrealistic to expect the modulator to observe the entire sequence
D1, . . . , Dk before producing any input to the channel.
The engineering solution to this problem is to ﬁnd some positive integer L such
that, for all practical purposes, g(t) is zero whenever |t| > LTs, i.e.,
g(t) ≈0,
|t| > LTs.
(10.31)
In this case we have that, irrespective of t′ ∈R, only 2L + 1 terms (approximately)
determine X(t′). Indeed, if κ is an integer such that
κTs ≤t′ < (κ + 1)Ts,
(10.32)
then
X(t′) ≈A
κ+L

ℓ=max{1,κ−L}
Xℓg(t −ℓTs),
κTs ≤t′ < (κ + 1)Ts,
(10.33)
4Information-theoretic considerations suggest that this is a good approach.
192
Mapping Bits to Waveforms
where the sum is assumed to be zero if κ + L < 1.
Thus, if (10.31) holds, then the approximate calculation of X(t′) can be performed
without knowledge of the entire sequence X1, . . . , Xn and the modulator can start
producing the waveform X(·) as soon as it knows X1, . . . , XL.
10.12.2
Causality
The reader may object to the fact that, even if (10.31) holds, the signal X(·) may
be nonzero at negative times. It might therefore seem as though the transmitter
needs to transmit a signal before the system has been turned on and that, worse
still, this signal depends on the data bits that will be fed to the system in the
future when the system is turned on. But this is not really an issue. It all has
to do with how we deﬁne the epoch t = 0, i.e., to what physical time instant
does t = 0 correspond. We never said it corresponded to the instant when the
system was turned on and, in fact, there is no reason to set the time origin at
that time instant or at the “Big Bang.” For example, we can set the time origin
at LTs seconds-past-system-turn-on, and the problem disappears. Similarly, if the
transmitted waveform depends on X1, . . . , XL, and if these real numbers can only
be computed once the data bits D1, . . . , Dκ have been fed to the encoder, then it
would make sense to set the time origin to the moment at which the last of these κ
data bits has been fed to the encoder.
Some problems in Digital Communications that appear like tough causality prob-
lems end up being easily solved by time delays and the redeﬁnition of the time
origin. Others can be much harder. It is sometimes diﬃcult for the novice to de-
termine which causality problem is of the former type and which of the latter. As
a rule of thumb, you should be extra cautious when the system contains feedback
loops.
10.12.3
Digital Implementation
Even when all the symbols among X1, . . . , Xn that are relevant for the calculation
of X(t′) are known, the actual computation may be tricky, particularly if the
formula describing the pulse shape is diﬃcult to implement in hardware. In such
cases one may opt for a digital implementation using look-up tables. The idea is
to compute only samples of X(·) and to then interpolate using a digital-to-analog
(D/A) converter and an anti-aliasing ﬁlter. The samples must be computed at a
rate determined by the Sampling Theorem, i.e., at least once every 1/(2W) seconds,
where W is the bandwidth of the pulse shape.
The computation of the values of X(·) at its samples can be done by choosing L
suﬃciently large so that (10.31) holds and by then approximating the sum (10.30)
for t′ satisfying (10.32) by the sum (10.33). The samples of this latter sum can be
computed with a digital computer or—as is more common if the symbols take on a
ﬁnite (and small) number of values—using a pre-programmed look-up table. The
size of the look-up table thus depends on two parameters: the number of samples
one needs to compute every Ts seconds (determined via the bandwidth of g(·) and
the Sampling Theorem), and the number of addresses needed (as determined by L
and by the constellation size).
10.13 Exercises
193
10.13
Exercises
Exercise 10.1 (Exploiting Orthogonality). Let the energy-limited real signals φ1 and φ2
be orthogonal, and let A(1) and A(2) be positive constants. Let the waveform X be given
by
X =
	
A(1)X(1) + A(2)X(2)
φ1 +
	
A(1)X(1) −A(2)X(2)
φ2,
where X(1) and X(2) are unknown real numbers. How can you recover X(1) and X(2)
from X?
Exercise 10.2 (More Orthogonality). Extend Exercise 10.1 to the case where φ1, . . . φη
are orthonormal;
X =
	
a(1,1)A(1)X(1) + · · · + a(η,1)A(η)X(η)
φ1 + · · ·
+
	
a(1,η)A(1)X(1) + · · · + a(η,η)A(η)X(η)
φη;
and where the real numbers a(ι,ν) for ι, ν ∈{1, . . . , η} satisfy the orthogonality condition
η

ν=1
a(ι,ν)a(ι′,ν) =

η
if ι = ι′,
0
if ι ̸= ι′,
ι, ι′ ∈{1, . . . , η}.
Exercise 10.3 (8-PPM). To send three bits D0, D1, and D2 using 8-PPM we send the
signal t →A g

t−(D0+2D1+4D2)Ts

, where g: t →I

|t| ≤Tc/2

and Tc is some positive
constant. Here PPM stands for Pulse Position Modulation. Show that this mapping can
be represented as a mapping of D0, D1, D2 to real numbers X1, . . . , X8 followed by the
mapping of X1, . . . , X8 to the signal t →A 8
ℓ=1 Xℓ˜g

t −ℓTs

. What is ˜g?
Exercise 10.4 (A Constellation and its Second Moment). What is the constellation cor-
responding to the (1, 3) binary-to-reals block encoder that maps 0 to (+1, +2, +2) and
maps 1 to (−1, −2, −2)? What is its second moment? Let the real symbols

Xℓ, ℓ∈Z

be generated from IID random bits

Dj, j ∈Z

(Deﬁnition 14.5.1) in block mode using
this block encoder. Compute
lim
L→∞
1
2L + 1
L

ℓ=−L
E
$
X2
ℓ
%
.
Exercise 10.5 (The Rate and Constellation Size). A PAM encoder ϕ: {0, 1}k →Rn has
constellation X of size # X. Show that its rate k/n is upper-bounded by log2(# X).
Exercise 10.6 (Smoothing a PAM Signal). Let

X(t)

be the result of mapping the IID
random bits D1, . . . , DK to the real numbers X1, . . . , XN using enc: {0, 1}K →RN and
then mapping these symbols to the waveform
X(t) = A
N

ℓ=1
Xℓg(t −ℓTs),
t ∈R,
where A, Ts are positive and g is an energy-limited pulse shape. Deﬁne
Y (t) = 1
17
 t+17
t
X(τ) dτ,
t ∈R.
Can

Y (t)

be viewed as a PAM signal? If so, of what pulse shape?
194
Mapping Bits to Waveforms
Exercise 10.7 (Orthonormal Signal Representation). Prove Theorem 10.5.1.
Hint: Recall the Gram-Schmidt procedure.
Exercise 10.8 (Unbounded PAM Signal). Consider the formal expression
X(t) =
∞

ℓ=−∞
Xℓsinc
	 t
Ts −ℓ

,
t ∈R.
(i) Show that even if the Xℓ’s can only take on the values ±1, the value of X(Ts/2)
can be arbitrarily high. That is, ﬁnd a sequence {xℓ}∞
−∞such that xℓ∈{+1, −1}
for every ℓ∈Z and
lim
L→∞
L

ℓ=−L
xℓsinc
	1
2 −ℓ

= ∞.
(ii) Suppose now that g: R →R satisﬁes
g(t)
 ≤
β
1 + |t/Ts|1+α ,
t ∈R
for some α, β > 0. Show that if for some γ > 0 we have |xℓ| ≤γ for all ℓ∈Z, then
the sum
∞

ℓ=−∞
xℓg(t −ℓTs)
converges at every t and is a bounded function of t.
Exercise 10.9 (Etymology). Let g be an integrable real signal. Express the frequency
response of the matched ﬁlter for g in terms of the FT of g. Repeat when g is a complex
signal. Can you guess the origin of the term “Matched Filter”?
Hint: Recall the notion of a “matched impedance.”
Exercise 10.10 (Recovering the Symbols from a Filtered PAM Signal). Let X(·) be the
PAM signal (10.17), where A > 0, and where g(t) is zero for |t| ≥Ts/2 and positive for
|t| < Ts/2.
(i) Suppose that X(·) is fed to a ﬁlter of impulse response h: t →I{|t| ≤Ts/2}. Is
it true that for every ℓ∈{1, . . . , n} one can recover Xℓfrom the ﬁlter’s output at
time ℓTs? If so, how?
(ii) Suppose now that the ﬁlter’s impulse response is h: t →I{−Ts/2 ≤t ≤3Ts/4}.
Can one always recover Xℓfrom the ﬁlter’s output at time ℓTs? Can one recover
the sequence (X1, . . . , Xn) from the n samples of the ﬁlter’s output at the times
Ts, . . . , nTs?
Exercise 10.11 (Continuous Phase Modulation). In Continuous Phase Modulation (CPM)
the symbols

Xℓ

are mapped to the waveform
X(t) = A cos

2πfct + 2πh
∞

ℓ=−∞
Xℓq(t −ℓTs)

,
t ∈R,
where fc, h > 0 are constants and q is a mapping from R to R. Is CPM a special case of
linear modulation?
Chapter 11
Nyquist’s Criterion
11.1
Introduction
In Section 10.7 we discussed the beneﬁt of choosing the pulse shape φ in Pulse
Amplitude Modulation so that its time shifts by integer multiples of the baud
period Ts be orthonormal. We saw that if the real transmitted signal is given by
X(t) = A
n

ℓ=1
Xℓφ(t −ℓTs),
t ∈R,
where for all integers ℓ, ℓ′ ∈{1, . . . , n}
 ∞
−∞
φ(t −ℓTs) φ(t −ℓ′Ts) dt = I{ℓ= ℓ′},
then
Xℓ= 1
A
 ∞
−∞
X(t) φ(t −ℓTs) dt,
ℓ= 1, . . . , n,
and all the inner products
 ∞
−∞
X(t) φ(t −ℓTs) dt,
ℓ= 1, . . . , n
can be computed using one circuit by feeding the signal X(·) to a matched ﬁlter of
impulse response ~φ and sampling the output at the times t = ℓTs, for ℓ= 1, . . . , n.
(In the complex case the matched ﬁlter is of impulse response ~φ∗.)
In this chapter we shall address the design of and the limitations on signals that are
orthogonal to their time shifts. While our focus so far has been on real functions φ,
for reasons that will become apparent in Chapter 16 when we discuss Quadrature
Amplitude Modulation, we prefer to generalize the discussion and allow φ to be
complex. The main results of this chapter are Corollary 11.3.4 and Corollary 11.3.5.
An obvious way of choosing a signal φ that is orthogonal to its time shifts by
nonzero integer multiples of Ts is by choosing a pulse that is zero outside some
interval of length Ts, say [−Ts/2, Ts/2).
This guarantees that the pulse and its
time shifts by nonzero integer multiples of Ts do not overlap in time and that they
195
196
Nyquist’s Criterion
are thus orthogonal.
But this choice limits us to pulses of inﬁnite bandwidth,
because no nonzero bandlimited signal can vanish outside a ﬁnite (time) interval
(Theorem 6.8.2).
Fortunately, as we shall see, there exist signals that are orthogonal to their time
shifts and that are also bandlimited.
This does not contradict Theorem 6.8.2
because these signals are not time-limited.
They are orthogonal to their time
shifts in spite of overlapping with them in time.
Since we have in mind using the pulse to send a very large number of symbols n
(where n corresponds to the number of symbols sent during the lifetime of the
system) we shall strengthen the orthonormality requirement to
 ∞
−∞
φ(t −ℓTs) φ∗(t −ℓ′Ts) dt = I{ℓ= ℓ′},
for all integers ℓ, ℓ′
(11.1)
and not only to those ℓ, ℓ′ in {1, . . . , n}.
We shall refer to Condition (11.1) as
saying that “the time shifts of φ by integer multiples of Ts are orthonormal.”
Condition (11.1) can also be phrased as a condition on φ’s self-similarity function,
which we introduce next.
11.2
The Self-Similarity Function of Energy-Limited Signals
We next introduce the self-similarity function of energy-limited signals. This
term is not standard; more common in the literature is the term “autocorrelation
function.” I prefer “self-similarity function,” which was proposed to me by Jim
Massey, because it reduces the risk of confusion with the autocovariance function
and the autocorrelation function of stochastic processes. There is nothing random
in our current setup.
Deﬁnition 11.2.1 (Self-Similarity Function). The self-similarity function Rvv
of an energy-limited signal v ∈L2 is deﬁned as the mapping
Rvv : τ 	→
 ∞
−∞
v(t + τ) v∗(t) dt,
τ ∈R.
(11.2)
If v is real, then the self-similarity function has a nice pictorial interpretation: one
plots the original signal and the result of shifting the signal by τ on the same graph,
and one then takes the pointwise product and integrates over time.
The main properties of the self-similarity function are summarized in the following
proposition.
Proposition 11.2.2 (Properties of the Self-Similarity Function). Let Rvv be the
self-similarity function of some energy-limited signal v ∈L2.
(i) Value at zero:
Rvv(0) =
 ∞
−∞
|v(t)|2 dt.
(11.3)
11.2 The Self-Similarity Function of Energy-Limited Signals
197
(ii) Maximum at zero:
|Rvv(τ)| ≤Rvv(0),
τ ∈R.
(11.4)
(iii) Conjugate symmetry:
Rvv(−τ) = R∗
vv(τ),
τ ∈R.
(11.5)
(iv) Integral representation:
Rvv(τ) =
 ∞
−∞
|ˆv(f)|2 ei2πfτ df,
τ ∈R,
(11.6)
where ˆv is the L2-Fourier Transform of v.
(v) Uniform Continuity: Rvv is uniformly continuous.
(vi) Convolution Representation:
Rvv(τ) = (v ⋆~v∗) (τ),
τ ∈R.
(11.7)
Proof. Part (i) follows by substituting τ = 0 in (11.2).
Part (ii) follows by noting that Rvv(τ) is the inner product between the mapping
t 	→v(t + τ) and the mapping t 	→v(t); by the Cauchy-Schwarz Inequality (Theo-
rem 3.3.1); and by noting that both of the above mappings have the same energy,
namely, the energy of v:
|Rvv(τ)| =

 ∞
−∞
v(t + τ) v∗(t) dt

≤
	 ∞
−∞
|v(t + τ)|2 dt

1/2	  ∞
−∞
|v∗(t)|2 dt

1/2
= ∥v∥2
2
= Rvv(0),
τ ∈R.
Part (iii) follows from the substitution s ≜t + τ in the following:
Rvv(τ) =
 ∞
−∞
v(t + τ) v∗(t) dt
=
 ∞
−∞
v(s) v∗(s −τ) ds
=
	 ∞
−∞
v(s −τ) v∗(s) ds

∗
= R∗
vv(−τ),
τ ∈R.
Part (iv) follows from the representation of Rvv(τ) as the inner product between
the mapping t 	→v(t + τ) and the mapping t 	→v(t); by Parseval’s Theorem;
198
Nyquist’s Criterion
and by noting that the L2-Fourier Transform of the mapping t 	→v(t + τ) is the
(equivalence class of the) mapping f 	→ei2πfτ ˆv(f):
Rvv(τ) =
 ∞
−∞
v(t + τ) v∗(t) dt
=

t 	→v(t + τ), t 	→v(t)

=

f 	→ei2πfτ ˆv(f), f 	→ˆv(f)

=
 ∞
−∞
ei2πfτ|ˆv(f)|2 df,
τ ∈R.
Part (v) follows from the integral representation of Part (iv) and the integrability
of the function f 	→|ˆv(f)|2 using Theorem 6.2.11 (ii).
Part (vi) follows from the substitution s ≜t + τ and by rearranging terms:
Rvv(τ) =
 ∞
−∞
v(t + τ) v∗(t) dt
=
 ∞
−∞
v(s) v∗(s −τ) ds
=
 ∞
−∞
v(s)~v∗(τ −s) ds
= (v ⋆~v∗)(τ).
With the above deﬁnition we can restate the orthonormality condition (11.1) in
terms of the self-similarity function Rφφ of φ:
Proposition 11.2.3 (Shift-Orthonormality and Self-Similarity). If φ is energy-
limited, then the shift-orthonormality condition
 ∞
−∞
φ(t −ℓTs) φ∗(t −ℓ′Ts) dt = I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z
(11.8)
is equivalent to the condition
Rφφ(ℓTs) = I{ℓ= 0},
ℓ∈Z.
(11.9)
Proof. The proposition follows by substituting s ≜t −ℓ′Ts in the LHS of (11.8)
to obtain
 ∞
−∞
φ(t −ℓTs) φ∗(t −ℓ′Ts) dt =
 ∞
−∞
φ

s + (ℓ′ −ℓ)Ts

φ∗(s) ds
= Rφφ

(ℓ′ −ℓ)Ts

.
At this point, Proposition 11.2.3 does not seem particularly helpful because Con-
dition (11.9) is not easy to verify. But, as we shall see in the next section, this
condition can be phrased very elegantly in the frequency domain.
11.3 Nyquist’s Criterion
199
11.3
Nyquist’s Criterion
Deﬁnition 11.3.1 (Nyquist Pulse). We say that a complex signal v: R 	→C is a
Nyquist Pulse of parameter Ts if
v(ℓTs) = I{ℓ= 0},
ℓ∈Z.
(11.10)
Theorem 11.3.2 (Nyquist’s Criterion). Let Ts > 0 be given, and let the signal v(·)
be given by
v(t) =
 ∞
−∞
g(f) ei2πft df,
t ∈R,
(11.11)
for some integrable function g: f 	→g(f). Then v(·) is a Nyquist Pulse of param-
eter Ts if, and only if,
lim
J→∞
 1/(2Ts)
−1/(2Ts)
Ts −
J

j=−J
g

f + j
Ts
 df = 0.
(11.12)
Note 11.3.3. Condition (11.12) is sometimes written informally1 in the form
∞

j=−∞
g

f + j
Ts

= Ts,
−1
2Ts
≤f ≤
1
2Ts
,
(11.13)
or, in view of the periodicity of the LHS of (11.13), as
∞

j=−∞
g

f + j
Ts

= Ts,
f ∈R.
(11.14)
Neither form is mathematically precise.
Proof. We will show that v(−ℓTs) is the ℓ-th Fourier Series Coeﬃcient of the
function2
f 	→
1
√Ts
∞

j=−∞
g

f + j
Ts

,
−1
2Ts
≤f ≤
1
2Ts
.
(11.15)
It will then follow that the condition that v is a Nyquist Pulse of parameter Ts is
equivalent to the condition that the function in (11.15) has Fourier Series Coeﬃ-
cients that are all zero except for the zeroth coeﬃcient, which is one. The theorem
will then follow by noting that a function is indistinguishable from a constant if,
and only if, all but its zeroth Fourier Series Coeﬃcient are zero. (This can be
proved by applying Theorem A.2.3 with g1 chosen as the constant function.) The
1There is no guarantee that the sum converges at every frequency f.
2Since, by hypothesis, g is integrable, it follows that the sum in (11.15) converges in the L1
sense, i.e., that there exists some integrable function s∞such that
lim
J→∞
 1/(2Ts)
−1/(2Ts)
s∞(f) −
J

j=−J
g

f + j
Ts
 df = 0.
By writing 	∞
j=−∞g

f +
j
Ts

we are referring to this function s∞.
200
Nyquist’s Criterion
value of the constant can be computed from the zeroth Fourier Series Coeﬃcient.
To conclude the proof we thus need to relate v(−ℓTs) to the ℓ-th Fourier Series
Coeﬃcient of the function in (11.15). The calculation is straightforward: for every
integer ℓ,
v(−ℓTs) =
 ∞
−∞
g(f) e−i2πfℓTs df
=
∞

j=−∞

j
Ts +
1
2Ts
j
Ts −
1
2Ts
g(f) e−i2πfℓTs df
=
∞

j=−∞

1
2Ts
−
1
2Ts
g

˜f + j
Ts

e−i2π( ˜
f+ j
Ts )ℓTs d ˜f
=
∞

j=−∞

1
2Ts
−
1
2Ts
g

˜f + j
Ts

e−i2π ˜
fℓTs d ˜f
=

1
2Ts
−
1
2Ts
∞

j=−∞
g

˜f + j
Ts

e−i2π ˜
fℓTs d ˜f
=

1
2Ts
−
1
2Ts
	 1
√Ts
∞

j=−∞
g

˜f + j
Ts


Ts e−i2π ˜
fℓTs d ˜f,
(11.16)
which is the ℓ-th Fourier Series Coeﬃcient of the function in (11.15). Here the ﬁrst
equality follows by substituting −ℓTs for t in (11.11); the second by partitioning the
region of integration into intervals of length
1
Ts ; the third by the change of variable
˜f ≜f −j
Ts ; the fourth by the periodicity of the complex exponentials; the ﬁfth by
Fubini’s Theorem, which allows us to swap the order summation and integration;
and the ﬁnal equality by multiplying and dividing by √Ts.
An example of a function f 	→g(f) satisfying (11.12) is plotted in Figure 11.1.
Corollary 11.3.4 (Characterization of Shift-Orthonormal Pulses). Let φ: R 	→C
be energy-limited and let Ts be positive. Then the condition
 ∞
−∞
φ(t −ℓTs) φ∗(t −ℓ′Ts) dt = I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z
(11.17)
is equivalent to the condition
∞

j=−∞
ˆφ

f + j
Ts

2
≡Ts,
(11.18)
i.e., to the condition that the set of frequencies f ∈R for which the LHS of (11.18)
is not equal to Ts is of Lebesgue measure zero.3
3It is a simple technical matter to verify that the question as to whether or not (11.18) is
satisﬁed outside a set of frequencies of Lebesgue measure zero does not depend on which element
in the equivalence class of the L2 -Fourier Transform of φ is considered.
11.3 Nyquist’s Criterion
201
f
f
f
f
Ts
Ts
Ts
1
2Ts
1
2Ts
−1
2Ts
−1
2Ts
−1
Ts
−1
Ts
1
Ts
1
Ts
2
Ts
−2
Ts
g(f)
g
	
f + 1
Ts

g
	
f −1
Ts

∞

j=−∞
g
	
f + j
Ts

= Ts
Figure 11.1: A function g(·) satisfying (11.12).
202
Nyquist’s Criterion
Proof. By Proposition 11.2.3, Condition (11.17) can be equivalently expressed in
terms of the self-similarity function as
Rφφ(mTs) = I{m = 0},
m ∈Z.
(11.19)
The result now follows from the integral representation of the self-similarity func-
tion Rφφ (Proposition 11.2.2 (iv)) and from Theorem 11.3.2 (with the additional
simpliﬁcation that for every j ∈Z the function f 	→
ˆφ

f + j
Ts
2 is nonnegative, so
the sum on the LHS of (11.18) converges (possibly to +∞) for every f ∈R).
An extremely important consequence of Corollary 11.3.4 is the following corollary
about the minimum bandwidth of a pulse φ satisfying the orthonormality condition
(11.1).
Corollary 11.3.5 (Minimum Bandwidth of Shift-Orthonormal Pulses). Let Ts > 0
be ﬁxed, and let φ be an energy-limited signal that is bandlimited to W Hz. If the
time shifts of φ by integer multiples of Ts are orthonormal, then
W ≥
1
2Ts
.
(11.20)
Equality is achieved if
ˆφ(f)
 =

Ts I
'
|f| ≤
1
2Ts
(
,
f ∈R
(11.21)
and, in particular, by the sinc(·) pulse
φ(t) =
1
√Ts
sinc
 t
Ts

,
t ∈R
(11.22)
or any time shift thereof.
Proof. Figure 11.2 illustrates why φ cannot satisfy (11.18) if (11.20) is violated.
The ﬁgure should also convince you of the conditions for equality in (11.20).
For the algebraically-inclined readers we prove the corollary by showing that if
W ≤1/(2Ts), then (11.18) can only be satisﬁed if φ satisﬁes (11.21) (outside a set
of frequencies of Lebesgue measure zero).4 To see this, consider the sum
∞

j=−∞
ˆφ

f + j
Ts

2
(11.23)
for frequencies f in the open interval

−1
2Ts , + 1
2Ts

. The key observation in the
proof is that for frequencies in this open interval, if W ≤1/(2Ts), then all the terms
in the sum (11.23) are zero, except for the j = 0 term. That is,
∞

j=−∞
ˆφ

f + j
Ts

2
=
ˆφ(f)
2,
	
W ≤
1
2Ts
, f ∈

−1
2Ts
, + 1
2Ts

.
(11.24)
4In the remainder of the proof we assume that ˆφ(f) is zero for frequencies f satisfying |f| > W.
The proof can be easily adjusted to account for the fact that, for frequencies |f| > W, it is possible
that ˆφ(·) be nonzero on a set of Lebesgue measure zero.
11.3 Nyquist’s Criterion
203
To convince yourself of (11.24), consider, for example, the term corresponding to
j = 1, namely, |ˆφ(f + 1/Ts)|2. By the deﬁnition of bandwidth, it is zero whenever
|f + 1/Ts| > W, i.e., whenever f > −1/Ts + W or f < −1/Ts −W. Since the
former category f > −1/Ts + W includes—by our assumption that W ≤1/(2Ts)—
all frequencies f > −1/(2Ts), we conclude that the term corresponding to j = 1
is zero for all the frequencies f in the open interval

−1
2Ts , + 1
2Ts

. More generally,
the j-th term |ˆφ(f + j/Ts)|2 is zero for all frequencies f satisfying the condition
|f +j/Ts| > W, a condition that is satisﬁed—assuming j ̸= 0 and W ≤1/(2Ts)—by
the frequencies in the open interval that is of interest to us

−1
2Ts , + 1
2Ts

.
For W ≤1/(2Ts) we thus obtain from (11.24) that the condition (11.18) implies
(11.21), and, in particular, that W = 1/(2Ts).
Functions satisfying (11.21) are seldom used in digital communication because they
typically decay like 1/t so that even if the transmitted symbols Xℓare bounded,
the signal X(t) may take on very high values (albeit quite rarely). Consequently,
the pulses φ that are used in practice have a larger bandwidth than 1/(2Ts).
This leads to the following deﬁnition.
Deﬁnition 11.3.6 (Excess Bandwidth). The excess bandwidth in percent of a
signal φ relative to Ts > 0 is deﬁned as
100%
	bandwidth of φ
1/(2Ts)
−1

.
(11.25)
The following corollary to Corollary 11.3.4 is useful for the understanding of real
signals of excess bandwidth smaller than 100%.
Corollary 11.3.7 (Band-Edge Symmetry). Let Ts be positive, and let φ be a real
energy-limited signal that is bandlimited to W Hz, where W < 1/Ts so φ is of excess
bandwidth smaller than 100%. Then the time shifts of φ by integer multiples of Ts
are orthonormal if, and only if, f 	→|ˆφ(f)|2 satisﬁes the band-edge symmetry
condition5
ˆφ
 1
2Ts
−f

2
+
ˆφ
 1
2Ts
+ f

2
≡Ts,
0 < f ≤
1
2Ts
.
(11.26)
Proof. We ﬁrst note that, since we have assumed that W < 1/Ts, only the terms
corresponding to j = −1, j = 0, and j = 1 contribute to the sum on the LHS of
(11.18) for f ∈

−1
2Ts , + 1
2Ts

. Moreover, since φ is by hypothesis real, it follows
that |ˆφ(−f)| = |ˆφ(f)|, so the sum on the LHS of (11.18) is a symmetric function
of f. Thus, the sum is equal to Ts on the interval

−1
2Ts , + 1
2Ts

if, and only if, it is
equal to Ts on the interval

0, + 1
2Ts

. For frequencies in this shorter interval only
two terms in the sum contribute: those corresponding to j = 0 and j = −1. We
5Condition (11.26) should be understood to indicate that the LHS and RHS of (11.26) are
equal for all frequencies 0 ≤f ≤1/(2Ts) outside a set of Lebesgue measure zero. Again, we ignore
this issue in the proof and assume that ˆφ(f) is zero for all |f| > W.
204
Nyquist’s Criterion
f
f
f
f
f
W
W
W
−W
−W
−W
1
Ts
−1
Ts
1
Ts −W
−1
Ts + W
1
2Ts
−1
2Ts
ˆφ(f)
ˆφ(f)
2
ˆφ

f −
1
Ts
2
ˆφ

f +
1
Ts
2
ˆφ

f +
1
Ts
2 +
ˆφ(f)
2 +
ˆφ

f −
1
Ts
2
Figure 11.2: If W < 1/(2Ts), then all the terms of the form
ˆφ

f + j
Ts
2 are zero
over the shaded frequencies W < |f| < 1/(2Ts). Thus, for W < 1/(2Ts) the sum
∞
j=−∞
ˆφ

f + j
Ts
2 cannot be equal to Ts at any of the shaded frequencies.
11.3 Nyquist’s Criterion
205
f
ˆφ(f)
2
Ts
Ts
2
1
2Ts
1
Ts
f ′
ˆφ

f ′ +
1
2Ts
2 −Ts
2
Figure 11.3: An example of a choice for |ˆφ(·)|2 satisfying the band-edge symmetry
condition (11.26).
thus conclude that, for real signals of excess bandwidth smaller than 100%, the
condition (11.18) is equivalent to the condition
ˆφ(f)
2 +
ˆφ(f −1/Ts)
2 ≡Ts,
0 ≤f <
1
2Ts
.
Substituting f ′ ≜
1
2Ts −f in this condition leads to the condition
ˆφ
 1
2Ts
−f ′
2
+
ˆφ

−f ′ −1
2Ts

2
≡Ts,
0 < f ′ ≤
1
2Ts
,
which, in view of the symmetry of |ˆφ(·)|, is equivalent to
ˆφ
 1
2Ts
−f ′
2
+
ˆφ

f ′ + 1
2Ts

2
≡Ts,
0 < f ′ ≤
1
2Ts
,
i.e., to (11.26).
Note 11.3.8. The band-edge symmetry condition (11.26) has a nice geometric
interpretation. This is best seen by rewriting the condition in the form
ˆφ
 1
2Ts
−f ′
2
−Ts
2



=˜g(−f ′)
= −
	ˆφ
 1
2Ts
+ f ′
2
−Ts
2




=˜g(f ′)
,
0 < f ′ ≤
1
2Ts
,
(11.27)
which demonstrates that the band-edge condition is equivalent to the condition
that the plot of f 	→|ˆφ(f)|2 in the interval 0 < f < 1/Ts be invariant with
respect to a 180◦-rotation around the point
 1
2Ts , Ts
2

. In other words, the function
˜g: f ′ 	→
ˆφ
 1
2Ts + f ′2 −Ts
2 should be anti-symmetric for 0 < f ′ ≤
1
2Ts . I.e., it
should satisfy
˜g(−f ′) = −˜g(f ′),
0 < f ′ ≤
1
2Ts
.
206
Nyquist’s Criterion
f
ˆφ(f)
2
Ts
1−β
2Ts
1
2Ts
1+β
2Ts
Figure 11.4: A plot of f 	→| ˆφ(f)|2 as given in (11.30) with β = 0.5.
Figure 11.3 is a plot over the interval [0, 1/Ts) of a mapping f 	→|ˆφ(f)|2 that
satisﬁes the band-edge symmetry condition (11.26).
A popular choice of φ is based on the raised-cosine family of functions. For every
0 < β ≤1 and every Ts > 0, the raised-cosine function is given by the mapping
f 	→
⎧
⎪
⎪
⎨
⎪
⎪
⎩
Ts
if
0 ≤|f| ≤1−β
2Ts ,
Ts
2

1 + cos

πTs
β (|f| −1−β
2Ts )

if
1−β
2Ts < |f| ≤1+β
2Ts ,
0
if
|f| > 1+β
2Ts .
(11.28)
Choosing φ so that its Fourier Transform is the square root of the raised-cosine
mapping (11.28)
ˆφ(f) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
√Ts
if
0 ≤|f| ≤1−β
2Ts ,
3
Ts
2
4
1 + cos

πTs
β (|f| −1−β
2Ts )

if
1−β
2Ts < |f| ≤1+β
2Ts ,
0
if
|f| > 1+β
2Ts ,
(11.29)
results in φ being real with
|ˆφ(f)|2 =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
Ts
if
0 ≤|f| ≤1−β
2Ts ,
Ts
2

1 + cos

πTs
β (|f| −1−β
2Ts )

if
1−β
2Ts < |f| ≤1+β
2Ts ,
0
if
|f| > 1+β
2Ts ,
(11.30)
as depicted in Figure 11.4 for β = 0.5.
Using (11.29) and using the band-edge symmetry criterion (Corollary 11.3.7), it
can be readily veriﬁed that the time shifts of φ by integer multiples of Ts are
orthonormal. Moreover, by (11.29), φ is bandlimited to (1 + β)/(2Ts) Hz. It is
thus of excess bandwidth β × 100%. For every 0 < β ≤1 we have thus found a
pulse φ of excess bandwidth β × 100% whose time shifts by integer multiples of Ts
are orthonormal.
11.3 Nyquist’s Criterion
207
t
τ
φ(t)
Rφφ(τ)
1
1
Ts
2Ts
−Ts
−2Ts
Figure 11.5: The pulse φ(·) of (11.31) with β = 0.5 and its self-similarity func-
tion Rφφ(·) of (11.32).
In the time domain
φ(t) =
4β
π√Ts
cos

(1 + β)π t
Ts

+
sin ((1−β)π t
Ts )
4β t
Ts
1 −(4β t
Ts )2
,
t ∈R,
(11.31)
with corresponding self-similarity function
Rφφ(τ) = sinc
 τ
Ts
 cos(πβτ/Ts)
1 −4β2τ 2/T2
s
,
τ ∈R.
(11.32)
The pulse φ of (11.31) is plotted in Figure 11.5 (top) for β = 0.5. Its self-similarity
function (11.32) is plotted in the same ﬁgure (bottom). That the time shifts of φ
by integer multiples of Ts are orthonormal can be veriﬁed again by observing that
Rφφ as given in (11.32) satisﬁes Rφφ(ℓTs) = I{ℓ= 0} for all ℓ∈Z.
Notice also that if φ(·) is chosen as in (11.31), then for all 0 < β ≤1, the pulse φ(·)
decays like 1/t2. This decay property combined with the fact that the inﬁnite sum
∞
ν=1 ν−2 converges (Rudin, 1976, Chapter 3, Theorem 3.28) will prove useful in
Section 14.3 when we discuss the power in PAM.
208
Nyquist’s Criterion
11.4
The Self-Similarity Function of Integrable Signals
This section is a bit technical and can be omitted at ﬁrst reading. In it we deﬁne
the self-similarity function for integrable signals that are not necessarily energy-
limited, and we then compute the Fourier Transform of the so-deﬁned self-similarity
function.
Recall that a Lebesgue measurable complex signal v: R →C is integrable if
 ∞
−∞|v(t)| dt < ∞and that the class of integrable signal is denoted by L1. For
such signals there may be τ’s for which the integral in (11.2) is undeﬁned. For
example, if v is not energy-limited, then the integral in (11.2) will be inﬁnite at
τ = 0. Nevertheless, we can discuss the self-similarity function of such signals by
adopting the convolution representation of Proposition 11.2.2 (vi) as the deﬁnition.
We thus deﬁne the self-similarity function Rvv of an integrable signal v ∈L1 as
Rvv ≜v ⋆~v∗,
v ∈L1,
(11.33)
but we need some clariﬁcation. Since v is integrable, and since this implies that
its mirror image ~v is also integrable, it follows that the convolution in (11.33) is
a convolution between two integrable signals. As such, we are guaranteed by the
discussion leading to (5.9) that the integral
 ∞
−∞
v(σ)~v∗(τ −σ) dσ =
 ∞
−∞
v(t + τ) v∗(t) dt
is deﬁned for all τ’s outside a set of Lebesgue measure zero. (This set of Lebesgue
measure zero will include the point τ = 0 if v is not of ﬁnite energy.) For τ’s inside
this set of measure zero we deﬁne the self-similarity function to be zero. The value
zero is quite arbitrary because, irrespective of the value we choose for such τ’s, we
are guaranteed by (5.9) that the so-deﬁned self-similarity function Rvv is integrable
 ∞
−∞
Rvv(τ)
 dτ ≤∥v∥2
1 ,
v ∈L1,
(11.34)
and that its L1-Fourier Transform is given by the product of the L1-Fourier Trans-
form of v and the L1-Fourier Transform of ~v∗, i.e.,
ˆRvv(f) = |ˆv(f)|2,

v ∈L1, f ∈R

.
(11.35)
11.5
Exercises
Exercise 11.1 (Passband Signaling). Let f0, Ts > 0 be ﬁxed.
(i) Show that a signal x is a Nyquist Pulse of parameter Ts if, and only if, the signal
t →ei2πf0t x(t) is such a pulse.
(ii) Show that if x is a Nyquist Pulse of parameter Ts, then so is t →cos(2πf0t) x(t).
(iii) If t →cos(2πf0t) x(t) is a Nyquist Pulse of parameter Ts, must x also be one?
11.5 Exercises
209
Exercise 11.2 (Self-Similarity of a Nonnegative Signal). Show that the self-similarity
function of a nonnegative signal is nonnegative.
Exercise 11.3 (The Self-Similarity Function of a Delayed Signal). Let u be an energy-
limited signal, and let the signal v be given by v: t →u(t−t0). Express the self-similarity
function of v in terms of the self-similarity of u and t0.
Exercise 11.4 (Transforming an Energy-Limited Signal). Let v be an energy-limited signal
of self-similarity function Rvv, and let p be the signal
p(t) = Av(αt),
t ∈R,
where A and α are real numbers (not necessarily positive) and α ̸= 0. Is p energy-limited?
If so, relate its self-similarity function Rpp to Rvv.
Exercise 11.5 (Reﬂection and the Self-Similarity Function). Let v be a complex energy-
limited signal. Relate its self-similarity function Rvv to that of its mirror image ~v.
Exercise 11.6 (The Self-Similarity Function of a Frequency Shifted Signal). Let u be
an energy-limited complex signal, and let the signal v be given by v: t →u(t) ei2πf0t for
some f0 ∈R. Express the self-similarity function of v in terms of f0 and the self-similarity
function of u.
Exercise 11.7 (A Self-Similarity Function). Compute and plot the self-similarity function
of the signal t →A

1 −|t|/T

I

|t| ≤T

.
Exercise 11.8 (Symmetry of the FT of the Self-Similarity Function of a Real Signal).
Show that if φ is an integrable real signal, then the FT of its self-similarity function is
symmetric:
	
ˆRφφ(f) = ˆRφφ(−f),
f ∈R

,
φ ∈L1 is real.
Exercise 11.9 (When Is the Convolution a Nyquist Pulse?). Let s and h be energy-
limited.
(i) Show that their convolution can be expressed as

s ⋆h

(t) =
 ∞
−∞
g(f) ei2πft df,
t ∈R,
where g is some integrable function. Express g in terms of ˆs and ˆh.
(ii) Let the L2-Fourier Transform of h be (the equivalence class of)
f →4
3

1 −|f|Ts

I
 
|f| ≤
1
2Ts
!
,
f ∈R.
What condition on ˆs is equivalent to the condition that s⋆h is a Nyquist pulse with
respect to Ts? Which is the signal s of least bandwidth that satisﬁes this condition?
Hint: For Part (i) adapt the proof of Proposition 6.6.1.
210
Nyquist’s Criterion
Exercise 11.10 (The Self-Similarity Function is Positive Deﬁnite). Show that if v is an
energy-limited signal, n is a positive integer, α1, . . . , αn ∈C, and t1, . . . , tn ∈R, then
n

j=1
n

ℓ=1
αjα∗
ℓRvv(tj −tℓ) ≥0.
Hint: Compute the energy in the signal t →n
j=1 αj v(t + tj).
Exercise 11.11 (Self-Similarity Functions and Characteristic Functions). Show that Φ(·)
is the characteristic function of some RV having a probability density function if, and
only if, Φ = Ruu for some unit-energy (possibly complex) signal u. (This is the Wiener-
Khinchin criterion.)
Hint: If Φ(·) is the characteristic function of the RV X of density fX(·), consider the
L2-Fourier Transform of √fX. For the other direction use Proposition 11.2.2 (iv).
Exercise 11.12 (Bounds on the Self-Similarity Function). Let v be an energy-limited
signal that is bandlimited to W Hz, and let Rvv be its self-similarity function. Prove that
Re

Rvv(τ)

≥∥v∥2
2
	
1 −min

2, 2π2 W2τ 2
,
τ ∈R.
Hint: See Exercise 6.11.
Exercise 11.13 (Relaxing the Orthonormality Condition). What is the minimal bandwidth
of an energy-limited signal whose time shifts by even multiples of Ts are orthonormal?
What is the minimal bandwidth of an energy-limited signal whose time shifts by odd
multiples of Ts are orthonormal?
Exercise 11.14 (Bandwidth and Shift-Orthonormality). Consider the signal
g(t) = α
 β
−β
&
1 −|f|
β ei2πft df,
t ∈R,
where α and β are positive numbers.
(i) Is g an energy-limited bandlimited signal? If so, of what bandwidth and energy?
(ii) Given Ts > 0, ﬁnd positive numbers α and β for which g satisﬁes
 ∞
−∞
g(t −ℓTs) g(t −ℓ′Ts) dt = I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z.
Exercise 11.15 (A Speciﬁc Signal). Let p be the complex energy-limited bandlimited
signal whose FT ˆp is given by
ˆp(f) = Ts

1 −|Tsf −1|

I
 
0 ≤f ≤2
Ts
!
,
f ∈R.
(i) Plot ˆp(·).
(ii) Is p(·) a Nyquist Pulse of parameter Ts?
(iii) Is the real part of p(·) a Nyquist Pulse of parameter Ts?
(iv) What about the imaginary part of p(·)?
11.5 Exercises
211
Exercise 11.16 (Orthogonality of Time Shifts of Diﬀerent Signals).
(i) Let u and v be energy-limited signals of L2-Fourier Transforms ˆu and ˆv. Let Ts be
a positive constant. Show that the condition
 ∞
−∞
u(t −ℓTs) v∗(t −ℓ′Ts) dt = 0,
ℓ, ℓ′ ∈Z
is equivalent to the condition
lim
J→∞

1
2Ts
−
1
2Ts

J

j=−J
ˆu
	
f + j
Ts

ˆv∗	
f + j
Ts

 df = 0.
If either u or v is bandlimited to W Hz, then the latter condition is equivalent to
f →
∞

j=−∞
ˆu
	
f + j
Ts

ˆv∗	
f + j
Ts

being indistinguishable from the all-zero function.
(ii) Assume that u and v are energy-limited signals that are bandlimited to W Hz and
that for every integer ℓ
Ruu(ℓTs) = Rvv(ℓTs) = I{ℓ= 0}
and
 ∞
−∞
u(t) v∗(t −ℓTs) dt = 0.
Prove that W ≥1/Ts.
Exercise 11.17 (Nyquist’s Third Criterion). We say that an energy-limited signal ψ(·)
satisﬁes Nyquist’s Third Criterion if
 (2ν+1)Ts/2
(2ν−1)Ts/2
ψ(t) dt =

1
if ν = 0,
0
if ν ∈Z \ {0}.
(11.36)
(i) Express the LHS of (11.36) as an inner product between ψ and some function gν.
(ii) Show that (11.36) is equivalent to
Ts
 ∞
−∞
ˆψ(f) e−i2πfνTs sinc(Tsf) df =

1
if ν = 0,
0
if ν ∈Z \ {0}.
(iii) Show that, loosely speaking, ψ satisﬁes Nyquist’s Third Criterion if, and only if,
f →
∞

j=−∞
ˆψ
	
f −j
Ts

sinc(Tsf −j)
is indistinguishable from the all-one function. More precisely, if and only if,
lim
J→∞

1
2Ts
−
1
2Ts
1 −
J

j=−J
ˆψ
	
f −j
Ts

sinc(Tsf −j)
 df = 0.
(iv) What is the FT of the pulse of least bandwidth that satisﬁes Nyquist’s Third
Criterion with respect to the baud Ts? What is its bandwidth?
212
Nyquist’s Criterion
Exercise 11.18 (Multiplication by a Carrier).
(i) Let u be an energy-limited complex signal that is bandlimited to W Hz, and let
f0 > W be given. Let v be the signal v: t →u(t) cos(2πf0t). Express the self-
similarity function of v in terms of f0 and the self-similarity function of u.
(ii) Let the signal φ be given by φ: t →
√
2 cos(2πfct) ψ(t), where fc > W/2 > 0;
where 4fcTs is an odd integer; and where ψ is a real energy-limited signal that
is bandlimited to W/2 Hz and whose time shifts by integer multiples of (2Ts) are
orthonormal. Show that the time shifts of φ by integer multiples of Ts are orthonor-
mal.
Exercise 11.19 (The Self-Similarity of a Convolution). Let p and q be integrable signals
of self-similarity functions Rpp and Rqq. Show that the self-similarity function of their
convolution p ⋆q is indistinguishable from Rpp ⋆Rqq.
Exercise 11.20 (The Integral of the Self-Similarity Function). Relate the integral of an
integrable signal x to the integral of its self-similarity function Rxx.
Chapter 12
Stochastic Processes: Deﬁnition
12.1
Introduction and Continuous-Time Heuristics
In this chapter we shall deﬁne stochastic processes. Our deﬁnition will be general so
as to include the continuous-time stochastic processes of the type we encountered
in Section 10.2 and also discrete-time processes.
In Section 10.2 we saw that since the data bits that we wish to communicate
are random, the transmitted waveform is a stochastic process.
But stochastic
processes play an important role in Digital Communications not only in modeling
the transmitted signals: they are also used to model the noise in the system and
other sources of impairments.
The stochastic processes we encountered in Section 10.2 are continuous-time pro-
cesses. We proposed that you think about such a process as a real-valued function
of two variables: “time” and “luck.” By “luck” we mean the realization of all the
random components of the system, e.g., the bits to be sent, the realization of the
noise processes (that we shall discuss later), or any other sources of randomness in
the system.
Somewhat more precisely, recall that a probability space is deﬁned as a triple
(Ω, F, P), where the set Ω is the set of experiment outcomes, the set F is the set
of events, and where P(·) assigns probabilities to the various events. A measurable
real-valued function of the outcome is a random variable, and a function of time and
the experiment outcome is a random process or a stochastic process. A continuous-
time stochastic process X is thus a mapping
X: Ω × R →R
(ω, t) 	→X(ω, t).
If we ﬁx some experiment outcome ω ∈Ω, then the random process can be regarded
as a function of one argument: time. This function is sometimes called a sample-
path, trajectory, sample-path realization, or a sample function
X(ω, ·): R →R
t 	→X(ω, t).
213
214
Stochastic Processes: Deﬁnition
t
t
Ts
2
−Ts
2
Ts
−Ts
g(t)
4
ℓ=−4 xℓg(t −ℓTs)
Figure 12.1: The pulse shape g: t 	→

1 −4|t|/Ts

I

|t| < Ts/4

, and the sample
function t 	→4
ℓ=−4 xℓg(t −ℓTs) when

x−4, x−3, x−2, x−1, x0, x1, x2, x3, x4

=
(−1, −1, +1, +1, −1, +1, −1, −1, −1).
Similarly, if we ﬁx an epoch t ∈R and view the stochastic process as a function of
“luck” only, we obtain a random variable:
X(·, t): Ω →R
ω 	→X(ω, t).
This random variable is sometimes called the value of the process at time t or
the time-t sample of the process.
Figure 12.1 shows the pulse shape g: t 	→

1 −4|t|/Ts

I{|t| < Ts/4} and a sample-
path of the PAM signal
X(t) =
4

ℓ=−4
Xℓg(t −ℓTs)
(12.1)
with {Xℓ} taking values in the set {−1, +1}.
Notice that in this example the
functions t 	→g(t −ℓTs) and t 	→g(t −ℓ′Ts) do not “overlap” if ℓ̸= ℓ′.
Figure 12.2 shows the pulse shape
g: t 	→

1 −
4
3Ts |t|
|t| ≤3Ts
4 ,
0
|t| > 3Ts
4 ,
t ∈R
(12.2)
and a sample-path of the PAM signal (12.1) for {Xℓ} taking values in the set
{−1, +1}. In this example the mappings t 	→g(t −ℓTs) and t 	→g(t −ℓ′Ts) do
overlap (when ℓ′ ∈{ℓ−1, ℓ, ℓ+ 1}).
12.2
A Formal Deﬁnition
We next give a formal deﬁnition of a stochastic process, which is also called a
random process, or a random function.
12.2 A Formal Deﬁnition
215
t
t
Ts
2
−Ts
2
Ts
−Ts
g(t)
4
ℓ=−4 xℓg(t −ℓTs)
Figure 12.2: The pulse shape g of (12.2) and the trajectory t 	→4
ℓ=−4 xℓg(t−ℓTs)
for

x−4, x−3, x−2, x−1, x0, x1, x2, x3, x4

= (−1, −1, +1, +1, −1, +1, −1, −1, −1).
Deﬁnition 12.2.1 (Stochastic Process). A stochastic process

X(t), t ∈T

is an
indexed family of random variables that are deﬁned on a common probability space
(Ω, F, P). Here T denotes the indexing set and X(t) (or sometimes Xt) denotes
the random variable indexed by t.
Thus, X(t) is the random variable to which t ∈T is mapped. For each t ∈T
we have that X(t) is a random variable, i.e., a measurable mapping from the
experiment outcomes set Ω to the reals.1
A stochastic process

X(t), t ∈T

is said to be centered or of zero mean if all
the random variables in the family are of zero mean, i.e., if for every t ∈T we have
E[X(t)] = 0. It is said to be of ﬁnite variance if all the random variables in the
family are of ﬁnite variance, i.e., if E

X2(t)

< ∞for all t ∈T .
The case where the indexing set T comprises only one element is not particularly
exciting because in this case the stochastic process is just a random variable with
fancy packaging. Similarly, when T is ﬁnite, the SP is just a random vector or a
tuple of random variables in disguise. The cases that will be of most interest are
enumerated below.
(i) When the indexing set T is the set of integers Z, the stochastic process is
said to be a discrete-time stochastic process and in this case it is simply
a bi-inﬁnite sequence of random variables
. . . , X−2, X−1, X0, X1, X2, . . .
1Some authors, e.g., (Doob, 1990), allow for X(t) to take on the values ±∞provided that
at each t ∈T this occurs with zero probability, but we, following (Lo`eve, 1963), insist that X(t)
only take on ﬁnite values.
216
Stochastic Processes: Deﬁnition
For discrete-time stochastic processes it is customary to denote the random
variable to which ν ∈Z is mapped by Xν rather than X(ν) and to refer to
Xν as the time-ν sample of the process

Xν, ν ∈Z

.
(ii) When the indexing set is the set of positive integers N, the stochastic process
is said to be a one-sided discrete-time stochastic process and it is simply
a one-sided sequence of random variables
X1, X2, . . .
Again, we refer to Xν as the time-ν sample of

Xν, ν ∈N

.
(iii) When the indexing set T is the real line R, the stochastic process is said to
be a continuous-time stochastic process and the random variable X(t)
is the time-t sample of

X(t), t ∈R

.
In dealing with continuous-time stochastic processes we shall usually denote the
process by

X(t), t ∈R

, by X, by X(·), or by

X(t)

. The random variable to
which t is mapped, i.e., the time-t sample of the process will be denoted by X(t).
Its realization will be denoted by x(t), and the sample-path of the process by x or
x(·).
Discrete-time processes will typically be denoted by

Xν, ν ∈Z

or by

Xν

.
We shall need only a few results on discrete-time stochastic processes, and those will
be presented in Chapter 13. Continuous-time stochastic processes will be discussed
in Chapter 25.
12.3
Describing Stochastic Processes
The description of a continuous-time stochastic process in terms of a random vari-
able (as in Section 10.2), in terms of a ﬁnite number of random variables (as in
PAM signaling), or in terms of an inﬁnite sequence of random variables (as in the
transmission using PAM signaling of an inﬁnite binary data stream) is particularly
well suited for describing human-generated stochastic processes or stochastic pro-
cesses that are generated using a mechanism that we fully understand. We simply
describe how the stochastic process is synthesized from the random variables. The
method is less useful when the stochastic process denotes a random signal (such
as thermal noise or some other interference of unknown origin) that we observe
rather than generate. In this case we can use measurements and statistical meth-
ods to analyze the process. Often, the best we can hope for is to be informed
of the ﬁnite-dimensional distributions of the process, a concept that will be
introduced in Section 25.2.
12.4
Additional Reading
Classic references on stochastic processes to which we shall frequently refer are
(Doob, 1990) and (Lo`eve, 1963). We also recommend (Gikhman and Skorokhod,
1996), (Cram´er and Leadbetter, 2004), and (Grimmett and Stirzaker, 2001). For
discrete-time stochastic processes, see (Pourahmadi, 2001) and (Porat, 2008).
12.5 Exercises
217
12.5
Exercises
Exercise 12.1 (Objects in a Basement). Let T1, T2, . . . be a sequence of positive random
variables, and let N1, N2, . . . be a sequence of random variables taking values in N. Deﬁne
X(t) =
∞

j=1
Nj I

t ≥Tj

,
t ∈R.
Draw some sample paths of

X(t), t ∈R

. Assume that at time zero a basement is empty
and that Nj denotes the number of objects in the j-th box, which is brought down to the
basement at time Tj. Explain why you can think of X(t) as the number of objects in the
basement at time t.
Exercise 12.2 (A Queue). Let S1, S2, . . . be a sequence of positive random variables. A
system is turned on at time zero. The ﬁrst customer arrives at the system at time S1
and the next at time S1 + S2.
More generally, Customer η arrives Sη minutes after
Customer (η −1). The system serves one customer at a time. It takes the system one
minute to serve each customer, and a customer leaves the system once it has been served.
Let X(t) denote the number of customers in the system at time t. Express X(t) in terms
of S1, S2, . . . Is

X(t), t ∈R

a stochastic process? If so, draw a few of its sample paths.
Compute Pr
$
X(0.5) > 0
%
. Express your answer in terms of the distribution of S1, S2, . . .
Exercise 12.3 (A Continuous-Time Markov SP). A particle is in State Zero at time t = 0.
It stays in that state for T (0)
1
seconds and then jumps to State One. It stays in State One
for T (1)
1
seconds and then jumps back to State Zero, where it stays for T (0)
2
seconds. In
general, T (0)
ν
is the duration of the particle’s stay in State Zero on its ν-th visit to that
state. Similarly, T (1)
ν
is the duration of its stay in State One on its ν-th visit. Assume
that T (0)
1
, T (1)
1
, T (0)
2
, T (1)
2
, T (0)
3
, T (1)
3
, . . . are independent with T (0)
ν
being a mean-μ0
exponential and with T (1)
ν
being a mean-μ1 exponential for all ν ∈N.
Let X(t) be deterministically equal to zero for t < 0, and equal to the particle’s state for
t ≥0.
(i) Plot some sample paths of

X(t), t ∈R

.
(ii) What is the probability that the sample path t →X(ω, t) is continuous in the
interval [0, t)?
(iii) Conditional on X(t) = 0, where t ≥0, what is the distribution of the remaining
duration of the particle’s stay in State Zero?
Hint: An exponential RV X has the memoryless property, i.e., that for every s, t ≥0 we
have Pr[X > s + t|X > t] = Pr[X ≥s].
Exercise 12.4 (Peak Power). Let the random variables

Dj, j ∈Z

be independent and
identically distributed, each taking on the values 0 and 1 equiprobably. Let
X(t) = A
∞

ℓ=−∞

1 −2Dℓ

g(t −ℓTs),
t ∈R,
where A, Ts > 0 and g: t →I{|t| ≤3Ts/4}. Find the distribution of the random variable
sup
t∈R
X(t)
.
218
Stochastic Processes: Deﬁnition
Exercise 12.5 (Sample-Path Continuity). Let the random variables

Dj, j ∈Z

be
independent and identically distributed, each taking on the values 0 and 1 equiprobably.
Let
X(t) = A
∞

ℓ=−∞

1 −2Dℓ

g(t −ℓTs),
t ∈R,
where A, Ts > 0. Suppose that the function g: R →R is continuous and is zero outside
some interval, so g(t) = 0 whenever |t| ≥T. Show that for every ω ∈Ω, the sample-path
t →X(ω, t) is a continuous function of time.
Exercise 12.6 (Random Sampling Time). Consider the setup of Exercise 12.5, with the
pulse shape g: t →

1 −2|t|/Ts

I

|t| ≤Ts/2

. Further assume that the RV T is inde-
pendent of

Dj, j ∈Z

and uniformly distributed over the interval [−δ, δ]. Find the
distribution of X(kTs + T) for any integer k.
Exercise 12.7 (A Strange SP). Let T be a mean-one exponential RV, and deﬁne the SP

X(t), t ∈R

by
X(t) =

1
if t = T,
0
otherwise.
Compute the distribution of X(t1) and the joint distribution of X(t1) and X(t2) for
t1, t2 ∈R. What is the probability that the sample-path t →X(ω, t) is continuous at t1?
What is the probability that the sample-path is a continuous function (everywhere)?
Exercise 12.8 (The Sum of Stochastic Processes: Formalities). Let the stochastic pro-
cesses

X1(t), t ∈R

and

X2(t), t ∈R

be deﬁned on the same probability space
(Ω, F, P). Let

Y (t), t ∈R

be the SP corresponding to their sum. Express Y as a
mapping from Ω × R to R. What is Y (ω, t) for (ω, t) ∈Ω × R?
Exercise 12.9 (Independent Stochastic Processes). Let the SP

X1(t), t ∈R

be de-
ﬁned on the probability space (Ω1, F1, P1), and let

X2(t), t ∈R

be deﬁned on the
space (Ω2, F2, P2). Deﬁne a new probability space (Ω, F, P) with two stochastic processes
 ˜
X1(t), t ∈R

and
 ˜
X2(t), t ∈R

such that for every η ∈N and epochs t1, . . . , tη ∈R
the following three conditions hold:
1) The joint law of ˜
X1(t1), . . . , ˜
X1(tη) is the same as the joint law of X1(t1), . . . , X1(tη).
2) The joint law of ˜
X2(t1), . . . , ˜
X2(tη) is the same as the joint law of X2(t1), . . . , X2(tη).
3) The η-tuple ˜
X1(t1), . . . , ˜
X1(tη) is independent of the η-tuple ˜
X2(t1), . . . , ˜
X2(tη).
Hint: Consider Ω = Ω1 × Ω2.
Exercise 12.10 (Pathwise Integration). Let

Xj, j ∈Z

be IID random variables deﬁned
over the probability space (Ω, F, P), with Xj taking on the values 0 and 1 equiprobably.
Deﬁne the stochastic process

X(t), t ∈R

as
X(t) =
∞

j=−∞
Xj I{j ≤t < j + 1},
t ∈R.
For a given n ∈N, compute the distribution of the random variable
ω →
 n
0
X(ω, t) dt.
Chapter 13
Stationary Discrete-Time Stochastic
Processes
13.1
Introduction
This chapter discusses some of the properties of real discrete-time stochastic pro-
cesses. Extensions to complex discrete-time stochastic processes are discussed in
Chapter 17.
13.2
Stationary Processes
A discrete-time stochastic process is said to be stationary if all equal-length tuples
of consecutive samples have the same joint law. Thus:
Deﬁnition 13.2.1 (Stationary Discrete-Time Processes). A discrete-time SP

Xν

is said to be stationary or strict sense stationary or strongly stationary
if for every n ∈N and all integers η, η′ the joint distribution of the n-tuple
(Xη, . . . Xη+n−1) is identical to that of the n-tuple (Xη′, . . . , Xη′+n−1):

Xη, . . . Xη+n−1
 L=

Xη′, . . . Xη′+n−1

.
(13.1)
Here
L= denotes equality of distribution (law) so X
L= Y indicates that the random
variables X and Y have the same distribution; (X, Y )
L= (W, Z) indicates that the
pair (X, Y ) and the pair (W, Z) have the same joint distribution; and similarly for
n-tuples.
By considering the case where n = 1 we obtain that if

Xν

is stationary, then the
distribution of Xη is the same as the distribution of Xη′, for all η, η′ ∈Z. That
is, if

Xν

is stationary, then all the random variables in the family

Xν, ν ∈Z

have the same distribution: the random variable X1 has the same distribution as
the random variable X2, etc. Thus,

Xν, ν ∈Z

stationary

=⇒

Xν
L= X1, ν ∈Z

.
(13.2)
By considering in the above deﬁnition the case where n = 2 we obtain that for a
stationary process

Xν

the joint distribution of X1, X2 is the same as the joint
219
220
Stationary Discrete-Time Stochastic Processes
distribution of Xη, Xη+1 for any integer η. More, however, is true. If

Xν

is
stationary, then the joint distribution of Xν, Xν′ is the same as the joint distribution
of Xη+ν, Xη+ν′:

Xν, ν ∈Z

stationary

=⇒

(Xν, Xν′)
L= (Xη+ν, Xη+ν′), ν, ν′, η ∈Z

. (13.3)
To prove (13.3) ﬁrst note that it suﬃces to treat the case where ν ≥ν′ because
(X, Y )
L= (W, Z) if, and only if, (Y, X)
L= (Z, W). Next note that stationarity
implies that

Xν′, . . . , Xν
 L=

Xη+ν′, . . . , Xη+ν

(13.4)
because both are (ν −ν′ + 1)-length tuples of consecutive samples of the process.
Finally, (13.4) implies that the joint distribution of (Xν′, Xν) is identical to the
joint distribution of (Xη+ν′, Xη+ν) and (13.3) follows.
The above argument can be generalized to more samples. This yields the following
proposition, which gives an alternative deﬁnition of stationarity, a deﬁnition that
more easily generalizes to continuous-time stochastic processes.
Proposition 13.2.2. A discrete-time SP

Xν, ν ∈Z

is stationary if, and only if,
for every n ∈N, all integers ν1, . . . , νn ∈Z, and every η ∈Z

Xν1, . . . , Xνn
 L=

Xη+ν1, . . . , Xη+νn

.
(13.5)
Proof. One direction is trivial and simply follows by substituting consecutive in-
tegers for ν1, . . . , νn in (13.5). The proof of the other direction is a straightforward
extension of the argument we used to prove (13.3).
By noting that (W1, . . . , Wn)
L= (Z1, . . . , Zn) if, and only if,1 
j αjWj
L= 
j αjZj
for all α1, . . . , αn ∈R we obtain the following equivalent characterization of sta-
tionary processes:
Proposition 13.2.3. A discrete-time SP

Xν

is stationary if, and only if, for every
n ∈N, all η, ν1, . . . , νn ∈Z, and all α1, . . . , αn ∈R
n

j=1
αjXνj
L=
n

j=1
αjXνj+η.
(13.6)
13.3
Wide-Sense Stationary Stochastic Processes
Deﬁnition 13.3.1 (Wide-Sense Stationary Discrete-Time SP). We say that a
discrete-time SP

Xν, ν ∈Z

is wide-sense stationary (WSS) or weakly
stationary or covariance stationary or second-order stationary or weak-
sense stationary if the following three conditions are satisﬁed:
1This follows because the multivariate characteristic function determines the joint distribution
(see Proposition 23.4.4 or (Dudley, 2003, Chapter 9, Section 5, Theorem 9.5.1)) and because
the characteristic functions of all the linear combinations of the components of a random vector
determine the multivariate characteristic function of the random vector (Feller, 1971, Chapter XV,
Section 7).
13.4 Stationarity and Wide-Sense Stationarity
221
1) The random variables Xν, ν ∈Z, are all of ﬁnite variance:
Var[Xν] < ∞,
ν ∈Z.
(13.7a)
2) The random variables Xν, ν ∈Z, have identical means:
E[Xν] = E[X1] ,
ν ∈Z.
(13.7b)
3) The quantity E[Xν′Xν] depends on ν′ and ν only via ν −ν′:
E[Xν′Xν] = E[Xη+ν′Xη+ν] ,
ν, ν′, η ∈Z.
(13.7c)
Note 13.3.2. By considering (13.7c) when ν = ν′ we obtain that all the samples
of a WSS SP have identical second moments. And since, by (13.7b), they also all
have identical means, it follows that all the samples of a WSS SP have identical
variances:

Xν, ν ∈Z

WSS

=⇒

Var[Xν] = Var[X1] ,
ν ∈Z

.
(13.8)
An alternative deﬁnition of a WSS process in terms of the variance of linear func-
tionals of the process is given below.
Proposition 13.3.3. A ﬁnite-variance discrete-time SP

Xν

is WSS if, and only
if, for every n ∈N, every η, ν1, . . . , νn ∈Z, and every α1, . . . , αn ∈R
n

j=1
αjXνj and
n

j=1
αjXνj+η
have the same mean & variance.
(13.9)
Proof. The proof is left as an exercise. Alternatively, see the proof of Proposi-
tion 17.5.5.
13.4
Stationarity and Wide-Sense Stationarity
Comparing (13.9) with (13.6) we see that, for ﬁnite-variance stochastic processes,
stationarity implies wide-sense stationarity, which is the content of the following
proposition. This explains why stationary processes are sometimes called strong-
sense stationary and why wide-sense stationary processes are sometimes called
weak-sense stationary.
Proposition 13.4.1 (Finite-Variance Stationary Stochastic Processes Are WSS).
Every ﬁnite-variance discrete-time stationary SP is WSS.
Proof. While this is obvious from (13.9) and (13.6) we shall nevertheless give an
alternative proof because the proof of Proposition 13.3.3 was left as an exercise. The
proof is straightforward and follows directly from (13.2) and (13.3) by noting that if
X
L= Y , then E[X] = E[Y ] and that if (X, Y )
L= (W, Z), then E[XY ] = E[WZ].
222
Stationary Discrete-Time Stochastic Processes
It is not surprising that not every WSS process is stationary. Indeed, the deﬁnition
of WSS processes only involves means and covariances, so it cannot possibly say
everything regarding the distribution. For example, the process whose samples
are independent with the odd ones taking on the value ±1 equiprobably and with
the even ones uniformly distributed over the interval [−
√
3, +
√
3] is WSS but not
stationary.
13.5
The Autocovariance Function
Deﬁnition 13.5.1 (Autocovariance Function). The autocovariance function
KXX : Z →R of a WSS discrete-time SP

Xν

is deﬁned by
KXX(η) ≜Cov[Xν+η, Xν] ,
η ∈Z.
(13.10)
Thus, the autocovariance function at η is the covariance between two samples of
the process taken η units of time apart. Note that because

Xν

is WSS, the RHS
of (13.10) does not depend on ν. Also, for WSS processes all samples are of equal
mean (13.7b), so
KXX(η) = Cov[Xν+η, Xν]
= E[Xν+ηXν] −E[Xν+η] E[Xν]
= E[Xν+ηXν] −

E[X1]
2,
η ∈Z.
In some engineering texts the autocovariance function is called “autocorrelation
function.” We prefer the former because KXX(η) does not measure the correlation
coeﬃcient between Xν and Xν+η but rather the covariance. These concepts are
diﬀerent also for zero-mean processes. Following (Grimmett and Stirzaker, 2001)
we deﬁne the autocorrelation function of a WSS process of nonzero variance as
ρXX(η) ≜Cov[Xν+η, Xν]
Var[X1]
,
η ∈Z,
(13.11)
i.e., as the correlation coeﬃcient between Xν+η and Xν. (Recall that for a WSS
process all samples are of the same variance (13.8), so for such a process the
denominator in (13.11) is equal to

Var[Xν] Var[Xν+η].)
Not every function from the integers to the reals is the autocovariance function of
some WSS SP. For example, the autocovariance function must be symmetric in the
sense that
KXX(−η) = KXX(η),
η ∈Z,
(13.12)
because, by (13.10),
KXX(η) = Cov[Xν+η, Xν]
= Cov[X˜ν, X˜ν−η]
= Cov[X˜ν−η, X˜ν]
= KXX(−η),
η ∈Z,
13.5 The Autocovariance Function
223
where in the second equality we deﬁned ˜ν ≜ν + η, and where in the third equal-
ity we used the fact that for real random variables the covariance is symmetric:
Cov[X, Y ] = Cov[Y, X].
Another property that the autocovariance function must satisfy is that for every
positive integer n
n

ν=1
n

ν′=1
αναν′ KXX(ν −ν′) ≥0,
α1, . . . , αn ∈R,
(13.13)
because
n

ν=1
n

ν′=1
αναν′ KXX(ν −ν′) =
n

ν=1
n

ν′=1
αναν′Cov[Xν, Xν′]
= Cov

n

ν=1
ανXν,
n

ν′=1
αν′Xν′

= Var

n

ν=1
ανXν

≥0.
It turns out that (13.12) and (13.13) fully characterize the autocovariance functions
of discrete-time WSS stochastic processes in a sense that is made precise in the
following theorem.
Theorem 13.5.2 (Characterizing Autocovariance Functions).
(i) If KXX is the autocovariance function of some discrete-time WSS SP

Xν

,
then KXX must satisfy (13.12) & (13.13) for every positive integer n.
(ii) If K: Z →R is some function satisfying
K(−η) = K(η),
η ∈Z
(13.14)
and
n

ν=1
n

ν′=1
αναν′K(ν −ν′) ≥0,

n ∈N, α1, . . . , αn ∈R

,
(13.15)
then there exists a discrete-time WSS SP

Xν

whose autocovariance func-
tion KXX is given by KXX(η) = K(η) for all η ∈Z.
Proof. We have already proved Part (i). For a proof of Part (ii) see, for example,
(Doob, 1990, Chapter X, § 3, Theorem 3.1) or (Pourahmadi, 2001, Theorem 5.1 in
Section 5.1 and Section 9.7).2
2For the beneﬁt of readers who have already encountered Gaussian stochastic processes, we
mention here that if K(·) satisﬁes (13.14) & (13.15) then we can even ﬁnd a Gaussian SP whose
autocovariance function is equal to K(·).
224
Stationary Discrete-Time Stochastic Processes
A function K: Z →R satisfying (13.14) & (13.15) is called a positive deﬁnite
function. Such functions have been extensively studied in the literature, and in
Section 13.7 we shall give an alternative characterization of autocovariance func-
tions based on these studies. But ﬁrst we introduce the power spectral density.
13.6
The Power Spectral Density Function
Roughly speaking, the power spectral density (PSD) of a discrete-time WSS
SP (Xν) of autocovariance function KXX is the Discrete-Time Fourier Transform
of the bi-inﬁnite sequence . . . , KXX(−1), KXX(0), KXX(1), . . . (Appendix B). More
precisely, it is an integrable function on the interval [−1/2, 1/2) whose (−η)-th
Fourier Series Coeﬃcient is equal to KXX(η). Such a function does not always
exist. When it does, it is unique in the sense that any two such functions can
only diﬀer on a subset of the interval [−1/2, 1/2) of Lebesgue measure zero. (This
follows because integrable functions on the interval [−1/2, 1/2) that have identical
Fourier Series Coeﬃcients can diﬀer only on a subset of [−1/2, 1/2) of Lebesgue
measure zero; see Theorem A.2.3.) Consequently, we shall speak of “the” PSD but
try to remember that this does not always exist and that, when it does, it is only
unique in this restricted sense.
Deﬁnition 13.6.1 (Power Spectral Density). We say that the discrete-time WSS
SP

Xν

is of power spectral density
SXX if
SXX is an integrable mapping
from the interval [−1/2, 1/2) to the reals such that
KXX(η) =
 1/2
−1/2
SXX(θ) ei2πηθ dθ,
η ∈Z.
(13.16)
But see also Note 13.6.5 ahead.
Note 13.6.2. We shall sometimes abuse notation and, rather than say that the
stochastic process

Xν, ν ∈Z

is of PSD SXX, we shall say that the autocovariance
function KXX is of PSD SXX.
By considering the special case of η = 0 in (13.16) we obtain that
Var[Xν] = KXX(0)
=
 1/2
−1/2
SXX(θ) dθ,
ν ∈Z.
(13.17)
The main result of the following proposition is that power spectral densities are
nonnegative (except possibly on a set of Lebesgue measure zero).
Proposition 13.6.3 (PSDs Are Nonnegative and Symmetric).
(i) If the WSS SP

Xν, ν ∈Z

of autocovariance KXX is of PSD SXX, then,
except on a subset of (−1/2, 1/2) of Lebesgue measure zero,
SXX(θ) ≥0
(13.18)
and
SXX(θ) = SXX(−θ).
(13.19)
13.6 The Power Spectral Density Function
225
(ii) If the function S: [−1/2, 1/2) →R is integrable, nonnegative, and symmetric
(in the sense that S(θ) = S(−θ) for all θ ∈(−1/2, 1/2)), then there exists a
WSS SP

Xν

whose PSD SXX is given by
SXX(θ) = S(θ),
θ ∈[−1/2, 1/2).
Proof. The nonnegativity of the PSD (13.18) will be established later in the more
general setting of complex stochastic processes (Proposition 17.5.7 ahead). Here we
only prove the symmetry (13.19) and establish the second half of the proposition.
That (13.19) holds (except on a set of Lebesgue measure zero) follows because KXX
is symmetric. Indeed, for any η ∈Z we have
 1/2
−1/2

SXX(θ) −SXX(−θ)

ei2πηθ dθ
=
 1/2
−1/2
SXX(θ) ei2πηθ dθ −
 1/2
−1/2
SXX(−θ) ei2πηθ dθ
= KXX(η) −
 1/2
−1/2
SXX(˜θ) ei2π(−η)˜θ d˜θ
= KXX(η) −KXX(−η)
= 0,
η ∈Z,
(13.20)
so all the Fourier Series Coeﬃcients of the function θ 	→SXX(θ)−SXX(−θ) are zero,
thus establishing that this function is zero except on a set of Lebesgue measure
zero (Theorem A.2.3).
We next prove that if the function S: [−1/2, 1/2) →R is symmetric, nonnegative,
and integrable, then it is the PSD of some WSS real SP. We cheat a bit because
our proof relies on Theorem 13.5.2, which we never proved. From Theorem 13.5.2
it follows that it suﬃces to establish that the sequence K: Z →R deﬁned by
K(η) =
 1/2
−1/2
S(θ) ei2πηθ dθ,
η ∈Z
(13.21)
satisﬁes (13.14) & (13.15).
Verifying (13.14) is straightforward: by hypothesis, S(·) is symmetric so
K(−η) =
 1/2
−1/2
S(θ) ei2π(−η)θ dθ
=
 1/2
−1/2
S(−˜θ) ei2πη˜θ d˜θ
=
 1/2
−1/2
S(˜θ) ei2πη˜θ d˜θ
= K(η),
η ∈Z,
where the ﬁrst equality follows from (13.21); the second from the change of variable
˜θ ≜−θ; the third from the symmetry of S(·), which implies that S(−˜θ) = S(˜θ);
and the last equality again from (13.21).
226
Stationary Discrete-Time Stochastic Processes
We next verify (13.15). To this end we ﬁx arbitrary α1, . . . , αn ∈R and compute
n

ν=1
n

ν′=1
αν αν′ K(ν −ν′) =
n

ν=1
n

ν′=1
αν αν′
 1/2
−1/2
S(θ) ei2π(ν−ν′)θ dθ
=
 1/2
−1/2
S(θ)
	
n

ν=1
n

ν′=1
αν αν′ ei2π(ν−ν′)θ

dθ
=
 1/2
−1/2
S(θ)
	
n

ν=1
n

ν′=1
αν ei2πνθ αν′ e−i2πν′θ

dθ
=
 1/2
−1/2
S(θ)
	
n

ν=1
αν ei2πνθ

	
n

ν′=1
αν′ ei2πν′θ

∗
dθ
=
 1/2
−1/2
S(θ)

n

ν=1
αν ei2πνθ

2
dθ
≥0,
(13.22)
where the ﬁrst equality follows from (13.21); the subsequent equalities by simple
algebraic manipulation; and the ﬁnal inequality from the nonnegativity of S(·).
Corollary 13.6.4. If a discrete-time WSS SP

Xν

has a PSD, then it also has a
PSD SXX for which (13.18) holds for every θ ∈[−1/2, 1/2) and for which (13.19)
holds for every θ ∈(−1/2, 1/2) (and not only outside a subset of Lebesgue measure
zero).
Proof. Suppose that

Xν

is of PSD SXX. Deﬁne the mapping S: [−1/2, 1/2) →R
by3
S(θ) =

1
2

|SXX(θ)| + |SXX(−θ)|

if θ ∈(−1/2, 1/2)
1
if θ = −1/2.
(13.23)
By the proposition, SXX(·) and S(·) diﬀer only on a set of Lebesgue measure zero,
so they must have identical Fourier Series Coeﬃcients. Since the Fourier Series
Coeﬃcients of SXX agree with KXX, it follows that so must those of S(·). Thus, S(·)
is a PSD for

Xν

, and it is by (13.23) nonnegative on [−1/2, 1/2) and symmetric
on (−1/2, 1/2).
Note 13.6.5. In view of Corollary 13.6.4 we shall only say that

Xν

is of PSD SXX
if the function SXX—in addition to being integrable and to satisfying (13.16)—is
also nonnegative and symmetric.
As we have noted, not every WSS SP has a PSD. For example, the SP
Xν = X,
ν ∈Z,
where X is some zero-mean unit-variance RV has the all-one autocovariance func-
tion KXX(η) = 1, η ∈Z, and this all-one sequence cannot be the Fourier Series
Coeﬃcients sequence of an integrable function because, by the Riemann-Lebesgue
3Our choice of S(−1/2) as 1 is arbitrary; any nonnegative value would do.
13.6 The Power Spectral Density Function
227
lemma (Theorem A.2.4), the Fourier Series Coeﬃcients of an integrable function
must converge to zero.4
In general, it is very diﬃcult to characterize the autocovariance functions having
a PSD. We know by the Riemann-Lebesgue lemma that such autocovariance func-
tions must tend to zero, but this necessary condition is not suﬃcient. A very useful
suﬃcient (but not necessary) condition is the following:
Proposition 13.6.6 (PSD when KXX Is Absolutely Summable). If the autoco-
variance function KXX is absolutely summable, i.e.,
∞

η=−∞
KXX(η)
 < ∞,
(13.24)
then its Discrete-Time Fourier Transform
S(θ) =
∞

η=−∞
KXX(η) e−i2πηθ,
θ ∈[−1/2, 1/2]
(13.25)
is continuous, symmetric, nonnegative, and satisﬁes
 1/2
−1/2
S(θ) ei2πηθ dθ = KXX(η),
η ∈Z.
(13.26)
Consequently, S(·) is a PSD for KXX.
Proof. First note that because |KXX(η) e−i2πθη| = |KXX(η)|, it follows that (13.24)
guarantees that the sum in (13.25) converges uniformly and absolutely. And since
each term in the sum is a continuous function, the uniform convergence of the
sum guarantees that S(·) is continuous (Rudin, 1976, Chapter 7, Theorem 7.12).
Consequently,
 1/2
−1/2
|S(θ)| dθ < ∞,
(13.27)
and it is meaningful to discuss the Fourier Series Coeﬃcients of S(·).
We next prove that the Fourier Series Coeﬃcients of S(·) are equal to KXX, i.e.,
that (13.26) holds. This can be shown by swapping integration and summation
and using the orthonormality property
 1/2
−1/2
ei2π(η−η′)θ dθ = I{η = η′},
η, η′ ∈Z
(13.28)
4One could say that the PSD of this process is Dirac’s Delta, but we shall refrain from doing
so because we do not use Dirac’s Delta in this book and because there is not much to be gained
from this. (There exist processes that do not have a PSD even if one allows for Dirac’s Deltas.)
228
Stationary Discrete-Time Stochastic Processes
as follows:
 1/2
−1/2
S(θ) ei2πηθ dθ =
 1/2
−1/2
	
∞

η′=−∞
KXX(η′) e−i2πη′θ

ei2πηθ dθ
=
∞

η′=−∞
KXX(η′)
 1/2
−1/2
e−i2πη′θ ei2πηθ dθ
=
∞

η′=−∞
KXX(η′)
 1/2
−1/2
ei2π(η−η′)θ dθ
=
∞

η′=−∞
KXX(η′) I{η = η′}
= KXX(η),
η ∈Z.
It remains to show that S(·) is symmetric, i.e., that S(θ) = S(−θ), and that it is
nonnegative. The symmetry of S(·) follows directly from its deﬁnition (13.25) and
from the fact that KXX, like every autocovariance function, is symmetric (Theo-
rem 13.5.2 (i)).
We next prove that S(·) is nonnegative.
From (13.26) it follows that S(·) can
only be negative on a subset of the interval [−1/2, 1/2) of Lebesgue measure zero
(Proposition 13.6.3 (i)).
And since S(·) is continuous, this implies that S(·) is
nonnegative.
13.7
The Spectral Distribution Function
We next brieﬂy discuss the case where (Xν) does not necessarily have a power
spectral density function. We shall see that in this case too we can express the
autocovariance function as the Fourier Series of “something,” but this “something”
is not an integrable function. (It is, in fact, a measure.) The theorem will also
yield a characterization of positive deﬁnite functions. The proof, which is based on
Herglotz’s Theorem (Feller, 1971, Chapter XIX, Section 6, Theorem 2), (Pourah-
madi, 2001, Theorem 9.22), is omitted. The results of this section will not be used
in subsequent chapters.
Recall that a random variable taking values in the interval [−α, α] is said to be
symmetric (or to have a symmetric distribution) if Pr[X ≤−ξ] = Pr[X ≥ξ] for
all ξ ∈[−α, α].
Theorem 13.7.1. A function ρ: Z →R is the autocorrelation function of a real
WSS SP if, and only if, there exists a symmetric random variable Θ taking values
in the interval [−1/2, 1/2] such that
ρ(η) = E

ei2πηΘ
,
η ∈Z.
(13.29)
The cumulative distribution function of Θ is fully determined by ρ.
Proof. See (Doob, 1990, Chapter X, § 3, Theorem 3.2), (Pourahmadi, 2001, The-
orem 9.22), (Shiryaev, 1996, Chapter VI, § 1.1), or (Porat, 2008, Section 2.8).
13.8 Exercises
229
This theorem also characterizes autocovariance functions: a function K: Z →R
is the autocovariance function of a real WSS SP if, and only if, there exists a
symmetric random variable Θ taking values in the interval [−1/2, 1/2] and some
constant α ≥0 such that
K(η) = α E

ei2πηΘ
,
η ∈Z.
(13.30)
(By equating (13.30) at η = 0 we obtain that α = K(0), i.e., the variance of the
stochastic process.)
Equivalently, we can state the theorem as follows. If

Xν

is a real WSS SP, then
its autocovariance function KXX can be expressed as
KXX(η) = Var[X1] E

ei2πηΘ
,
η ∈Z
(13.31)
for some random variable Θ taking values in the interval [−1/2, 1/2] according to
some symmetric distribution. If, additionally, Var[X1] > 0, then the cumulative
distribution function FΘ(·) of Θ is uniquely determined by KXX.
Note 13.7.2.
(i) If the random variable Θ above has a symmetric density fΘ(·), then the
process is of PSD θ 	→Var[X1] fΘ(θ). Indeed, by (13.31) we have for every
integer η
KXX(η) = Var[X1] E

ei2πηΘ
= Var[X1]
 1/2
−1/2
fΘ(θ) ei2πηθ dθ
=
 1/2
−1/2

Var[X1] fΘ(θ)

ei2πηθ dθ.
(ii) Some authors, e.g., (Grimmett and Stirzaker, 2001) refer to the cumulative
distribution function FΘ(·) of Θ, i.e., to the mapping θ 	→Pr[Θ ≤θ], as
the Spectral Distribution Function of

Xν

. This, however, is not stan-
dard. It is only in agreement with the more common usage in the case where
Var[X1] = 1.5
13.8
Exercises
Exercise 13.1 (Discrete-Time WSS Stochastic Processes). Prove Proposition 13.3.3.
Exercise 13.2 (Empirical Averages). Let

Xν

be a WSS discrete-time SP of autocovari-
ance function KXX.
5The more common deﬁnition is that θ →Var[X1] Pr[Θ ≤θ] is the spectral measure or
spectral distribution function. But this is not a distribution function in the probabilistic sense
because its value at θ = ∞is Var[X1] which may be diﬀerent from one.
230
Stationary Discrete-Time Stochastic Processes
(i) Prove that
Var
' 1
n
n

ν=1
Xν
(
= 1
n KXX(0) + 1
n
n

m=1
2
	
1 −m
n

KXX(m).
(ii) Prove that
)
lim
η→∞KXX(η) = 0
*
=⇒
)
lim
n→∞Var
' 1
n
n

ν=1
Xν
(
= 0
*
.
Hint: For Part (ii) recall that if a sequence (aj) converges to a as j tends to inﬁnity, then
n−1 n
k=1 ak tends to a as n tends to inﬁnity (Rudin, 1976, Ch. 3 Exercise 14).
Exercise 13.3 (Mapping a Discrete-Time Stationary SP). Let

Xν

be a stationary
discrete-time SP, and let g: R →R be some arbitrary (Borel measurable) function. For
every ν ∈Z, let Yν = g(Xν). Prove that the discrete-time SP

Yν

is stationary.
Exercise 13.4 (Mapping a Discrete-Time WSS SP). Let

Xν

be a WSS discrete-time
SP, and let g: R →R be some arbitrary (Borel measurable) bounded function. For every
ν ∈Z, let Yν = g(Xν). Must the SP

Yν

be WSS?
Exercise 13.5 (A Sliding-Window Mapping of a Stationary SP). Let

Xν

be a stationary
discrete-time SP, and let g: R2 →R be some arbitrary (Borel measurable) function. For
every ν ∈Z deﬁne Yν = g(Xν−1, Xν). Must

Yν

be stationary?
Exercise 13.6 (A Sliding-Window Mapping of a WSS SP). Let

Xν

be a WSS discrete-
time SP, and let g: R2 →R be some arbitrary bounded (Borel measurable) function. For
every ν ∈Z deﬁne Yν = g(Xν−1, Xν). Must

Yν

be WSS?
Exercise 13.7 (Existence of a SP). For which values of α, β ∈R is the function
KXX(m) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
if m = 0,
α
if m = 1,
β
if m = −1,
0
otherwise,
m ∈Z
the autocovariance function of some WSS SP

Xν, ν ∈Z

?
Exercise 13.8 (Dilating a Stationary SP). Let

Xν

be a stationary discrete-time SP, and
deﬁne Yν = X2ν for every ν ∈Z. Must

Yν

be stationary?
Exercise 13.9 (Inserting Zeros Periodically). Let

Xν

be a stationary discrete-time SP,
and let the RV U be independent of it and take on the values 0 and 1 equiprobably. Deﬁne
for every ν ∈Z
Yν =

0
if ν is odd,
Xν/2
if ν is even
and
Zν = Yν+U.
(13.32)
Under what conditions is

Yν

stationary? Under what conditions is

Zν

stationary?
13.8 Exercises
231
Exercise 13.10 (The Autocovariance Function of a Dilated WSS SP). Let

Xν

be a
WSS discrete-time SP of autocovariance function KXX. Deﬁne Yν = X2ν for every ν ∈Z.
Must

Yν

be WSS? If so, express its autocovariance function KYY in terms of KXX.
Exercise 13.11 (Inserting Zeros Periodically: the Autocovariance Function). Let

Xν

be
a WSS discrete-time SP of autocovariance function KXX, and let the RV U be independent
of it and take on the values 0 and 1 equiprobably. Deﬁne

Zν

as in (13.32). Must

Zν

be WSS? If yes, express its autocovariance function in terms of KXX.
Exercise 13.12 (Stationary But Not WSS). Construct a discrete-time stationary SP that
is not WSS.
Exercise 13.13 (Complex Coeﬃcients). Show that (13.13) will hold for complex numbers
α1, . . . , αn provided that we replace the product αν αν′ with αν α∗
ν′. That is, show that
if KXX is the autocovariance function of a real discrete-time WSS SP, then
n

ν=1
n

ν′=1
αν α∗
ν′ KXX(ν −ν′) ≥0,
α1, . . . , αn ∈C.
Exercise 13.14 (Existence of a Power Spectral Density). Does there exist a WSS SP

Xν, ν ∈Z

whose autocovariance function KXX is given by KXX(η) = (−1)η for all
η ∈Z? If yes, does it have a power spectral density?
Hint: Recall the Riemann-Lebesgue Lemma (Theorem A.2.4).
Chapter 14
Energy and Power in PAM
14.1
Introduction
Energy is an important resource in Digital Communications. The rate at which
it is transmitted—the “transmit power”—is critical in battery-operated devices.
In satellite applications it is a major consideration in determining the size of the
required solar panels, and in wireless systems it inﬂuences the interference that one
system causes to another. In this chapter we shall discuss the power in PAM signals.
To deﬁne power we shall need some modeling trickery which will allow us to pretend
that the system has been operating since “time −∞” and that it will continue
to operate indeﬁnitely.
Our deﬁnitions and derivations will be mathematically
somewhat informal. A more formal account for readers with background in Measure
Theory is provided in Section 14.6.
Before discussing power we begin with a discussion of the expected energy in trans-
mitting a ﬁnite number of bits.
14.2
Energy in PAM
We begin with a seemingly completely artiﬁcial problem. Suppose that K inde-
pendent data bits D1, . . . , DK, each taking on the values 0 and 1 equiprobably,
are mapped by a mapping enc: {0, 1}K →RN to an N-tuple of real numbers
(X1, . . . , XN), where Xℓis the ℓ-th component of the N-tuple enc

D1, . . . , DK

.
Suppose further that the symbols X1, . . . , XN are then mapped to the waveform
X(t) = A
N

ℓ=1
Xℓg(t −ℓTs),
t ∈R,
(14.1)
where g ∈L2 is an energy-limited real pulse shape, A ≥0 is a scaling factor, and
Ts > 0 is the baud period. We seek the expected energy in the waveform X(·).
We assume that X(·) corresponds to the voltage across a unit-load or to the current
through a unit-load, so the transmitted energy is the time integral of the mapping
t 	→X2(t).
Because the data bits are random variables, the signal X(·) is a
232
14.2 Energy in PAM
233
stochastic process. Its energy
 ∞
−∞X2(t) dt is thus a random variable.1 If (Ω, F, P)
is the probability space under consideration, then this RV is the mapping from Ω
to R deﬁned by
ω 	→
 ∞
−∞
X2(ω, t) dt.
This RV’s expectation—the expected energy—is denoted by E and is given by
E ≜E
 ∞
−∞
X2(t) dt

.
(14.2)
Note that even though we are considering the transmission of a ﬁnite number of
symbols (N), the waveform X(·) may extend in time from −∞to +∞.
We next derive an explicit expression for E. Starting from (14.2) and using (14.1),
E = E
 ∞
−∞
X2(t) dt

= A2E
5 ∞
−∞
	 N

ℓ=1
Xℓg(t −ℓTs)

2
dt
6
= A2E
5 ∞
−∞
	 N

ℓ=1
Xℓg(t −ℓTs)

	
N

ℓ′=1
Xℓ′ g(t −ℓ′Ts)

dt
6
= A2E
5 ∞
−∞
N

ℓ=1
N

ℓ′=1
XℓXℓ′ g(t −ℓTs) g(t −ℓ′Ts) dt
6
= A2
 ∞
−∞
N

ℓ=1
N

ℓ′=1
E[XℓXℓ′] g(t −ℓTs) g(t −ℓ′Ts) dt
= A2
N

ℓ=1
N

ℓ′=1
E[XℓXℓ′]
 ∞
−∞
g(t −ℓTs) g(t −ℓ′Ts) dt
= A2
N

ℓ=1
N

ℓ′=1
E[XℓXℓ′] Rgg

(ℓ−ℓ′)Ts

,
(14.3)
where Rgg is the self-similarity function of the pulse g(·) (Section 11.2). Here the
ﬁrst equality follows from (14.2); the second from (14.1); the third by writing the
square of a number as its product with itself (ξ2 = ξξ); the fourth by writing the
product of sums as the double sum of products; the ﬁfth by swapping expectation
with integration and by the linearity of expectation; the sixth by swapping integra-
tion and summation; and the ﬁnal equality by the deﬁnition of the self-similarity
function (Deﬁnition 11.2.1).
Using Proposition 11.2.2 (iv) we can also express Rgg as
Rgg(τ) =
 ∞
−∞
ˆg(f)
2 ei2πfτ df,
τ ∈R
(14.4)
1There are some slight measure-theoretic mathematical technicalities that we are sweeping
under the rug. Those are resolved in Section 14.6.
234
Energy and Power in PAM
and hence rewrite (14.3) as
E = A2
 ∞
−∞
N

ℓ=1
N

ℓ′=1
E[XℓXℓ′] ei2πf(ℓ−ℓ′)Tsˆg(f)
2 df.
(14.5)
We deﬁne the energy per bit as
Eb
%energy
bit
&
≜E
K
(14.6)
and the energy per real symbol as
Es

energy
real symbol

≜E
N.
(14.7)
As we shall see in Section 14.5.2, if inﬁnite data are transmitted using the binary-
to-reals (K, N) block encoder enc(·), then the resulting transmitted power P is given
by
P = Es
Ts
.
(14.8)
This result will be proved in Section 14.5.2 after we carefully deﬁne the average
power. The units work out because if we think of Ts as having units of seconds per
real symbol then:
Es
%
energy
real symbol
&
Ts
%
second
real symbol
& = Es
Ts
%energy
second
&
.
(14.9)
Expression (14.3) for the expected energy E is greatly simpliﬁed in two cases that
we discuss next. The ﬁrst is when the pulse shape g satisﬁes the orthogonality
condition
 ∞
−∞
g(t) g(t −κTs) dt = ∥g∥2
2 I{κ = 0},
κ ∈{0, 1, . . . , N −1}.
(14.10)
In this case (14.3) simpliﬁes to
E = A2 ∥g∥2
2
N

ℓ=1
E

X2
ℓ

,

t 	→g(t −ℓTs)
N−1
ℓ=0 orthogonal

.
(14.11)
(In this case one need not even go through the calculation leading to (14.3); the
result simply follows from (14.1) and the Pythagorean Theorem (Theorem 4.5.2).)
The second case for which the computation of E is simpliﬁed is when the distribu-
tion of D1, . . . , DK and the mapping enc(·) result in the real symbols X1, . . . , XN
being of zero mean and uncorrelated:2
E[Xℓ] = 0,
ℓ∈{1, . . . , N}
(14.12a)
2Actually, it suﬃces that (14.12b) hold; (14.12a) is not needed.
14.3 Deﬁning the Power in PAM
235
and
E[XℓXℓ′] = E

X2
ℓ

I{ℓ= ℓ′},
ℓ, ℓ′ ∈{1, . . . , N}.
(14.12b)
In this case too (14.3) simpliﬁes to
E = A2 ∥g∥2
2
N

ℓ=1
E

X2
ℓ

,

Xℓ, ℓ∈Z

zero-mean & uncorrelated

.
(14.13)
14.3
Deﬁning the Power in PAM
If

X(t), t ∈R

is a continuous-time stochastic process describing the voltage
across a unit-load or the current through a unit-load, then it is reasonable to
deﬁne the power P in

X(t), t ∈R

as the limit
P ≜lim
T→∞
1
2T E
 T
−T
X2(t) dt

.
(14.14)
But there is a problem. Over its lifetime, a communication system is only used
to transmit a ﬁnite number of bits, and it only sends a ﬁnite amount of energy.
Consequently, if

X(t), t ∈R

corresponds to the transmitted waveform over the
system’s lifetime, then P as deﬁned in (14.14) will always end up being zero. The
deﬁnition in (14.14) is thus useless when discussing the transmission of a ﬁnite
number of bits.
To deﬁne power in a useful way we need some modeling trickery. Instead of thinking
about the encoder as producing a ﬁnite number of symbols, we should now pretend
that the encoder produces an inﬁnite sequence of symbols

Xℓ, ℓ∈Z

, which are
then mapped to the inﬁnite sum
X(t) = A
∞

ℓ=−∞
Xℓg(t −ℓTs),
t ∈R.
(14.15)
For the waveform in (14.15), the deﬁnition of P in (14.14) makes perfect sense.
Philosophically speaking, the modeling trickery we employ corresponds to mea-
suring power on a time scale much greater than the signaling period Ts but much
shorter than the system’s lifetime.
But philosophy aside, there are still two problems we must address: how to model
the generation of the inﬁnite sequence

Xℓ, ℓ∈Z

, and how to guarantee that
the sum in (14.15) converges for every t ∈R. We begin with the latter. If g is of
ﬁnite duration, then at every epoch t ∈R only a ﬁnite number of terms in (14.15)
are nonzero and convergence is thus guaranteed. But we do not want to restrict
ourselves to ﬁnite-duration pulse shapes because those, by Theorem 6.8.2, cannot
be bandlimited. Instead, to guarantee convergence, we shall assume throughout
that the following conditions both hold:
1) The symbols

Xℓ, ℓ∈Z

are uniformly bounded in the sense that there
exists some constant γ such that
|Xℓ| ≤γ,
ℓ∈Z.
(14.16)
236
Energy and Power in PAM
D−K+1,
. . .
, D0,
enc(·)
, X−N+1,
. . .
, X0,
enc(D−K+1, . . . , D0)
D1, . . .
, DK,
enc(·)
X1,
. . .
, XN,
enc(D1, . . . , DK)
DK+1,
· · · , D2K
enc(·)
XN+1,
· · · , X2N,
enc(DK+1, . . . , D2K)
Figure 14.1: Bi-Inﬁnite Block Encoding.
2) The pulse shape t 	→g(t) decays faster than 1/t in the sense that there exist
positive constants α, β > 0 such that
|g(t)| ≤
β
1 + |t/Ts|1+α ,
t ∈R.
(14.17)
Using the fact that the sum 
n≥1 n−(1+α) converges whenever α > 0 (Rudin,
1976, Theorem 3.28), it is not diﬃcult to show that if both (14.16) and (14.17)
hold, then the inﬁnite sum (14.15) converges at every epoch t ∈R.
As to the generation of

Xℓ, ℓ∈Z

, we shall consider three scenarios. In the
ﬁrst, which we analyze in Section 14.5.1, we ignore this issue and simply assume
that

Xℓ, ℓ∈Z

is a WSS discrete-time SP of a given autocovariance function.
In the second scenario, which we analyze in Section 14.5.2, we tweak the block-
encoding mode that we introduced in Section 10.4 to account for a bi-inﬁnite data
sequence. We call this tweaked mode bi-inﬁnite block encoding and describe
it more precisely in Section 14.5.2. It is illustrated in Figure 14.1. Finally, the
third scenario, which we analyze in Section 14.5.3, is similar to the ﬁrst except
that we relax some of the statistical assumptions on

Xℓ, ℓ∈Z

. But we only
treat the case where the time shifts of the pulse shape by integer multiples of Ts
are orthonormal.
Except in the third scenario, we shall only analyze the power in the stochastic
process (14.15) assuming that the symbols

Xℓ, ℓ∈Z

are of zero mean
E[Xℓ] = 0,
ℓ∈Z.
(14.18)
This not only simpliﬁes the analysis but also makes engineering sense, because it
guarantees that

X(t), t ∈R

is centered
E[X(t)] = 0,
t ∈R,
(14.19)
and, for the reasons that we outline in Section 14.4, transmitting zero-mean wave-
forms is usually power eﬃcient.
14.4 On the Mean of Transmitted Waveforms
237
+
+
+
TX1
TX2
RX1
RX2
X
X −c
Y = X −c + N
X + N
{Dj}
{Dest
j }
−c
c
N
TX1
RX1
X
Y = X + N
+
N
{Dj}
{Dest
j }
Figure 14.2: The above two systems have identical performance. In the former
the transmitted power is the power in t 	→X(t) whereas in the second it is the
power in t 	→X(t) −c(t).
14.4
On the Mean of Transmitted Waveforms
We next explain why the transmitted waveforms in digital communications are
usually designed to be of zero mean.3 We focus on the case where the transmitted
signal suﬀers only from an additive disturbance. The key observation is that given
any transmitter that transmits the SP

X(t), t ∈R

and any receiver, we can
design a new transmitter that transmits the waveform t 	→X(t) −c(t) and a
new receiver with identical performance.
Here c(·) is any deterministic signal.
Indeed, the new receiver can simply add c(·) to the received signal and then pass
on the result to the old receiver. That the old and the new systems have identical
performance follows by noting that if

N(t), t ∈R

is the added disturbance, then
the received signal on which the old receiver operates is given by t 	→X(t) + N(t).
And the received signal in the new system is t 	→X(t) −c(t) + N(t), so after we
add c(·) to this signal we obtain the signal X(t) + N(t), which is equal the signal
that the old receiver operated on. Thus, the performance of a system transmitting
X(·) can be mimicked on a system transmitting X(·) −c(·) by simply adding c(·)
at the receiver. See Figure 14.2.
The addition at the receiver of c(·) entails no change in the transmitted power.
Therefore, if a system transmits X(·), then we might be able to improve its power
eﬃciency without hurting its performance by cleverly choosing c(·) so that the
power in X(·) −c(·) be smaller than the power in X(·) and by then transmitting
t 	→X(t) −c(t) instead of t 	→X(t). The only additional change we would need
to make is to add c(·) at the receiver.
How should we choose c(·)? To answer this we shall need the following lemma.
3This, however, is not the case with some wireless systems that transmit training sequences
to help the receiver learn the channel and acquire timing information.
238
Energy and Power in PAM
Lemma 14.4.1. If W is a random variable of ﬁnite variance, then
E

(W −c)2
≥Var[W] ,
c ∈R
(14.20)
with equality if, and only if,
c = E[W] .
(14.21)
Proof.
E

(W −c)2
= E
%
(W −E[W]) + (E[W] −c)
2&
= E

(W −E[W])2
+ 2 E

W −E[W]




0

E[W] −c

+

E[W] −c
2
= E

(W −E[W])2
+ (E[W] −c)2
≥E

(W −E[W])2
= Var[W] ,
with equality if, and only if, c = E[W].
With the aid of Lemma 14.4.1 we can now choose c(·) to minimize the power in
t 	→X(t) −c(t) as follows. Keeping the deﬁnition of power (14.14) in mind, we
study
1
2T
 T
−T
E
%
X(t) −c(t)
2&
dt
and note that this expression is minimized over all choices of the waveform c(·) by
minimizing the integrand, i.e., by choosing at every epoch t the value of c(t) to be
the one that minimizes E
%
X(t) −c(t)
2&
. By Lemma 14.4.1 this corresponds to
choosing c(t) to be E[X(t)]. It is thus optimal to choose c(·) as
c(t) = E[X(t)] ,
t ∈R.
(14.22)
This choice results in the transmitted waveform being t 	→X(t) −E[X(t)], i.e., in
the transmitted waveform being of zero mean.
Stated diﬀerently, if in a given system the transmitted waveform is not of zero
mean, then a new system can be built that transmits a waveform of lower (or
equal) average power and whose performance on any additive noise channel is
identical.
14.5
Computing the Power in PAM
We proceed to compute the power in the signal
X(t) = A
∞

ℓ=−∞
Xℓg(t −ℓTs),
t ∈R
(14.23)
under various assumptions on the bi-inﬁnite random sequence

Xℓ, ℓ∈Z

. We
assume throughout that Conditions (14.16) & (14.17) are satisﬁed so the inﬁnite
14.5 Computing the Power in PAM
239
sum in (14.23) converges at every epoch t ∈R.
The power P is deﬁned as in
(14.14).4
14.5.1

Xℓ

Is Zero-Mean and WSS
Here we compute the power in the signal (14.23) when

Xℓ, ℓ∈Z

is a centered
WSS SP of autocovariance function KXX:
E[Xℓ] = 0,
ℓ∈Z,
(14.24a)
E[XℓXℓ+m] = KXX(m) ,
ℓ, m ∈Z.
(14.24b)
We further assume that the pulse shape satisﬁes the decay condition (14.17) and
that the process

Xℓ, ℓ∈Z

satisﬁes the boundedness condition (14.16).
We begin by calculating the expected energy of X(·) in a half-open interval [τ, τ+Ts)
of length Ts and in showing that this expected energy does not depend on τ, i.e.,
that the expected energy in all intervals of length Ts are identical. We calculate
the energy in the interval [τ, τ + Ts) as follows:
E
 τ+Ts
τ
X2(t) dt

= A2
 τ+Ts
τ
E
5	
∞

ℓ=−∞
Xℓg(t −ℓTs)

26
dt
(14.25)
= A2
 τ+Ts
τ
E

∞

ℓ=−∞
∞

ℓ′=−∞
XℓXℓ′ g(t −ℓTs) g(t −ℓ′Ts)

dt
= A2
 τ+Ts
τ
∞

ℓ=−∞
∞

ℓ′=−∞
E[XℓXℓ′] g(t −ℓTs) g(t −ℓ′Ts) dt
= A2
 τ+Ts
τ
∞

ℓ=−∞
∞

m=−∞
E[XℓXℓ+m] g(t −ℓTs) g

t −(ℓ+ m)Ts

dt
= A2
 τ+Ts
τ
∞

m=−∞
KXX(m)
∞

ℓ=−∞
g(t −ℓTs) g

t −(ℓ+ m)Ts

dt
= A2
∞

m=−∞
KXX(m)
∞

ℓ=−∞
 τ+Ts−ℓTs
τ−ℓTs
g(t′) g(t′ −mTs) dt′
(14.26)
= A2
∞

m=−∞
KXX(m)
 ∞
−∞
g(t′) g(t′ −mTs) dt′
= A2
∞

m=−∞
KXX(m) Rgg(mTs),
τ ∈R,
(14.27)
where the ﬁrst equality follows by the structure of X(·) (14.15); the second by
writing X2(t) as X(t) X(t) and rearranging terms; the third by the linearity of the
4A general mathematical deﬁnition of the power of a stochastic process is given in Deﬁni-
tion 14.6.1 ahead.
240
Energy and Power in PAM
expectation, which allows us to swap the double sum and the expectation and to
take the deterministic term g(t−ℓTs)g(t−ℓ′Ts) outside the expectation; the fourth
by deﬁning m ≜ℓ′ −ℓ; the ﬁfth by (14.24b); the sixth by deﬁning t′ ≜t −ℓTs; the
seventh by noting that the integrals of a function over all the intervals of the form
[τ −ℓTs, τ −ℓTs + Ts) where ℓ∈Z sum to the integral over the entire real line; and
the ﬁnal by the deﬁnition of the self-similarity function Rgg (Section 11.2).
Note that, indeed, the RHS of (14.27) does not depend on the epoch τ at which
the length-Ts time interval starts. This observation will now help us to compute
the power in X(·). Since the interval [−T, +T) contains ⌊(2T)/Ts⌋disjoint intervals
of the form [τ, τ + Ts), and since it is contained in the union of ⌈(2T)/Ts⌉such
intervals, it follows that
72T
Ts
8
E
 τ+Ts
τ
X2(t) dt

≤E
 T
−T
X2(t) dt

≤
/2T
Ts
0
E
 τ+Ts
τ
X2(t) dt

, (14.28)
where we use ⌊ξ⌋to denote the greatest integer smaller than or equal to ξ (e.g.,
⌊4.2⌋= 4), and where we use ⌈ξ⌉to denote the smallest integer that is greater than
or equal to ξ (e.g., ⌈4.2⌉= 5) so
ξ −1 < ⌊ξ⌋≤⌈ξ⌉< ξ + 1,
ξ ∈R.
(14.29)
Note that from (14.29) and the Sandwich Theorem it follows that
lim
T→∞
1
2T
72T
Ts
8
= lim
T→∞
1
2T
/2T
Ts
0
= 1
Ts
,
Ts > 0.
(14.30)
Dividing (14.28) by 2T and using (14.30) we obtain that
lim
T→∞
1
2T E
 T
−T
X2(t) dt

= 1
Ts
E
 τ+Ts
τ
X2(t) dt

,
which combines with (14.27) to yield
P = 1
Ts
A2
∞

m=−∞
KXX(m) Rgg(mTs).
(14.31)
The power P can be alternatively expressed in the frequency domain using (14.31)
and (14.4) as
P = A2
Ts
 ∞
−∞
∞

m=−∞
KXX(m) ei2πfmTs |ˆg(f)|2 df.
(14.32)
An important special case of (14.31) is when the symbols

Xℓ

are zero-mean,
uncorrelated, and of equal variance σ2
X. In this case KXX(m) = σ2
X I{m = 0}, and
the only nonzero term in (14.31) is the term corresponding to m = 0 so
P = 1
Ts
A2 ∥g∥2
2 σ2
X,

Xℓ

centered, variance σ2
X, uncorrelated

.
(14.33)
14.5 Computing the Power in PAM
241
14.5.2
Bi-Inﬁnite Block-Mode
The bi-inﬁnite block-mode with a (K, N) binary-to-reals block encoder
enc: {0, 1}K →RN
is depicted in Figure 14.1 and can be described as follows. A bi-inﬁnite sequence
of data bits

Dj, j ∈Z

is fed to an encoder. The encoder parses this sequence
into K-tuples and deﬁnes for every integer ν ∈Z the “ν-th data block” Dν
Dν ≜

DνK+1, . . . , DνK+K

,
ν ∈Z.
(14.34)
Each data block Dν is then mapped by enc(·) to a real N-tuple, which we denote
by Xν:
Xν ≜enc(Dν),
ν ∈Z.
(14.35)
The bi-inﬁnite sequence

Xℓ, ℓ∈Z

produced by the encoder is the concatenation
of these N-tuples so

XνN+1, . . . , XνN+N

= Xν,
ν ∈Z.
(14.36)
Stated diﬀerently, for every ν ∈Z and η ∈{1, . . . , N}, the symbol XνN+η is the
η-th component of the N-tuple Xν. The transmitted signal X(·) is as in (14.15)
with the pulse shape g satisfying the decay condition (14.17) and with Ts > 0 being
arbitrary. (The boundedness condition (14.16) is always guaranteed in bi-inﬁnite
block encoding.)
We next compute the power P in X(·) under the assumption that the data bits

Dj, j ∈Z

are IID random bits, where we adopt the following deﬁnition.
Deﬁnition 14.5.1 (IID Random Bits). We say that a collection of random variables
comprises IID random bits if the random variables are binary, independent, and
each of them takes on the values 0 and 1 equiprobably.
The assumption that the bi-inﬁnite data sequence

Dj, j ∈Z

consists of IID
random bits is equivalent to the assumption that the K-tuples

Dν, ν ∈Z

are
IID with Dν being uniformly distributed over the set of binary K-tuples {0, 1}K.
We shall also assume that the real N-tuple enc(D) is of zero mean whenever the
binary K-tuple is uniformly distributed over {0, 1}K. We will show that, subject to
these assumptions,
P =
1
NTs
E
5 ∞
−∞
	
A
N

ℓ=1
Xℓg(t −ℓTs)

2
dt
6
.
(14.37)
This expression has an interesting interpretation.
On the LHS is the power in
the transmitted signal in bi-inﬁnite block encoding using the (K, N) binary-to-reals
block encoder enc(·). On the RHS is the quantity E/(NTs), where E, as in (14.3), is
the expected energy in the signal that results when only the K-tuple (D1, . . . , DK)
is transmitted from time −∞to time +∞.
Using the deﬁnition of the energy
per-symbol Es (14.7) we can also rewrite (14.37) as in (14.8). Thus, in bi-inﬁnite
242
Energy and Power in PAM
block-mode, the transmitted power is the energy per real symbol Es normalized by
the signaling period Ts. Also, by (14.5), we can rewrite (14.37) as
P = A2
NTs
 ∞
−∞
N

ℓ=1
N

ℓ′=1
E[XℓXℓ′] ei2πf(ℓ−ℓ′)Ts ˆg(f)
2 df.
(14.38)
To derive (14.37) we ﬁrst express the transmitted waveform X(·) as
X(t) = A
∞

ℓ=−∞
Xℓg(t −ℓTs)
= A
∞

ν=−∞
N

η=1
XνN+η g

t −(νN + η)Ts

= A
∞

ν=−∞
u

Xν, t −νNTs

,
t ∈R,
(14.39)
where the function u: RN × R →R is given by
u: (x1, . . . , xN, t) 	→
N

η=1
xη g(t −ηTs).
(14.40)
We now make three observations. The ﬁrst is that because the law of Dν does not
depend on ν, neither does the law of Xν (= enc(Dν)):
Xν
L= Xν′,
ν, ν′ ∈Z.
(14.41)
The second is that the assumption that enc(D) is of zero mean whenever D is
uniformly distributed over {0, 1}K implies by (14.40) that
E

u

Xν, t

= 0,

ν ∈Z, t ∈R

.
(14.42)
The third is that the hypothesis that the data bits

Dj, j ∈Z

are IID implies
that

Dν, ν ∈Z

are IID and hence that

Xν, ν ∈Z

are also IID. Consequently,
since the independence of Xν and Xν′ implies the independence of u

Xν, t

and
u

Xν′t′
, it follows from (14.42) that
E

u

Xν, t

u

Xν′, t′
= 0,

t, t′ ∈R, ν ̸= ν′, ν, ν′ ∈Z

.
(14.43)
Using (14.39) and these three observations we can now compute for any epoch τ ∈R
the expected energy in the time interval [τ, τ + NTs) as
 τ+NTs
τ
E

X2(t)

dt
=
 τ+NTs
τ
E
5	
A
∞

ν=−∞
u

Xν, t −νNTs

26
dt
14.5 Computing the Power in PAM
243
= A2
 τ+NTs
τ
∞

ν=−∞
∞

ν′=−∞
E
%
u

Xν, t −νNTs

u

Xν′, t −ν′NTs
&
dt
= A2
 τ+NTs
τ
∞

ν=−∞
E

u2
Xν, t −νNTs

dt
= A2
 τ+NTs
τ
∞

ν=−∞
E

u2
X0, t −νNTs

dt
= A2
∞

ν=−∞
 τ−(ν−1)NTs
τ−νNTs
E

u2
X0, t′
dt′
= A2
 ∞
−∞
E

u2
X0, t′
dt′
= E
5 ∞
−∞
	
A
N

ℓ=1
Xℓg(t′ −ℓTs)

2
dt′
6
,
τ ∈R,
(14.44)
where the ﬁrst equality follows from(14.39); the second by writing the square as
a product and by using the linearity of expectation; the third from (14.43); the
fourth because the law of Xν does not depend on ν (14.41); the ﬁfth by changing
the integration variable to t′ ≜t −NTs; the sixth because the sum of the integrals
is equal to the integral over R; and the seventh by (14.40).
Note that, indeed, the RHS of (14.44) does not depend on the starting epoch τ of
the interval. Because there are ⌊2T/(NTs)⌋disjoint length-NTs half-open intervals
contained in the interval [−T, T) and because ⌈2T/(NTs)⌉such intervals suﬃce to
cover the interval [−T, T), it follows that
7 2T
NTs
8
E
5 ∞
−∞
	
A
N

ℓ=1
Xℓg(t −ℓTs)

2
dt
6
≤E
 T
−T
X2(t) dt

≤
/ 2T
NTs
0
E
5 ∞
−∞
	
A
N

ℓ=1
Xℓg(t −ℓTs)

2
dt
6
.
Dividing by 2T and then letting T tend to inﬁnity establishes (14.37).
14.5.3
Time Shifts of Pulse Shape Are Orthonormal
We next consider the power in PAM when the time shifts of the real pulse shape by
integer multiples of Ts are orthonormal. To remind the reader of this assumption,
we change notation and denote the pulse shape by φ(·) and express the orthonor-
mality condition as
 ∞
−∞
φ(t −ℓTs) φ(t −ℓ′Ts) dt = I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z.
(14.45)
The calculation of the power is a bit tricky because (14.45) only guarantees that the
time shifts of the pulse shape are orthogonal over the interval (−∞, ∞); they need
244
Energy and Power in PAM
not be orthogonal over the interval [−T, +T ] (even for very large T). Nevertheless,
intuition suggests that if ℓTs and ℓ′Ts are both much smaller than T, then the
orthogonality of t 	→φ(t −ℓTs) and t 	→φ(t −ℓ′Ts) over the interval (−∞, ∞)
should imply that they are nearly orthogonal over [−T, T ]. Making this intuition
rigorous is a bit tricky and the calculation of the energy in the interval [−T, T ]
requires a fair number of approximations that must be justiﬁed.
To control these approximations we shall assume a decay condition on the pulse
shape that is identical to (14.17). Thus, we shall assume that there exist positive
constants α and β such that
φ(t)
 ≤
β
1 + |t/Ts|1+α ,
t ∈R.
(14.46)
(The pulse shapes used in practice, like those we encountered in (11.31), typically
decay like 1/t2 so this is not a serious restriction.) We shall also continue to assume
the boundedness condition (14.16) but otherwise make no statistical assumptions
on the symbols

Xℓ, ℓ∈Z

.
The main result of this section is the next theorem.
Theorem 14.5.2. Let the continuous-time SP

X(t), t ∈R

be given by
X(t) = A
∞

ℓ=−∞
Xℓφ(t −ℓTs),
t ∈R,
(14.47)
where A ≥0; Ts > 0; the pulse shape φ(·) is a Borel measurable function satisfying
the orthonormality condition (14.45) and the decay condition (14.46); and where
the random sequence

Xℓ, ℓ∈Z

satisﬁes the boundedness condition (14.16). Then
lim
T→∞
1
2T E
 T
−T
X2(t) dt

= A2
Ts
lim
L→∞
1
2L + 1
L

ℓ=−L
E

X2
ℓ

,
(14.48)
whenever the limit on the RHS exists.
Proof. The proof is somewhat technical and may be skipped. Both sides of (14.48)
scale like A2, so it suﬃces to prove the theorem, as we shall, for A = 1. We next
argue that it suﬃces to prove the theorem for the case where Ts = 1. To see this,
assume that Ts > 0 is not necessarily equal to 1. Deﬁne the function
˜φ(t) =

Ts φ(Tst),
t ∈R,
(14.49)
and note that, by changing the integration variable to τ ≜tTs,
 ∞
−∞
˜φ(t −ℓ) ˜φ(t −ℓ′) dt =
 ∞
−∞
φ(τ −ℓTs) φ(τ −ℓ′Ts) dτ
= I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z,
(14.50a)
14.5 Computing the Power in PAM
245
where the second equality follows from the theorem’s assumption about the or-
thonormality of the time shifts of φ by integer multiples of Ts. Also, by (14.49)
and (14.46) we obtain
|˜φ(t)| =

Ts |φ(Tst)|
≤

Ts
β
1 + |t|1+α
=
β′
1 + |t|1+α ,
t ∈R,
(14.50b)
for some β′ > 0 and α > 0.
As to the power, by changing the integration variable to σ ≜t/Ts we obtain
1
2T
 T
−T
	
ℓ∈Z
Xℓφ(t−ℓTs)

2
dt = 1
Ts
1
2(T/Ts)
 T/Ts
−T/Ts
	
ℓ∈Z
Xℓ˜φ(σ−ℓ)

2
dσ. (14.50c)
It now follows from (14.50a) & (14.50b) that if we prove the theorem for the pulse
shape ˜φ with Ts = 1, it will then follow that the power in ∞
ℓ=−∞Xℓ˜φ(σ −ℓ)
is equal to limL→∞(2L + 1)−1 L
ℓ=−L E

X2
ℓ

and that consequently, by (14.50c),
the power in ∞
ℓ=−∞Xℓφ(t −ℓTs) is T−1
s
limL→∞(2L + 1)−1 L
ℓ=−L E

X2
ℓ

. In the
remainder of the proof we shall thus assume that Ts = 1 and express the decay
condition (14.46) as
|φ(t)| ≤
β
1 + |t|1+α ,
t ∈R
(14.51)
for some β, α > 0.
To further simplify notation we shall assume that T is a positive integer. Indeed,
if the limit is proved for positive integers, then the general result follows from the
Sandwich Theorem by noting that for T > 0 (not necessarily an integer)
⌊T⌋
T
1
⌊T⌋
 ⌊T⌋
−⌊T⌋
	
ℓ∈Z
Xℓφ(t −ℓ)

2
dt
≤1
T
 T
−T
	
ℓ∈Z
Xℓφ(t −ℓ)

2
dt ≤
⌈T⌉
T
1
⌈T⌉
 ⌈T⌉
−⌈T⌉
	
ℓ∈Z
Xℓφ(t −ℓ)

2
dt
(14.52)
and by noting that both ⌊T⌋/T and ⌈T⌉/T tend to 1, as T →∞.
We thus proceed to prove (14.48) for the case where A = 1, where Ts = 1, and
where the limit T →∞is only over positive integers. We begin by introducing
some notation. For every integer ℓwe denote the mapping t 	→φ(t −ℓ) by φℓ, and
for every positive integer T we denote the windowed mapping t 	→φ(t−ℓ) I{|t| ≤T}
by φℓ,w. Finally, we ﬁx some integer ν > 0 and deﬁne for every T > ν, the random
246
Energy and Power in PAM
processes
X0 =

|ℓ|≤T−ν
Xℓφℓ,w,
(14.53)
X1 =

T−ν<|ℓ|≤T+ν
Xℓφℓ,w,
(14.54)
X2 =

T+ν<|ℓ|<∞
Xℓφℓ,w,
(14.55)
and the unwindowed version of X0
Xu
0 =

|ℓ|≤T−ν
Xℓφℓ
(14.56)
so
X(t) I{|t| ≤T} = X0(t) + X1(t) + X2(t)
= Xu
0 +

X0(t) −Xu
0 (t)

+ X1(t) + X2(t),
t ∈R.
(14.57)
Using arguments very similar to the ones leading to (4.14) (with integration re-
placed by integration and expectation) one can show that (14.57) leads to the
bound
	4
E
%
∥Xu
0∥2
2
&
−
4
E
%
X0 −Xu
0

+ X1 + X2
2
2
&
2
≤E
 T
−T
X2(t) dt

≤
	4
E
%
∥Xu
0∥2
2
&
+
4
E
%
X0 −Xu
0

+ X1 + X2
2
2
&
2
.
(14.58)
Note that, by the orthonormality assumption on the time shifts of φ,
∥Xu
0∥2
2 =

|ℓ|≤T−ν
X2
ℓ
so
lim
T→∞
1
2T E
%
∥Xu
0∥2
2
&
= lim
L→∞
1
2L + 1

|ℓ|≤L
E

X2
ℓ

.
(14.59)
It follows from (14.58) and (14.59) that to conclude the proof of the theorem it
suﬃces to show that for every ﬁxed ν ≥2
lim
T→∞
1
2T E
%
∥X1∥2
2
&
= 0,
(14.60)
lim
T→∞
1
2T E
%
∥X0 −Xu
0∥2
2
&
= 0,
(14.61)
and that
lim
ν→∞lim
T→∞
1
2T E
%
∥X2∥2
2
&
= 0.
(14.62)
14.5 Computing the Power in PAM
247
We begin with (14.60), which follows directly from the Triangle Inequality,
∥X1∥2 ≤

T−ν<|ℓ|≤T+ν
|Xℓ| ∥φℓ,w∥2
≤4νγ,
where the second inequality follows from the boundedness condition (14.16), from
the fact that φℓ,w is a windowed version of the unit-energy signal φℓso ∥φℓ,w∥2 ≤
∥φ∥2 = 1, and because there are 4ν terms in the sum (whenever T exceeds ν).
We next prove (14.62). To that end we upper-bound |X2(t)| for |t| ≤T as follows:
|X2(t)| =


T+ν<|ℓ|<∞
Xℓφ(t −ℓ)
,
|t| ≤T
≤γ

T+ν<|ℓ|<∞
|φ(t −ℓ)|
≤γ

T+ν<|ℓ|<∞
β
|t −ℓ|1+α
≤γ

T+ν<|ℓ|<∞
β
|ℓ| −|t|
1+α
≤γ

T+ν<|ℓ|<∞
β
(|ℓ| −T)1+α ,
|t| ≤T
= 2γβ
∞

ℓ=T+ν+1
1
(ℓ−T)1+α
= 2γβ
∞

˜ℓ=ν+1
1
˜ℓ1+α
≤2γβ
 ∞
ν
ξ−1−α dξ
= 2γβ
α ν−α,
(14.63)
where the equality in the ﬁrst line follows from the deﬁnition of X2 (14.55) by
noting that for |t| ≤T we have φℓ(t) = φℓ,w(t); the inequality in the second line
follows from the boundedness condition (14.16) and from the Triangle Inequality for
Complex Numbers (2.12); the inequality in the third line from the decay condition
(14.51); the inequality in the fourth line because |ξ −ζ| ≥
|ξ| −|ζ|
 whenever
ξ, ζ ∈R; the inequality in the ﬁfth line because we are only considering |t| ≤T and
because over the range of this summation |ℓ| > T + ν; the equality in the sixth line
from the symmetry of the summand; the equality in the seventh line by deﬁning
˜ℓ≜ℓ−T; the inequality in the eighth line from the monotonicity of the function
ξ 	→ξ−1−α (for α > 0), which implies that
1
˜ℓ1+α ≤
 ˜ℓ
˜ℓ−1
1
ξ1+α dξ;
248
Energy and Power in PAM
and where the ﬁnal equality on the ninth line follows by computing the integral.
Using (14.63), which holds for all t ∈R because X2(t) is zero for |t| > T, we
conclude that
∥X2∥2
2 ≤2T
2γβ
α
2
ν−2α,
(14.64)
from which (14.62) follows.
We next turn to proving (14.61). We begin by using the Triangle Inequality and
the boundedness condition (14.16) to obtain
∥X0 −Xu
0∥2
2 =


|ℓ|≤T−ν
Xℓφℓ,w −

|ℓ|≤T−ν
Xℓφℓ

2
2
=


|ℓ|≤T−ν
Xℓ

φℓ,w −φℓ

2
2
≤γ2
	

|ℓ|≤T−ν
∥φℓ,w −φℓ∥2

2
.
(14.65)
We next proceed to upper-bound the RHS of (14.65) by ﬁrst deﬁning the function
ρ(τ) =

|t|>τ
φ2(t) dt
(14.66)
and by then using this function to upper-bound ∥φℓ−φℓ,w∥2 as
∥φℓ−φℓ,w∥2 ≤ρ(T −|ℓ|),
|ℓ| ≤T,
(14.67)
because
∥φℓ−φℓ,w∥2
2 =
 −T
−∞
φ2(t −ℓ) dt +
 ∞
T
φ2(t −ℓ) dt
=
 −T−ℓ
−∞
φ2(s) ds +
 ∞
T−ℓ
φ2(s) ds
≤
 −T+|ℓ|
−∞
φ2(s) ds +
 ∞
T−|ℓ|
φ2(s) ds
=

|s|≥T−|ℓ|
φ2(s) ds,
|ℓ| ≤T
= ρ2(T −|ℓ|).
It follows from (14.65) and (14.67) that
∥X0 −Xu
0∥2
2 ≤γ2
	 
|ℓ|≤T−ν
∥φℓ,w −φℓ∥2

2
≤γ2
	

|ℓ|≤T−ν
ρ(T −|ℓ|)

2
14.6 A More Formal Account
249
≤γ2
	
2

0≤ℓ≤T−ν
ρ(T −ℓ)

2
= 4γ2
	
T

η=ν
ρ(η)

2
.
(14.68)
We next note that the decay condition (14.51) implies that
ρ(τ) ≤
 2β2
1 + 2α
1/2
τ −1
2 −α,
τ > 0,
(14.69)
because for every τ > 0,
ρ2(τ) =

|t|>τ
φ2(t) dt
≤

|t|>τ
β2
|t|2+2α dt
= 2β2
 ∞
τ
t−2−2α dt
=
2β2
1 + 2ατ −1−2α.
It now follows from (14.69) that
T

η=ν
ρ(η) ≤
 2β2
1 + 2α
1/2
T

η=ν
η−1
2 −α
≤
 2β2
1 + 2α
1/2  T
ν−1
ξ−1
2 −α dξ
and hence, by evaluating the integral explicitly, that
lim
T→∞
1
T1/2
T

η=ν
ρ(η) = 0.
(14.70)
From (14.68) and (14.70) we thus obtain (14.61).
14.6
A More Formal Account
In this section we present a more formal deﬁnition of power and justify some of
the mathematical steps that we took in deriving the power in PAM signals. This
section is quite mathematical and is recommended for readers who have had some
exposure to Measure Theory.
Let R denote the σ-algebra generated by the open sets in R, i.e., the Borel σ-
algebra over the reals. A continuous-time stochastic process

X(t)

deﬁned over
250
Energy and Power in PAM
the probability space (Ω, F, P) is said to be a measurable stochastic process
if the mapping (ω, t) 	→X(ω, t) from Ω × R to R is measurable when its codomain
R is endowed with the σ-algebra R and when its domain Ω × R is endowed with
the product σ-algebra F × R (Deﬁnition 25.9.1 ahead). Thus,

X(t), t ∈R

is
measurable if the mapping (ω, t) 	→X(ω, t) is F×R/R measurable.5
From Fubini’s Theorem it follows that if

X(t), t ∈R

is measurable and if T > 0
is deterministic, then:
(i) For every ω ∈Ω, the mapping t 	→X2(ω, t) is Borel measurable;
(ii) the mapping
ω 	→
 T
−T
X2(ω, t) dt
is a random variable (i.e., F measurable) possibly taking on the value +∞;
(iii) and
E
 T
−T
X2(t) dt

=
 T
−T
E

X2(t)

dt,
T ∈R.
(14.71)
Deﬁnition 14.6.1 (Power of a Stochastic Process). We say that a measurable
stochastic process

X(t), t ∈R

is of power P if the limit
lim
T→∞
1
2T E
 T
−T
X2(t) dt

(14.72)
exists and is equal to P.
Proposition 14.6.2. If the pulse shape g is a Borel measurable function satisfying
the decay condition (14.17) for some positive α, β, Ts, and if the discrete-time SP

Xℓ, ℓ∈Z

satisﬁes the boundedness condition (14.16) for some γ ≥0, then the
stochastic process
X: (ω, t) 	→A
∞

ℓ=−∞
Xℓ(ω) g(t −ℓTs)
(14.73)
is a measurable stochastic process.
Proof. The mapping (ω, t) 	→Xℓ(ω) is F×R/R measurable because Xℓis a ran-
dom variable, so the mapping ω 	→Xℓ(ω) is F/R measurable.
The mapping
(ω, t) 	→A g(t −ℓTs) is F×R/R measurable because g is Borel measurable, so
t 	→g(t −ℓTs) is R/R measurable. Since the product of measurable functions is
measurable (Rudin, 1987, Chapter 1, Section 1.9 (c)), it follows that the mapping
(ω, t) 	→AXℓ(ω) g(t −ℓTs) is F×R/R measurable. And since the sum of measur-
able functions is measurable (Rudin, 1987, Chapter 1, Section 1.9 (c)), it follows
that for every positive integer L ∈Z, the mapping
(ω, t) 	→A
L

ℓ=−L
Xℓ(ω) g(t −ℓTs)
5See (Billingsley, 1995, Section 37, p. 503) or (Lo`eve, 1963, Section 35) on the deﬁnition of a
measurable stochastic process and see (Billingsley, 1995, Section 18) or (Lo`eve, 1963, Section 8.2)
or (Halmos, 1950, Chapter VII) for the deﬁnition of the product σ-algebra.
14.6 A More Formal Account
251
is F×R/R measurable. The proposition now follows by recalling that the pointwise
limit of every pointwise convergent sequence of measurable functions is measurable
(Rudin, 1987, Theorem 1.14).
Having established that the PAM signal (14.73) is a measurable stochastic process
we would next like to justify the calculations leading to (14.31). To justify the
swapping of integration and summations in (14.26) we shall need the following
lemma, which also explains why the sum in (14.27) converges.
Lemma 14.6.3. If g(·) is a Borel measurable function satisfying the decay condition
|g(t)| ≤
β
1 + |t/Ts|1+α ,
t ∈R
(14.74)
for some positive α, Ts, and β, then
∞

m=−∞
 ∞
−∞
g(t) g(t −mTs)
 dt < ∞.
(14.75)
Proof. The decay condition (14.74) guarantees that g is of ﬁnite energy. From the
Cauchy-Schwarz Inequality it thus follows that the terms in (14.75) are all ﬁnite.
Also, by symmetry, the term in (14.75) corresponding to m is the same as the one
corresponding to −m. Consequently, to establish (14.75), it suﬃces to prove
∞

m=2
 ∞
−∞
g(t) g(t −mTs)
 dt < ∞.
(14.76)
Deﬁne the function
gu(t) ≜

1
if |t| ≤1,
|t|−1−α
otherwise,
t ∈R.
By (14.74) it follows that |g(t)| ≤β gu (t/Ts) for all t ∈R. Consequently,
 ∞
−∞
g(t) g(t −mTs)
 dt ≤β2
 ∞
−∞
gu(t/Ts) gu(t/Ts −m) dt
= β2Ts
 ∞
−∞
gu(τ) gu(τ −m) dτ,
and to establish (14.76) it thus suﬃces to prove
∞

m=2
 ∞
−∞
gu(τ) gu(τ −m) dτ < ∞.
(14.77)
Since the integrand in (14.77) is symmetric around τ = m/2, it follows that
 ∞
−∞
gu(τ) gu(τ −m) dτ = 2
 ∞
m/2
gu(τ) gu(τ −m) dτ,
(14.78)
252
Energy and Power in PAM
and it thus suﬃces to establish
∞

m=2
 ∞
m/2
gu(τ) gu(τ −m) dτ < ∞.
(14.79)
We next upper-bound the integral in (14.79) for every m ≥2 by ﬁrst expressing it
as
 ∞
m/2
gu(τ) gu(τ −m) dτ = I1 + I2 + I3,
where
I1 ≜
 m−1
m/2
1
τ 1+α
1
(m −τ)1+α dτ,
I2 ≜
 m+1
m−1
1
τ 1+α dτ,
I3 ≜
 ∞
m+1
1
τ 1+α
1
(τ −m)1+α dτ.
We next upper-bound each of these terms for m ≥2. Starting with I1 we obtain
upon deﬁning ξ ≜m −τ
I1 =
 m−1
m/2
1
τ 1+α
1
(m −τ)1+α dτ
=
 m/2
1
1
(m −ξ)1+α
1
ξ1+α dξ
≤
 m/2
1
1
(m/2)1+α
1
ξ1+α dξ
= 1
α 21+α
1
m1+α
	
1 −2α
mα

,
m ≥2
≤1
α 21+α
1
m1+α ,
m ≥2,
which is summable over m. As to I2 we have
I2 =
 m+1
m−1
1
τ 1+α dτ
≤
2
(m −1)1+α ,
m ≥2,
which is summable over m. Finally we upper-bound I3 by deﬁning ξ ≜τ −m
I3 =
 ∞
m+1
1
τ 1+α
1
(τ −m)1+α dτ
=
 ∞
1
1
(ξ + m)1+α
1
ξ1+α dξ
=
 m
1
1
(ξ + m)1+α
1
ξ1+α dξ +
 ∞
m
1
(ξ + m)1+α
1
ξ1+α dξ
14.7 Exercises
253
≤
1
m1+α
 m
1
1
ξ1+α dξ +
 ∞
m
1
ξ1+α
1
ξ1+α dξ
= 1
α
1
m1+α
	
1 −
1
mα

+
1
1 + 2α
1
m1+2α ,
m ≥2
≤1
α
1
m1+α ,
m ≥2,
which is summable over m.
We can now state (14.31) as a theorem.
Theorem 14.6.4. Let the pulse shape g: R →R be a Borel measurable function sat-
isfying the decay condition (14.17) for some positive α, β, and Ts. Let

Xℓ, ℓ∈Z

be a centered WSS SP of autocovariance function KXX and satisfying the bound-
edness condition (14.16) for some γ ≥0. Then the stochastic process (14.73) is
measurable and is of the power P given in (14.31).
Proof. The measurability of

X(t), t ∈R

follows from Proposition 14.6.2. The
power can be derived as in the derivation of (14.31) from (14.27) with the derivation
of (14.27) now being justiﬁable by noting that (14.25) follows from (14.71) and by
noting that (14.26) follows from Lemma 14.6.3 and Fubini’s Theorem.
Similarly, we can state (14.37) as a theorem.
Theorem 14.6.5 (Power in Bi-Inﬁnite Block-Mode PAM). Let

Dj, j ∈Z

be
IID random bits. Let the (K, N) binary-to-reals encoder enc: {0, 1}K →RN be
such that enc(D1, . . . , DK) is of zero mean whenever the K-tuple (D1, . . . , DK) is
uniformly distributed over {0, 1}K. Let

Xℓ, ℓ∈Z

be generated from

Dj, j ∈Z

in bi-inﬁnite block encoding mode using enc(·). Assume that the pulse shape g is a
Borel measurable function satisfying the decay condition (14.17) for some positive
α, β, and Ts.
Then the stochastic process (14.73) is measurable and is of the
power P as given in (14.37).
Proof. Measurability follows from Proposition 14.6.2. The derivation of (14.37) is
justiﬁed using Fubini’s Theorem.
14.7
Exercises
Exercise 14.1 (Superimposing Independent Transmissions). Let the two PAM signals

X(1)(t)

and

X(2)(t)

be given at every epoch t ∈R by
X(1)(t) = A(1)
∞

ℓ=−∞
X(1)
ℓ
g(1)(t −ℓTs),
X(2)(t) = A(2)
∞

ℓ=−∞
X(2)
ℓ
g(2)(t −ℓTs),
where the zero-mean real symbols

X(1)
ℓ

are generated from the data bits

D(1)
j

and
the zero-mean real symbols

X(2)
ℓ

from

D(2)
j

. Assume that the bit streams

D(1)
j

and

D(2)
j

are independent and that

X(1)(t)

and

X(2)(t)

are of powers P(1) and P(2).
Find the power in the sum of

X(1)(t)

and

X(2)(t)

.
254
Energy and Power in PAM
Exercise 14.2 (The Minimum Distance of a Constellation and Power). Consider the
PAM signal (14.47) where the time shifts of the pulse shape φ by integer multiples of Ts
are orthonormal, and where the symbols

Xℓ

are IID and uniformly distributed over the
set

± d
2, ± 3d
2 , . . . , ±(2ν −1) d
2

. Relate the power in X(·) to the minimum distance d
and the constant A.
Exercise 14.3 (PAM with Nonorthogonal Pulses). Let the IID random bits

Dj, j ∈Z

be modulated using PAM with the pulse shape g: t →I{|t| ≤Ts} and the repetition block
encoding map 0 →(+1, +1) and 1 →(−1, −1). Compute the average transmitted power.
Exercise 14.4 (Non-IID Data Bits). Expression (14.37) for the power in bi-inﬁnite block
mode was derived under the assumption that the data bits are IID. Show that it need
not otherwise hold.
Exercise 14.5 (The Power in Nonorthogonal PAM). Consider the PAM signal (14.23)
with the pulse shape g: t →I{|t| ≤Ts}.
(i) Compute the power in X(·) when

Xℓ

are IID of zero mean and unit variance.
(ii) Repeat when

Xℓ

is a zero-mean WSS SP of autocovariance function
KXX(m) =
⎧
⎪
⎨
⎪
⎩
1
m = 0,
1
2
|m| = 1,
0
otherwise,
m ∈Z.
Note that in both parts E[Xℓ] = 0 and E
$
X2
ℓ
%
= 1.
Exercise 14.6 (A Non-PAM Signal). The bi-inﬁnite data sequence

Dj, j ∈Z

of IID
random bits is mapped to the waveform
X(t) = A
∞

ℓ=−∞
	
Dℓg1(t −ℓTs) + (1 −Dℓ) g0(t −ℓTs)

,
t ∈R,
where g0 and g1 are pulse shapes that satisfy the decay condition (14.17). Compute the
mean E[X(t)]; the energy in a length-Ts interval
E
' τ+Ts
τ
X2(t) dt
(
;
and the power in the SP

X(t), t ∈R

.
Exercise 14.7 (The Power in a Periodic Signal). Let x be a real, Lebesgue measurable,
periodic signal of period Tp > 0. Prove that the power in x is
1
Tp
 Tp
0
x2(t) dt.
Exercise 14.8 (Pre-Encoding). Rather than applying the mapping enc: {0, 1}K →RN to
the IID random bits D1, . . . , DK directly, we ﬁrst map the data bits using a one-to-one
mapping φ: {0, 1}K →{0, 1}K to D′
1, . . . , D′
K, and we then map D′
1, . . . , D′
K using enc to
X1, . . . , XN. Does this change the transmitted energy?
14.7 Exercises
255
Exercise 14.9 (Binary Linear Encoders Producing Pairwise-Independent Symbols). Bi-
nary linear encoders with the antipodal mapping can be described as follows. Using a de-
terministic binary K×N matrix G, the encoder ﬁrst maps the row-vector d = (d1, . . . , dK)
to the row-vector dG, where dG is computed using matrix multiplication over the binary
ﬁeld. (Recall that in the binary ﬁeld multiplication is deﬁned as 0 · 0 = 0 · 1 = 1 · 0 = 0,
and 1 · 1 = 1; and addition is modulo 2, so 0 ⊕0 = 1 ⊕1 = 0 and 0 ⊕1 = 1 ⊕0 = 1).
Thus, the ℓ-th component cℓof dG is given by
cℓ= d1 · g(1,ℓ) ⊕d2 · g(2,ℓ) ⊕· · · ⊕dK · g(K,ℓ).
The real symbol xℓis then computed according to the rule
xℓ=

+1
if cℓ= 0,
−1
if cℓ= 1,
ℓ= 1, . . . , N.
Let X1, X2, . . . , XN be the symbols produced by the encoder when it is fed IID random
bits D1, D2, . . . , DK. Show that:
(i) Unless all the entries in the ℓ-th column of G are zero, E[Xℓ] = 0.
(ii) The statement “Xℓis independent of Xℓ′” is equivalent to the statement “the ℓ-th
and ℓ′-th columns of G diﬀer or are both all-zero.”
You may ﬁnd it useful to ﬁrst prove the following.
(i) If a RV E takes value in the set {0, 1}, and if F takes on the values 0 and 1 equiprob-
ably and independently of E, then E⊕F is uniform on {0, 1} and independent of E.
(ii) If X and Y are binary random variables and X is uniform, then they are indepen-
dent if, and only if, X ⊕Y is uniform.
Exercise 14.10 (Zero-Mean Signals for Linearly Dispersive Channels). Suppose that the
transmitted signal X suﬀers not only from an additive random disturbance but also
from a deterministic linear distortion. Thus, the received signal Y can be expressed as
Y = X ⋆h + N, where h is a known (deterministic) impulse response, and where N is
an unknown (random) additive disturbance. Show heuristically that transmitting signals
of nonzero mean is power ineﬃcient. How would you mimic the performance of a system
transmitting X(·) using a system transmitting X(·) −c(·)?
Exercise 14.11 (The Power in Orthogonal Code-Division Multi-Accessing). Suppose that
the data bits

D(1)
j

are mapped to the real symbols

X(1)
ℓ

and that the data bits

D(2)
j

are mapped to

X(2)
ℓ

. Assume that

A(1)2
Ts
lim
L→∞
1
2L + 1
L

ℓ=−L
E
+
X(1)
ℓ
2,
= P(1),
and similarly for P(2). Further assume that the time shifts of φ by integer multiples of Ts
are orthonormal and that φ satisﬁes the decay condition (14.46). Finally assume that

X(1)
ℓ

and

X(2)
ℓ

are bounded in the sense of (14.16). Compute the power in the signal
∞

ℓ=−∞
	
A(1)X(1)
ℓ
+ A(2)X(2)
ℓ

φ

t −2ℓTs

+
	
A(1)X(1)
ℓ
−A(2)X(2)
ℓ

φ

t −(2ℓ+ 1)Ts

.
256
Energy and Power in PAM
Exercise 14.12 (More on Orthogonal Code-Division Multi-Accessing). Extend the result
of Exercise 14.11 to the case with η data streams, where the transmitted signal is given
by
∞

ℓ=−∞
	
a(1,1)A(1)X(1)
ℓ
+ · · · + a(η,1)A(η)X(η)
ℓ

φ

t −ηℓTs

+ · · · +
	
a(1,η)A(1)X(1)
ℓ
+ · · · + a(η,η)A(η)X(η)
ℓ

φ

t −(ηℓ+ η −1)Ts

and where the real numbers a(ι,ν) for ι, ν ∈{1, . . . , η} satisfy the orthogonality condition
η

ν=1
a(ι,ν)a(ι′,ν) =

η
if ι = ι′,
0
if ι ̸= ι′,
ι, ι′ ∈{1, . . . , η}.
The sequence a(ι,1), . . . , a(ι,η) is sometimes called the signature of the ι-th stream.
Exercise 14.13 (The Samples of the Self-Similarity Function). Let g: R →R be of ﬁnite
energy, and let Rgg be its self-similarity function.
(i) Show that there exists an integrable nonnegative function G: [−1/2, 1/2) →[0, ∞)
such that
Rgg(mTs) =
 1/2
−1/2
G(θ) ei2πmθ dθ,
m ∈Z,
(14.80)
and such that G(−θ) = G(θ) for all |θ| < 1/2. Express G(·) in terms of the FT of g.
(ii) Show that if the samples of the self-similarity function are absolutely summable,
i.e.,

m∈Z
Rgg(mTs)
 < ∞,
(14.81)
then the function
θ →
∞

m=−∞
Rgg(mTs) e−i2πmθ,
θ ∈[−1/2, 1/2),
(14.82)
is such a function and is continuous.
(iii) Assuming the above summability, show that if

Xℓ

is of PSD SXX, then the RHS
of (14.31) can be expressed as
1
Ts A2
 1/2
−1/2
G(θ) SXX(θ) dθ.
(14.83)
Exercise 14.14 (A Bound on the Power in PAM). Let g be as in Exercise 14.13, and let
its self-similarity function satisfy (14.81). Let G(·) be the mapping in (14.82) so that the
RHS of (14.31) can be expressed as in (14.83).
(i) Show that if

Xℓ

is of zero mean, of unit variance, and has a PSD, then the RHS
of (14.31) is upper-bounded by
1
Ts A2
max
−1/2≤θ<1/2 G(θ).
(14.84)
(ii) Show that for every ϵ > 0, there exists a zero-mean unit-variance SP

Xℓ

with a
PSD for which the RHS of (14.31) is within ϵ of (14.84).
Chapter 15
Operational Power Spectral Density
15.1
Introduction
The Power Spectral Density of a stochastic process tells us more about the SP than
just its power. It tells us something about how this power is distributed among
the diﬀerent frequencies that the SP occupies. The purpose of this chapter is to
clarify this statement and to derive the PSD of PAM signals. Most of this chapter
is written informally with an emphasis on ideas and intuition as opposed to math-
ematical rigor. The mathematically-inclined readers will ﬁnd precise statements
of the key results of this chapter in Section 15.5. We emphasize that this chapter
only deals with real continuous-time stochastic processes.
The classical deﬁnition of the PSD of continuous-time stochastic processes (Deﬁni-
tion 25.7.2 ahead) is only applicable to wide-sense stationary stochastic processes,
and PAM signals are not WSS.1 Consequently, we shall have to introduce a new
concept, which we call the operational power spectral density, or the op-
erational PSD for short.2 This new concept is applicable to a large family of
stochastic processes that includes most WSS processes and most PAM signals.
For WSS stochastic processes, the operational PSD and the classical PSD coin-
cide (Section 25.14). In addition to being more general, the operational PSD is
more intuitive in that it clariﬁes the origin of the words “power spectral density.”
Moreover, it gives an operational meaning to the concept.
Section 15.2 provides some motivation for our deﬁnition of the operational PSD.
Readers in a rush to get to the deﬁnition can skip it and jump directly to Sec-
tion 15.3.
Section 15.4 derives the operational PSD of PAM signals, and Sec-
tion 15.5 does so a bit more rigorously. Sections 15.6–15.8 expand on the oper-
ational PSD. Section 15.6 explores its relationship to the average autocovariance
function; Section 15.7 discusses the eﬀect of ﬁltering on the operational PSD; and
Section 15.8 explores the technical assumptions that are required for the power to
equal the integral of the operational PSD over all the frequencies.
1If the discrete-time symbol sequence is stationary then the PAM signal is cyclostationary.
But this term will not be used in this book.
2These terms are not standard. Most of the literature does not seem to distinguish between
the PSD in the sense of Deﬁnition 25.7.2 and what we call the operational PSD.
257
258
Operational Power Spectral Density
function
quantity of interest
per unit of
charge (spatial) density
charge
space
mass (spatial) density
mass
space
mass line density
mass
length
probability (per unit of X) density
probability
unit of X
power spectral density
power
spectrum (Hz)
Table 15.1: Various densities and their units.
15.2
Motivation
To motivate the new deﬁnition we shall ﬁrst brieﬂy discuss other “densities” such
as charge density, mass density, and probability density.
In electromagnetism one encounters the concept of charge density, which is often
denoted by ϱ(·). It measures the amount of charge per unit volume. Since the
charge need not be uniformly distributed, ϱ(·) is typically not constant so the charge
density is a function of location. Thus, we usually write ϱ(x, y, z) for the charge
density at the location (x, y, z). This can be deﬁned diﬀerentially or integrally.
The diﬀerential deﬁnition is
ϱ(x, y, z)
= lim
Δ↓0
Charge in Box

(x′, y′, z′) : |x −x′| ≤Δ
2 , |y −y′| ≤Δ
2 , |z −z′| ≤Δ
2

Volume of Box

(x′, y′, z′) : |x −x′| ≤Δ
2 , |y −y′| ≤Δ
2 , |z −z′| ≤Δ
2

= lim
Δ↓0
Charge in box

(x′, y′, z′) : |x −x′| ≤Δ
2 , |y −y′| ≤Δ
2 , |z −z′| ≤Δ
2

Δ3
,
and the integral deﬁnition is that a function ϱ(·) is the charge density if for every
region D ⊂R3
Charge in D =

(x,y,z)∈D
ϱ(x, y, z) dx dy dz,
D ⊂R3.
Ignoring some mathematical subtleties, the two deﬁnitions are equivalent. Perhaps
a more appropriate name for charge density is “Charge Spatial Density,” which
makes it clear that the quantity of interest is charge and that we are interested in
the way it is distributed in space. The units of ϱ(x, y, z) are those of charge per
unit volume.
Mass density—or as we would prefer to call it, “Mass Spatial Density”—is analo-
gously deﬁned. Either diﬀerentially, as
ϱ(x, y, z)
= lim
Δ↓0
Mass in Box

(x′, y′, z′) : |x −x′| ≤Δ
2 , |y −y′| ≤Δ
2 , |z −z′| ≤Δ
2

Volume of Box

(x′, y′, z′) : |x −x′| ≤Δ
2 , |y −y′| ≤Δ
2 , |z −z′| ≤Δ
2

= lim
Δ↓0
Mass in box

(x′, y′, z′) : |x −x′| ≤Δ
2 , |y −y′| ≤Δ
2 , |z −z′| ≤Δ
2

Δ3
,
15.2 Motivation
259
or integrally as the function ϱ(x, y, z) such that for every subset D ⊂R3
Mass in D =

(x,y,z)∈D
ϱ(x, y, z) dx dy dz,
D ⊂R3.
The units are those of mass per unit volume. Since mass is nonnegative, the dif-
ferential deﬁnition of mass density makes it clear that mass density must also be
nonnegative. This is slightly less apparent from the integral deﬁnition, but (exclud-
ing subsets of R3 of Lebesgue measure zero) is true nonetheless. By convention, if
one deﬁnes mass density integrally, then one typically insists that the density be
nonnegative.
Similarly, in discussing mass line density one envisions a one-dimensional object,
and its density with respect to unit length is deﬁned diﬀerentially as
ϱ(x) = lim
Δ↓0
Mass in Interval

x′ : |x −x′| ≤Δ
2

Δ
,
or integrally as the nonnegative function ϱ(·) such that for every subset D ⊂R of
the real line
Mass in D =

x∈D
ϱ(x) dx,
D ⊂R.
The units are those of mass per unit length.
In probability theory one encounters the probability density function of a random
variable X. Here the quantity of interest is probability, and we are interested in
how it is distributed on the real line. The units depend on the units of X. Thus, if
X measures the time in days until at least one piece in your new china set breaks,
then the units of the probability density function fX(·) of X are those of probability
(unit-less) per day. The probability density function can be deﬁned diﬀerentially
as
fX(x) = lim
Δ↓0
Pr

X ∈

x −Δ
2 , x + Δ
2

Δ
or integrally by requiring that for every subset E ⊂R
Pr[X ∈E] =

x∈E
fX(x) dx,
E ⊂R.
(15.1)
Again, since probabilities are nonnegative, the diﬀerential deﬁnition makes it clear
that the probability density function is nonnegative. In the integral deﬁnition we
typically add the nonnegativity as a condition. That is, we say that fX(·) is a
density function for the random variable X if fX(·) is nonnegative and if (15.1)
holds. (There is a technical uniqueness issue that we are sweeping under the rug
here: if fX(·) is a probability density function for X and if ξ(·) is a nonnegative
function that diﬀers from fX(·) only on a set of Lebesgue measure zero, then ξ(·)
is also a probability density function for X.)
With these examples in mind, it is natural to interpret the power spectral density
of a stochastic process

X(t), t ∈R

as the distribution of the power of X(·)
260
Operational Power Spectral Density
among the diﬀerent frequencies. See Table 15.1 on Page 258. Heuristically, we
would deﬁne the power spectral density SXX at the frequency f diﬀerentially as
SXX(f) = lim
Δ↓0
Power in the frequencies

f −Δ
2 , f + Δ
2

Δ
or integrally by requiring that for any subset D of the spectrum
Power of X in D =

f∈D
SXX(f) df,
D ⊂R.
(15.2)
To make this meaningful we next explain what we mean by “the power of X in
the frequencies D.” To that end it is best to envision a ﬁlter of impulse response h
whose frequency response ˆh is given by
ˆh(f) =

1
if f ∈D,
0
otherwise,
(15.3)
and to think of the power of X(·) in the frequencies D as the power at the output
of that ﬁlter when it is fed X(·), i.e., the power of the stochastic process X ⋆h.3
We are now almost ready to give a heuristic deﬁnition of the power spectral density.
But there are three more points we would like to discuss ﬁrst. The ﬁrst is that
(15.2) can also be rewritten as
Power of X in D =

all frequencies
I{f ∈D} SXX(f) df,
D ⊂R.
(15.4)
It turns out that if (15.2) holds for all sets D ⊂R of frequencies, then it also holds
for all “nice” ﬁlters (of a frequency response that is not necessarily {0, 1} valued):
Power of X ⋆h =

all frequencies
|ˆh(f)|2 SXX(f) df,
h “nice.”
(15.5)
That (15.4) typically implies (15.5) can be heuristically argued as follows.
By
(15.4) the set of frequency responses ˆh for which (15.5) holds includes all frequency
responses of the form ˆh(f) = I{f ∈D}. But if (15.5) holds for some frequency
response ˆh, then it must also hold for α ˆh, where α is any complex number, because
scaling the frequency response by α merely multiplies the output power by |α|2.
Also, if (15.5) holds for two responses ˆh1 and ˆh2 for which
ˆh1(f) ˆh2(f) = 0,
f ∈R,
(15.6)
then it must also hold for h1 + h2, because Parseval’s Theorem and (15.6) imply
that X ⋆h1 and X ⋆h2 must be orthogonal. Thus, (15.6) implies that the power
in X ⋆(h1 + h2) is the sum of the power in X ⋆h1 and the power in X ⋆h2. It
thus intuitively follows that if (15.4) holds for all subsets D of the spectrum, then
it holds for all step functions ˆh(f) = 
ν αν I{f ∈Dν}, where {Dν} are disjoint.
3We are ignoring the fact that the RHS of (15.3) is typically not the frequency response of a
stable ﬁlter. Stable ﬁlters have (uniformly) continuous frequency responses (Theorem 6.2.11 (i)).
15.2 Motivation
261
And since any “nice” frequency response ˆh can be arbitrarily well approximated
by such step functions, we expect that (15.5) would hold for all “nice” responses.
Having heuristically established that (15.2) implies (15.5), we prefer to deﬁne the
PSD as a function SXX for which (15.5) holds, where “nice” will be taken to mean
stable.
The second point we would like to make is regarding uniqueness. For real stochastic
processes it is reasonable to require that (15.5) hold only for ﬁlters of real impulse
response. Thus we would require
Power of X ⋆h =

all frequencies
|ˆh(f)|2 SXX(f) df,
h real and “nice.”
(15.7a)
But since for ﬁlters of real impulse response the mapping f 	→|ˆh(f)|2 is symmetric,
(15.7a) can be rewritten as
 ∞
0
|ˆh(f)|2 
SXX(f) + SXX(−f)

df,
h real and “nice.”
(15.7b)
This form makes it clear that for real stochastic processes, (15.7a) (or its equivalent
form (15.7b)) can only specify the function f 	→SXX(f)+SXX(−f); it cannot fully
specify the mapping f 	→SXX(f).
For example, if a symmetric function SXX
satisﬁes (15.7a), then so does
f 	→

2 SXX(f)
if f > 0,
0
otherwise,
f ∈R.
In fact, if SXX satisﬁes (15.7a), then so does any function ˜S(·) such that
˜S(f) + ˜S(−f) = SXX(f) + SXX(−f),
f ∈R.
Thus, for the sake of uniqueness, we deﬁne the power spectral density SXX to be
a function of frequency that satisﬁes (15.7a) and that is additionally symmetric.
It can be shown that this deﬁnes SXX (to within indistinguishability) uniquely.
In fact, once one has identiﬁed a nonnegative function S(·) such that for any real
impulse response h the integral
 ∞
−∞
S(f) |ˆh(f)|2 df
corresponds to the power in X ⋆h, then the PSD SXX of X is given by the sym-
metrized version of S(·), i.e.,
SXX(f) = 1
2

S(f) + S(−f)

,
f ∈R.
(15.8)
Note that the diﬀerential deﬁnition of the PSD would not have resolved the unique-
ness issue because a ﬁlter of frequency response f 	→I

f ∈

f0 −Δ
2 , f0 + Δ
2

is
not real.
The ﬁnal point we would like to make is regarding additivity. Apart from some
mathematical details, what makes the deﬁnition of charge density possible is the
262
Operational Power Spectral Density
fact that the total charge in the union of two disjoint regions in space is the sum
of charges in the individual regions. The same holds for mass. For the probability
densities the crucial property is that the probability of the union of two disjoint
events is the sum of the probabilities. Consequently, if D1 and D2 are disjoint
subsets of R, then Pr[X ∈D1 ∪D2] = Pr[X ∈D1] + Pr[X ∈D2].
Does this
hold for power? In general the power in the sum of two signals is not the sum of
the individual powers. But if the signals are orthogonal, then their powers do add.
Thus, while Parseval’s theorem will not appear explicitly in our analysis of the PSD,
it is really what makes it all possible. It demonstrates that if D1, D2 ⊂R are disjoint
frequency bands, then the signals X ⋆h1 and X ⋆h2 that result when X is passed
through the ﬁlters of frequency response ˆh1(f) = I{f ∈D1} and ˆh2(f) = I{f ∈D2}
are orthogonal, so their powers add. We will not bother to formulate this result
precisely, because it does not show up in our analysis explicitly, but it is this result
that allows us to deﬁne the power spectral density.
15.3
Deﬁning the Operational PSD
Recall that in (14.14) we deﬁned the power P in a SP

Y (t), t ∈R

as
P = lim
T→∞
1
2T E
5 T
−T
Y 2(t) dt
6
whenever the limit exists. Thus, the power is the limit, as T tends to inﬁnity, of
the ratio of the expected energy in the interval [−T, T] to the interval’s duration 2T.
We deﬁne the operational power spectral density of a stochastic process as follows.
Deﬁnition 15.3.1 (Operational PSD of a Real SP). We say that the continuous-
time real stochastic process

X(t), t ∈R

is of operational power spectral
density SXX if

X(t), t ∈R

is a measurable SP; the mapping SXX : R →R is
integrable and symmetric; and for every stable real ﬁlter of impulse response h ∈L1
the power at the ﬁlter’s output when it is fed

X(t), t ∈R

is given by
Power in X ⋆h =
 ∞
−∞
SXX(f) |ˆh(f)|2 df.
(15.9)
We chose our words very carefully in the above deﬁnition, and, in doing so, we
avoided two issues. The ﬁrst is whether every SP is of some operational PSD. The
answer is “no.” (But most stochastic processes encountered in Digital Communi-
cations are.) The second issue we avoided is the uniqueness issue. Our wording
did not indicate whether a SP could be of two diﬀerent operational PSDs. It turns
out that if a SP is of two diﬀerent operational PSDs, then they must be indistin-
guishable in the sense that they are equal outside a set of frequencies of Lebesgue
measure zero. Consequently, somewhat loosely, we shall speak of the operational
power spectral density of

X(t), t ∈R

even though the uniqueness is only to
within indistinguishability. The uniqueness is established in Corollary 15.3.6 af-
ter we discuss nonnegativity, which is a consequence of the following somewhat
technical lemma.
15.3 Deﬁning the Operational PSD
263
Lemma 15.3.2.
(i) If s: R →R is an integrable function such that
 ∞
−∞
s(f) |ˆh(f)|2 df ≥0
(15.10)
for every integrable complex function h: R →C, then s(f) is nonnegative
at every frequency f outside a set of frequencies of Lebesgue measure zero.
(ii) If s: R →R is a symmetric function (~s = s) such that (15.10) holds for
every integrable real function h: R →R, then s(f) is nonnegative at every
frequency f outside a set of frequencies of Lebesgue measure zero.
Proof. We begin with a proof of Part (i). For every λ > 0 and f0 ∈R deﬁne the
function h: R →C by
h(t) =
1
√
λ
I
'
|t| ≤λ
2
(
ei2πf0t,
t ∈R.
(15.11)
This function is in both L1 and L2.
Since it is in L2, its self-similarity func-
tion Rhh(τ) is deﬁned at every τ ∈R. In fact,
Rhh(τ) =
	
1 −|τ|
λ

I{|τ| ≤λ} ei2πf0τ,
τ ∈R.
(15.12)
And since h ∈L1, it follows from (11.35) that the Fourier Transform of Rhh is the
mapping f 	→|ˆh(f)|2. Consequently, by Proposition 6.2.3 (i) (with the substitution
of ~Rhh for g), the mapping f 	→|ˆh(f)|2 can be expressed as the Inverse Fourier
Transform of ~Rhh. Thus, by (6.8) (with the substitutions of s for x and ~Rhh for g),
 ∞
−∞
s(f) |ˆh(f)|2 df =
 ∞
−∞
ˆs(f) ~R∗
hh(f) df.
(15.13)
It now follows from (15.10), (15.13), and (15.12) that
 λ
−λ
	
1 −|f|
λ

ˆs(f) ei2πf0f df ≥0,
λ > 0, f0 ∈R.
(15.14)
Part (i) now follows from (15.14) and from Theorem 6.2.12 (ii) (with the substitu-
tion of s for x and of f0 for t).
We next turn to Part (ii). For any integrable complex function h: R →C, deﬁne
hR ≜Re(h) and hI ≜Im(h) so
ˆhR(f) =
ˆh(f) + ˆh∗(−f)
2
,
f ∈R,
ˆhI(f) =
ˆh(f) −ˆh∗(−f)
2i
,
f ∈R.
264
Operational Power Spectral Density
Consequently,
ˆhR(f)
2 = 1
4
ˆh(f)
2 +
ˆh(−f)
2 + 2 Re
ˆh(f) ˆh(−f)

,
f ∈R
ˆhI(f)
2 = 1
4
ˆh(f)
2 +
ˆh(−f)
2 −2 Re
ˆh(f) ˆh(−f)

,
f ∈R,
and
ˆhR(f)
2 +
ˆhI(f)
2 = 1
2
ˆh(f)
2 +
ˆh(−f)
2
,
f ∈R.
(15.15)
Applying the lemma’s hypothesis (15.10) to the real functions hR and hI we obtain
0 ≤
 ∞
−∞
s(f)
ˆhR(f)
2 df,
0 ≤
 ∞
−∞
s(f)
ˆhI(f)
2 df,
and thus, upon adding the equations,
0 ≤
 ∞
−∞
s(f)
ˆhR(f)
2 +
ˆhI(f)
2
df
= 1
2
 ∞
−∞
s(f)
ˆh(f)
2 +
ˆh(−f)
2
df
=
 ∞
−∞
s(f) + s(−f)
2
ˆh(f)
2 df
=
 ∞
−∞
s(f)
ˆh(f)
2 df,
(15.16)
where the second equality follows from (15.15); the third by writing the integral
of the sum as a sum of integrals and by changing the integration variable in the
integral involving ˆh(−f); and the last equality from the hypothesis that s is sym-
metric. Since we have established (15.16) for every complex h: R →C, we can
now apply Part (i) to conclude that s(f) is nonnegative at all frequencies f outside
a set of Lebesgue measure zero.
Going back to the deﬁnition of the operational PSD and speciﬁcally to (15.9),
and noting that the power in X ⋆h is nonnegative, we conclude that if X is of
operational PSD SXX, then SXX is symmetric and satisﬁes
 ∞
−∞
SXX(f) |ˆh(f)|2 df ≥0,
(15.17)
for every integrable h: R →R. From this and Lemma 15.3.2 (ii) we conclude:
Corollary 15.3.3 (The Operational PSD Is Nonnegative). If

X(t), t ∈R

is of
operational PSD SXX, then SXX(f) must be nonnegative at all frequencies f outside
a set of frequencies of Lebesgue measure zero.
15.3 Deﬁning the Operational PSD
265
It follows from the corollary that if

X(t)

is of operational PSD SXX then the
function
S(f) =

SXX(f)
if SXX(f) ≥0,
0
otherwise
(which results when we change the values of SXX to zero at those frequencies where
it is negative) is indistinguishable from SXX. Consequently, (15.9) must also hold
when we replace SXX with S. Thus,

X(t)

is also of operational PSD S, which
is nonnegative at all frequencies.
Consequently, there is no harm in adding to
Deﬁnition 15.3.1 the requirement that SXX be a nonnegative function:
Remark 15.3.4. Henceforth we require from every operational PSD that it be
nonnegative.
Next, as promised, we address the uniqueness of the operational PSD. For that
we will need the following lemma, which is similar to Lemma 15.3.2 but with the
greater-or-equal sign in (15.10) replaced with equality.
Lemma 15.3.5.
(i) If s: R →R is an integrable function such that
 ∞
−∞
s(f) |ˆh(f)|2 df = 0
(15.18)
for every integrable complex function h: R →C, then s(f) is zero at all
frequencies f outside a set of Lebesgue measure zero.
(ii) If s: R →R is a symmetric function (~s = s) such that (15.18) holds for
every integrable real function h: R →R, then s(f) is zero at all frequencies
f outside a set of Lebesgue measure zero.
Proof. The equality sign in (15.18) implies that if s satisﬁes the hypotheses of the
lemma then both s and −s satisfy the hypotheses of Lemma 15.3.2. Consequently,
by the latter lemma, s as well as −s is nonnegative outside an exception set of
frequencies of Lebesgue measure zero. Outside the union of their respective excep-
tion sets—a union which is also of Lebesgue measure zero (Exercise 2.11)—both s
and −s are nonnegative, and s must therefore be zero.
Corollary 15.3.6 (Uniqueness of the Operational PSD). If both SXX and S′
XX are
operational PSDs of some SP, then the set of frequencies at which they diﬀer is of
Lebesgue measure zero.
Proof. Apply Lemma 15.3.5 (ii) to the function s: f 	→SXX(f) −S′
XX(f).
As noted above, we make here no general claims about the existence of opera-
tional PSDs. Under certain restrictions that are made precise in Section 15.5, the
operational PSD is deﬁned for PAM signals. And by Proposition 25.7.1 and The-
orem 25.14.1, the operational PSD always exists for measurable, centered, WSS
stochastic processes of continuous and integrable autocovariance functions.
266
Operational Power Spectral Density
Deﬁnition 15.3.7 (Bandlimited Stochastic Processes). A SP

X(t), t ∈R

of
operational PSD SXX is said to be bandlimited to W Hz if, except on a set of
frequencies of Lebesgue measure zero, SXX(f) is zero whenever |f| > W.
The smallest W to which

X(t), t ∈R

is bandlimited is its bandwidth.
15.4
The Operational PSD of Real PAM Signals
Computing the operational PSD of PAM signals is much easier than one might
expect. This is because, as we next show, passing a PAM signal of pulse shape g
through a stable ﬁlter of impulse response h is tantamount to changing its pulse
shape from g to g ⋆h:
	
σ 	→A

ℓ
Xℓg(σ −ℓTs)

⋆h

(t) = A

ℓ
Xℓ(g ⋆h)(t −ℓTs),
t ∈R. (15.19)
(For a formal statement of this result, see Corollary 18.6.2, which also addresses the
diﬃculty that arises when the sum is inﬁnite.) Consequently, if one can compute
the power in a PAM signal of arbitrary pulse shape (as explained in Chapter 14),
then one can also compute the power in a ﬁltered PAM signal.
That ﬁltering a PAM signal is tantamount to convolving its pulse shape with the
impulse response follows from two properties of the convolution: that it is linear
(α u + β v) ⋆h = α u ⋆h + β v ⋆h
and that convolving a delayed version of a signal with h is equivalent to convolving
the original signal and delaying the result

σ 	→u(σ −t0)

⋆h

(t) = (u ⋆h)(t −t0),
t, t0 ∈R.
Indeed, if X is the PAM signal
X(t) = A
∞

ℓ=−∞
Xℓg(t −ℓTs),
(15.20)
then (15.19) follows from the calculation

X ⋆h

(t) =
	
σ 	→A
∞

ℓ=−∞
Xℓg(σ −ℓTs)

⋆h

(t)
= A
∞

ℓ=−∞
Xℓ
 ∞
−∞
h(s) g(t −s −ℓTs) ds
= A
∞

ℓ=−∞
Xℓ(g ⋆h)(t −ℓTs),
t ∈R.
(15.21)
We are now ready to apply the results of Chapter 14 on the power in PAM signals
to study the power in ﬁltered PAM signals and hence to derive the operational
PSD of PAM signals. We will not treat the case discussed in Section 14.5.3 where
the only assumption is that the time shifts of the pulse shape by integer multiples
of Ts are orthonormal, because this orthonormality is typically lost under ﬁltering.
15.4 The Operational PSD of Real PAM Signals
267
15.4.1
The Symbols Are Centered, Uncorrelated, and of Equal
Variance
We begin with the case where the symbols

Xℓ, ℓ∈Z

are of zero mean, uncor-
related, and of equal variance σ2
X. As in (15.20) we denote the PAM signal by

X(t), t ∈R

and study its operational PSD by studying the power in X ⋆h.
Using (15.21) we obtain that X⋆h is the PAM signal X but with the pulse shape g
replaced by g ⋆h. Consequently, using Expression (14.33) for the power in PAM
with zero-mean, uncorrelated, variance-σ2
X symbols, we obtain that the power in
X ⋆h is given by
Power in X ⋆h = A2
Ts
σ2
X ∥g ⋆h∥2
2
= A2σ2
X
Ts
 ∞
−∞
|ˆg(f)|2 |ˆh(f)|2 df
=
 ∞
−∞
	A2σ2
X
Ts
|ˆg(f)|2



SXX(f)

|ˆh(f)|2 df,
(15.22)
where the ﬁrst equality follows from (14.33) applied to the PAM signal of pulse
shape g⋆h; the second follows from Parseval’s Theorem by noting that the Fourier
Transform of a convolution of two signals is the product of their Fourier Transforms;
and where the third equality follows by rearranging terms. From (15.22) and from
the fact that f 	→|ˆg(f)|2 is a symmetric function (because g is real), it follows
that the operational PSD of the PAM signal

X(t), t ∈R

when

Xℓ, ℓ∈Z

are
zero-mean, uncorrelated, and of variance σ2
X is given by
SXX(f) = A2σ2
X
Ts
|ˆg(f)|2,
f ∈R.
(15.23)
15.4.2

Xℓ

Is Centered and WSS
The more general case where the symbols

Xℓ, ℓ∈Z

are not necessarily un-
correlated but form a centered, WSS, discrete-time SP can be treated with the
same ease via (14.31) or (14.32). As above, passing X through a ﬁlter of impulse
response h results in a PAM signal with identical symbols but with pulse shape
g ⋆h. Consequently, the resulting power can be computed by substituting g ⋆h
for g in (14.32) to obtain that the power in X ⋆h is given by
Power in X ⋆h =
 ∞
−∞
	A2
Ts
∞

m=−∞
KXX(m) ei2πfmTs |ˆg(f)|2



SXX(f)

|ˆh(f)|2 df,
268
Operational Power Spectral Density
where again we are using the fact that the FT of g ⋆h is f 	→ˆg(f) ˆh(f). The
operational PSD is thus
SXX(f) = A2
Ts
∞

m=−∞
KXX(m) ei2πfmTs |ˆg(f)|2,
f ∈R,
(15.24)
because, as we next argue, the RHS of the above is a symmetric function of f.
This symmetry follows from the symmetry of |ˆg(·)| (because the pulse shape g
is real) and from the symmetry of the autocovariance function KXX (because the
symbols

Xℓ, ℓ∈Z

are real; see (13.12)). Note that (15.24) reduces to (15.23) if
KXX(m) = σ2
X I{m = 0}.
15.4.3
The Operational PSD in Bi-Inﬁnite Block-Mode
We now assume, as in Section 14.5.2, that the (K, N) binary-to-reals block encoder
enc: {0, 1}K →RN is used in bi-inﬁnite block encoding mode to map the bi-
inﬁnite IID random bits

Dj, j ∈Z

to the bi-inﬁnite sequence of real numbers

Xℓ, ℓ∈Z

, and that the transmitted signal is
X(t) = A
∞

ℓ=−∞
Xℓg(t −ℓTs),
(15.25)
where Ts > 0 is the baud period, and where g(·) is a pulse shape satisfying the
decay condition (14.17). We do not assume that the time shifts of g(·) by integer
multiples of Ts are orthogonal, or that the symbols

Xℓ, ℓ∈Z

are uncorrelated.
We do, however, continue to assume that the N-tuple enc(D1, . . . , DK) is of zero
mean whenever D1, . . . , DK are IID random bits.
We shall determine the operational PSD of X by computing the power of the signal
that results when X is fed to a stable ﬁlter of impulse response h. As before, we note
that feeding X through a ﬁlter of impulse response h is tantamount to replacing
its pulse shape g by g ⋆h. The power of this output signal can be thus computed
from our expression for the power in bi-inﬁnite block encoding with PAM signaling
(14.38) but with the pulse shape being g ⋆h and hence of FT f 	→ˆg(f) ˆh(f):
Power in X ⋆h =
 ∞
−∞
	 A2
NTs
N

ℓ=1
N

ℓ′=1
E[XℓXℓ′] ei2πf(ℓ−ℓ′)Ts |ˆg(f)|2



SXX(f)

|ˆh(f)|2 df.
As we next show, the underbraced term is a symmetric function of f, and we thus
conclude that the PSD of X is:
SXX(f) = A2
NTs
N

ℓ=1
N

ℓ′=1
E[XℓXℓ′] ei2πf(ℓ−ℓ′)Ts |ˆg(f)|2,
f ∈R.
(15.26)
To see that the RHS of (15.26) is a symmetric function of f, use the identities
N

ℓ=1
N

ℓ′=1
aℓ,ℓ′ =
N

ℓ=1
aℓ,ℓ+
N

ℓ=2
ℓ−1

ℓ′=1
(aℓ,ℓ′ + aℓ′,ℓ)
15.4 The Operational PSD of Real PAM Signals
269
and E[XℓXℓ′] = E[Xℓ′Xℓ] to rewrite the RHS of (15.26) in the symmetric form
A2
NTs
- N

ℓ=1
E

X2
ℓ

+
N

ℓ=2
ℓ−1

ℓ′=1
2 E[XℓXℓ′] cos

2πf(ℓ−ℓ′)Ts

.
|ˆg(f)|2.
From (15.26) we obtain:
Theorem 15.4.1 (The Bandwidth of PAM Is that of the Pulse Shape). Suppose
that the operational PSD in bi-inﬁnite block-mode of a PAM signal

X(t)

is as
given in (15.26), e.g., that the conditions of Theorem 15.5.2 ahead are satisﬁed.
Further assume
A > 0,
N

ℓ=1
E

X2
ℓ

> 0,
(15.27)
e.g., that

X(t)

is not deterministically zero. Then the bandwidth of the SP

X(t)

is equal to the bandwidth of the pulse shape g.
Proof. The proof is very similar to the proof of Lemma 10.10.1. If g is bandlimited
to W Hz, then so is

X(t)

, because, by (15.26),

ˆg(f) = 0

=⇒

SXX(f) = 0

.
We next complete the proof by showing that there are at most a countable number
of frequencies f such that SXX(f) = 0 but ˆg(f) ̸= 0.
From (15.26) it follows
that to show this it suﬃces to show that there are at most a countable number of
frequencies f such that σ(f) = 0, where
σ(f) ≜A2
NTs
N

ℓ=1
N

ℓ′=1
E[XℓXℓ′] ei2πf(ℓ−ℓ′)Ts
=
N−1

m=−N+1
γm ei2πfmTs
=
N−1

m=−N+1
γm zm 
z=ei2πfTs ,
(15.28)
and
γm = A2
NTs
min{N,N+m}

ℓ=max{1,m+1}
E[XℓXℓ−m] ,
m ∈{−N + 1, . . . , N −1}.
(15.29)
It follows from (15.28) that σ(f) is zero if, and only if, ei2πfTs is a root of the
mapping
z 	→
N−1

m=−N+1
γm zm.
270
Operational Power Spectral Density
Since ei2πfTs is of unit magnitude, it follows that σ(f) is zero if, and only if, ei2πfTs
is a root of the polynomial
z 	→
2N−2

ν=0
γν−N+1 zν.
(15.30)
From (15.29) and (15.27) it follows that γ0 > 0, so the polynomial in (15.30) is
not zero. Consequently, since its degree is at most 2N −2, it has at most 2N −2
distinct roots and, a fortiori, at most 2N −2 distinct roots of unit magnitude.
Denote these roots by
eiθ1, . . . , eiθd,
where d ≤2N −2 and θ1, . . . , θd ∈[−π, π). Since f satisﬁes ei2πfTs = eiθ if, and
only if,
f =
θ
2πTs
+ η
Ts
for some η ∈Z, we conclude that the set of frequencies f satisfying σ(f) = 0 is the
set
1 θ1
2πTs
+ η
Ts
: η ∈Z
2
∪· · · ∪
1 θd
2πTs
+ η
Ts
: η ∈Z
2
,
and is thus countable. (The union of a ﬁnite (or countable) number of countable
sets is countable.)
15.5
A More Formal Account
In this section we shall give a more formal account of the power at the output of
a stable ﬁlter that is fed a PAM signal. There are two approaches to this. The
ﬁrst is based on carefully justifying the steps in our informal derivation.4
This
approach is pursued in Section 18.6.5, where the results are generalized to complex
pulse shapes and complex symbols. The second approach is to convert the problem
into one about WSS stochastic processes and to then rely heavily on Sections 25.13
and 25.14 on the ﬁltering of WSS stochastic processes and, in particular, on the
Wiener-Khinchin Theorem (Theorem 25.14.1). For the beneﬁt of readers who have
already encountered the Wiener-Khinchin Theorem we follow this latter approach
here. We ask the readers to note that the Wiener-Khinchin Theorem is not directly
applicable here because the PAM signal is not WSS. A “stationarization argument”
is thus needed.
The key results of this section are the following two theorems.
Theorem 15.5.1. Consider the setup of Theorem 14.6.4 with the additional as-
sumption that the autocovariance function KXX of

Xℓ

is absolutely summable:
∞

m=−∞
KXX(m)
 < ∞.
(15.31)
Let h ∈L1 be the impulse response of a stable real ﬁlter. Then:
4The main diﬃculties in the justiﬁcation are in making (15.19) rigorous and in controlling
the decay of g ⋆h for arbitrary h ∈L1 .
15.5 A More Formal Account
271
(i) The PAM signal
X: (ω, t) 	→A
∞

ℓ=−∞
Xℓ(ω) g(t −ℓTs)
(15.32)
is bounded in the sense that there exists a constant Γ such that
|X(ω, t)| < Γ,

ω ∈Ω, t ∈R

.
(15.33)
(ii) For every ω ∈Ω the convolution of the sample-path t 	→X(ω, t) with h is
deﬁned at every epoch.
(iii) The stochastic process
(ω, t) 	→
 ∞
−∞
x(ω, σ) h(t −σ) dσ,

ω ∈Ω, t ∈R

(15.34)
that results when the sample-paths of X are convolved with h is a measurable
stochastic process of power
P =
 ∞
−∞
-
A2
Ts
∞

m=−∞
KXX(m) ei2πfmTs |ˆg(f)|2
.
|ˆh(f)|2 df.
(15.35)
Theorem 15.5.2. Consider the setup of Theorem 14.6.5. Let h ∈L1 be the impulse
response of a real stable ﬁlter. Then:
(i) The sample-paths of the PAM stochastic process
X: (ω, t) 	→A
∞

ℓ=−∞
Xℓ(ω) g(t −ℓTs)
(15.36)
are bounded in the sense of (15.33).
(ii) For every ω ∈Ω the convolution of the sample-path t 	→X(ω, t) and h is
deﬁned at every epoch.
(iii) The stochastic process

X(t), t ∈R

⋆h that results when the sample-paths
of X are convolved with h is a measurable stochastic process of power
P =
 ∞
−∞
-
A2
NTs
N

ℓ=1
N

ℓ′=1
E[XℓXℓ′] ei2πf(ℓ−ℓ′)Ts |ˆg(f)|2
.
|ˆh(f)|2 df,
(15.37)
where

X1, . . . , XN

= enc

D1, . . . , DK

, and where D1, . . . , DK are IID ran-
dom bits.
Proof of Theorem 15.5.1. Part (i) is a consequence of the assumption that

Xℓ

is bounded in the sense of (14.16) and that the pulse shape g decays faster than 1/t
in the sense of (14.17).
272
Operational Power Spectral Density
Part (ii) is a consequence of the fact that the convolution of a bounded function
with an integrable function is deﬁned at every epoch; see Section 5.5.
We next turn to Part (iii). The proof of the measurability of the convolution of

X(t), t ∈R

with h is a bit technical. It is very similar to the proof of Theo-
rem 25.13.2 (i). As in that proof, we ﬁrst note that it suﬃces to prove the result
for functions h that are Borel measurable; the extension to Lebesgue measurable
functions will then follow by approximating h by a Borel measurable function
that diﬀers from it on a set of Lebesgue measure zero (Rudin, 1987, Chapter 8,
Lemma 1 or Chapter 2, Exercise 14) and by then noting that the convolution of
t 	→X(ω, t) with h is unaltered when h is replaced by a function that diﬀers from
it on a set of Lebesgue measure zero. We thus assume that h is Borel measur-
able. Consequently, the mapping from R2 to R deﬁned by (t, σ) 	→h(t −σ) is also
Borel measurable, because it is the composition of the continuous (and hence Borel
measurable) mapping (t, σ) 	→t −σ with the Borel measurable mapping t 	→h(t).
As in the proof of Theorem 25.13.2, we prove the measurability of the convolution
of

X(t), t ∈R

with h by proving the measurability of the mapping deﬁned by
(ω, t) 	→(1 + t2)−1  ∞
−∞X(ω, σ) h(t −σ) dσ. To this end we study the function

(ω, t), σ

	→X(ω, σ) h(t −σ)
1 + t2
,

(ω, t) ∈Ω × R, σ ∈R

.
(15.38)
This function is measurable because, as noted above, (t, σ) 	→h(t −σ) is measur-
able; because, by Proposition 14.6.2,

X(t), t ∈R

is measurable; and because the
product of Borel measurable functions is Borel measurable (Rudin, 1987, Chap-
ter 1, Section 1.9 (c)). Moreover, using (15.33) and Fubini’s Theorem it can be
readily veriﬁed that this function is integrable. Using Fubini’s Theorem again, we
conclude that the function
(ω, t) 	→
1
1 + t2
 ∞
−∞
X(ω, σ) h(t −σ) dσ
is measurable. Consequently, so is X ⋆h.
To conclude the proof we now need to compute the power in the measurable (non-
stationary) SP X ⋆h. This will be done in a roundabout way. We shall ﬁrst deﬁne
a new SP X′. This SP is centered, measurable, and WSS so the power in X′⋆h can
be computed using Theorem 25.14.1. We shall then show that the powers of X ⋆h
and X′ ⋆h are equal and hence that from the power in X′ ⋆h we can immediately
obtain the power in X ⋆h.
We begin by deﬁning the SP

X′(t), t ∈R

as
X′(t) = X(t + S),
t ∈R,
(15.39a)
where S is independent of

X(t)

and uniformly distributed over the interval [0, Ts],
S ∼U ([0, Ts]) .
(15.39b)
That

X′(t)

is centered follows from the calculation
E[X′(t)] = E[X(t + S)]
15.5 A More Formal Account
273
=
 Ts
0
1
Ts
E[X(t + s)] ds
= 0,
where the ﬁrst equality follows from the deﬁnition of

X′(t)

; the second from the
independence of

X(t)

and S and from the speciﬁc form of the density of S; and
the third because

X(t)

is centered. That

X′(t)

is measurable follows because
the mapping

(ω, s), t

	→X(ω, t + s) can be written as the composition of the
mapping

(ω, s), t

	→(ω, t + s) with the mapping (ω, t) 	→X(ω, t). And that it is
WSS follows from the calculation
E[X′(t) X′(t + τ)]
= E[X(t + S) X(t + S + τ)]
= 1
Ts
 Ts
0
E[X(t + s) X(t + s + τ)] ds
= A2
Ts
 Ts
0
E

∞

ℓ=−∞
Xℓg(t + s −ℓTs)
∞

ℓ′=−∞
Xℓ′ g(t + s + τ −ℓ′Ts)

ds
= A2
Ts
∞

ℓ=−∞
∞

ℓ′=−∞
E[XℓXℓ′]
 Ts
0
g(t + s −ℓTs) g(t + s + τ −ℓ′Ts) ds
= A2
Ts
∞

ℓ=−∞
∞

ℓ′=−∞
KXX(ℓ−ℓ′)
 Ts
0
g(t + s −ℓTs) g(t + s + τ −ℓ′Ts) ds
= A2
Ts
∞

ℓ=−∞
∞

m=−∞
KXX(m)
 Ts
0
g

t + s −ℓTs

g

t + s + τ −(ℓ−m)Ts

ds
= A2
Ts
∞

m=−∞
KXX(m)
∞

ℓ=−∞
 −ℓTs+Ts+t
−ℓTs+t
g(ξ) g(ξ + τ + mTs) dξ
= A2
Ts
∞

m=−∞
KXX(m)
 ∞
−∞
g(ξ) g(ξ + τ + mTs) dξ
= A2
Ts
∞

m=−∞
KXX(m) Rgg(mTs + τ),
τ, t ∈R.
(15.40)
Note that (15.40) also shows that

X′(t)

is of PSD (as deﬁned in Deﬁnition 25.7.2)
SX′X′(f) = A2
Ts
∞

m=−∞
KXX(m) ei2πfmTs |ˆg(f)|2,
f ∈R,
(15.41)
which is integrable by the absolute summability of KXX.
Deﬁning

Y ′(t), t ∈R

to be

X′(t), t ∈R

⋆h we can now use Theorem 25.14.1
to compute the power in

Y ′(t), t ∈R

:
lim
T→∞
1
2TE
5 T
−T

Y ′(t)
2 dt
6
=
 ∞
−∞
	A2
Ts
∞

m=−∞
KXX(m) ei2πfmTs|ˆg(f)|2

|ˆh(f)|2 df.
274
Operational Power Spectral Density
To conclude the proof we next show that the power in Y is the same as the power
in Y′. To that end we ﬁrst note that from (15.39a) it follows that

X′ ⋆h

(ω, s), t

=

X ⋆h

(ω, t + s),

ω ∈Ω, 0 ≤s ≤Ts, t ∈R

,
i.e., that
Y ′
(ω, s), t

= Y (ω, t + s),

ω ∈Ω, 0 ≤s ≤Ts, t ∈R

.
(15.42)
It thus follows that
 T
−T
Y 2(ω, t) dt ≤
 T
−T−Ts

Y ′((ω, s), t)
2 dt,

ω ∈Ω, 0 ≤s ≤Ts, t ∈R

,
(15.43)
because
 T
−T−Ts

Y ′((ω, s), t)
2 dt =
 T
−T−Ts
Y 2(ω, t + s) dt
=
 T+s
−T−Ts+s
Y 2(ω, σ) dσ
≥
 T
−T
Y 2(ω, σ) dσ,
0 ≤s ≤Ts,
where the equality in the ﬁrst line follows from (15.42); the equality in the second
line from the substitution σ ≜t+s; and the ﬁnal inequality from the nonnegativity
of the integrand and because 0 ≤s ≤Ts.
Similarly,
 T
−T
Y 2(ω, t) dt ≥
 T−Ts
−T

Y ′((ω, s), t)
2 dt,

ω ∈Ω, 0 ≤s ≤Ts, t ∈R

, (15.44)
because
 T−Ts
−T

Y ′((ω, s), t)
2 dt =
 T−Ts
−T
Y 2(ω, t + s) dt
=
 T−Ts+s
−T+s
Y 2(ω, σ) dσ
≤
 T
−T
Y 2(ω, σ) dσ,
0 ≤s ≤Ts.
Combining (15.43) and (15.44) and using the nonnegativity of the integrand we
obtain that for every ω ∈Ω and s ∈[0, Ts]
 T−Ts
−T+Ts

Y ′((ω, s), t)
2 dt ≤
 T
−T
Y 2(ω, σ) dσ ≤
 T+Ts
−T−Ts

Y ′((ω, s), t)
2 dt.
(15.45)
15.5 A More Formal Account
275
Dividing by 2T and taking expectations we obtain
2T −2Ts
2T
1
2T −2Ts
E
5 T−Ts
−T+Ts

Y ′(t)
2dt
6
≤1
2T E
5 T
−T
Y 2(σ) dσ
6
≤
2T + 2Ts
2T
1
2T + 2Ts
E
5 T+Ts
−T−Ts

Y ′(t)
2 dt
6
,
(15.46)
from which the equality between the power in Y′ and in Y follows by letting T
tend to inﬁnity and using the Sandwich Theorem.
Proof of Theorem 15.5.2. The proof of Theorem 15.5.2 is very similar to the proof
of Theorem 15.5.1, so most of the details will be omitted. The main diﬀerence is
that the process

X′(t), t ∈R

is now deﬁned as
X′(t) = X(t + S)
where the random variable S is now uniformly distributed over the interval [0, NTs],
S ∼U ([0, NTs]) .
With this deﬁnition, the autocovariance of

X′(t), t ∈R

can be computed as
KX′X′(τ)
= E

X(t + S) X(t + τ + S)

=
1
NTs
 NTs
0
E

X(t + s) X(t + τ + s)

ds
= A2
NTs
E
5 NTs
0
	
∞

ν=−∞
u

Xν, t + s −νNTs

∞

ν′=−∞
u

Xν′, t + τ + s −ν′NTs

ds
6
= A2
NTs
 NTs
0
∞

ν=−∞
∞

ν′=−∞
E

u

Xν, t + s −νNTs

u

Xν′, t + τ + s −ν′NTs

ds
= A2
NTs
 NTs
0
∞

ν=−∞
E

u

Xν, t + s −νNTs

u

Xν, t + τ + s −νNTs

ds
= A2
NTs
 NTs
0
∞

ν=−∞
E

u

X0, t + s −νNTs

u

X0, t + τ + s −νNTs

ds
= A2
NTs
 ∞
−∞
E

u

X0, ξ

u

X0, ξ + τ

dξ
= A2
NTs
 ∞
−∞
E
5 N

η=1
Xη g(ξ −ηTs)
N

η′=1
Xη′ g(ξ + τ −η′Ts)
6
dξ
= A2
NTs
N

η=1
N

η′=1
E

XηXη′
Rgg

τ + (η −η′)Ts

,
t, τ ∈R,
276
Operational Power Spectral Density
where the third equality follows from (14.36), (14.39), and (14.40); the ﬁfth follows
from (14.43); the sixth because the N-tuples

Xη, η ∈Z

are IID; the seventh
by deﬁning ξ = t + s, swapping the integration and summation, and expressing
the integral over the real line as an inﬁnite sum of the integrals over the disjoint
intervals

t −ν N Ts, t −(ν −1) N Ts)

with ν ∈Z; the eighth by the deﬁnition
(14.40) of the function u(·); and the ﬁnal equality by swapping the summations
and the expectation.
The process

X′(t)

is thus a WSS process of PSD (as deﬁned in Deﬁnition 25.7.2)
SX′X′(f) = A2
NTs
N

ℓ=1
N

ℓ′=1
E

XℓXℓ′
ei2πf(ℓ−ℓ′)Ts |ˆg(f)|2.
(15.47)
The proof proceeds now along the same lines as the proof of Theorem 15.5.1.
15.6
Operational PSD and Average Autocovariance Function
The operational PSD can often be calculated from the average autocovariance
function, which we deﬁne and study next. The main result is Theorem 15.6.6,
which provides an operational meaning to the average autocovariance function and
which relates it to the operational PSD. Its proof is somewhat technical and is
recommended for the more advanced readers. Those should read Chapter 25 ﬁrst.
Throughout this section we restrict ourselves to measurable stochastic processes
that are centered and of bounded variance, where a SP

X(t)

is said to be of
bounded variance if there exists some constant γ such that at every epoch t ∈R
the variance of the RV X(t) is bounded by γ:
Var[X(t)] ≤γ,
t ∈R.
(15.48)
From the Covariance Inequality (Corollary 3.5.2)
Cov[X(t), X(t′)]
 ≤

Var[X(t)] Var[X(t′)],
t, t′ ∈R,
(15.49)
so

Var[X(t)] ≤γ,
t ∈R

=⇒
Cov[X(t), X(t′)]
 ≤γ,
t, t′ ∈R

.
(15.50)
We focus on stochastic processes for which the limit deﬁning ¯KXX(τ) for every
τ ∈R as
¯KXX(τ) ≜lim
T→∞
1
2T
 T
−T
Cov[X(t), X(t + τ)] dt,
τ ∈R
(15.51)
exists and is ﬁnite. This is clearly the case when the integrand does not depend
on t,5 but there are many other cases of interest.
5The integrand does not depend on t whenever
X(t)

is wide-sense stationary; see Deﬁni-
tion 25.4.2 in Chapter 25.
15.6 Operational PSD and Average Autocovariance Function
277
Deﬁnition 15.6.1 (Average Autocovariance Function). We say that a SP

X(t)

is of average autocovariance function ¯KXX if it is measurable, of bounded
variance, and
lim
T→∞
1
2T
 T
−T
Cov[X(t), X(t + τ)] dt = ¯KXX(τ),
τ ∈R.
(15.52)
Substituting 0 for τ in (15.52) and recalling the deﬁnition of power (14.14), we
obtain:
Note 15.6.2 (The Power and the Average Autocovariance Function). If

X(t)

is a centered SP of power P and of average autocovariance function ¯KXX, then
P = ¯KXX(0).
(15.53)
An example of a SP that has an average autocovariance function is a PAM signal:
Example 15.6.3 (The Average Autocovariance Function of a PAM Signal). Con-
sider the PAM signal
X(t) = A
∞

ℓ=−∞
Xℓg(t −ℓTs)
(15.54)
that we encountered in Section 15.4.1. Here A is a positive constant; {Xℓ} are cen-
tered, bounded, uncorrelated, and of variance σ2
X; the pulse shape g is measurable
and satisﬁes the decay condition (14.17); and the baud period Ts is positive. Since
the symbols {Xℓ} are centered, so is the SP

X(t)

. We claim that

X(t)

has the
average autocovariance function
¯KXX(τ) = A2σ2
X
Ts
Rgg(τ),
τ ∈R.
(15.55)
Proof. For every t, τ ∈R
E

X(t) X(t + τ)

= E

A
	
ℓ
Xℓg(t −ℓTs)

	
A

ℓ′
Xℓ′ g(t + τ −ℓ′Ts)


= A2 
ℓ

ℓ′
E

XℓXℓ′
g(t −ℓTs) g(t + τ −ℓ′Ts)
= A2σ2
X

ℓ
g(t −ℓTs) g(t + τ −ℓTs),
t, τ ∈R,
(15.56)
where in the last equality we used the fact that {Xℓ} are centered, uncorrelated,
and of variance σ2
X. To study the limit on the RHS of (15.51), we ﬁrst consider
the integral over an arbitrary length-Ts interval [σ, σ + Ts]. Using (15.56),
 σ+Ts
σ
E

X(t) X(t + τ)

dt = A2σ2
X
 σ+Ts
σ

ℓ
g(t −ℓTs) g(t + τ −ℓTs) dt
= A2σ2
X

ℓ
 σ+Ts
σ
g(t −ℓTs) g(t + τ −ℓTs) dt
278
Operational Power Spectral Density
= A2σ2
X

ℓ
 σ−ℓTs+Ts
σ−ℓTs
g(α) g(α + τ) dα
= A2σ2
X
 ∞
−∞
g(α) g(α + τ) dα
= A2σ2
X Rgg(τ),
t, τ, σ ∈R,
(15.57)
where in the third equality we have substituted α for t −ℓTs. Since this holds
for every length-Ts interval, we can establish (15.55) using the same argument we
employed in Section 14.5.1 when we computed the power in PAM; see (14.28)–
(14.30).
The main result of this section is that if ¯KXX is integrable, then its FT is the
operational PSD of

X(t)

. This shows that, when

X(t)

has an integrable average
autocovariance function, our deﬁnition of the operational PSD and the deﬁnition in
the literature of the operational PSD as the FT of ¯KXX (Yaglom, 1986, Chapter 4,
Section 26.6) coincide. It also provides a method for computing the operational
PSD: compute ¯KXX and take its FT. In the above example, this allows us to recover
the operational PSD of (15.23) by taking the FT of the RHS of (15.55).
To prove this result we shall need a technical lemma and a proposition describing
the key properties of the average autocovariance function. Readers who intend to
skip the theorem’s proof can also skip these results and proceed directly to the
theorem.
The lemma deals with the integration interval in the deﬁnition of ¯KXX (15.51).
The integral in (15.51) is over the interval [−T, T ], which is symmetric around the
origin. The lemma shows that a constant oﬀset (that does not depend on T) in the
range of integration does not alter the limit:
Lemma 15.6.4. If the SP

X(t)

is of average autocovariance function ¯KXX, then
for every τ ∈R and α ∈R
lim
T→∞
1
2T
 T−α
−T−α
Cov

X(t), X(t + τ)

dt = ¯KXX(τ).
(15.58)
Proof. Since

X(t)

is of average autocovariance function ¯KXX, it is a fortiori of
bounded variance, and there exists some constant γ for which (15.48) holds. It
thus follows from (15.50) that
Cov

X(t), X(t′)
 ≤γ,
t, t′ ∈R.
(15.59)
With the aid of this inequality we are now ready to prove the lemma:
1
2T
 T−α
−T−α
Cov

X(t), X(t + τ)

dt
= 1
2T
 T
−T
Cov

X(t), X(t + τ)

dt + 1
2T
- −T
−T−α
+
 T−α
T
.
Cov

X(t), X(t + τ)

dt,
where, as T tends to inﬁnity, the ﬁrst term on the RHS converges to ¯KXX(τ) by
the deﬁnition of ¯KXX(τ) (15.52), and where the second term on the RHS converges
15.6 Operational PSD and Average Autocovariance Function
279
to zero because the integrals over these short intervals (compared to T) can be
bounded by
1
2T

- −T
−T−α
+
 T−α
T
.
Cov

X(t), X(t + τ)

dt
 ≤|α| γ
T
using (15.59) (and Proposition 2.4.1).
In the next proposition we shall need the concept of a positive deﬁnite function,
which is discussed in Appendix C.
Proposition 15.6.5 (Properties of ¯KXX). If the SP

X(t)

is of average autoco-
variance function ¯KXX, then ¯KXX is a measurable, real, positive deﬁnite function.
Consequently
¯KXX(τ)
 ≤¯KXX(0),
τ ∈R;
(15.60)
¯KXX(−τ) = ¯KXX(τ),
τ ∈R;
(15.61)
and
n

ν=1
n

ν′=1
αν αν′ ¯KXX(tν −tν′) ≥0,
(15.62)
for every n ∈N, epochs t1, . . . , tn ∈R, and real coeﬃcients α1, . . . , αn ∈R.
Proof. The measurability of ¯KXX follows (using Fubini’s Theorem) from the mea-
surability of the SP

X(t)

and from the fact that the limit of measurable functions
is measurable. The details are omitted.
We shall next establish (15.61) and that (15.62) holds for every n ∈N, epochs
t1, . . . , tn ∈R, and real coeﬃcients α1, . . . , αn ∈R. From this it will then follow
that ¯KXX is positive deﬁnite using Proposition C.2. Proposition C.3 (iii) will then
establish (15.60).
To prove the symmetry (15.61) we note that for every τ ∈R,
¯KXX(−τ) = lim
T→∞
1
2T
 T
−T
Cov

X(t), X(t −τ)

dt
= lim
T→∞
1
2T
 T−τ
−T−τ
Cov

X(s + τ), X(s)

ds
= lim
T→∞
1
2T
 T−τ
−T−τ
Cov

X(s), X(s + τ)

ds
= ¯KXX(τ),
where the second equality follows by deﬁning s as t−τ and the last equality follows
from Lemma 15.6.4 by replacing α there with τ.
280
Operational Power Spectral Density
As to (15.62), for all epochs t1, . . . , tn ∈R, all real coeﬃcients α1, . . . , αn ∈R and
any T > 0, the nonnegativity of the variance implies
0 ≤1
2T
 T
−T
Var

n

ν=1
αν X(t + tν)

dt
= 1
2T
 T
−T
n

ν=1
n

ν′=1
αν αν′ Cov

X(t + tν′), X(t + tν)

dt
=
n

ν=1
n

ν′=1
αν αν′ 1
2T
 T
−T
Cov

X(t + tν′), X(t + tν)

dt
=
n

ν=1
n

ν′=1
αν αν′ 1
2T
 T+tν′
−T+tν′
Cov

X(s), X(s + tν −tν′)

ds,
where in the last equality we deﬁned s to be t + tν′. Letting T tend to inﬁnity
and using Lemma 15.6.4 (by replacing α there with −tν′ and τ there with tν −tν′)
establishes (15.62).
The key parts of the next theorem are Part (i), which provides an operational
meaning to the average autocovariance function and Part (iii) that relates it to the
operational PSD; the other parts are technical.
Theorem 15.6.6 (The Operational PSD and Average Autocovariance Function).
Let

X(t)

be a centered SP of average autocovariance function ¯KXX.
(i) If h is the impulse response of some stable ﬁlter, then
Power in X ⋆h =
 ∞
−∞
¯KXX(σ) Rhh(σ) dσ,
h ∈L1.
(15.63)
(ii) If ¯KXX and the IFT of some integrable function S(·) diﬀer on a set of Lebesgue
measure zero, and if h ∈L1, then
Power in X ⋆h =
 ∞
−∞
S(f)
ˆh(f)
2 df,
h ∈L1.
(15.64)
(iii) If ¯KXX is integrable, then its FT is the operational PSD of

X(t)

:
ˆ¯KXX = SXX.
(15.65)
(iv) If

X(t)

is of operational PSD SXX, then, outside a set of Lebesgue measure
zero, ¯KXX equals the IFT of SXX.
Proof. Since

X(t)

is of average autocovariance function ¯KXX, it is a fortiori of
bounded variance, and there exists some constant γ for which (15.48) holds. It
thus follows from (15.50) and the fact that

X(t)

is centered that
E

X(t) X(t′)
 ≤γ,
t, t′ ∈R.
(15.66)
15.6 Operational PSD and Average Autocovariance Function
281
We now turn to Part (i). Let h ∈L1 be any stable impulse response. To compute
the power in X⋆h we use Fubini’s Theorem to express the expectation of (X⋆h)2(t)
at every epoch t ∈R as
E

(X ⋆h)2(t)

= E
	 ∞
−∞
h(τ) X(t −τ) dτ

	 ∞
−∞
h(σ) X(t −σ) dσ


=
 ∞
−∞
 ∞
−∞
h(τ) h(σ) E

X(t −τ) X(t −σ)

dτ dσ.
Consequently, for every T > 0,
1
2T
 T
−T
E

(X ⋆h)2(t)

dt
=
 ∞
−∞
 ∞
−∞
h(τ) h(σ) 1
2T
 T
−T
E

X(t −τ) X(t −σ)

dt dτ dσ
=
 ∞
−∞
 ∞
−∞
h(τ) h(σ) 1
2T
 T−τ
−T−τ
E

X(s) X(s + τ −σ)

ds dτ dσ,
(15.67)
where in the last equality we substituted s for t −τ. Letting T tend to inﬁnity, we
now obtain
lim
T→∞
1
2T
 T
−T
E

(X ⋆h)2(t)

dt
= lim
T→∞
 ∞
−∞
 ∞
−∞
h(τ) h(σ) 1
2T
 T−τ
−T−τ
E

X(s) X(s + τ −σ)

ds dτ dσ
=
 ∞
−∞
 ∞
−∞
lim
T→∞h(τ) h(σ) 1
2T
 T−τ
−T−τ
E

X(s) X(s + τ −σ)

ds dτ dσ
=
 ∞
−∞
 ∞
−∞
h(τ) h(σ) ¯KXX(τ −σ) dτ dσ,
(15.68)
where the swapping of the limit and the (double) integral in the second equality
can be justiﬁed based on the Dominated Convergence Theorem (using (15.66) and
the integrability of h), and where the last equality follows from Lemma 15.6.4 (by
replacing α there with τ, and τ there with τ −σ). From here the proof of (15.63)
is but a change of variable away:
Power in X ⋆h = lim
T→∞
1
2T
 T
−T
E

(X ⋆h)2(t)

dt
=
 ∞
−∞
 ∞
−∞
h(τ) h(σ) ¯KXX(τ −σ) dτ dσ
=
 ∞
−∞
¯KXX(α)
 ∞
−∞
h(σ) h(σ + α) dσ dα
=
 ∞
−∞
¯KXX(α) Rhh(α) dα,
where the second equality follows from (15.68) and the third by deﬁning α as τ −σ.
This concludes the proof of Part (i).
282
Operational Power Spectral Density
To prove Part (ii), note that if ˇS and ¯KXX are indistinguishable, then
 ∞
−∞
¯KXX(σ) Rhh(σ) dσ =
 ∞
−∞
ˇS(σ) Rhh(σ) dσ
=
 ∞
−∞
S(f) ˆRhh(f) df
=
 ∞
−∞
S(f)
ˆh(f)
2 df,
from which (15.64) follows using (15.63). Here the second equality follows from
Proposition 6.2.4 and the fact that ˆRhh is real, and the last equality holds because
the FT of Rhh is f 	→
ˆh(f)
2; see (11.35).
We next turn to Part (iii). Since ¯KXX is real and symmetric, its FT ˆ¯KXX is real and
symmetric. And since it is positive deﬁnite, its FT is nonnegative (Appendix C,
Corollary C.8 (ii)). To prove that ˆ¯KXX equals SXX it thus remains to establish
that
Power in X ⋆h =
 ∞
−∞
ˆ¯KXX(f)
ˆh(f)
2 df,
h ∈L1.
(15.69)
This follows from Part (ii) because ¯KXX is indistinguishable from the IFT of ˆ¯KXX
(Appendix C, Corollary C.8 (i)).
We next prove Part (iv). For every h ∈L1 we can express the power in X ⋆h
either in terms of SXX (using the deﬁnition of the operational PSD) or in terms of
the average autocovariance function ¯KXX (15.63), so
 ∞
−∞
¯KXX(σ) Rhh(σ) dσ =
 ∞
−∞
SXX(f)
ˆh(f)
2 df
=
 ∞
−∞
ˇSXX(σ) Rhh(σ) dσ,
h ∈L1,
(15.70)
where the second equality follows from Proposition 6.2.4.
Since ¯KXX is a measurable positive deﬁnite function (Proposition 15.6.5), it follows
from the Riesz-Crum Theorem (Appendix C, Theorem C.6 (i)), that it can be
expressed as
¯KXX = ¯K
c
XX + ¯K
s
XX,
(15.71)
where ¯K
c
XX is a positive deﬁnite function that is continuous, and ¯K
s
XX is a positive
deﬁnite function that is zero outside a set of Lebesgue measure zero. It follows
from (15.70), (15.71), and from the fact that ¯K
s
XX is indistinguishable from the
all-zero function that
 ∞
−∞
¯K
c
XX(σ) Rhh(σ) dσ =
 ∞
−∞
ˇSXX(σ) Rhh(σ) dσ,
h ∈L1.
(15.72)
Since both ¯K
c
XX and ˇSXX are continuous, this implies that they are identical
(Lemma 25.14.2). Thus, the IFT of SXX is identical to ¯K
c
XX, and the latter is
indistinguishable from ¯KXX (because ¯K
s
XX is indistinguishable from the all-zero
function), so the IFT of SXX must be indistinguishable from ¯KXX.
15.7 The Operational PSD of a Filtered Stochastic Process
283
15.7
The Operational PSD of a Filtered Stochastic Process
From the operational PSD of a SP X it should not be diﬃcult to compute the
operational PSD of X ⋆r whenever r is deterministic. The intuition is as follows.
To compute the operational PSD of the SP X ⋆r we need to know the power in
(X ⋆r) ⋆h for every h ∈L1. But, since convolution is (usually) associative, we
expect that the SP (X⋆r)⋆h be (usually) identical to the SP X⋆(r⋆h) and hence
of the same power. The power in the latter is easily computed from SXX: we view
r ⋆h as an impulse response of a ﬁlter; we view X ⋆(r ⋆h) as the result of passing
X through this ﬁlter; and we recall that X is of operational PSD SXX so the power
in X ⋆(r ⋆h)—and hence also in (X ⋆r) ⋆h—is
 ∞
−∞
SXX(f)
ˆr(f) ˆh(f)
2 df.
Rewriting this as
 ∞
−∞

SXX(f)
ˆr(f)
2 ˆh(f)
2 df,
we conclude that the operational PSD of X ⋆r ought to be
f 	→SXX(f)
ˆr(f)
2.
As we next show, under some very mild technical conditions, this is indeed the case.
The technical assumptions are needed to justify the claim that the convolution is
associative. This is the content of the next lemma. The proof is rather technical
and can be skipped.
Lemma 15.7.1 (Convolution Is Associative: Stochastic Processes). Let

X(t)

be a measurable SP satisfying
sup
t∈R
E

|X(t)|

< ∞.
(15.73)
Let r and h be real integrable signals, and let the epoch t ∈R be arbitrary but ﬁxed.
Then, with probability one, the two integrals
 ∞
−∞
X(t −τ)

r ⋆h

(τ) dτ
and
 ∞
−∞

X ⋆r

(t −σ) h(σ) dσ
(15.74)
are both deﬁned and are equal.
Note 15.7.2. Condition (15.73) holds whenever

X(t)

is centered and of bounded
variance, because the nonnegativity of the variance of |X(t)| implies that E[|X(t)|]
is upper-bounded by the square root of E

X2(t)

.
Proof. Since

X(t)

is a measurable SP, and since both r and h are measurable,
the mapping from Ω × (R × R) to the reals

ω, (σ, τ)

	→X(ω, t −τ) r(τ −σ) h(σ)
(15.75)
284
Operational Power Spectral Density
is measurable. It is integrable in the sense that
 ∞
−∞
 ∞
−∞
E

|X(t −τ)|

|r(τ −σ)| |h(σ)| dσ dτ < ∞,
(15.76)
because
 ∞
−∞
 ∞
−∞
E

|X(t −τ)|

|r(τ −σ)| |h(σ)| dσ dτ
≤sup
t∈R
E

|X(t)|
  ∞
−∞
 ∞
−∞
|r(τ −σ)| |h(σ)| dσ dτ
= sup
t∈R
E

|X(t)|

∥r∥1 ∥h∥1 .
Using (15.76) and Fubini’s Theorem, we now conclude that there exists an event
N of probability zero such that
 ∞
−∞
 ∞
−∞
|X(ω, t −τ)| |r(τ −σ)| |h(σ)| dσ dτ < ∞,
ω /∈N.
(15.77)
Consequently, by Fubini’s Theorem,
 ∞
−∞
	 ∞
−∞
X(ω, t −τ) r(τ −σ) h(σ) dσ

dτ =
 ∞
−∞
	 ∞
−∞
X(ω, t −τ) r(τ −σ) h(σ) dτ

dσ,
ω /∈N.
(15.78)
The LHS of the above is the integral on the LHS of (15.74), and the RHS is the
integral on the RHS of (15.74).
Having established the associativity of the convolution, we are now ready for the
main result on the operational PSD of ﬁltered stochastic processes.
Theorem 15.7.3 (The Operational PSD of a Filtered SP). Let the SP

X(t)

satisfy (15.73) and be of operational PSD SXX. Let r be any real integrable deter-
ministic signal. Then the operational PSD of X ⋆r is
f 	→SXX(f)
ˆr(f)
2.
(15.79)
Proof. Since the mapping in (15.79) is symmetric, it only remains to verify that
Power in (X ⋆r) ⋆h =
 ∞
−∞
SXX(f)
ˆr(f)
2 ˆh(f)
2 df,
h ∈L1.
(15.80)
This is now straightforward because for any h ∈L1
Power in (X ⋆r) ⋆h = Power in X ⋆(r ⋆h)
=
 ∞
−∞
SXX(f)
ˆr(f) ˆh(f)
2 df
=
 ∞
−∞
SXX(f)
ˆr(f)
2 ˆh(f)
2 df,
15.8 The Operational PSD and Power
285
as we next justify. The ﬁrst equality follows from Lemma 15.7.1, which shows that
at every epoch t ∈R the random variables

(X ⋆r) ⋆h

(t) and

X ⋆(r ⋆h)

(t) are
equal with probability one and hence of identical second moment:
E
%
(X ⋆r) ⋆h
2(t)
&
= E
%
X ⋆(r ⋆h)
2(t)
&
,
t ∈R.
The second equality holds because

X(t)

is of operational PSD SXX.
15.8
The Operational PSD and Power
It would be a sin of omission not to discuss the relationship between the operational
PSD of a stochastic process and its power. Intuition suggests that the integral of
the operational PSD ought to equal the power. To see why, recall that if X is of
operational PSD SXX, then
Power in X ⋆h =
 ∞
−∞
SXX(f)
ˆh(f)
2 df,
h ∈L1.
(15.81)
Suppose we now substitute for h the impulse response of a ﬁlter whose frequency
response resembles that of an ideal unit-gain lowpass ﬁlter of very large cutoﬀ
frequency W ≫1. In this case the RHS of (15.81) would resemble the integral of
SXX(f) from −W to +W, which is approximately the integral from −∞to +∞
when W is very large. And as to the LHS, if W is very large, then intuition suggests
that X will hardly be altered by the ﬁlter, and the LHS would approximately equal
the power in X.
This intuition is excellent, and for most stochastic processes of interest the opera-
tional PSD indeed integrates to the power. However, as our next example shows,
there are some pathological counter-examples.
Before presenting our example in detail, we begin with the big picture. In our
example the SP X takes on the values ±1 only, so its power is 1. However, X
changes between the values +1 and −1 progressively faster the further time is from
the origin. As we next explain, this results in the power in X⋆h being zero for every
stable ﬁlter h, so X is of zero operational PSD. The integral of the operational PSD
is thus zero, while the power is one.
We next oﬀer some intuition as to why the power in X⋆h is zero. Recall that when h
is stable, its frequency response decays to zero (Theorem 6.2.11 (i)). Consequently,
above some cutoﬀfrequency, the frequency response of the ﬁlter is nearly zero.
Since our SP varies faster and faster the further we are from the origin of time,
when we are suﬃciently far from the origin of time the dynamics of our SP are
much faster than the ﬁlter’s cutoﬀfrequency. Consequently, except for transients
that result from the behavior of our SP near the origin of time, in steady state the
response of h to X will be nearly zero. Since the transients do not inﬂuence the
power in X ⋆h, the power in X ⋆h is zero. We next present the example in greater
detail.
Example 15.8.1. Consider the SP

X(t), t ∈R

whose value in the time interval
[ν, ν +1) is deﬁned for every integer ν as follows: The interval is divided into |ν|+1
286
Operational Power Spectral Density
nonoverlapping half-open subintervals of length 1/(|ν| + 1)
%
ν +
κ
|ν| + 1, ν + κ + 1
|ν| + 1

,
κ ∈{0, . . . , |ν|},
and in each such subinterval the SP is constant and is equal to the RV Xν,κ, which
takes on the values ±1 equiprobably with
{Xν,κ},
ν ∈Z, κ ∈{0, . . . , |ν|}
being IID. Thus,
X(t) =
∞

ν=−∞
|ν|

κ=0
Xν,κ I
'
ν +
κ
|ν| + 1 ≤t < ν + κ + 1
|ν| + 1
(
,
(15.82a)

Xν,κ} ∼IID U ({±1}) .
(15.82b)
This SP is centered, of power P = 1, and yet its operational PSD is zero at all
frequencies. The integral of the operational PSD of X is thus strictly smaller than
the power in X.
Analysis of Example 15.8.1. At every epoch t the RV X(t) takes on the values
±1 equiprobably and is thus centered. Moreover, X2(t) is deterministically 1, so
the power in

X(t)

is one. We next show that

X(t)

is of average autocovariance
function
¯KXX(τ) =

1
if τ = 0,
0
otherwise,
τ ∈R.
(15.83)
For τ equal to zero this follows immediately from our observation that X2(t) is
deterministically equal to one. By symmetry, it suﬃces to establish (15.83) for
positive τ.
When τ is 1 or larger, the epochs t and t + τ fall—irrespective of
t—in diﬀerent intervals, so X(t) and X(t + τ) are uncorrelated for all t.
For
such τ’s ¯KXX(τ) is thus zero, in agreement with (15.83). It thus only remains to
establish (15.83) for 0 < τ < 1. In this case t and t + τ are guaranteed to fall in
diﬀerent subintervals whenever
τ ≥
1
⌊t⌋
 + 1,
(15.84)
where the RHS is the length of the subintervals to which the interval containing t—
namely the interval [ν, ν + 1), where ν is ⌊t⌋—is subdivided. (If this inequality is
not satisﬁed, then X(t) and X(t + τ) may or may not be in diﬀerent subintervals.)
For τ ∈(0, 1), Inequality (15.84) holds whenever
⌊t⌋
 ≥τ −1 −1. Thus, when t is
outside the ﬁnite interval

t′ ∈R :
⌊t′⌋
 < τ −1 −1

the random variables X(t) and X(t + τ) are uncorrelated. For t inside this ﬁnite
interval the correlation between X(t) and X(t + τ) is upper bounded by 1. Con-
sequently, when we average E[X(t) X(t + τ)] over t, the contribution of t’s inside
this interval washes out and the result is zero.
15.8 The Operational PSD and Power
287
Having established (15.83), we conclude from Theorem 15.6.6 (i) that the power
in X ⋆h is zero for any stable ﬁlter h. The operational PSD of

X(t)

is thus
zero.
In Example 15.8.1 the power is strictly larger than the integral of the operational
PSD, and the average autocovariance function is discontinuous at the origin. As
we shall next see this is no coincidence.
The integral of the operational PSD
never exceeds the power, and the two are equal whenever the SP has an average
autocovariance function that is continuous at the origin. To emphasize that the
prevalent case is when the two are the same, we shall state the results in two
separate theorems.
Theorem 15.8.2 (The Power and the Integral of the Operational PSD). Let

X(t)

be a centered SP of power P, of operational PSD SXX, and of an average
autocovariance function ¯KXX that is continuous at the origin. Then
P =
 ∞
−∞
SXX(f) df.
(15.85)
Proof. For every stable impulse response h ∈L1 we can express the power in X⋆h
in two diﬀerent ways: as in (15.63) of Theorem 15.6.6 (i) and as in the deﬁnition
of the operational PSD (Deﬁnition 15.3.1). Consequently,
 ∞
−∞
¯KXX(τ) Rhh(τ) dτ =
 ∞
−∞
SXX(f)
ˆh(f)
2 df,
h ∈L1.
(15.86)
The gist of the proof is to choose h to be a very narrow pulse centered at the origin
with Rhh integrating to 1 (i.e., with
ˆh(0)
 = 1) so that the LHS of (15.86) would
approximate ¯KXX(0)—which, by Note 15.6.2, is equal to P—and so that the RHS
would approximate the integral of the operational PSD.
For every positive λ, we choose the impulse response h as
h(t) = 1
λ I
'
|t| ≤λ
2
(
,
t ∈R
(15.87a)
with corresponding Fourier Transform
ˆh(f) = sinc (λf),
f ∈R,
(15.87b)
and self-similarity function
Rhh(τ) = 1
λ

1 −|τ|
λ

I

|τ| ≤λ

,
τ ∈R.
(15.87c)
288
Operational Power Spectral Density
We now conclude the proof by justifying the calculation
P = ¯KXX(0)
(15.88a)
= lim
λ↓0
 ∞
−∞
¯KXX(τ) Rhh(τ) dτ
(15.88b)
= lim
λ↓0
 ∞
−∞
SXX(f)
ˆh(f)
2 df
(15.88c)
=
 ∞
−∞
SXX(f) df.
(15.88d)
The ﬁrst equality follows from Note 15.6.2; the second from the hypothesis that
¯KXX is continuous at the origin and from the explicit form of Rhh (15.87c); and
the third equality follows from (15.86). The ﬁnal equality can be justiﬁed using
the Dominated Convergence Theorem because the integrand converges to SXX(f)
at every f ∈R because
lim
λ↓0 SXX(f)
ˆh(f)
2 = lim
λ↓0 SXX(f) sinc2 (λf)
= SXX(f),
f ∈R,
and the magnitude of the integrand can be upper-bounded as
SXX(f)
ˆh(f)
2 = SXX(f)
ˆh(f)
2
= SXX(f) sinc2 (λf)
≤SXX(f),
f ∈R,
which is integrable.
To prepare for the result that the integral of the operational PSD cannot exceed the
power, we send forward the following lemma, which bounds the power ampliﬁcation
of a ﬁlter. For technical reasons it only deals with impulse responses of compact
support, i.e., impulse responses that are zero outside some interval of the form
[−Δ, Δ]. This restriction allows us to deal with inputs having ﬁnite power but
inﬁnite energy.
Lemma 15.8.3. Let

X(t)

be a SP of power P, and let h be an integrable function
of compact support. If the power in X ⋆h is deﬁned, then it is bounded by
Power in X ⋆h ≤P max
f∈R
ˆh(f)
2.
(15.89)
Proof. Denote the SP X ⋆h by Y. Since h is of compact support, we can ﬁnd
some Δ > 0 such that h(t) is zero whenever |t| > Δ. We will show that for any
T > 0 and sample-path ω ∈Ω,
 T
−T
Y 2(ω, t) dt ≤max
f∈R
ˆh(f)
2  T+Δ
−T−Δ
X2(ω, t) dt.
(15.90)
15.8 The Operational PSD and Power
289
The result will then follow by taking expectations of both sides of the inequality;
dividing by 2T; and letting T tend to inﬁnity.
It thus remains to prove (15.90). This inequality is obvious when its RHS is inﬁnite.
Hereafter we thus focus on the case where
 T+Δ
−T−Δ
X2(ω, t) dt < ∞.
(15.91)
Starting from the deﬁnition of the convolution and recalling that h(t) is zero when-
ever |t| exceeds Δ, we obtain
Y (t) =
 ∞
−∞
X(τ) h(t −τ) dτ
=
 t+Δ
t−Δ
X(τ) h(t −τ) dτ,
and thus establish that the value of Y (t) depends only on the values of X at epochs
in the interval [t−Δ, t+Δ]. Hence, for any T > 0, the values of Y (t) for t ∈[−T, +T ]
are determined by the values of X at epochs in the interval [−T −Δ, T + Δ]. Thus,
if we deﬁne the time-windowed SP Xw as
Xw(ω, t) = X(ω, t) I

|t| ≤T + Δ

,
(15.92)
then, rather than as X ⋆h, we can also express Y for |t| ≤T as
Y (t) = (Xw ⋆h)(t),
|t| ≤T.
(15.93)
Consequently,
 T
−T
Y 2(t) dt =
 T
−T
(Xw ⋆h)2(t) dt
≤∥Xw ⋆h∥2
2
=
 ∞
−∞
 ˆXw(f) ˆh(f)
2 df
≤max
f∈R
ˆh(f)
2  ∞
−∞
 ˆXw(f)
2 df
= max
f∈R
ˆh(f)
2 ∥Xw∥2
2
= max
f∈R
ˆh(f)
2  T+Δ
−T−Δ
X2(t) dt,
with the following justiﬁcation.
The ﬁrst line follows from (15.93); the second
by extending the region of integration from [−T, +T ] to the entire real line; the
third by Parseval’s Theorem; the fourth by upper-bounding the integrand; the
ﬁfth by Parseval’s Theorem; and the last by the deﬁnition of Xw (15.92). This
establishes (15.90) and hence concludes the proof.
290
Operational Power Spectral Density
Theorem 15.8.4 (The Integral of the Operational PSD Never Exceeds the
Power). If

X(t)

is of operational PSD SXX and of power P, then
P ≥
 ∞
−∞
SXX(f) df.
(15.94)
Proof. We will show that for every W > 0
P ≥
 W
−W
SXX(f) df,
(15.95)
from which the result follows by letting W tend to inﬁnity. Fix some W > 0 and
some δ > 0. Let h ∈L1 be the impulse response of some stable ﬁlter that resembles
an ideal unit-gain lowpass ﬁlter of cutoﬀfrequency W in the sense that
ˆh(f) = 1,
|f| ≤W,
(15.96a)
ˆh(f) = 0,
|f| ≥W + δ,
(15.96b)
and
ˆh(f)
 ≤1,
f ∈R.
(15.96c)
(For example, h could be the IFT of the function g in (7.16) when we substitute
2(W + δ) for Wc, zero for fc, and 2W for W; see also (7.18) and Figure 7.8.) For
this ﬁlter,
Power in X ⋆h =
 ∞
−∞
SXX(f)
ˆh(f)
2 df
≥
 W
−W
SXX(f) df,
(15.97)
where the inequality follows from the nonnegativity of SXX and from (15.96a). The
desired inequality (15.95) will follow from (15.97) once we establish that
Power in X ⋆h ≤P.
(15.98)
To this end, ﬁx any (small) ϵ > 0, and let hc be a function of compact support
that approximates h within ϵ in the L1 norm. That is,
∥h −hc∥1 ≤ϵ,
(15.99a)
and
hc(t) = 0,
|t| ≥Δ,
(15.99b)
where Δ > 0 typically depends on ϵ. Deﬁning ˜h = h−hc, we obtain from (15.99a)
(using Theorem 6.2.11) that
ˆ˜h(f)
 =
ˆh(f) −ˆhc(f)
 ≤ϵ,
f ∈R,
(15.100)
and hence, by (15.96c),
ˆhc(f)
 ≤1 + ϵ,
f ∈R.
(15.101)
15.8 The Operational PSD and Power
291
By expressing h as hc+˜h and using the Triangle Inequality for Stochastic Processes
(Proposition 18.5.1), we obtain for every T > 0,
1
2T E
 T
−T
(X ⋆h)2(t) dt

≤
-
1
2T E
 T
−T
(X ⋆hc)2(t) dt

+

1
2T E
 T
−T
(X ⋆˜h)2(t) dt
 .2
.
(15.102)
From this we obtain upon letting T tend to inﬁnity that
Power in X ⋆h ≤
-

(1 + ϵ)2 P
1/2 +
	
ϵ2
 ∞
−∞
SXX(f) df

1/2.2
,
(15.103)
because, by Lemma 15.8.3 and (15.101),
Power in X ⋆hc ≤(1 + ϵ)2 P,
(15.104)
and
Power in X ⋆˜h =
 ∞
−∞
SXX(f)
ˆ˜h(f)

2
df
≤max
f∈R
ˆ˜h(f)

2  ∞
−∞
SXX(f) df
≤ϵ2
 ∞
−∞
SXX(f) df,
(15.105)
where the last inequality follows from (15.100).
The desired inequality (15.98) now follows from (15.103), because the latter holds
for every ϵ > 0, and the RHS of (15.103) converges to the RHS of (15.98) when ϵ
tends to zero.
We have seen that the integral of the operational PSD never exceeds the power
(Theorem 15.8.4) and that, for stochastic processes having an average autocovari-
ance function, a suﬃcient condition for equality is that this function be continuous
at the origin (Theorem 15.8.2). The next result shows that for such stochastic
processes this is also a necessary condition. (Recall that in Example 15.8.1 this
condition does not hold and the integral of the operational PSD is strictly smaller
than the power.)
Theorem 15.8.5 (Continuity at the Origin Is Necessary). Let

X(t)

be a cen-
tered SP of power P, of operational PSD SXX, and of average autocovariance func-
tion ¯KXX that is discontinuous at the origin. Then
P >
 ∞
−∞
SXX(f) df.
(15.106)
Proof. Since ¯KXX is a measurable positive deﬁnite function (Proposition 15.6.5),
it follows from the Riesz-Crum Theorem (Appendix C, Theorem C.6), that it can
be expressed as
¯KXX = ¯K
c
XX + ¯K
s
XX,
(15.107)
292
Operational Power Spectral Density
where ¯K
c
XX is a positive deﬁnite function that is continuous, and ¯K
s
XX is a positive
deﬁnite function that is zero outside a set of Lebesgue measure zero. Since ¯KXX is
discontinuous, it follows that ¯K
s
XX is not identically zero. Consequently its value
at zero must be strictly positive
¯K
s
XX(0) > 0
(15.108)
because ¯K
s
XX is positive deﬁnite and hence achieves its maximum at the origin
(Proposition C.3 (iii)). It follows from (15.107) and (15.108) that
¯KXX(0) > ¯K
c
XX(0).
(15.109)
Using the same choice of h as in (15.87a), we now obtain as in (15.88)
P = ¯KXX(0)
(15.110a)
> ¯K
c
XX(0)
(15.110b)
= lim
λ↓0
 ∞
−∞
¯K
c
XX(τ) Rhh(τ) dτ
(15.110c)
= lim
λ↓0
 ∞
−∞
¯KXX(τ) Rhh(τ) dτ
(15.110d)
= lim
λ↓0
 ∞
−∞
SXX(f)
ˆh(f)
2 df
(15.110e)
=
 ∞
−∞
SXX(f) df,
(15.110f)
where (15.110b) follows from (15.109); (15.110c) follows from the continuity of
¯K
c
XX at the origin; (15.110d) follows because ¯K
s
XX is zero outside a set of Lebesgue
measure zero; (15.110e) follows from (15.86); and (15.110f) is justiﬁed like (15.88d).
15.9
Exercises
Exercise 15.1 (Scaling a SP). Let

Y (t)

be the result of scaling the SP

X(t)

by the
real number α. Thus, Y (t) = α X(t) for every epoch t ∈R. Show that if

X(t)

is of
operational PSD SXX, then

Y (t)

is of operational PSD f →α2 SXX(f).
Exercise 15.2 (The Operational PSD of a Sum of Independent SPs). Intuition suggests
that if

X(t)

and

Y (t)

are centered independent stochastic processes of operational
PSDs SXX and SYY , then their sum should be of operational PSD f →SXX(f) + SYY (f).
Explain why.
Exercise 15.3 (Operational PSD of a Deterministic SP). Let

X(t)

be deterministically
equal to the energy-limited signal g: R →R in the sense that, at every epoch t ∈R, the
RV X(t) is deterministically equal to g(t). Find the operational PSD of

X(t)

.
Exercise 15.4 (Stretching Time). Let

X(t)

be of operational PSD SXX, and let a > 0
be ﬁxed. Deﬁne the SP

Y (t)

at every epoch t ∈R as Y (t) = X(t/a). Show that

Y (t)

is of operational PSD f →a SXX(af).
15.9 Exercises
293
Exercise 15.5 (The Operational PSD of PAM). Let

Xℓ, ℓ∈Z

be IID with Xℓtaking
on the values ±1 equiprobably. Let
g(t) = I
 
|t| ≤Ts
2
!
,
t ∈R,
X(t) = A
∞

ℓ=−∞
Xℓg(t −ℓTs),
t ∈R,
where A, Ts > 0 are deterministic.
(i) Plot a sample function of X for a realization of

Xℓ, ℓ∈Z

of your choice.
(ii) Compute the operational PSD of X.
(iii) Repeat Parts (i) and (ii) for
˜
X(t) = A
∞

ℓ=−∞
Xℓg(t −2 ℓTs),
t ∈R.
(iv) How do the operational PSDs of X and ˜X compare?
Exercise 15.6 (Spectral Shaping via Precoding). Let

Xℓ, ℓ∈Z

be IID with Xℓtaking
on the values ±1 equiprobably. Let ˜
Xℓ= Xℓ+ Xℓ−1 for every ℓ∈Z.
(i) Compute the operational PSD of the PAM signal
˜
X(t) =
∞

ℓ=−∞
˜
Xℓg(t −ℓTs),
t ∈R
for g(·) decaying to zero suﬃciently fast as |t| →∞, e.g., satisfying (14.17).
(ii) Throw mathematical caution to the wind and evaluate your answer for the pulse
shape whose FT is
ˆg(f) = I
 
|f| ≤
1
2Ts
!
,
f ∈R.
(Ignore the fact that this pulse shape does not satisfy (14.17).) Plot your answer
and compare it to the operational PSD of the PAM signal
X(t) =
∞

ℓ=−∞
Xℓg(t −ℓTs),
t ∈R.
(iii) Show that ˜X can also be written as a PAM signal with IID symbols but with a
diﬀerent pulse shape. That is,
˜
X(t) =
∞

ℓ=−∞
Xℓh(t −ℓTs),
t ∈R,
h: t →g(t) + g(t −Ts).
Exercise 15.7 (The Operational PSD and Block Codes). PAM is used in block-mode in
conjunction with the (1, 2) binary-to-reals block encoder
0 →(+1, −1),
1 →(−1, +1)
to transmit IID random bits. The pulse shape g(·) satisﬁes the decay condition (14.17).
Compute the power and operational PSD of the signal.
294
Operational Power Spectral Density
Exercise 15.8 (Repetitions and the Operational PSD). Let

X(t)

be the signal (15.25)
that results when the (1, 2) binary-to-reals block-encoder (10.4) is used in bi-inﬁnite block-
mode. Find the operational PSD of

X(t)

.
Exercise 15.9 (Direct-Sequence Spread-Spectrum Communications). This problem is
motivated by uncoded Direct-Sequence Spread-Spectrum communications with process-
ing gain N. Let the (1, N) binary-to-reals block encoder map 0 to the sequence a1, . . . , aN
and 1 to −a1, . . . , −aN. Consider PAM with bi-inﬁnite block encoding using this map-
ping. Express the operational PSD of the resulting PAM signal in terms of the sequence
a1, . . . , aN and the pulse shape g. Calculate explicitly when the pulse shape is the map-
ping t →I{|t| ≤Ts/2} for two cases: when the sequence a1, . . . , aN is the Barker-7 code
(+1, +1, +1, −1, −1, +1, −1) and when it is the sequence (+1, +1, +1, +1, +1, +1, +1).
Compare the latter case with the case where the mapping is the antipodal mapping
0 →+1, and 1 →−1, the baud period is 7Ts, and the pulse shape is t →I{|t| ≤7Ts/2}
Exercise 15.10 (A Non-PAM Signal). For every t ∈R deﬁne Y (t) = X(t) −E[X(t)],
where

X(t)

is the SP of Exercise 14.6. Find the operational PSD of

Y (t)

.
Exercise 15.11 (8-PPM). Let

Y (t)

be given at every t ∈R by Y (t) = X(t) −E[X(t)],
where

X(t)

is the result of bi-inﬁnite block encoding IID random bits using 8-PPM; see
Exercise 10.3. Find the operational PSD of

Y (t)

.
Exercise 15.12 (The Power in a Filtered Period Signal). Let x be a periodic signal of
period Tp > 0 and of ﬁnite power. Show that for every h ∈L1
Power of x ⋆h = 1
Tp
∞

η=−∞
|cη|2 ˆh
	 η
Tp


2
,
where cη is the η-th Fourier Series Coeﬃcient of x w.r.t. the interval [−Tp/2, Tp/2].
Hint: Recall Exercises 5.11, 14.7, 6.23, and Theorem A.3.3 (ii).
Exercise 15.13 (Periodicity and the Average Autocovariance Function). Let the SP

X(t)

be centered and of bounded variance.
Show that if for every lag τ ∈R the
mapping t →Cov[X(t), X(t + τ)] is periodic in t with period Tp > 0, then

X(t)

has the
average autocovariance function
¯KXX(τ) = 1
Tp
 Tp
0
Cov[X(t), X(t + τ)] dt,
τ ∈R.
Exercise 15.14 (On the Power and Average Autocovariance Function). Let

X(t)

be a
centered, measurable SP of bounded variance, and let τ ∈R be any lag. Show that the
existence of the limits deﬁning the power in the SP

X(t)

and in the SP whose time-t
value is X(t) + X(t + τ) guarantees the existence of the limit on the RHS of (15.51).
Chapter 16
Quadrature Amplitude Modulation
16.1
Introduction
We next discuss linear modulation in passband. We envision being allocated band-
width W around the carrier frequency fc, so we can only send real signals whose
Fourier Transform is zero at frequencies f satisfying
|f| −fc
 > W/2.
That
is, the FT of the transmitted signal is allowed to be nonzero only in the fre-
quency interval [fc −W/2, fc + W/2] and in its negative frequency counterpart
[−fc −W/2, −fc + W/2] (Deﬁnition 7.3.1). We assume throughout this chapter
that
fc > W
2 .
(16.1)
There are numerous ways to communicate in passband and, to complicate things
further, sometimes seemingly diﬀerent approaches lead to identical signals. Thus,
while we would like to motivate the scheme we shall focus on—Quadrature Ampli-
tude Modulation (QAM)—we cannot prove or claim that it is the only “optimal”
solution.1 Nevertheless, we shall try to motivate it by discussing some features
that one would typically like to have and by then showing that QAM has these
features.
From our studies of PAM and of Nyquist’s Criterion we recall that if we are al-
located (baseband) bandwidth W Hz and if Ts ≥1/(2W), then we can ﬁnd a
bandwidth-W pulse shape whose time shifts by integer multiples of Ts are or-
thonormal. If Ts = 1/(2W), then such a pulse is the bandwidth-W unit-energy
pulse t 	→
√
2W sinc(2Wt). (You may recall that such pulses are rarely used be-
cause they decay to zero too slowly over time, thus rendering the computation
of the PAM signal unstable and the resulting peak power unbounded.)
And if
Ts < 1/(2W), then no such pulse shape exists. (Corollary 11.3.5.)
From a somewhat more abstract perspective, PAM with the above pulse shape (or
with the square root of a raised-cosine pulse shape (11.29) with very small excess
bandwidth) allows us to send symbols arriving at rate
Rs
real symbol
second

≜1
Ts
1There are information theoretic considerations that show that QAM can achieve the capacity
of the bandlimited passband additive white Gaussian noise channel.
295
296
Quadrature Amplitude Modulation
as the coeﬃcients in a linear combination of orthonormal signals whose bandwidth
does not exceed (or only slightly exceeds)
Rs
2 [Hz] .
That is, for each spectral sliver of 1 Hz at baseband we obtain 2 real dimensions
per second, i.e., we can communicate at spectral eﬃciency
2 [real dimension/sec]
[baseband Hz]
.
This is an achievement that we would like to replicate for passband signaling:
First Objective: Find a way to transmit real symbols arriving at rate Rs real sym-
bols per second as the coeﬃcients in a linear combination of orthonormal passband
signals occupying a (passband) bandwidth of W Hz around the carrier frequency fc,
where the bandwidth W is equal to (or only slightly exceeds) Rs/2. That is, we
would like to ﬁnd a communication scheme that would allow us to communicate at
2 [real dimension/sec]
[passband Hz]
.
Equivalently, since any stream of real symbols arriving at rate Rs real symbols
per second can be viewed as a stream of complex symbols arriving at rate Rs/2
complex symbols per second (simply by pairing tuples (a, b) of real numbers a, b ∈R
into single complex numbers a + ib), we can restate our objective as follows: ﬁnd
a way to transmit complex symbols arriving at rate Rs/2 complex symbols per
second as the coeﬃcients in a linear combination of orthonormal passband signals
occupying a (passband) bandwidth of W Hz around the carrier frequency fc, where
the bandwidth W is equal to, or only slightly exceeds Rs/2. That is, we would like
to ﬁnd a communication scheme that would allow us to communicate at
1 [complex dimension/sec]
[passband Hz]
.
(16.2)
In addition, we would like our modulation scheme to be of reasonable complexity.
One of the beneﬁts of the baseband PAM scheme is that we can compute all the
inner products required to reconstruct the coeﬃcients (symbols) using the matched
ﬁlter by feeding it with the transmitted signal and sampling its output at the
appropriate times.
A naive approach that does not achieve our objective is to use real baseband PAM
of the type we studied in Chapter 10 and to up-convert the PAM signal to passband
by multiplying it by the mapping t 	→cos(2πfct). The problem with this approach
is that the up-conversion doubles the bandwidth (Proposition 7.3.3).
16.2
PAM for Passband?
A natural approach to passband signaling might be to consider PAM directly with-
out any up-conversion. We merely have to look for a pulse shape φ whose Fourier
16.3 The QAM Signal
297
Transform is zero outside the band
|f| −fc
 ≤W/2 and whose self-similarity
function Rφφ is a Nyquist Pulse. It turns out that with this approach we can only
achieve our objective if 4fcTs is an odd integer. Indeed, the reader is encouraged
to use Corollary 11.3.4 to verify that if a pulse φ is an energy-limited passband
signal that is bandlimited to W Hz around the carrier frequency fc, and if its time
shifts by integer multiples of Ts are orthonormal, then
Ts ≥
1
2W
with equality being achievable only if both
|ˆφ(f)|2 = Ts I
|f| −fc
 ≤W/2

(for all frequencies f ∈R outside a set of Lebesgue measure zero) and
4fcTs
is an odd integer.
(16.3)
In fact, it can be shown that if (16.3) is satisﬁed and if ψ is any energy-limited
signal that is bandlimited to W/2 Hz and whose time shifts by integer multiples
of 2Ts are orthonormal, then the passband signal
φ(t) =
√
2 cos(2πfct) ψ(t),
t ∈R
is an energy-limited passband signal that is bandlimited to W Hz around the carrier
frequency fc, and its time shifts by integer multiples of Ts are orthonormal.
It would thus seem that if (16.3) is satisﬁed, then PAM would be a viable solution
to our problem. Nevertheless, this is not the standard solution. The reason may
have to do with implementation. If the above approach is used, then the carrier
frequency inﬂuences the choice of the pulse shape. Thus, a radio with a selectable
carrier frequency would require a diﬀerent pulse shape for each frequency! More-
over, the implementation of the modulator becomes carrier-dependent and fairly
complex. This discussion motivates our second objective:
Second Objective: To allow for ﬂexibility in the choice of the carrier, it is desir-
able to decouple the pulse shape selection from the carrier frequency.
16.3
The QAM Signal
Quadrature Amplitude Modulation achieves both our objectives. It achieves our
desired spectral eﬃciency (16.2) and also decouples the signal design from the
carrier frequency. It is easiest to describe QAM by describing the baseband repre-
sentation xBB(·) of the transmitted passband signal xPB(·). Indeed, the baseband
representation of the transmitted signal has the structure of PAM but with one
important diﬀerence: we allow for complex symbols and for complex pulse shapes.2
In QAM the encoder
ϕ: {0, 1}k →Cn
(16.4)
2Allowing complex pulse shapes is not critical. Crucial is that we allow complex symbols.
298
Quadrature Amplitude Modulation
maps k-tuples of data bits (D1, . . . , Dk) to n-tuples of complex symbols (C1, . . . , Cn),
and the baseband representation of the transmitted signal is
XBB(t) = A
n

ℓ=1
Cℓg(t −ℓTs),
t ∈R,
(16.5a)
where the pulse shape g(·) may be complex (though it is often chosen to be real),
A ≥0 is a real constant, Ts > 0 is the baud period, and 1/Ts is the baud rate. The
rate of the encoder is given by
k
n

bit
complex symbol

,
(16.5b)
and the transmitted real passband QAM signal XPB(·) is given by
XPB(t) = 2 Re

XBB(t) ei2πfct
,
t ∈R.
(16.5c)
Using (16.5a) & (16.5c) we can also express the QAM signal as
XPB(t) = 2 Re
	
A
n

ℓ=1
Cℓg(t −ℓTs) ei2πfct

,
t ∈R.
(16.6)
Alternatively, we can use the identities
Re(wz) = Re(w) Re(z) −Im(w) Im(z),
w, z ∈C,
Im(z) = −Re(iz),
z ∈C
to express the QAM signal as
XPB(t) =
√
2A
n

ℓ=1
Re(Cℓ)
gI,ℓ(t)



2 Re
-
1
√
2 g(t −ℓTs)



gI,ℓ,BB(t)
ei2πfct
.
+
√
2A
n

ℓ=1
Im(Cℓ)
gQ,ℓ(t)



2 Re
-
i 1
√
2 g(t −ℓTs)



gQ,ℓ,BB(t)
ei2πfct
.
,
t ∈R,
(16.7)
where we deﬁne
gI,ℓ(t) ≜2 Re
	 1
√
2 g(t −ℓTs) ei2πfct

(16.8a)
= 2 Re

gI,ℓ,BB(t) ei2πfct
,
t ∈R,
and
gQ,ℓ(t) ≜2 Re
	
i 1
√
2 g(t −ℓTs) ei2πfct

(16.8b)
= 2 Re

gQ,ℓ,BB(t) ei2πfct
,
t ∈R,
16.4 Bandwidth Considerations
299
with corresponding baseband representations:
gI,ℓ,BB(t) ≜
1
√
2 g(t −ℓTs),
t ∈R,
(16.9a)
gQ,ℓ,BB(t) ≜i 1
√
2 g(t −ℓTs),
t ∈R.
(16.9b)
Some comments about the QAM signal:
(i) The representation (16.7) demonstrates that the QAM signal is a linear com-
bination of the waveforms {gI,ℓ} and {gQ,ℓ}, where the coeﬃcients are pro-
portional to the real parts and the imaginary parts of the symbols {Cℓ}.
(ii) The normalization factor of 1/
√
2 in the deﬁnition of the functions {gI,ℓ} and
{gQ,ℓ} is for convenience only. Its role will become clearer in Section 16.5,
where the pulse shape is chosen to be of unit energy. In this case the factor of
1/
√
2 guarantees that the functions {gI,ℓ} and {gQ,ℓ} are also of unit energy.
(iii) We could also view QAM slightly diﬀerently as a modulation scheme where
data bits D1, . . . , Dk are mapped to 2n real numbers X1, . . . , X2n, which are
then grouped in pairs to form the n complex numbers Cℓ= X2ℓ−1 + iX2ℓ
for ℓ= 1, . . . , n and where these complex numbers are then mapped into the
passband signal whose baseband representation is given in (16.5a). The two
views are, of course, completely equivalent.
The expression for the QAM signal XPB(·) is simpliﬁed if the pulse shape g is real.
In this case we obtain from (16.6) for every t ∈R
XPB(t) = 2A
n

ℓ=1
Re(Cℓ) g(t −ℓTs) cos(2πfct)
−2A
n

ℓ=1
Im(Cℓ) g(t −ℓTs) sin(2πfct),
g real.
(16.10)
Thus, if the pulse shape g is real, then the QAM signal can be viewed as the
sum of two signals: the ﬁrst is the result of feeding {Re(Cℓ)} to a baseband PAM
modulator of pulse shape g and multiplying the result by cos(2πfct), and the second
is the result of feeding {Im(Cℓ)} to a baseband PAM modulator of pulse shape g
and multiplying the result by −sin(2πfct). Figure 16.1 illustrates the generation
of the QAM signal when the pulse shape g is real.
16.4
Bandwidth Considerations
The baseband representation of the QAM signal XPB(·) of (16.5c) is the baseband
PAM signal XBB(·) of (16.5a). By Corollary 10.10.2 (which also holds for complex
pulse shapes and symbols), the bandwidth of XBB(·) is the bandwidth of the pulse
shape g. Consequently, since the bandwidth of a passband signal around the carrier
frequency is twice the bandwidth of its baseband representation (Proposition 7.6.7
and Theorem 7.7.12 (i)), we conclude:
Note 16.4.1 (Bandwidth around fc of QAM). Unless the QAM signal is zero, its
bandwidth around the carrier frequency is twice the bandwidth of the pulse shape.
300
Quadrature Amplitude Modulation
cos(2πfct)
−sin(2πfct)
90◦
×
×
+
{Cℓ}
Re(·)
Im(·)
Re(Cℓ)
Im(Cℓ)
PAM
PAM
A 
ℓRe(Cℓ)g(t −ℓTs)
A 
ℓIm(Cℓ)g(t −ℓTs)
A 
ℓRe(Cℓ)g(t −ℓTs) cos(2πfct)
−A 
ℓIm(Cℓ)g(t −ℓTs) sin(2πfct)
xPB(t)/2
Figure 16.1: Generating a QAM signal when the pulse shape g is real.
16.5
Orthogonality Considerations
We next study the consequences of choosing the pulse shape g(·) so that its time
shifts by integer multiples of Ts be orthonormal. As in our treatment of PAM, we
change notation and denote the pulse shape in this case by φ(·). The orthonormal-
ity condition is thus
 ∞
−∞
φ(t −ℓTs) φ∗(t −ℓ′Ts) dt = I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z.
(16.11)
By Corollary 11.3.4, this is equivalent to requiring that
∞

ℓ=−∞
ˆφ

f + ℓ
Ts

2
= Ts,
(16.12)
for all frequencies f outside a set of Lebesgue measure zero.
When the pulse shape satisﬁes the orthogonality condition (16.11) we refer to 1/Ts
as having units of complex dimensions per second. In analogy to Deﬁnition 11.3.6,
we deﬁne the excess bandwidth as
100%
	bandwidth of φ
1/(2Ts)
−1

.
(16.13)
Proposition 16.5.1. If the energy-limited pulse shape φ satisﬁes (16.11), then the
QAM signal XPB(·) can be expressed as
XPB =
√
2A
n

ℓ=1
Re(Cℓ) ψI,ℓ+
√
2A
n

ℓ=1
Im(Cℓ) ψQ,ℓ
(16.14)
16.5 Orthogonality Considerations
301
where
. . . , ψI,−1, ψQ,−1, ψI,0, ψQ,0, ψI,1, ψQ,1, . . .
are orthonormal functions that are given by
ψI,ℓ: t 	→2 Re
	 1
√
2 φ(t −ℓTs) ei2πfct

,
ℓ∈Z
(16.15a)
ψQ,ℓ: t 	→2 Re
	
i 1
√
2 φ(t −ℓTs) ei2πfct

,
ℓ∈Z.
(16.15b)
Proof. Substituting φ for g in (16.7) we obtain
XPB(t) =
√
2A
n

ℓ=1
Re(Cℓ)
ψI,ℓ(t)



2 Re
	 1
√
2 φ(t −ℓTs)



ψI,ℓ,BB(t)
ei2πfct

+
√
2A
n

ℓ=1
Im(Cℓ)
ψQ,ℓ(t)



2 Re
	
i 1
√
2 φ(t −ℓTs)



ψQ,ℓ,BB(t)
ei2πfct

,
t ∈R,
where for every t ∈R
ψI,ℓ(t) ≜2 Re
	 1
√
2 φ(t −ℓTs) ei2πfct

(16.16a)
= 2 Re

ψI,ℓ,BB(t) ei2πfct
,
ψQ,ℓ(t) ≜2 Re
	
i 1
√
2 φ(t −ℓTs) ei2πfct

(16.16b)
= 2 Re

ψQ,ℓ,BB(t) ei2πfct
,
and the baseband representations are given by
ψI,ℓ,BB(t) ≜
1
√
2 φ(t −ℓTs)
(16.17a)
and
ψQ,ℓ,BB(t) ≜i 1
√
2 φ(t −ℓTs).
(16.17b)
We next verify that, when φ satisﬁes (16.11), the functions
. . . , ψI,−1, ψQ,−1, ψI,0, ψQ,0, ψI,1, ψQ,1, . . .
are orthonormal. To this end we recall that the inner product between two real
passband signals is twice the real part of the inner product between their baseband
302
Quadrature Amplitude Modulation
representations (Theorem 7.6.10). For ℓ̸= ℓ′ we thus have by (16.11)
⟨ψI,ℓ, ψI,ℓ′⟩= 2 Re

ψI,ℓ,BB, ψI,ℓ′,BB

= 2 Re
#
t 	→
1
√
2 φ(t −ℓTs), t 	→
1
√
2 φ(t −ℓ′Ts)
$
= 0,
⟨ψQ,ℓ, ψQ,ℓ′⟩= 2 Re

ψQ,ℓ,BB, ψQ,ℓ′,BB

= 2 Re
#
t 	→i 1
√
2 φ(t −ℓTs), t 	→i 1
√
2 φ(t −ℓ′Ts)
$
= 0,
and
⟨ψI,ℓ, ψQ,ℓ′⟩= 2 Re
#
t 	→
1
√
2 φ(t −ℓTs), t 	→i 1
√
2 φ(t −ℓ′Ts)
$
= 0.
And for ℓ= ℓ′ we have, again by (16.11),
⟨ψI,ℓ, ψI,ℓ⟩= 2 Re
#
t 	→
1
√
2 φ(t −ℓTs), t 	→
1
√
2 φ(t −ℓTs)
$
= 1,
⟨ψI,ℓ, ψQ,ℓ⟩= 2 Re
#
t 	→
1
√
2 φ(t −ℓTs), t 	→i 1
√
2 φ(t −ℓTs)
$
= Re

−i ∥φ∥2
2

= 0,
and
⟨ψQ,ℓ, ψQ,ℓ⟩= 2 Re
#
t 	→i 1
√
2 φ(t −ℓTs), t 	→i 1
√
2 φ(t −ℓTs)
$
= 1.
Notice that (16.14)–(16.15) can be simpliﬁed when φ is real:
Corollary 16.5.2. If, in addition to the assumptions of Proposition 16.5.1, we also
assume that the pulse shape φ is real, then the QAM signal can be written as
XPB(t) =
√
2A
n

ℓ=1
Re(Cℓ)
√
2 φ(t −ℓTs) cos(2πfct)
−
√
2A
n

ℓ=1
Im(Cℓ)
√
2 φ(t −ℓTs) sin(2πfct),
t ∈R,
(16.18)
and
'
t 	→
√
2 φ(t −ℓTs) cos(2πfct)
(∞
ℓ=−∞,
'
t 	→
√
2 φ(t −ℓTs) sin(2πfct)
(∞
ℓ=−∞
are orthonormal.
16.6 Spectral Eﬃciency
303
16.6
Spectral Eﬃciency
We next show that QAM achieves our spectral eﬃciency objective. We assume
that we are only allowed to transmit signals of bandwidth W around the carrier
frequency fc, so the transmitted signal can only occupy the frequencies f satisfying
|f| −fc
 ≤W/2.
In order for the QAM signal to meet this constraint, we choose a pulse shape φ
that is bandlimited to W/2 Hz, because the up-conversion doubles the bandwidth
(Note 16.4.1). Thus, by Corollary 11.3.5, the orthogonality (16.11) can only hold
if the baud period Ts satisﬁes Ts ≥1/(2 × W/2) or
Ts ≥1
W,
with the RHS being achievable by choosing φ to be the bandwidth-W/2 unit-energy
signal t 	→
√
W sinc(Wt).
If we choose Ts equal to 1/W (or only slightly larger than that), then our modulation
will support the transmission of complex symbols arriving at a rate of 1/Ts ≈W
complex symbols per second.
And since our QAM signal only occupies W Hz
around the carrier frequency, our scheme achieves a spectral eﬃciency of 1 [complex
dimension per second] per Hz. QAM thus achieves our spectral eﬃciency objective.
This is so exciting that we highlight the achievement:
QAM with the bandwidth-W/2 unit-energy pulse shape given by
t 	→
√
W sinc(Wt) transmits a sequence of real symbols arriving at
a rate of 2W real symbols per second as the coeﬃcients in a linear
combination of orthogonal signals, with the resulting waveform
being bandlimited to W Hz around the carrier frequency fc. It
thus achieves a spectral eﬃciency of
2 [real dimension/sec]
[passband Hz]
= 1 [complex dimension/sec]
[passband Hz]
.
16.7
QAM Constellations
In analogy to the deﬁnition of the constellation of a PAM scheme (Section 10.8),
we deﬁne the constellation of a QAM scheme (or, perhaps more appropriately, of
the mapping ϕ(·) in (16.4)) as the smallest subset of C of which Cℓis an element
for every ℓ∈{1, . . . , n} and for every realization of the data bits.
We denote
the constellation by C. The number of points in the constellation C is just the
number of elements of C.
Important constellations include the square 4-QAM constellation (also known as
QPSK)
{+1 + i, −1 + i, −1 −i, +1 −i},
304
Quadrature Amplitude Modulation
4-QAM
8-PSK
16-QAM
32-QAM
Figure 16.2: Some QAM constellations (drawn to no particular scale).
the square QAM constellation with (2ν) × (2ν) points
'
a + ib : a, b ∈

−(2ν −1), . . . , −3, −1, +1, +3, . . . , (2ν −1)
(
,
(16.19)
and the M-PSK (M-ary Phase Shift Keying) constellation comprising the M com-
plex numbers on the unit circle whose M-th power is one, i.e.,
'
1, ei2π/M, ei4π/M, ei6π/M, . . . , ei(M−1)2π/M(
.
See Figure 16.2 for some common QAM constellations. Please note that the square
16-QAM and the 16-PSK are just two of many possible constellations with 16
points. However, some engineers omit the word “square” and write 4-QAM, 16-
QAM, 64-QAM, etc. for the respective square constellations.
We can also deﬁne the minimum distance δ of a constellation C in analogy to
(10.22) as
δ ≜min
c,c′∈C
c̸=c′
|c −c′|.
(16.20)
16.8 Recovering the Complex Symbols via Inner Products
305
The constellation is centered if

c∈C
c = 0,
(16.21)
and in analogy to (10.25), we deﬁne the second moment of a constellation C as
1
# C

c∈C
|c|2.
(16.22)
Finally, in analogy to our deﬁnition of uncoded PAM transmission (Section 10.9),
we say that the QAM transmission is uncoded when the range of ϕ equals Cn.
16.8
Recovering the Complex Symbols via Inner Products
Recall that, by Proposition 16.5.1, if the time shifts of φ by integer multiples of Ts
are orthonormal, then the QAM signal can be written as
XPB =
√
2A
n

ℓ=1
Re(Cℓ) ψI,ℓ+
√
2A
n

ℓ=1
Im(Cℓ) ψQ,ℓ,
where the signals . . . , ψI,−1, ψQ,−1, ψI,0, ψQ,0, ψI,1, ψQ,1, . . ., which are given in
(16.15), are orthonormal. Consequently, the complex symbols can be recovered
from the QAM signal (in the absence of noise) using the inner products:
Re(Cℓ) =
1
√
2A ⟨XPB, ψI,ℓ⟩,
ℓ∈{1, . . . , n},
(16.23a)
Im(Cℓ) =
1
√
2A ⟨XPB, ψQ,ℓ⟩,
ℓ∈{1, . . . , n}.
(16.23b)
We next describe circuits to compute these inner products. With a view to future
chapters where noise will be present, we shall describe more general circuits that
compute the inner products ⟨r, ψI,ℓ⟩and ⟨r, ψQ,ℓ⟩for an arbitrary (not necessarily
QAM) energy-limited signal r. Moreover, since the calculation of the inner products
will not exploit the orthogonality condition (16.11), we shall describe the more
general setting where the pulse shape is arbitrary and refer to the notation of
(16.7). Thus, we shall present circuits to compute
⟨r, gI,ℓ⟩, ⟨r, gQ,ℓ⟩,
where gI,ℓand gQ,ℓand their baseband representations are given in (16.8) and
(16.9). Here r is an arbitrary energy-limited signal. We present two approaches:
an approach based on baseband conversion and a direct approach.
16.8.1
Inner Products via Baseband Conversion
We begin by noting that if the pulse shape g is bandlimited to W/2 Hz then both
gI,ℓand gQ,ℓare bandlimited to W Hz around the carrier frequency fc. Conse-
quently, since they contain no energy outside the bands [fc −W/2, fc + W/2] and
306
Quadrature Amplitude Modulation
[−fc−W/2, −fc+W/2], it follows from Parseval’s Theorem that the Fourier Trans-
form of r outside these bands does not inﬂuence the value of the inner products.
Thus, if s is the result of passing r through an ideal unit-gain bandpass ﬁlter of
bandwidth W around the carrier frequency fc, i.e.,
s = r ⋆BPFW,fc,
(16.24)
then
⟨r, gI,ℓ⟩= ⟨s, gI,ℓ⟩,
(16.25a)
⟨r, gQ,ℓ⟩= ⟨s, gQ,ℓ⟩.
(16.25b)
If we denote the baseband representation of s by sBB, then
⟨r, gI,ℓ⟩= ⟨s, gI,ℓ⟩
= 2 Re

⟨sBB, gI,ℓ,BB⟩

=
√
2 Re

⟨sBB, t 	→g(t −ℓTs)⟩

,
(16.26a)
where the ﬁrst equality follows from (16.25a); the second from Theorem 7.6.10;
and the ﬁnal equality from (16.9a). Similarly,
⟨r, gQ,ℓ⟩= ⟨s, gQ,ℓ⟩
= 2 Re

⟨sBB, gQ,ℓ,BB⟩

=
√
2 Re

⟨sBB, t 	→i g(t −ℓTs)⟩

=
√
2 Im

⟨sBB, t 	→g(t −ℓTs)⟩

.
(16.26b)
We next describe circuits to compute the RHS of (16.26a) & (16.26b). The circuit
to produce sBB from s was already discussed in Section 7.6 on the baseband rep-
resentation of passband signals (Figure 7.11). One multiplies s(t) by e−i2πfct and
then passes the result through a lowpass ﬁlter whose cutoﬀfrequency Wc satisﬁes
W
2 ≤Wc ≤2fc −W
2 ,
i.e.,
sBB =

t 	→s(t) e−i2πfct
⋆LPFWc,
or, in terms of real operations:
Re

sBB

=

t 	→s(t) cos(2πfct)

⋆LPFWc,
Im

sBB

= −

t 	→s(t) sin(2πfct)

⋆LPFWc .
This circuit is depicted in Figure 16.3. Notice that this circuit depends only on
the carrier frequency fc and on the bandwidth W; it does not depend on the pulse
shape.
Once sBB has been computed, the calculation of the inner products on the RHS of
(16.26a) & (16.26b) is straightforward. For example, to compute the inner product
16.8 Recovering the Complex Symbols via Inner Products
307
cos(2πfct)
90◦
×
×
r(t)
s(t)
BPFW,fc
LPFWc
LPFWc
W
2 ≤Wc ≤2fc −W
2
Re(sBB)
Im(sBB)
−sin(2πfct)
Figure 16.3: QAM demodulation: the front-end.
on the RHS of (16.26a) we note that from (16.26a)
⟨r, gI,ℓ⟩=
√
2 Re
	 ∞
−∞
sBB(t) g∗(t −ℓTs) dt

=
√
2
 ∞
−∞
Re

sBB(t)

Re

g(t −ℓTs)

dt
+
√
2
 ∞
−∞
Im

sBB(t)

Im

g(t −ℓTs)

dt,
(16.27)
where the terms on the RHS can be computed by feeding Re(sBB) to a matched
ﬁlter matched to Re(g) and sampling the ﬁlter’s output at time ℓTs
 ∞
−∞
Re

sBB(t)

Re

g(t −ℓTs)

dt =

Re(sBB) ⋆Re(~g)

(ℓTs),
(16.28)
and by feeding Im(sBB) to a matched ﬁlter matched to Im(g) and sampling the
ﬁlter’s output at time ℓTs
 ∞
−∞
Im

sBB(t)

Im

g(t −ℓTs)

dt =

Im(sBB) ⋆Im(~g)

(ℓTs).
(16.29)
Similarly, to compute the inner product on the RHS of (16.26b) we note that from
(16.26b)
⟨r, gQ,ℓ⟩=
√
2 Im
	 ∞
−∞
sBB(t) g∗(t −ℓTs) dt

=
√
2
 ∞
−∞
Im

sBB(t)

Re

g(t −ℓTs)

dt
−
√
2
 ∞
−∞
Re

sBB(t)

Im

g(t −ℓTs)

dt,
(16.30)
308
Quadrature Amplitude Modulation
Im(sBB)
Re(sBB)
1
√
2 ⟨r, gQ,ℓ⟩
1
√
2 ⟨r, gI,ℓ⟩
~g
~g
ℓTs
ℓTs
Figure 16.4: QAM demodulation: matched ﬁltering (g real).
where the inner products can be computed again using a matched ﬁlter:
 ∞
−∞
Im

sBB(t)

Re

g(t −ℓTs)

dt =

Im(sBB) ⋆Re(~g)

(ℓTs),
 ∞
−∞
Re

sBB(t)

Im

g(t −ℓTs)

dt =

Re(sBB) ⋆Im(~g)

(ℓTs).
Things become simpler when the pulse shape g is real. In this case (16.27) and
(16.30) simplify to
⟨r, gI,ℓ⟩=
√
2

Re

sBB(t)

g(t −ℓTs) dt,
g real,
(16.31a)
⟨r, gQ,ℓ⟩=
√
2

Im

sBB(t)

g(t −ℓTs) dt,
g real.
(16.31b)
Diagrams demonstrating how these inner products are computed are given in Fig-
ures 16.3 and 16.4. We have already discussed the ﬁrst diagram, which includes the
front-end bandpass ﬁlter and the circuit for producing sBB. The second diagram
includes the matched ﬁltering needed to compute the RHS of (16.31a) and the
RHS of (16.31b). Notice that we have accomplished our second objective in that
the ﬁrst circuit depends only on the carrier frequency fc (and the bandwidth W)
and the second circuit depends on the pulse shape but not on the carrier frequency.
16.8.2
Computing Inner Products Directly
The astute reader may have noticed that neither the bandpass ﬁltering of the
signal r nor the image rejection ﬁlters that produce sBB are needed for the com-
putation of the inner products. Indeed, starting from (16.8a)
⟨r, gI,ℓ⟩=

r, t 	→2 Re

gI,ℓ,BB(t) ei2πfct
= 2 Re

r, t 	→gI,ℓ,BB(t) ei2πfct
= 2 Re

t 	→r(t) e−i2πfct, gI,ℓ,BB

=
√
2 Re

t 	→r(t) e−i2πfct, t 	→g(t −ℓTs)

,
(16.32a)
where the second equality follows because r is real and the last equality from
(16.9a). Similarly, starting from (16.8b)
⟨r, gQ,ℓ⟩=

r, t 	→2 Re

gQ,ℓ,BB(t) ei2πfct
16.9 Filtering QAM Signals
309
= 2 Re

r, t 	→gQ,ℓ,BB(t) ei2πfct
= 2 Re

t 	→r(t) e−i2πfct, gQ,ℓ,BB

=
√
2 Re

t 	→r(t) e−i2πfct, t 	→i g(t −ℓTs)

=
√
2 Im

t 	→r(t) e−i2πfct, t 	→g(t −ℓTs)

,
(16.32b)
where the fourth equality follows from (16.9b). Notice that the RHS of (16.32a)
and the RHS of (16.32b) do not involve any ﬁltering. To see how to implement
them with real operations we can write them more explicitly as:
⟨r, gI,ℓ⟩=
√
2 Re
	 ∞
−∞
r(t) e−i2πfct g∗(t −ℓTs) dt

,
⟨r, gQ,ℓ⟩=
√
2 Im
	 ∞
−∞
r(t) e−i2πfct g∗(t −ℓTs) dt

,
or even more explicitly in terms of real operations as:
⟨r, gI,ℓ⟩=
√
2
 ∞
−∞
r(t) cos(2πfct) Re

g(t −ℓTs)

dt
−
√
2
 ∞
−∞
r(t) sin(2πfct) Im

g(t −ℓTs)

dt,
(16.33a)
⟨r, gQ,ℓ⟩= −
√
2
 ∞
−∞
r(t) cos(2πfct) Im

g(t −ℓTs)

dt
−
√
2
 ∞
−∞
r(t) sin(2πfct) Re

g(t −ℓTs)

dt.
(16.33b)
The two approaches we discussed for computing the inner products are, of course,
mathematically equivalent. The former makes more engineering sense, because the
bandpass ﬁlter typically guarantees that the energy in s is signiﬁcantly smaller
than in r, thus reducing the dynamic range required from the rest of the receiver.
The latter approach is mathematically cleaner because it requires less mathemat-
ical justiﬁcation. One need not check that the various ﬁlters satisfy the required
integrability conditions.
Moreover, this approach is more useful when r is not
energy-limited and when this is compensated for by the fast decay of the pulse
shape. (See, for example, the situation addressed by Proposition 3.4.4.)
16.9
Filtering QAM Signals
With all the simpliﬁcations aﬀorded by using a real pulse shape, why use a complex
one?
Why not restrict attention to real pulse shapes?
One reason is that we
sometimes end up with a complex pulse shape through no choice of ours. This can
happen, for example, when the transmitted signal that we have carefully designed is
passed through some ﬁlter over which we have no control. Indeed, as we next show,
passing a QAM signal through a ﬁlter is tantamount to replacing its pulse shape
by a pulse shape that might be complex even when the original one is real. Thus,
a linearly-dispersive channel of the kind we shall study in Chapter 32 transforms
310
Quadrature Amplitude Modulation
a QAM signal of a real pulse shape into one of a complex pulse shape (and adds
noise to boot).
Proposition 16.9.1 (QAM Signals through Stable Filters). Let XPB be a QAM
signal of the form (16.6), where the pulse shape g is an integrable signal that is
bandlimited to W/2 Hz and where fc > W/2. Passing XPB through a stable real
ﬁlter of impulse response h ∈L1 is tantamount to replacing its pulse shape g by
the pulse shape p, which is deﬁned as
p(t) =
 ∞
−∞
ˆg(f) ˆh(f + fc) ei2πft df,
t ∈R
(16.34a)
and which is a complex integrable signal that is bandlimited to W/2 Hz and whose
FT is
ˆp(f) = ˆg(f) ˆh(f + fc),
f ∈R.
(16.34b)
Proof. To identify the role of p, consider the signal t 	→2 Re

g(t) ei2πfct
. Since g
is an integrable signal that is bandlimited to W/2 Hz, this signal is a real integrable
passband signal that is bandlimited to W Hz around fc. Its baseband representa-
tion is g. It follows from Proposition 7.6.13 that the result of convolving this signal
with some real h ∈L1 is a real integrable passband signal that is bandlimited to
W Hz around fc and whose baseband representation is p
p =

t 	→2 Re

g(t) ei2πfct
⋆h

BB.
(16.35)
Moreover, by the same proposition, the FT of p is given by (16.34b).
Having identiﬁed the role of p and having established (16.34b), we next proceed
to study the result of ﬁltering XPB. From Proposition 7.6.13 we conclude that
XPB ⋆h is an integrable passband signal that is bandlimited to W Hz around fc,
and that its baseband representation (XPB ⋆h)BB is of FT
f 	→ˆXBB(f) ˆh(f + fc),
f ∈R,
(16.36)
where XBB is the baseband representation of XPB and is given in (16.5a). The FT
of XBB can be readily computed from (16.5a) using the basic properties of the FT
(Table 6.1):
ˆXBB(f) = A
n

ℓ=1
Cℓe−i2πfℓTs ˆg(f),
f ∈R.
(16.37)
Consequently, the FT of (XPB ⋆h)BB is
f 	→A
n

ℓ=1
Cℓe−i2πfℓTs ˆg(f) ˆh(f + fc),
f ∈R.
16.10 Exercises
311
Taking the IFT (which recovers (XPB ⋆h)BB by Proposition 7.6.13), we obtain
(XPB ⋆h)BB(t) =
 ∞
−∞
ˆXBB(f) ˆh(f + fc) ei2πft df
= A
n

ℓ=1
Cℓ
 ∞
−∞
e−i2πfℓTs ˆg(f) ˆh(f + fc) ei2πft df
= A
n

ℓ=1
Cℓ
 ∞
−∞
ˆg(f) ˆh(f + fc) ei2πf(t−ℓTs) df
= A
n

ℓ=1
Cℓp(t −ℓTs),
t ∈R,
(16.38)
where the last equality follows from (16.34a).
From (16.38) we conclude that
XPB ⋆h is also a QAM signal and that it is identical to XPB except that its pulse
shape is not g but p.
16.10
Exercises
Exercise 16.1 (Nyquist’s Criterion and Passband Signals). Corollary 11.3.4 provides con-
ditions under which the time shifts of a signal by integer multiples of Ts are orthonormal.
Discuss how these conditions apply to real passband signals of bandwidth W around the
carrier frequency fc. Speciﬁcally:
(i) Plot the function
f →
∞

ℓ=−∞
ˆy
	
f + ℓ
Ts


2
for the passband signal y of Figure 7.2. Pay attention to how the sum at positive
frequencies is inﬂuenced by the signal’s FT at negative frequencies.
(ii) Show that there exists a passband signal φ(·) whose bandwidth W around the
carrier frequency fc is 1/(2Ts) and whose time shifts by integer multiples of Ts are
orthonormal if, and only if, 4Tsfc is an odd integer. Show that such a signal must
satisfy (outside a set of frequencies of Lebesgue measure zero)
ˆφ(f)
 =
√
Ts I
 |f| −fc
 ≤
1
4Ts
!
,
f ∈R.
(iii) Let φ be an energy-limited baseband signal of bandwidth W/2 whose FT is a
symmetric function of frequency and whose time shifts by integer multiples of (2Ts)
are orthonormal.
Let the carrier frequency fc be larger than W/2 and satisfy
that 4Tsfc is an odd integer. Show that the (possibly complex) passband signal
t →
√
2 cos(2πfct) φ(t) is of bandwidth W around the carrier fc, and its time shifts
by integer multiples of Ts are orthonormal.
Exercise 16.2 (How General is QAM?). Under what conditions on A, fc, φ, W, and Ts
can we view the signal
t →A Re

ei(2πfct+φ)
n

ℓ=1
Cℓsinc

W(t −ℓTs)

as a QAM signal?
312
Quadrature Amplitude Modulation
Exercise 16.3 (M-PSK). Consider a QAM signal XPB of the form (16.6) with the pulse
shape g: t →I{−Ts/2 ≤t < Ts/2} and symbols

Cℓ

that are IID and uniformly dis-
tributed over the set
{ei2π/8, ei4π/8, . . . , ei14π/8, 1}.
(i) Plot a sample function of

XPB(t), t ∈R

.
(ii) Are the sample paths continuous?
(iii) Express XPB(t) in the form 2A cos

2πfct + Φ(t)

and describe Φ(t). Plot a sample
path of

Φ(t)

.
Exercise 16.4 (Transmission Rate, Encoder Rate, and Bandwidth). Data bits are to be
transmitted at rate Rb bits per second using QAM with a pulse shape φ satisfying the
orthonormality condition (16.11).
(i) Let W be the allotted bandwidth around the carrier frequency. What is the minimal
constellation size required for the data bits to be reliably communicated in the
absence of noise?
(ii) Repeat Part (i) if you are required to use a pulse shape of excess bandwidth 15%
or more.
Exercise 16.5 (Synthesis of 16-QAM). Let X1(·) and X2(·) be 4-QAM (QPSK) signals
that are given for every t ∈R by
Xν(t) = 2 Re

A
n

ℓ=1
C(ν)
ℓ
g(t −ℓTs) ei2πfct

,
ν = 1, 2,
where the symbols

C(ν)
ℓ

take on the values ±1 ± i. Show that for the right choice of the
constant α ∈R, the signal
X(t) = αX1(t) + X2(t),
t ∈R
can be viewed as a 16-QAM signal with a square constellation.
Exercise 16.6 (Orthogonality of the In-Phase and Quadrature Components). Let the
pulse shape g be a real integrable signal that is bandlimited to W/2 Hz, and let the
carrier frequency fc be larger than W/2. Show that, even if the time shifts of g by integer
multiples of Ts are not orthonormal, the signals
t →g(t −ℓTs) cos(2πfct + ϕ) and t →g(t −ℓ′Ts) sin(2πfct + ϕ)
are orthogonal for all integers ℓ, ℓ′ (not necessarily distinct). Here ϕ ∈[−π, π) is arbitrary.
Exercise 16.7 (The Importance of the Phase). Let x and y be real integrable signals
that are bandlimited to W/2 Hz. Let the transmitted signal s be
s(t) = Re
	
x(t) + iy(t)

ei(2πfct+φT)
= x(t) cos(2πfct + φT) −y(t) sin(2πfct + φT),
t ∈R,
where fc > W/2, and where φT denotes the phase of the transmitted carrier. The receiver
multiplies s(t) by 2 cos(2πfct+φR) (where φR denotes the phase of the receiver’s oscillator)
16.10 Exercises
313
and passes the resulting product through a lowpass ﬁlter of cutoﬀfrequency W/2 to
produce the signal ˜x:
˜x(t) =
	
τ →s(τ) 2 cos(2πfcτ + φR)

⋆LPFW/2

(t),
t ∈R.
Express ˜x(·) in terms of x(·), y(·), φT and φR. Evaluate your expression in the following
cases: φT = φR, φT −φR = π, φT −φR = π/2, and φT −φR = π/4.
Exercise 16.8 (Phase Imprecision). Consider QAM with a real pulse shape and a receiver
that performs a conversion to baseband followed by matched ﬁltering (Section 16.8.1).
Write an expression for the output of the receiver if its oscillator is at the right frequency
but lags the phase of the transmitter’s oscillator by Δφ.
Exercise 16.9 (Rotating a QAM Constellation). Show that rotating a QAM constellation
changes neither its second moment nor its minimum distance.
Exercise 16.10 (Optimal Rectangular Constellation). Consider all rectangular constella-
tions of the form
{a + ib, a −ib, −a + ib, −a −ib},
where a and b are real. Which of these constellations whose second moment is one has
the largest minimum distance?
Exercise 16.11 (Delaying and Reﬂecting QAM Signals).
(i) Is a delayed QAM signal a QAM signal?
(ii) Is the mirror image of a QAM signal a QAM signal?
Chapter 17
Complex Random Variables and Processes
17.1
Introduction
We ﬁrst encountered complex random variables in Chapter 16 on QAM. There we
considered an encoder that maps k-tuples of bits into n-tuples of complex numbers,
and we then considered the result of applying this encoder to random bits. The
resulting symbols were therefore random and were taking values in the complex
ﬁeld, i.e., they were complex random variables. Complex random variables are
functions that map “luck” into the complex ﬁeld: they map every outcome of the
experiment ω ∈Ω to a complex number. Thus, they are very much like regular
random variables, except that they take values in the complex ﬁeld. They can
always be considered as pairs of real variables: their real and imaginary parts.
It is perfectly meaningful to discuss their expectation and variance.
If C is a
complex random variable, then
E[C] = E

Re(C)

+ i E

Im(C)

,
E
%
|C|2&
= E
%
Re(C)
2&
+ E
%
Im(C)
2&
,
and
Var[C] = E
%C −E[C]
2&
= E

|C|2
−
E[C]
2.
In this chapter we shall make the above deﬁnition of complex random variables
more formal and also discuss complex random vectors and complex stochastic pro-
cesses.
Complex random variables can be avoided if one treats such variables as pairs of
real variables. However, we do not recommend this approach. Many of the complex
random variables and processes encountered in Digital Communications possess ad-
ditional properties that simplify their manipulation, and complex random variables
are better suited to take advantage of these simpliﬁcations.
We begin this chapter with some notation followed by some basic deﬁnitions for
complex random variables.
We next introduce a property that simpliﬁes their
314
17.2 Notation
315
manipulation: properness. (Another such property, circular symmetry, is described
in Chapter 24.) Finally, we extend the discussion to complex random vectors and
conclude with a discussion of complex stochastic processes.
17.2
Notation
The notation we use in this chapter is fairly standard. The only issue that may
need clariﬁcation is the diﬀerence between three matrix/vector operations: trans-
position, conjugation, and Hermitian conjugation. These operations are described
next.
All vectors in this chapter are column vectors. Thus, a vector a whose components
are a(1), . . . , a(n) is the column vector
a =
⎛
⎜
⎜
⎜
⎝
a(1)
a(2)
...
a(n)
⎞
⎟
⎟
⎟
⎠.
(17.1)
We shall sometimes refer to such a vector a as an n-vector to make the number of
its components explicit. For typesetting reasons, we shall usually use the notation
a =

a(1), . . . , a(n)T,
(17.2)
which is more space eﬃcient. Here the operator (·)T denotes the matrix trans-
pose. Thus if we think of (a(1), . . . a(n)) as a 1 × n matrix, then (a(1), . . . a(n))T is
this matrix’s transpose, i.e., an n × 1 matrix, or a vector. More generally, if A is
an n × m matrix, then AT is an m × n matrix whose Row-j Column-ℓcomponent
is the Row-ℓColumn-j component of A. We say that A is symmetric if AT = A.
We use (·)∗to denote componentwise complex conjugation. Thus, if a is as
in (17.1), then
a∗=
⎛
⎜
⎜
⎜
⎝

a(1)∗

a(2)∗
...

a(n)∗
⎞
⎟
⎟
⎟
⎠.
(17.3)
We use (·)† to denote Hermitian conjugation, i.e., the componentwise conjugate
of the transposed matrix. Thus, if a is as in (17.1), then a† is the 1 × n matrix
a† =

a(1)∗, . . . ,

a(n)∗
.
(17.4)
The Hermitian conjugate A† of an n×m matrix A is an m×n matrix whose Row-j
Column-ℓcomponent is the complex conjugate of the Row-ℓColumn-j component
of the matrix A. We say that a matrix A is conjugate-symmetric or self-adjoint
or Hermitian if A† = A.
Note that if a and b are n-vectors, then aTb is a scalar
aTb =
n

j=1
a(j) b(j),
(17.5)
316
Complex Random Variables and Processes
whereas abT is the n × n matrix
abT =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
a(1)b(1)
a(1)b(2)
. . .
a(1)b(n)
a(2)b(1)
a(2)b(2)
. . .
a(2)b(n)
...
...
...
...
...
...
...
...
a(n)b(1)
a(n)b(2)
. . .
a(n)b(n)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
17.3
Complex Random Variables
We say that C is a complex random variable (CRV) on the probability space
(Ω, F, P) if C : Ω →C is a mapping from Ω to the complex ﬁeld C such that both
Re(C) and Im(C) are random variables on (Ω, F, P).
Any CRV Z can be written in the form Z = X + i Y , where X and Y are real
random variables. But there are some advantages to studying complex random
variables over pairs of real random variables. Those will become apparent when we
discuss analytic functions of complex random variables and when we discuss com-
plex random variables that have special properties such as that of being “proper”
or that of being “circularly-symmetric.”
Many of the deﬁnitions related to complex random variables are similar to the
analogous deﬁnitions for pairs of real random variables, but some are not. We
shall try to emphasize the latter.
17.3.1
Distribution and Density
Since it makes no sense to say that one complex number is smaller than another, we
cannot deﬁne the cumulative distribution function (CDF) of a CRV as in the real
case: an expression like “Pr[Z ≤1 + i]” is meaningless. We can, however, discuss
the joint distribution function of the real and imaginary parts of a CRV, which
speciﬁes Pr[Re(Z) ≤x, Im(Z) ≤y] for all x, y ∈R. We say that two complex
random variables W and Z are of equal law (or have the same distribution) and
write W
L= Z, if the joint distribution of the pair (Re(W), Im(W)) is identical to
the joint distribution of the pair (Re(Z), Im(Z)):

W
L= Z

⇐⇒

Pr

Re(W) ≤x, Im(W) ≤y

= Pr

Re(Z) ≤x, Im(Z) ≤y

, x, y ∈R

.
(17.6)
Similarly, we can deﬁne the density function fZ(·) (if it exists) of a CRV Z at the
point z ∈C as the joint density of the real pair (Re(Z), Im(Z)) at (Re(z), Im(z)):
fZ(z) ≜fRe(Z),Im(Z)

Re(z), Im(z)

,
z ∈C,
(17.7)
which can also be written as
fZ(z) =
∂2
∂x ∂y Pr

Re(Z) ≤x, Im(Z) ≤y

x=Re(z),y=Im(z)
,
z ∈C.
(17.8)
17.3 Complex Random Variables
317
The notions of distribution function and density of a CRV extend immediately to
pairs of complex variables and, more generally, to n-tuples.
17.3.2
The Expectation
The expectation of a CRV can be deﬁned in terms of the expectations of its real
and imaginary parts:
E[Z] = E[Re(Z)] + i E[Im(Z)] ,
(17.9)
provided that the two real expectations E[Re(Z)] and E[Im(Z)] are ﬁnite. With
this deﬁnition one can readily verify that, whenever E[Z] is deﬁned, conjugation
and expectation commute
E[Z∗] = (E[Z])∗,
(17.10)
and
Re

E[Z]

= E

Re(Z)

,
(17.11a)
Im

E[Z]

= E

Im(Z)

.
(17.11b)
If the CRV Z has a density fZ(·), then the expectation E[g(Z)] for some measurable
function g: C →C can be formally written as
E

g(Z)

=

z∈C
fZ(z) g(z) dz
(17.12)
or, in terms of real integrals, as
E

g(Z)

=
 ∞
−∞
 ∞
−∞
fZ(x + iy) Re

g(x + iy)

dx dy
+ i
 ∞
−∞
 ∞
−∞
fZ(x + iy) Im

g(x + iy)

dx dy.
(17.13)
Thus, rather than computing the distribution of g(Z) and then computing the
expectations of its real and imaginary parts, one can use (17.12).
17.3.3
The Variance
The deﬁnition of the variance of a CRV is not consistent with viewing the CRV as
a pair of real random variables. The variance Var[Z] of a CRV Z is deﬁned as
Var[Z] ≜E

|Z −E[Z]|2
(17.14a)
= E

|Z|2
−|E[Z]|2
(17.14b)
= Var

Re(Z)

+ Var

Im(Z)

.
(17.14c)
This deﬁnition should be contrasted with the deﬁnition of the covariance matrix
of the pair (Re(Z), Im(Z))
	
Var

Re(Z)

Cov

Re(Z), Im(Z)

Cov

Re(Z), Im(Z)

Var

Im(Z)


.
318
Complex Random Variables and Processes
One can compute the variance of Z from the covariance matrix of (Re(Z), Im(Z)),
but not the other way around. Indeed, the variance of Z is just the trace of the
covariance matrix of (Re(Z), Im(Z)).
To derive (17.14b) from (17.14a) we note that
E

|Z −E[Z]|2
= E

(Z −E[Z])(Z −E[Z])∗
= E

(Z −E[Z])(Z∗−E[Z∗])

= E

(Z −E[Z])Z∗
−E

(Z −E[Z])

E

Z∗
= E

(Z −E[Z])Z∗
= E[ZZ∗] −E[Z] E[Z∗]
= E

|Z|2
−|E[Z]|2,
where we only used the linearity of expectation and (17.10). Here the ﬁrst equality
follows by writing |w|2 as ww∗; the second by (17.10); the third by simple algebra;
the fourth because the expectation of Z −E[Z] is zero; and the ﬁnal by (17.10).
To derive (17.14c) from (17.14b) we write E

|Z|2
as E

(Re(Z))2 + (Im(Z))2
and
express |E[Z]|2 using (17.9) as E[Re(Z)]2 + E[Im(Z)]2.
17.3.4
Proper Complex Random Variables
Many of the complex random variables that appear in Digital Communications
are proper. This is a concept that has no natural counterpart for real random
variables.
Deﬁnition 17.3.1 (Proper CRV). We say that the CRV Z is proper if the following
three conditions are all satisﬁed: it is of zero-mean; it is of ﬁnite variance; and
E

Z2
= 0.
(17.15)
Notice that the LHS of (17.15) is, in general, a complex number, so (17.15) is
equivalent to two real equations:
E

Re(Z)2
= E

Im(Z)2
(17.16a)
and
E

Re(Z) Im(Z)

= 0.
(17.16b)
This leads to the following characterization of proper complex random variables.
Proposition 17.3.2. A CRV Z is proper if, and only if, all three of the following
conditions are satisﬁed: Z is of zero mean; Re(Z) & Im(Z) have the same ﬁnite
variance; and Re(Z) & Im(Z) are uncorrelated.
An example of a proper CRV is one taking on the four values {±1, ±i} equiprobably.
We mentioned earlier in Section 17.3.3 that the variance of a CRV is not the
same as the covariance matrix of the tuple consisting of its real and imaginary
parts.
While the covariance matrix determines the variance, the variance does
17.3 Complex Random Variables
319
not uniquely determine the covariance matrix. However, if a CRV is proper, then
its variance uniquely determines the covariance matrix of its real and imaginary
parts. Indeed, by Proposition 17.3.2 and (17.14c), a zero-mean ﬁnite-variance CRV
is proper if, and only if, the covariance matrix of the pair (Re(Z), Im(Z)) is
	 1
2Var[Z]
0
0
1
2Var[Z]

.
17.3.5
The Covariance
The covariance Cov[Z, W] between the complex random variables Z and W is
deﬁned by
Cov[Z, W] ≜E
%
Z −E[Z]

W −E[W]
∗&
.
(17.17)
Again, this deﬁnition is diﬀerent from the one for pairs of real random variables:
the covariance between two pairs of real random variables is a real matrix, whereas
the covariance between two CRVs is a complex scalar.
Some of the key properties of the covariance are listed next. They hold whenever
the α’s and β’s are deterministic complex numbers and the covariances on the RHS
are deﬁned.
(i) Conjugate Symmetry:
Cov[Z, W] =

Cov[W, Z]
∗.
(17.18)
(ii) Sesquilinearity:
Cov[αZ, W] = α Cov[Z, W] ,
(17.19)
Cov[Z1 + Z2, W] = Cov[Z1, W] + Cov[Z2, W] ,
(17.20)
Cov[Z, βW] = β∗Cov[Z, W] ,
(17.21)
Cov[Z, W1 + W2] = Cov[Z, W1] + Cov[Z, W2] ,
(17.22)
and, more generally,
Cov

n

j=1
αjZj,
n′

j′=1
βj′Wj′

=
n

j=1
n′

j′=1
αj β∗
j′ Cov[Zj, Wj′] .
(17.23)
(iii) Relation with Variance:
Var[Z] = Cov[Z, Z] .
(17.24)
(iv) Variance of Linear Functionals:
Var

n

j=1
αjZj

=
n

j=1
n

j′=1
αj α∗
j′ Cov[Zj, Zj′] .
(17.25)
320
Complex Random Variables and Processes
17.3.6
The Characteristic Function
The deﬁnition of the characteristic function of a CRV is consistent with viewing it as
a pair of real random variables. Recall that the characteristic function ΦX : R →C
of a real random variable X is deﬁned by
ΦX : ϖ 	→E

eiϖX
,
ϖ ∈R.
(17.26)
For a pair of real random variables X, Y the joint characteristic function is the
mapping ΦX,Y : R2 →C deﬁned by
ΦX,Y : (ϖ1, ϖ2) 	→E
%
ei(ϖ1X+ϖ2Y )&
,
ϖ1, ϖ2 ∈R.
(17.27)
Note that the expectations in (17.26) and (17.27) are always deﬁned, because the
argument to the expectation operator is of modulus one (|eir| = 1, whenever r is
real). This motivates us to deﬁne the characteristic function for a complex random
variable as follows.
Deﬁnition 17.3.3 (Characteristic Function of a CRV). The characteristic func-
tion ΦZ : C →C of a complex random variable Z is deﬁned as
ΦZ(ϖ) ≜E
%
ei Re(ϖ∗Z)&
,
ϖ ∈C
= E
%
ei( Re(ϖ) Re(Z)+Im(ϖ) Im(Z))&
,
ϖ ∈C.
Here we can think of Re(ϖ) and Im(ϖ) as playing the role of ϖ1 and ϖ2 in (17.27).
17.3.7
Transforming Complex Variables
We next calculate the density of the result of applying a (deterministic) transfor-
mation to a CRV. The key to the calculation is to treat the CRV as a pair of real
random variables and to then apply the analogous result regarding the transfor-
mation of a random real tuple. To that end we recall the following basic theorem
regarding the transformation of real random vectors. In the theorem’s statement
we encounter the notion of an open subset of Rn. Loosely speaking, D ⊆Rn is an
open subset of Rn if to each x ∈D there corresponds some ϵ > 0 such that the
ball of radius ϵ and center x is fully contained in D.1
Theorem 17.3.4 (Transforming Real Random Vectors). Let g: D →R be a one-
to-one mapping from an open subset D of Rn onto a subset R of Rn. Assume
that g has continuous partial derivatives in D and that the Jacobian determinant
det (∂g(x)/∂x) is at no point of D zero.
Let the real random n-vector X have
the density function fX(·) and satisfy Pr[X ∈D] = 1. Then the random n-vector
Y = g(X) is of density
fY(y) =
fX(x)
det ∂g(x)
∂x


x=g−1(y)
· I{y ∈R}.
(17.28)
1Thus, D is an open subset of Rn if D ⊆Rn and if to each x ∈D there corresponds some
ϵ > 0 such that each y ∈Rn satisfying (x −y)T(x −y) ≤ϵ2 is in D.
17.3 Complex Random Variables
321
Using Theorem 17.3.4 we can relate the density of a CRV Z and the joint distri-
bution of its phase and magnitude.
Lemma 17.3.5 (The Joint Density of the Magnitude and Phase of a CRV). Let Z
be a CRV of density fZ(·), and let R = |Z| and Θ ∈[−π, π) be the magnitude and
argument of Z:
Z = R eiΘ,
R ≥0, Θ ∈[−π, π).
Then the joint distribution of the pair (R, Θ) is of density
fR,Θ(r, θ) = rfZ

r eiθ
,
r > 0, θ ∈[−π, π).
(17.29)
Proof. This result follows directly from Theorem 17.3.4 by computing the absolute
value of the Jacobian determinant of the transformation2 (x, y) 	→(r, θ) where
r =

x2 + y2 and θ = arctan(y/x):
det
-
∂r
∂x
∂r
∂y
∂θ
∂x
∂θ
∂y
. =
1

x2 + y2
= 1
r .
For the next change-of-variables result we recall some basic concepts from Complex
Analysis. Given some z0 ∈C and some nonnegative real number r ≥0, we denote
by D(z0, r) the disc of radius r that is centered at z0:
D(z0, r) ≜{z ∈C : |z −z0| < r}.
We say that a subset D of the complex plane is open if to each z ∈D there
corresponds some ϵ > 0 such that D(z0, ϵ) ⊆D. Let g: D →C be some function
from an open set D ⊆C to C. Let z0 be in D. We say that g(·) is diﬀerentiable
at z0 ∈D and that its derivative at z0 is the complex number g′(z0), if for every
ϵ > 0 there exists some δ > 0 such that

g

z0 + h

−g

z0

h
−g′
z0

 ≤ϵ,
(17.30)
whenever the complex number h ∈C satisﬁes 0 < |h| ≤δ. It is important to note
that here h is complex. If g is diﬀerentiable at every z ∈D, then we say that g is
holomorphic or analytic in D.3
Deﬁne the mappings
u, v: {x, y ∈R : x + iy ∈D} →R
(17.31a)
by
u(x, y) = Re

g(x + iy)

,
(17.31b)
and
v(x, y) = Im

g(x + iy)

.
(17.31c)
2Here D is the set R2 without the origin.
3There is some confusion in the literature about the terms analytic, holomorphic, and
regular. We are following here (Rudin, 1987, Chapter 10).
322
Complex Random Variables and Processes
Proposition 17.3.6 (The Cauchy-Riemann Equations). Let D ⊆C be open and
let g: D →C be analytic in D. Let u, v be deﬁned by (17.31). Then u and v
satisfy the Cauchy-Riemann equations
∂u(x, y)
∂x
= ∂v(x, y)
∂y
,
(17.32a)
∂u(x, y)
∂y
= −∂v(x, y)
∂x
(17.32b)
at every x, y ∈R such that x + iy ∈D, and
g′(z) =
	∂u(x, y)
∂x
+ i∂v(x, y)
∂x


(x,y)=( Re(z),Im(z))
,
z ∈D.
(17.33)
Moreover, the partial derivatives in (17.32) are continuous in the subset of R2
deﬁned by {x, y ∈R : x + iy ∈D}.
Proof. See (Rudin, 1987, Chapter 11, Theorem 11.2 & Theorem 11.4) or (Nehari,
1975, Chapter II, Section 5 & Chapter III, Section 3).
We can now state the change-of-variables theorem for CRVs.
Theorem 17.3.7 (Transforming Complex Random Variables). Let g: D →R be
a one-to-one mapping from an open subset D of C onto a subset R of C. Assume
that g is analytic in D and that at no point of D is the derivative of g zero. Let
the CRV have the density function fZ(·) and satisfy Pr[Z ∈D] = 1. Then the CRV
deﬁned by W = g(Z) is of density
fW (w) =
fZ(z)
|g′(z)|2

z=g−1(w)
I{w ∈R}.
(17.34)
Here g−1(w) denotes the point in D that is mapped by g to w.
Note 17.3.8. The square in (17.34) does not appear in dealing with real random
variables. It appears here because a mapping of complex numbers is essentially
two-dimensional: scaling by α ∈C translates to a scaling of area by |α|2.
Proof. To prove (17.34) we begin by expressing the function g(·) as
g(x + iy) = u(x, y) + iv(x, y),

x, y ∈R, x + iy ∈D

,
where u(x, y) = Re(g(x + iy)) and v(x, y) = Im(g(x + iy)) are deﬁned in (17.31b)
and (17.31c). The density of g(Z) is, by deﬁnition, the joint density of the pair
u(Re(Z), Im(Z)), v(Re(Z), Im(Z)). And the joint density of the pair(Re(Z), Im(Z))
is just the density of Z.
Thus, if we could relate the joint density of the pair
u(Re(Z), Im(Z)), v(Re(Z), Im(Z)) to the joint density of the pair (Re(Z), Im(Z)),
then we could relate the density of g(Z) to the density of Z.
To relate the joint density of the pair u(Re(Z), Im(Z)), v(Re(Z), Im(Z)) to the
joint density of the pair (Re(Z), Im(Z)) we employ Theorem 17.3.4. To that end
17.4 Complex Random Vectors
323
we need to compute the absolute value of the Jacobian determinant. This we do
as follows:
det
- ∂u
∂x
∂u
∂y
∂v
∂x
∂v
∂y
. =
det
- ∂u
∂x
−∂v
∂x
∂v
∂x
∂u
∂x
.
=
	∂u
∂x

2
+
	∂v
∂x

2
= |g′(x + iy)|2,
(17.35)
where the ﬁrst equality follows from the Cauchy-Riemann equations (17.32); the
second from a direct calculation of the determinant of a 2 × 2 matrix; and where
the last equality follows from (17.33). The theorem now follows from (17.35) and
Theorem 17.3.4.
17.4
Complex Random Vectors
We say that Z = (Z(1), . . . , Z(n))T is a complex random vector on the probability
space (Ω, F, P) if it is a mapping from the outcome set Ω to Cn such that the real
2n-vector

Re

Z(1)
, Im

Z(1)
, . . . , Re

Z(n)
, Im

Z(n)T
comprising the real and imaginary parts of its components is a real random vector
on (Ω, F, P), i.e., if each of the components of Z is a CRV.
We say that the complex random vector Z = (Z(1), . . . , Z(n))T and the complex
random vector W = (W (1), . . . , W (n))T are of equal law (or have the same distri-
bution) and write Z
L= W, if the real vector taking values in R2n whose components
are the real and imaginary parts of the components of Z has the same distribution
as the analogous vector for W, i.e., if for all x1, . . . , xn, y1, . . . , yn ∈R
Pr
%
Re

Z(1)
≤x1, Im

Z(1)
≤y1, . . . , Re

Z(n)
≤xn, Im

Z(n)
≤yn
&
= Pr
%
Re

W (1)
≤x1, Im

W (1)
≤y1, . . . , Re

W (n)
≤xn, Im

W (n)
≤yn
&
.
The expectation of a complex random vector is the vector consisting of the ex-
pectation of each of its components. We say that a complex random vector is of
ﬁnite variance if each of its components is a CRV of ﬁnite variance.
17.4.1
The Covariance Matrix
The discussion in Section 17.3.5 can be generalized to random complex vectors.
The covariance matrix KZZ of a ﬁnite-variance complex random n-vector Z is
deﬁned as the conjugate-symmetric n × n matrix
KZZ ≜E

(Z −E[Z])(Z −E[Z])†
.
(17.36)
Once again, this deﬁnition is not consistent with viewing the random complex
vector as a vector of length 2n of real random variables. The latter would have a
real symmetric 2n × 2n covariance matrix.
324
Complex Random Variables and Processes
The reader may wonder why we have chosen to deﬁne the covariance and the covari-
ance matrix with the conjugation sign. Why not look at E

(Z −E[Z])(Z −E[Z])T
?
The reason is that (17.36) is simply much more useful in applications. For example,
for any deterministic α1, . . . , αn ∈C the variance of n
j=1 αjZj can be computed
from KZZ (using (17.25)) but not from E

(Z −E[Z])(Z −E[Z])T
.
17.4.2
Proper Complex Random Vectors
The notion of proper complex random variables extends to vectors:
Deﬁnition 17.4.1 (Proper Complex Random Vector). A complex random vector Z
is said to be proper if the following three conditions are all met: it is of zero mean;
it is of ﬁnite variance; and
E

ZZT
= 0.
(17.37)
An alternative deﬁnition can be given based on linear functionals:
Proposition 17.4.2. The complex random n-vector Z is proper if, and only if, for
every deterministic vector α ∈Cn the CRV αTZ is proper.
Proof. We begin by noting that Z is of zero mean if, and only if, αTZ is of zero
mean for all α ∈Cn. This can be seen from the relation
E

αTZ

= αTE[Z] ,
α ∈Cn.
(17.38)
Indeed, (17.38) demonstrates that if Z is of zero mean then so is αTZ for every
α ∈Cn. Conversely, if αTZ is of zero mean for all α ∈Cn, then, a fortiori, it must
also be of zero mean for the choice of α = E[Z]∗, which yields that 0 = E[Z]† E[Z]
and hence that E[Z] must be zero (because E[Z]† E[Z] is the sum of the squared
magnitudes of the components of E[Z]).
We next note that Z is of ﬁnite variance if, and only if, αTZ is of ﬁnite variance
for every α ∈Cn. The proof is not diﬃcult and is omitted.
We thus continue with the proof under the assumption that Z is of zero mean and
of ﬁnite variance. We note that for any deterministic complex vector α ∈Cn
E

(αTZ)2
= E

(αTZ)(αTZ)

= E

(αTZ)(αTZ)T
= E

αTZZTα

= αTE

ZZT
α,
α ∈Cn,
(17.39)
where the ﬁrst equality follows by writing the square of a random variable as the
product of the variable by itself; the second because the transpose of a scalar is
the original scalar; the third by the transpose rule
(AB)T = BTAT;
(17.40)
and the ﬁnal equality because α is deterministic.
17.4 Complex Random Vectors
325
From (17.39) it follows that if Z is proper, then so is αTZ for all α ∈Cn. Actually,
(17.39) also proves the reverse implication by substituting A = E

ZZT
in the
following fact from Matrix Theory:

αTAα = 0, α ∈Cn
=⇒

A = 0

,
A symmetric.
(17.41)
To prove this fact from Matrix Theory assume that A is symmetric, i.e., that
a(j,ℓ) = a(ℓ,j),
j, ℓ∈{1, . . . , n}.
(17.42)
Let α = eℓwhere eℓis all-zero except for its ℓ-th component, which is one. The
equality eT
ℓAeℓ= 0 for every ℓ∈{1, . . . , n} is equivalent to
a(ℓ,ℓ) = 0,
ℓ∈{1, . . . , n}.
(17.43)
Next choose α = ej + eℓ. The equality
(ej + eℓ)TA(ej + eℓ) = 0
for every j, ℓ∈{1, . . . , n} is then equivalent to
a(j,ℓ) + a(j,j) + a(ℓ,j) + a(ℓ,ℓ) = 0,
j, ℓ∈{1, . . . , n}.
(17.44)
Equations (17.42), (17.43), and (17.44) guarantee that the matrix A is all-zero.
An important observation regarding complex random vectors is that a linearly-
transformed proper vector is also proper:
Proposition 17.4.3 (Linear Transformation of a Proper Random Vector). If the
complex random n-vector Z is proper, then so is the complex random m-vector AZ
for every deterministic m × n complex matrix A.
Proof. We leave it to the reader to verify that the hypothesis that Z is proper
implies that AZ must be of zero mean and of ﬁnite variance. To show that AZ
is proper, it thus remains to show that E

(AZ)(AZ)T
= 0. This we do by direct
calculation:
E

(AZ)(AZ)T
= E

AZZTAT
= AE

ZZT
AT
= 0,
where the ﬁrst equality follows from the rule for the transpose of a product (17.40);
the second because A is deterministic; and the last from the hypothesis that Z is
proper, so E

ZZT
= 0.
326
Complex Random Variables and Processes
17.4.3
The Characteristic Function
The deﬁnition we gave in Section 17.3.6 for the characteristic function of a CRV
extends naturally to vectors: the characteristic function ΦZ : Cn →C of a complex
random n-vector Z is deﬁned as
ΦZ(ϖ) ≜E
%
ei Re(ϖ†Z)&
,
ϖ ∈Cn.
The connection between the characteristic function of the complex n-vector Z and
that of the (real) random 2n-vector4

Re

Z(1)
, Im

Z(1)
, . . . , Re

Z(n)
, Im

Z(n)T
is revealed once we note that
Re

ϖ†Z

= Re

ϖ(1)
Re

Z(1)
+ Im

ϖ(1)
Im

Z(1)
+ · · ·
+ Re

ϖ(n)
Re

Z(n)
+ Im

ϖ(n)
Im

Z(n)
.
This connection and the fact that two random 2n-vectors have identical laws if,
and only if, they have identical characteristic functions (Proposition 23.4.4) can be
used to obtain the following theorem:
Theorem 17.4.4. Two complex n-vectors are of equal law if, and only if, their
characteristic functions are identical:

Z
L= W

⇐⇒

ΦZ(ϖ) = ΦW(ϖ), ϖ ∈Cn
.
(17.45)
Corollary 17.4.5. The complex random n-vectors Z and W are of equal law if, and
only if, for every deterministic vector α ∈Cn the complex random variables αTZ
and αTW are of equal law:

Z
L= W

⇐⇒

αTZ
L= αTW,
α ∈Cn
.
(17.46)
Proof. The direction that needs proof is that equality in law of all linear combi-
nations implies equality in law between the vectors. But this readily follows from
Theorem 17.4.4, because equality in law of the linear combinations implies that the
law of ϖ†Z is equal to the law of ϖ†W for every ϖ ∈Cn. This in turn implies
that ei Re(ϖ†Z) L= ei Re(ϖ†W), from which, upon taking expectations, we obtain that
Z and W have identical characteristic functions. Thus, by the theorem, they are
equal in law.
17.4.4
Transforming Complex Random Vectors
The change of density rule (17.34) can be generalized to analytic multi-variable
mappings (Exercise 17.13). But here we shall only present a version of this result
for linear mappings:
4The characteristic function of a (real) random vector is discussed in Section 23.4.4.
17.4 Complex Random Vectors
327
Lemma 17.4.6 (Linearly Transforming Complex Random Vectors). Let the com-
plex random n-vector W be given by
W = AZ,
where A is a nonsingular deterministic complex n×n matrix, and where the complex
random n-vector Z has the density fZ(·). Then W is of density
fW(w) =
1
|det A|2 fZ(A−1w),
w ∈Cn.
(17.47)
Proof. The proof is based on viewing the complex n × n linear transformation
from Z to W as a 2n×2n real transformation, and on then applying Theorem 17.3.4.
Stack the real parts of the components of Z on top of the imaginary parts in a real
random 2n-vector S:
S =

Re

Z(1)
, . . . , Re

Z(n)
, Im

Z(1)
, . . . , Im

Z(n)T
.
(17.48)
Similarly, stack the real parts of the components of W on top of the imaginary
parts in a real random 2n-vector T:
T =

Re

W (1)
, . . . , Re

W (n)
, Im

W (1)
, . . . , Im

W (n)T
.
We can then express T as the result of multiplying the random vector S by a
2n × 2n real matrix:
T =
	
Re(A)
−Im(A)
Im(A)
Re(A)

S,
where Re(A) and Im(A) denote the componentwise real and imaginary parts of A.
The result will follow from Theorem 17.3.4 once we show that the absolute value
of the Jacobian determinant of this transformation is |det A|2. Using elementary
row and column operations we compute:
det
	
Re(A)
−Im(A)
Im(A)
Re(A)

= det
	
A
−Im(A)
−iA
Re(A)

= det
	
A
−Im(A)
0
A∗

= (det A) (det A∗)
= |det A|2,
where the ﬁrst equality follows by the elementary column operations of multiplying
the right columns by (−i) and adding the result to the left columns; the second
from the elementary row operations of multiplying the top rows by i and adding
the result to the bottom rows; the third from the identity
det
	B
C
0
D

= (det B) (det D);
and the last by noting that for any square matrix B
det(B∗) = (det B)∗.
328
Complex Random Variables and Processes
17.5
Discrete-Time Complex Stochastic Processes
Deﬁnition 12.2.1 of a real stochastic process extends to the complex case as follows.
Deﬁnition 17.5.1 (Complex Stochastic Process). A complex stochastic pro-
cess (CSP)

Z(t), t ∈T

is a collection of complex random variables that are
deﬁned on a common probability space (Ω, F, P) and that are indexed by some
set T .
A CSP

Z(t), t ∈T

is said to be centered if for each t ∈T the CRV Z(t) is of
zero mean. Similarly, the CSP is said to be of ﬁnite variance if for each t ∈T the
CRV Z(t) is of ﬁnite variance. A discrete-time CSP corresponds to the case where
the index set T is the set of integers Z. Discrete-time complex stochastic processes
are not very diﬀerent from the real-valued ones we encountered in Chapter 13.
Consequently, we shall present the main deﬁnitions and results succinctly with
an emphasis on the issues where the complex and real processes diﬀer.
As in
Chapter 13, when dealing with a discrete-time CSP we shall use subscripts to
index the complex random variables and denote the process by

Zν, ν ∈Z

or,
more succinctly, by

Zν

.
A discrete-time CSP

Zν, ν ∈Z

is said to be stationary, or strict-sense sta-
tionary, or strongly stationary if for every positive integer n and for every
η, η′ ∈Z, the joint distribution of the n-tuple (Zη, . . . Zη+n−1) is identical to the
joint distribution of the n-tuple (Zη′, . . . , Zη′+n−1). This deﬁnition is essentially
identical to the analogous deﬁnition for real processes (Deﬁnition 13.2.1). Similarly,
Proposition 13.2.2 holds verbatim also for complex stochastic processes. Proposi-
tion 13.2.3 also holds for complex stochastic processes with the slight modiﬁcation
that the deterministic coeﬃcients α1, . . . , αn are now allowed to be arbitrary com-
plex numbers:
Proposition 17.5.2. A discrete-time CSP

Zν

is stationary if, and only if, for
every n ∈N, all η, ν1, . . . , νn ∈Z, and all α1, . . . , αn ∈C,
n

j=1
αjZνj
L=
n

j=1
αjZνj+η.
(17.49)
The deﬁnition of a wide-sense stationary CSP is very similar to the analogous
deﬁnition for real processes (Deﬁnition 13.3.1).
Deﬁnition 17.5.3 (Wide-Sense Stationary Discrete-Time CSP). We say that
a discrete-time CSP

Zν

is wide-sense stationary or weakly stationary or
covariance stationary if the following three conditions all hold:
1) For every ν ∈Z the CRV Zν is of ﬁnite variance.
2) The mean of Zν does not depend on ν.
3) The expectation E[Zν Z∗
ν′] depends on ν′ and ν only via their diﬀerence ν−ν′:
E[Zν Z∗
ν′] = E

Zν+η Z∗
ν′+η

,
ν, ν′, η ∈Z.
(17.50)
17.5 Discrete-Time Complex Stochastic Processes
329
Note the conjugation in (17.50). We do not require that E[Zν′Zν] be computable
from ν −ν′; it may or may not be. Thus, we do not require that the matrix
	
E[Re(Zν′) Re(Zν)]
E[Re(Zν′) Im(Zν)]
E[Im(Zν′) Re(Zν)]
E[Im(Zν′) Im(Zν)]

be computable from ν −ν′. This matrix is, however, computable from ν −ν′ if the
process is proper:
Deﬁnition 17.5.4 (Proper CSP). A discrete-time CSP

Zν

is said to be proper
if the following three conditions all hold: it is centered; it is of ﬁnite variance; and
E[Zν Zν′] = 0,
ν, ν′ ∈Z.
(17.51)
Equivalently, a discrete-time CSP

Zν

is proper if, and only if, for every positive
integer n and all ν1, . . . , νn ∈Z the complex random vector (Zν1, . . . , Zνn)T is
proper. Equivalently,

Zν

is proper if, and only if, for every positive integer n, all
α1, . . . , αn ∈C, and all ν1, . . . , νn ∈Z
n

j=1
αjZνj is proper
(17.52)
(Proposition 17.4.2).
The alternative deﬁnition of WSS real processes in terms of the variance of linear
functionals of the process (Proposition 13.3.3) requires little change:
Proposition 17.5.5. A ﬁnite-variance discrete-time CSP

Zν

is WSS if, and only
if, for every n ∈N, all η, ν1, . . . , νn ∈Z, and all α1, . . . , αn ∈C
n

j=1
αjZνj and
n

j=1
αjZνj+η
have the same mean & variance.
(17.53)
Proof. We begin by assuming that

Zν

is WSS and prove (17.53). The equality
of expectations follows directly from the linearity of expectation and from the fact
that because

Zν

is WSS the mean of Zν does not depend on ν. In proving the
equality of the variances we use (17.25):
Var

n

j=1
αjZνj+η

=
n

j=1
n

j′=1
αj α∗
j′ Cov

Zνj+η, Zνj′+η

=
n

j=1
n

j′=1
αj α∗
j′ Cov

Zνj, Zνj′

= Var

n

j=1
αjZνj

,
where the second equality follows from the wide-sense stationarity of

Zν

, and the
last equality follows again from (17.25).
330
Complex Random Variables and Processes
We next turn to proving that (17.53) implies that

Zν

is WSS. Choosing n = 1 and
α1 = 1 we obtain, by considering the equality of the means, that E[Zν] = E[Zν+η]
for all η ∈Z, i.e., that the mean of the process is constant. And, by considering
the equality of the variances, we obtain that the random variables

Zν

all have
the same variance
Var[Zν] = Var[Zν+η] ,
ν, η ∈Z.
(17.54)
Choosing n = 2 and α1 = α2 = 1 we obtain from the equality of the variances
Var[Zν1 + Zν2] = Var[Zν1+η + Zν2+η] .
(17.55)
But, by (17.25) and (17.54),
Var[Zν1 + Zν2] = 2 Var[Z1] + 2 Re

Cov[Zν1, Zν2]

(17.56)
and similarly
Var[Zν1+η + Zν2+η] = 2 Var[Z1] + 2 Re

Cov[Zν1+η, Zν2+η]

.
(17.57)
By (17.55), (17.56), and (17.57)
Re

Cov[Zν1+η, Zν2+η]

= Re

Cov[Zν1, Zν2]

,
η, ν1, ν2 ∈Z.
(17.58)
We now repeat the argument with α1 = 1 and α2 = i:
Var[Zν1 + i Zν2] = Var[Zν1] + Var[Zν2] + 2 Re

Cov[Zν1, i Zν2]

= 2 Var[Z1] + 2 Im

Cov[Zν1, Zν2]

and similarly
Var[Zν1+η + i Zν2+η] = 2 Var[Z1] + 2 Im

Cov[Zν1+η, Zν2+η]

,
so the equality of the variances implies
Im

Cov[Zν1+η, Zν2+η]

= Im

Cov[Zν1, Zν2]

,
η, ν1, ν2 ∈Z,
which combines with (17.58) to prove Cov[Zν1+η, Zν2+η] = Cov[Zν1, Zν2].
As with real processes, a comparison of Propositions 17.5.5 and 17.5.2 yields that
any ﬁnite-variance stationary CSP is also WSS. The reverse is not true.
Deﬁnition 17.5.6 (Autocovariance Function). We deﬁne the autocovariance func-
tion KZZ : Z →C of a discrete-time WSS CSP

Zν

as5
KZZ(η) ≜Cov[Zν+η, Zν]
(17.59)
= E
%
Zν+η −E[Z1]

Zν −E[Z1]
∗&
,
η ∈Z.
5Some authors, e.g., (Grimmett and Stirzaker, 2001), deﬁne KZZ(η) as Cov[Zν, Zν+η]. Our
deﬁnition follows (Doob, 1990).
17.5 Discrete-Time Complex Stochastic Processes
331
By mimicking the derivations of (13.12) (taking into account the conjugate symme-
try (17.18)) we obtain that the autocovariance function KZZ of every discrete-time
WSS CSP

Zν

satisﬁes the conjugate-symmetry condition
KZZ(−η) = K∗
ZZ(η) ,
η ∈Z.
(17.60)
Similarly, by mimicking the derivation of (13.13) (i.e., from the nonnegativity of
the variance and from (17.25)), we obtain that the autocovariance function of such
a process satisﬁes, for every positive integer n,
n

ν=1
n

ν′=1
αν α∗
ν′ KZZ(ν −ν′) ≥0,
α1, . . . , αn ∈C.
(17.61)
In analogy to the real case (Theorem 13.5.2), (17.60) and (17.61) fully characterize
the possible autocovariance functions in the sense that any function K: Z →C
satisfying
K(−η) = K∗(η),
η ∈Z
(17.62)
and
n

ν=1
n

ν′=1
αν α∗
ν′K(ν −ν′) ≥0,
α1, . . . , αn ∈C
(17.63)
is the autocovariance function of some discrete-time WSS CSP.6 If K: Z →C
satisﬁes (17.62) and (17.63), then we say that K(·) is a positive deﬁnite function
from the integers to the complex ﬁeld.
Deﬁnition 13.6.1 of the power spectral density SZZ requires no change.
We
require that SZZ be integrable on the interval [−1/2, 1/2) and that
KZZ(η) =
 1/2
−1/2
SZZ(θ) ei2πηθ dθ,
η ∈Z.
(17.64)
Proposition 13.6.3 does require some alteration. Indeed, for complex stochastic
processes the PSD need not be a symmetric function. However, the main result
(that the PSD is real and nonnegative) remains true:
Proposition 17.5.7 (PSDs of Complex Processes Are Nonnegative).
(i) If the discrete-time WSS CSP

Zν

is of PSD SZZ, then
SZZ(θ) ≥0,
(17.65)
except possibly on a subset of the interval [−1/2, 1/2) of Lebesgue measure
zero.
(ii) If a function S: [−1/2, 1/2) →R is integrable and nonnegative, then there
exists a proper discrete-time WSS CSP7 
Zν

whose PSD SZZ is given by
SZZ(θ) = S(θ),
θ ∈[−1/2, 1/2).
6In fact, it is the autocovariance function of some proper Gaussian stochastic process. Com-
plex Gaussian random processes will be discussed in Chapter 24.
7The process can be taken to be Gaussian; see Chapter 24.
332
Complex Random Variables and Processes
As in the real case, by possibly changing the value of SZZ on the set of Lebesgue
measure zero where (17.65) is violated, we can obtain a power spectral density that
is nonnegative for all θ ∈[−1/2, 1/2). Consequently, we shall always assume that
the PSD, if it exists, is nonnegative for all θ ∈[−1/2, 1/2).
Proof of Proposition 17.5.7. We begin with Part (i), where we need to prove the
nonnegativity of the PSD. We shall only sketch the proof. We recommend reading
Appendix A through Theorem A.2.2 before reading this proof.
Let KZZ denote the autocovariance function of the WSS CSP

Zν

.
Applying
(17.61) with
αν = e−i2πνθ,
ν ∈{1, . . . , n}
and thus
αν α∗
ν′ = ei2π(ν′−ν)θ,
ν, ν′ ∈{1, . . . , n},
we obtain
0 ≤
n

ν=1
n

ν′=1
αν α∗
ν′ KZZ(ν −ν′)
=
n

ν=1
n

ν′=1
ei2π(ν′−ν)θ KZZ(ν −ν′)
=
n−1

η=−(n−1)

n −|η|

ei2πηθ KZZ(−η),
θ ∈[−1/2, 1/2).
Dividing by n we obtain
0 ≤
n−1

η=−(n−1)
	
1 −|η|
n

ei2πηθ KZZ(−η)
=
n−1

η=−(n−1)
	
1 −|η|
n

ei2πηθ ˆSZZ(η)
=

kn−1 ⋆SZZ

(θ),
θ ∈[−1/2, 1/2),
where in the equality on the second line ˆSZZ(η) denotes the η-th Fourier Series
Coeﬃcient of SZZ and we use (17.64); and in the subsequent equality on the third
line kn denotes the degree-n Fej´er kernel (Deﬁnition A.1.3) and the convolution is
the periodic one of (A.6).
We have thus established that kn−1 ⋆SZZ is nonnegative. The result now follows
from Theorem A.2.2 which guarantees that
lim
n→∞
 1/2
−1/2
SZZ(θ) −

kn ⋆SZZ

(θ)
 dθ = 0.
The proof of Part (ii) is very similar to the proof of the analogous result for real
processes. As in (13.21), we deﬁne
K(η) ≜
 1/2
−1/2
S(θ) ei2πηθ dθ,
η ∈Z,
(17.66)
17.5 Discrete-Time Complex Stochastic Processes
333
and we prove that this function satisﬁes (17.62) and (17.63). To prove (17.62) we
compute
K(−η) =
 1/2
−1/2
S(θ) ei2π(−η)θ dθ
=
 1/2
−1/2
S∗(θ) e−i2πηθ dθ
=
	 1/2
−1/2
S(θ) ei2πηθ dθ

∗
= K∗(η),
η ∈Z,
where the ﬁrst equality follows from the deﬁnition of K(·) (17.66); the second
because S(·) is, by assumption, real; the third because conjugating the integrand
is equivalent to conjugating the integral; and the ﬁnal equality again by (17.66).
To prove (17.63) we mimic the derivation of (13.22) with the constants α1, . . . , αn
now being complex:
n

ν=1
n

ν′=1
αν α∗
ν′K(ν −ν′) =
n

ν=1
n

ν′=1
αν α∗
ν′
 1/2
−1/2
S(θ) ei2π(ν−ν′)θ dθ
=
 1/2
−1/2
S(θ)
	
n

ν=1
n

ν′=1
αν α∗
ν′ ei2π(ν−ν′)θ

dθ
=
 1/2
−1/2
S(θ)
	
n

ν=1
n

ν′=1
αν ei2πνθα∗
ν′ e−i2πν′θ

dθ
=
 1/2
−1/2
S(θ)
	
n

ν=1
αν ei2πνθ

	
n

ν′=1
αν′ ei2πν′θ

∗
dθ
=
 1/2
−1/2
S(θ)

n

ν=1
αν ei2πνθ

2
dθ
≥0,
(17.67)
where the last inequality follows from the assumption in (ii) that S is nonnegative
on [−1/2, 1/2). The proof is completed in view of the suﬃciency of the conditions
in (17.62) and (17.63) for a function K: Z →C to be the autocovariance function
of some discrete-time WSS CSP.
Proposition 13.6.6 needs very little alteration. We only need to drop the symmetry
property:
Proposition 17.5.8 (PSD when KZZ Is Absolutely Summable). If the autocovari-
ance function KZZ of a discrete-time WSS CSP is absolutely summable, i.e.,
∞

η=−∞
KZZ(η)
 < ∞,
(17.68)
334
Complex Random Variables and Processes
then the function
S(θ) =
∞

η=−∞
KZZ(η) e−i2πηθ
(17.69)
is continuous, nonnegative, and satisﬁes
 1/2
−1/2
S(θ) ei2πηθ dθ = KZZ(η),
η ∈Z.
(17.70)
The Spectral Distribution Function that we encountered in Section 13.7 has a
natural extension to discrete-time WSS CSPs. The main diﬀerence is that, unlike
in Theorem 13.7.1, here Θ need not have a symmetric distribution:
Theorem 17.5.9.
(i) If

Zν

is a WSS CSP of autocovariance function KZZ, then
KZZ(η) = KZZ(0) E

ei2πηΘ
,
η ∈Z,
(17.71)
for some random variable Θ taking values in the interval [−1/2, 1/2).
In
the nontrivial case where KZZ(0) > 0 the distribution function of Θ is fully
speciﬁed by KZZ.
(ii) If Θ is any random variable taking values in [−1/2, 1/2) and if α > 0, then
there exists a proper discrete-time WSS CSP

Zν

whose autocovariance func-
tion KZZ is given by
KZZ(η) = α E

ei2πηΘ
,
η ∈Z
(17.72)
and whose variance is consequently given by KZZ(0) = α.
Proof. See (Shiryaev, 1996, Chapter VI, Section § 1 Theorem 3), (Doob, 1990,
Chapter X § 3 Theorem 3.2), or (Feller, 1971, Chapter XIX, Section 6, Theorem 3).
Some authors refer to the mapping θ 	→Pr[Θ ≤θ] as the spectral distribution func-
tion of

Zν

, but others refer to θ 	→KZZ(0) Pr[Θ ≤θ] as the spectral distribution
function. The latter is more common.
17.6
Limits of Proper Complex Random Variables
Understanding limits is essential to understanding inﬁnite sums, and those will
appear when we discuss discrete convolutions and the response of discrete-time
ﬁlters to stochastic inputs (Chapter 31). Here we shall establish that the limit of
proper CRVs must also be proper and that this also holds for complex random
vectors. We begin with scalars.
17.6 Limits of Proper Complex Random Variables
335
Proposition 17.6.1. If the sequence Z1, Z2, . . . of proper complex random variables
converges to the CRV Z in the sense that
lim
n→∞E

|Z −Zn|2
= 0,
(17.73)
then Z must be proper.
Proof. We ﬁrst prove that
E

|Z|2
< ∞.
(17.74)
The proposition’s hypotheses guarantee that for any n ∈N the CRV Zn is proper
and hence, a fortiori,
E

|Zn|2
< ∞.
(17.75)
Expressing Z as Zn +(Z −Zn) we obtain from the Triangle Inequality for complex
random variables (Exercise 17.10) that
E

|Z|2
≤

E[|Zn|2] +

E[|Zn −Z|2]
2
,
which combines with (17.75) and (17.73) to establish (17.74).
We next prove that Z must be of zero mean. For every n ∈N the CRV Zn is
proper and hence
E[Zn] = 0.
(17.76)
This in combination with (17.73) implies that E[Z] must be zero, because
E

|Z −Zn|2
= |E[Z −Zn]|2 + Var[Z −Zn]
≥|E[Z −Zn]|2
= |E[Z]|2,
where in the last equality we used (17.76).
To conclude the proof that Z is proper, we next show that E[Z2] must be zero.
Since Zn is proper for every n ∈N,
E

Z2
n

= 0,
n ∈N.
(17.77)
We use this to upper-bound |E[Z2]| as follows:
E

Z2 =
E
%
(Z −Zn) + Zn
2&
=
E

(Z −Zn)2
+ 2 E

(Z −Zn)Zn

+ E

Z2
n

=
E

(Z −Zn)2
+ 2 E

(Z −Zn)Zn

≤
E

(Z −Zn)2 + 2
E[(Z −Zn)Zn]

≤E

|Z −Zn|2
+ 2
E[(Z −Zn)Zn]

≤E

|Z −Zn|2
+ 2

E[|Z −Zn|2]

E[|Zn|2],
(17.78)
336
Complex Random Variables and Processes
where the third line follows from (17.77), the ﬁfth from the inequality |E[Z]| ≤
E[|Z|] (Exercise 17.3), and the sixth line from the Cauchy-Schwarz Inequality for
complex random variables (Exercise 17.9).
We will establish that E[Z2] must be zero by showing that the proposition’s hy-
potheses imply that, as n tends to inﬁnity, the RHS of (17.78) tends to zero. In
view of (17.73), we only need to show that E

|Zn|2
must be bounded in n. This
follows from the Triangle Inequality for complex random variables by expressing
Zn as Z + (Zn −Z) to obtain
E

|Zn|2
≤

E[|Z|2] +

E[|Zn −Z|2]
2
,
from which the desired boundedness follows using (17.74) and (17.73).
The above proposition can be easily extended to vectors by recalling that if αTZ is
proper for every deterministic vector α, then Z must be a proper complex random
vector (Proposition 17.4.2):
Proposition 17.6.2 (Limits of Proper Complex Random Vectors). If the sequence
of proper complex random n-vectors Z1, Z2, . . . converges to the complex random
n-vector Z in the sense that
lim
ν→∞E
%Z(m)
ν
−Z(m)2&
= 0,
m ∈{1, . . . , n}
(17.79)
(where Z(m)
ν
is the m-th component of Zν and Z(m) is the m-th component of Z),
then Z must be proper.
Proof. To establish that Z is proper it suﬃces to show that αTZ is proper for
every α ∈Cn (Proposition 17.4.2), which is what we now do. Let α ∈Cn be any
deterministic n-vector. Since linear functionals of proper complex random vectors
are proper CRVs (Proposition 17.4.2), the sequence αTZ1, αTZ2, . . . is a sequence
of proper CRVs. We will now argue that (17.79) implies that it converges to αTZ
in the sense that
lim
ν→∞E
%αTZν −αTZ
2&
= 0,
(17.80)
from which it will then follow using Proposition 17.6.1 that αTZ is proper.
To see that (17.79) implies (17.80), one can use the Triangle Inequality for complex
random variables (see Exercise 17.10 but extended to the sum of n CRVs) to obtain
that
0 ≤E
%αTZν −αTZ
2&
= E
5 
n

j=1
α(j)
Z(j)
ν
−Z(j)
26
≤
	
n

j=1
α(j) E
%Z(j)
ν
−Z(j)2& 1
2 
2
.
This inequality demonstrates that (17.79) indeed implies (17.80), because (17.79)
implies that the RHS of the inequality tends to zero as ν tends to inﬁnity, and
hence (17.80) must hold by the Sandwich Theorem.
17.7 On the Eigenvalues of Large Toeplitz Matrices
337
17.7
On the Eigenvalues of Large Toeplitz Matrices
Although it will not be used in this book, we cannot resist stating the following
classic result, which is sometimes called “Szeg˝o’s Theorem.”
Let the function
s: [−1/2, 1/2] →[0, ∞) be Lebesgue integrable. Deﬁne
cη =
 1/2
−1/2
s(θ) ei2πηθ dθ,
η ∈Z.
(17.81)
(In some applications, see for example (Gray, 2006), s(·) is the PSD of a discrete-
time real or complex stochastic process and cη is the value of the corresponding
autocovariance function at η.)
The n × n matrix
⎛
⎜
⎜
⎜
⎝
c0
c1
. . .
cn−1
c−1
c0
. . .
cn−2
...
...
...
...
c−n+1
. . .
. . .
c0
⎞
⎟
⎟
⎟
⎠
is positive semideﬁnite and conjugate-symmetric. Consequently, it has n nonneg-
ative eigenvalues (counting multiplicity), which we denote by
λ(1)
n
≤λ(2)
n
≤· · · ≤λ(n)
n .
(17.82)
As n increases (with s(·) ﬁxed), the number of eigenvalues increases. It turns out
that we can say something quite precise about the distribution of these eigenvalues.
Theorem 17.7.1. Let s: [−1/2, 1/2] →[0, ∞) be integrable, and let λ(j)
n
be as in
(17.82). Let g: [0, ∞) →R be a continuous function such that the limit limξ→∞
g(ξ)
ξ
exists and is ﬁnite. Then
lim
n→∞
1
n
n

j=1
g

λ(j)
n

=
 1/2
−1/2
g

s(θ)

dθ.
(17.83)
Proof. For a proof of a more general statement of this theorem see (Simon, 2005,
Chapter 2, Section 7, Theorem 2.7.13). More accessible is (Gray, 2006).
17.8
Exercises
Exercise 17.1 (The Distribution of Re(Z) and |Z|). Let the CRV Z be uniformly dis-
tributed over the unit disc {z ∈C : |z| ≤1}.
(i) What is the density of its real part Re(Z)?
(ii) What is the density of its magnitude |Z|?
Exercise 17.2 (The Variance under Random Rotation). Let Z be a zero-mean CRV of
ﬁnite variance, and let Θ be a real random variable that is independent of it. Show that
the variance of ZeiΘ is equal to the variance of Z.
338
Complex Random Variables and Processes
Exercise 17.3 (Expectation of Modulus). Show that if Z is a CRV and E[|Z|] < ∞then
E[Z]
 ≤E[|Z|]
by justifying the following steps:
E[Z]
 = eiθ E[Z] = E
$
eiθZ
%
= E
$
Re

eiθZ
%
≤E[|Z|] ,
where θ ∈[−π, π) is such that the ﬁrst equality holds.
Exercise 17.4 (Constructing a Proper CRV). Let the CRV Z be of ﬁnite variance, and
let Θ take on the values 0, π/2, π, and 3π/2 equiprobably and independently of Z. Prove
that the CRV Z eiΘ is proper.
Exercise 17.5 (The Density of Z2). Let Z be a CRV of density fZ(·). Express the density
of Z2 in terms of fZ(·).
Exercise 17.6 (The Conjugate of a Proper CRV). Must the complex conjugate of a proper
CRV be proper?
Exercise 17.7 (Product of Proper CRVs). Show that the product of two independent
complex random variables is proper whenever one is proper and the other is of ﬁnite
variance.
Exercise 17.8 (Sums of Proper CRVs). Show that the sum of independent proper complex
random variables is proper. Is the assumption of independence essential?
Exercise 17.9 (Cauchy-Schwarz Inequality for Complex Random Variables). Prove that
if W and Z are complex random variables of ﬁnite variance then
E[WZ∗]
 ≤

E[|W|2]

E[|Z|2]
and
Cov[W, Z]
 ≤

Var[W]

Var[Z].
Hint: To prove the ﬁrst inequality, recall Exercise 17.3.
Exercise 17.10 (Triangle Inequality for Complex Random Variables). Show that if W
and Z are complex random variables of ﬁnite variance, then

E[|W + Z|2] ≤

E[|W|2] +

E[|Z|2].
Hint: Use the Cauchy-Schwarz Inequality for complex random variables (Exercise 17.9).
Exercise 17.11 (On the Characteristic Function of a CRV). Let Z be a CRV. Express
the characteristic functions of Re(Z) and Im(Z) in terms of the characteristic function
of Z.
Exercise 17.12 (A Complex Random Vector of Independent Components). Let V and
W be independent complex random variables. Express the characteristic function of the
complex random vector (V, W)T in terms of the characteristic functions ΦV (·) and ΦW (·).
17.8 Exercises
339
Exercise 17.13 (Transforming Complex Random Vectors). Let Z be a complex n-vector
of PDF fZ(·). Let W = g(Z), where g: D →R is a one-to-one function from an open
subset D of Cn to R ⊆Cn. Let the mappings u, v: R2n →Rn be deﬁned for x, y ∈Rn
as
u: (x, y) →Re

g(x + iy)

and
v: (x, y) →Im

g(x + iy)

.
Assume that g is diﬀerentiable in D in the sense that for all j, ℓ∈{1, . . . , n} the partial
derivatives
∂u(j)(x, y)
∂x(ℓ)
, ∂u(j)(x, y)
∂y(ℓ)
, ∂v(j)(x, y)
∂x(ℓ)
, ∂v(j)(x, y)
∂y(ℓ)
exist and are continuous in D, and that they satisfy
∂u(j)(x, y)
∂x(ℓ)
= ∂v(j)(x, y)
∂y(ℓ)
and
∂u(j)(x, y)
∂y(ℓ)
= −∂v(j)(x, y)
∂x(ℓ)
,
where a(j) denotes the j-th component of the vector a. Further assume that the determi-
nant of the Jacobian matrix
det g′(z) = det
⎛
⎜
⎜
⎜
⎜
⎝
∂u(1)(x, y)
∂x(1)
+ i∂v(1)(x, y)
∂x(1)
. . .
∂u(1)(x, y)
∂x(n)
+ i∂v(1)(x, y)
∂x(n)
...
...
...
∂u(n)(x, y)
∂x(1)
+ i∂v(n)(x, y)
∂x(1)
. . .
∂u(n)(x, y)
∂x(n)
+ i∂v(n)(x, y)
∂x(n)
⎞
⎟
⎟
⎟
⎟
⎠
is at no point in D zero. Show that the density fW(·) of W is given by
fW(w) =
fZ(z)
|det g′(z)|2

z=g−1(w)
· I{w ∈R}.
Exercise 17.14 (The Cauchy-Schwarz Inequality Revisited). Let

Zℓ

be a discrete-time
WSS CSP. Show that (17.61) implies
Cov[Zℓ, Zℓ′]
 ≤Var[Z1] ,
ℓ, ℓ′ ∈Z.
Exercise 17.15 (The Real Part of a WSS CSP Need Not Be WSS). Let X and Y
be independent zero-mean random variables of variances σ2
X and σ2
Y . Deﬁne the CRV
Z = X + iY , and deﬁne Zν = iνZ, for every integer ν.
(i) Show that the CSP

Zν, ν ∈Z

is WSS.
(ii) Show that if σ2
X ̸= σ2
Y then the real part of

Zν, ν ∈Z

is not WSS.
Exercise 17.16 (The Real Part of a Proper WSS CSP). Let

Zν, ν ∈Z

be a proper,
WSS, discrete-time, complex SP of autocovariance function KZZ. Prove that its real part
forms a real WSS SP and express its autocovariance function in terms of KZZ. Repeat
for the imaginary part of

Zν

.
Exercise 17.17 (The Real and Imaginary Parts of a WSS CSP). Show that if the real part
of a WSS CSP is WSS then so is the imaginary part, and the autocovariance functions of
the real and imaginary parts sum to the real part of the autocovariance function of the
CSP.
340
Complex Random Variables and Processes
Exercise 17.18 (On the Autocovariance Function of a Discrete-Time CSP). Show that
if KZZ is the autocovariance function of a discrete-time WSS CSP, then for every n ∈N,
the matrix
⎛
⎜
⎜
⎜
⎝
KZZ(0)
KZZ(1)
. . .
KZZ(n −1)
KZZ(−1)
KZZ(0)
. . .
KZZ(n −2)
...
...
...
...
KZZ(−n + 1)
KZZ(−n + 2)
. . .
KZZ(0)
⎞
⎟
⎟
⎟
⎠
is positive semideﬁnite.
Exercise 17.19 (Reversing the Direction of Time). Let KZZ be the autocovariance func-
tion of some discrete-time WSS CSP

Zν

. For every ν ∈Z deﬁne Yν = Z−ν. Show that
the time-reversed CSP

Yν

is also a WSS CSP, and express its autocovariance function
KYY in terms of KZZ.
Exercise 17.20 (The Sum of Autocovariance Functions). Show that the sum of the
autocovariance functions of two discrete-time WSS complex stochastic processes is the
autocovariance function of some discrete-time WSS CSP.
Exercise 17.21 (The Real Part of an Autocovariance Function). Let KZZ be the au-
tocovariance function of some discrete-time WSS CSP

Zν

.
Show that the mapping
m →Re

KZZ(m)

is the autocovariance function of some real SP. Is this also true for the
mapping m →Im

KZZ(m)

?
Exercise 17.22 (Rotating a WSS CSP). Let

Zℓ

be a zero-mean WSS discrete-time CSP,
and let α ∈C be ﬁxed. Deﬁne the new CSP

Wℓ

as Wℓ= αℓZℓfor every ℓ∈Z.
(i) Show that if |α| = 1 then

Wℓ

is WSS. Compute its autocovariance function.
(ii) Does your answer change if α is not of unit magnitude?
Chapter 18
Energy, Power, and PSD in QAM
18.1
Introduction
The calculations of the power and of the operational power spectral density in
QAM are not just repetitions of the analogous PAM calculations with complex
notation.
They contain two new elements that we shall try to highlight.
The
ﬁrst is the relationship between the power (as opposed to energy) in passband and
baseband, and the second is the fact that the energy and power in transmitting
the complex symbols {Cℓ} are only related to expectations of the form E[CℓC∗
ℓ′];
they are uninﬂuenced by those of the form E[CℓCℓ′].
The signal

X(t), t ∈R

(or X for short) that we consider is given by
X(t) = 2 Re

XBB(t) ei2πfct
,
t ∈R,
(18.1)
where
XBB(t) = A

ℓ
Cℓg(t −ℓTs),
t ∈R.
(18.2)
Here A > 0 is real; the symbols {Cℓ} are complex random variables; the pulse
shape g is an integrable complex function that is bandlimited to W/2 Hz; Ts is
positive; and fc > W/2. The range of the summation will depend on the modes
we discuss.
Our focus in this chapter is on X’s energy, power, and operational PSD. These
quantities are studied in Sections 18.2–18.4, albeit without all the ﬁne mathemat-
ical details. Those are provided in Sections 18.5 & 18.6, which are recommended
for the more mathematical readers. The deﬁnition of the operational PSD of com-
plex stochastic processes is very similar to the one of real stochastic processes
(Deﬁnition 15.3.1). It is given in Section 18.4 (Deﬁnition 18.4.1).
18.2
The Energy in QAM
As in our treatment in Chapter 14 of PAM, we begin with an analysis of the energy
in transmitting K IID random bits D1, . . . , DK.
We assume that the data bits
are mapped to N complex symbols C1, . . . , CN using a (K, N) binary-to-complex
block-encoder
enc: {0, 1}K →CN
(18.3)
341
342
Energy, Power, and PSD in QAM
of rate
K
N

bit
complex symbol

.
The transmitted signal is then:
X(t) = 2 Re

XBB(t) ei2πfct
(18.4)
= 2 Re
-
A
N

ℓ=1
Cℓg(t −ℓTs) ei2πfct
.
,
t ∈R,
(18.5)
where the baseband representation of the transmitted signal is
XBB(t) = A
N

ℓ=1
Cℓg(t −ℓTs),
t ∈R.
(18.6)
Our interest is in the energy E in X, which is deﬁned by
E ≜E
 ∞
−∞
X2(t) dt

.
(18.7)
Our assumption that the pulse shape g is bandlimited to W/2 Hz implies that
for every realization of the symbols {Cℓ}, the signal XBB(·) is also bandlimited
to W/2 Hz. And since we assume that fc > W/2, it follows from Theorem 7.6.10
that the energy in the passband signal X(·) is twice the energy in its baseband
representation XBB(·), i.e.,
E = 2E
 ∞
−∞
XBB(t)
2 dt

.
(18.8)
We can thus compute the energy in X(·) by computing the energy in XBB(·) and
doubling the result. The energy of the baseband signal can be computed in much
the same way that the energy was computed in Section 14.2 for PAM. The only
diﬀerence is that the baseband signal is now complex:
E
 ∞
−∞
XBB(t)
2 dt

=
 ∞
−∞
E
5A
N

ℓ=1
Cℓg(t −ℓTs)

26
dt
=
 ∞
−∞
E
5	
A
N

ℓ=1
Cℓg(t −ℓTs)

	
A
N

ℓ′=1
Cℓ′ g(t −ℓ′Ts)

∗6
dt
= A2
N

ℓ=1
N

ℓ′=1
E[CℓC∗
ℓ′]
 ∞
−∞
g(t −ℓTs) g∗(t −ℓ′Ts) dt
= A2
N

ℓ=1
N

ℓ′=1
E[CℓC∗
ℓ′] Rgg

(ℓ′ −ℓ)Ts

,
(18.9)
18.2 The Energy in QAM
343
where Rgg is the self-similarity function of the pulse shape g (Deﬁnition 11.2.1),
Rgg(τ) =
 ∞
−∞
g(t + τ) g∗(t) dt,
τ ∈R.
(18.10)
This expression for the energy in XBB(·) is greatly simpliﬁed if the symbols {Cℓ}
are of zero mean and uncorrelated:
E
 ∞
−∞
XBB(t)
2 dt

= A2 ∥g∥2
2
N

ℓ=1
E

|Cℓ|2
,

E[CℓC∗
ℓ′] = E

|Cℓ|2
I{ℓ= ℓ′},
ℓ, ℓ′ ∈{1, . . . , N}

,
(18.11)
or if the time shifts of the pulse shape by integer multiples of Ts are orthonormal
E
 ∞
−∞
XBB(t)
2 dt

= A2
N

ℓ=1
E

|Cℓ|2
,
	 ∞
−∞
g(t −ℓTs)g∗(t −ℓ′Ts) dt = I{ℓ= ℓ′},
ℓ, ℓ′ ∈{1, . . . , N}

.
(18.12)
Since g is an integrable function that is bandlimited to W/2 Hz, it is also energy-
limited (Note 6.4.12). Consequently, by Proposition 11.2.2 (iv), we can express
the self-similarity function Rgg in (18.9) as the Inverse Fourier Transform of the
mapping f 	→|ˆg(f)|2:
Rgg(τ) =
 ∞
−∞
|ˆg(f)|2 ei2πfτ df,
τ ∈R.
(18.13)
With this representation of Rgg we obtain from (18.9) an equivalent representation
of the energy as
E
 ∞
−∞
XBB(t)
2 dt

= A2
 ∞
−∞
N

ℓ=1
N

ℓ′=1
E[CℓC∗
ℓ′] ei2πf(ℓ′−ℓ)Ts |ˆg(f)|2 df.
(18.14)
Using (18.8), (18.9), and (18.14) we obtain:
Theorem 18.2.1 (Energy in QAM). Assume that A ≥0, that Ts > 0, that g: R →
C is an integrable signal that is bandlimited to W/2 Hz, and that fc > W/2. Then
the energy E in the QAM signal X(·) of (18.5) is given by
E = 2A2
N

ℓ=1
N

ℓ′=1
E[CℓC∗
ℓ′] Rgg

(ℓ′ −ℓ)Ts

(18.15)
= 2A2
 ∞
−∞
N

ℓ=1
N

ℓ′=1
E[CℓC∗
ℓ′] ei2πf(ℓ′−ℓ)Ts |ˆg(f)|2 df,
(18.16)
whenever all the complex random variables C1, . . . , CN are of ﬁnite variance
E

|Cℓ|2
< ∞,
ℓ= 1, . . . , N.
(18.17)
344
Energy, Power, and PSD in QAM
In analogy to PAM, we deﬁne the energy per bit Eb by
Eb ≜E
K
(18.18)
and the energy per complex symbol Es by
Es ≜E
N.
(18.19)
Using Theorem 18.2.1, we obtain
Es = 2
NA2
N

ℓ=1
N

ℓ′=1
E[CℓC∗
ℓ′] Rgg

(ℓ′ −ℓ)Ts

(18.20)
= 2
NA2
 ∞
−∞
N

ℓ=1
N

ℓ′=1
E[CℓC∗
ℓ′] ei2πf(ℓ′−ℓ)Ts |ˆg(f)|2 df.
(18.21)
Notice that, as promised, only terms of the form E[CℓC∗
ℓ′] inﬂuence the energy;
terms of the form E[CℓCℓ′] do not appear in this analysis.
18.3
The Power in QAM
In order to discuss the power in QAM we must consider the transmission of an
inﬁnite sequence of complex symbols

Cℓ

. To guarantee convergence, we shall
assume that the pulse shape g—in addition to being an integrable signal that is
bandlimited to W/2 Hz—also satisﬁes the decay condition
|g(t)| ≤
β
1 + |t/Ts|1+α ,
t ∈R
(18.22)
for some α, β > 0.
Also, we shall only consider the transmission of bi-inﬁnite
sequences

Cℓ

that are bounded in the sense that there exists some γ > 0 such
that every realization of

Cℓ

satisﬁes
Cℓ
 ≤γ,
ℓ∈Z.
(18.23)
As for PAM, we shall treat three diﬀerent scenarios for the generation of

Cℓ

. In
the ﬁrst, we simply ignore the mechanism by which the sequence

Cℓ

is generated
and assume that it forms a wide-sense stationary complex stochastic process. In
the second, we assume bi-inﬁnite block encoding. And in the third we relax the
statistical assumptions and consider the case where the time shifts of g by integer
multiples of Ts are orthonormal. In all these cases the transmitted waveform is
given by
X(t) = 2 Re

XBB(t) ei2πfct
,
t ∈R,
(18.24)
where
XBB(t) = A
∞

ℓ=−∞
Cℓg(t −ℓTs),
t ∈R.
(18.25)
18.3 The Power in QAM
345
It is tempting to derive the power in X(·) by using the complex version of the
PAM results of Section 14.5 to compute the power in XBB(·) and then doubling
the result. This turns out to be a valid approach, but its justiﬁcation requires some
work. The diﬃculty is that the powers are deﬁned as
lim
T→∞
1
2T E
5 T
−T
X2(t) dt
6
and
lim
T→∞
1
2T E
5 T
−T
XBB(t)
2 dt
6
,
and—Theorem 7.6.10 notwithstanding—
1
2T E
5 T
−T
X2(t) dt
6
̸= 2 1
2T E
5 T
−T
XBB(t)
2 dt
6
.
(18.26)
The reason we cannot claim equality in (18.26) is that t 	→X(t) I{|t| ≤T} is not
bandlimited around fc, so Theorem 7.6.10, which relates energies in passband and
baseband, is not applicable. Nevertheless, it turns out that the limits as T →∞of
the RHS and the LHS of (18.26) do agree:
lim
T→∞
1
2T E
5 T
−T
X2(t) dt
6
= 2 lim
T→∞
1
2T E
5 T
−T
XBB(t)
2 dt
6
.
(18.27)
Thus, the power in a QAM signal is, indeed, twice the power in its baseband
representation. This is stated more precisely in Theorem 18.5.2 and is proved in
Section 18.5. With the aid of (18.27) we can now readily compute the power in
QAM.
18.3.1

Cℓ

Is Zero-Mean and WSS
We next ignore the mechanism by which the symbols

Cℓ

are generated and merely
assume that they form a zero-mean WSS discrete-time CSP of autocovariance
function KCC:
E[Cℓ] = 0,
ℓ∈Z,
(18.28a)
E[Cℓ+mC∗
ℓ] = KCC(m) ,
m, ℓ∈Z.
(18.28b)
The calculation of the RHS of (18.27) is very similar to the analogous computation
in Section 14.5.1 for PAM. The only diﬀerence is that here XBB(·) is complex. As
in Section 14.5.1, we begin by computing the energy in a length-Ts interval:
E
 τ+Ts
τ
XBB(t)
2 dt

= A2
 τ+Ts
τ
E
5 
∞

ℓ=−∞
Cℓg(t −ℓTs)

26
dt
346
Energy, Power, and PSD in QAM
= A2
 τ+Ts
τ
E

∞

ℓ=−∞
∞

ℓ′=−∞
CℓC∗
ℓ′ g(t −ℓTs) g∗(t −ℓ′Ts)

dt
= A2
 τ+Ts
τ
∞

ℓ=−∞
∞

ℓ′=−∞
E[CℓC∗
ℓ′] g(t −ℓTs) g∗(t −ℓ′Ts) dt
= A2
 τ+Ts
τ
∞

m=−∞
∞

ℓ′=−∞
E[Cℓ′+mC∗
ℓ′] g

t −(ℓ′ + m)Ts

g∗(t −ℓ′Ts) dt
= A2
 τ+Ts
τ
∞

m=−∞
KCC(m)
∞

ℓ′=−∞
g

t −(ℓ′ + m)Ts

g∗(t −ℓ′Ts) dt
= A2
∞

m=−∞
KCC(m)
∞

ℓ′=−∞
 τ+Ts−ℓ′Ts
τ−ℓ′Ts
g(t′ −mTs) g∗(t′) dt′
= A2
∞

m=−∞
KCC(m)
 ∞
−∞
g∗(t′) g(t′ −mTs) dt′
= A2
∞

m=−∞
KCC(m) R∗
gg(mTs),
(18.29)
where we have substituted ℓ′ + m for ℓ(fourth equality); we have substituted t′
for t −ℓ′Ts (sixth equality); and we have used the cojugate-symmetry of the self-
similarity function (11.5) in the last equality.
As in the analogous analysis for real PAM signals, the RHS of (18.29) does not
depend on the starting point τ of the length-Ts interval [τ, τ +Ts], and we can hence
lower-bound the energy of XBB(·) in the interval [−T, +T ] by
72T
Ts
8
E
5 τ+Ts
τ
XBB(t)
2 dt
6
and upper-bound it by
/2T
Ts
0
E
5 τ+Ts
τ
XBB(t)
2 dt
6
,
so, by the Sandwich Theorem,
lim
T→∞
1
2T E
5 +T
−T
XBB(t)
2 dt
6
= 1
Ts
E
5 τ+Ts
τ
XBB(t)
2 dt
6
.
(18.30)
It thus follows from (18.30) and (18.29) that the power PBB in XBB(·) is
PBB = A2
Ts
∞

m=−∞
KCC(m) R∗
gg(mTs)
(18.31)
= A2
Ts
 ∞
−∞
∞

m=−∞
KCC(m) e−i2πfmTs |ˆg(f)|2 df,
(18.32)
where the second equality follows from (18.13).
Since the power in passband is twice the power in baseband (18.27), we conclude:
18.3 The Power in QAM
347
Theorem 18.3.1. Let the QAM SP

X(t)

be given by (18.24) & (18.25), where
A, Ts, g, W, and fc are as in Theorem 18.2.1. Further assume that g satisﬁes
the decay condition (18.22) and that the WSS centered discrete-time CSP

Cℓ

is bounded in the sense of (18.23).
If

Cℓ

satisﬁes (18.28), then

X(t)

is a
measurable SP,
lim
T→∞
1
2T E
5 T
−T
X2(t) dt
6
= 2A2
Ts
∞

m=−∞
KCC(m) R∗
gg(mTs),
(18.33)
and
lim
T→∞
1
2T E
5 T
−T
X2(t) dt
6
= 2A2
Ts
 ∞
−∞
∞

m=−∞
KCC(m) e−i2πfmTs |ˆg(f)|2 df.
(18.34)
Proof. Follows by combining (18.27) (Theorem 18.5.2) with (18.31) and (18.32)
(which can be justiﬁed by extending Theorem 14.6.4 to the case where the pulse
shape and the symbols are complex).
18.3.2
Bi-Inﬁnite Block-Mode
The second scenario we consider is when

Cℓ

is generated, as in Section 14.5.2, by
applying a binary-to-complex block-encoder enc: {0, 1}K →CN to bi-inﬁnite IID
random bits

Dj

. As in Section 14.5.2, we assume that the encoder, when fed IID
random bits, produces symbols of zero mean.
By extending the results of Section 14.5.2 to complex pulse shapes and complex
symbols, we obtain that the power in XBB(·) is given by:
PBB =
1
NTs
E
5  ∞
−∞
A
N

ℓ=1
Cℓg(t −ℓTs)

26
(18.35)
= A2
NTs
 ∞
−∞
N

ℓ=1
N

ℓ′=1
E[CℓC∗
ℓ′] ei2πf(ℓ′−ℓ)Ts |ˆg(f)|2 df.
(18.36)
Using the relationship between power in baseband and passband (18.27) and using
the deﬁnitions of E (18.8) and of Es (18.19), we obtain:
Theorem 18.3.2. Under the assumptions of Theorem 18.3.1, if the symbols

Cℓ

are generated from IID random bits

Dj

in bi-inﬁnite block-mode using the encoder
enc(·), where enc(·) produces zero-mean symbols when fed IID random bits, then

X(t)

is a measurable SP, and
lim
T→∞
1
2T E
5 T
−T
X2(t) dt
6
= Es
Ts
,
(18.37)
where the energy per symbol Es is deﬁned in (18.19) and is given by (18.20) or
(18.21).
348
Energy, Power, and PSD in QAM
Proof. Follows from Theorem 18.5.2 and by noting that Theorem 14.6.5 also ex-
tends to the case where the pulse shape and the symbols are complex.
18.3.3
Time Shifts of Pulse Shape Are Orthonormal
We ﬁnally address the third scenario where the time shifts of the pulse shape by
integer multiples of Ts are orthonormal. This situation is very prevalent in Digital
Communications and allows for signiﬁcant simpliﬁcations. In this setting we denote
the pulse shape by φ(·) and state the orthonormality as
 ∞
−∞
φ(t −ℓTs) φ∗(t −ℓ′Ts) dt = I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z.
(18.38)
The transmitted signal

X(t), t ∈R

is thus given as in (18.24) but with
XBB(t) = A
∞

ℓ=−∞
Cℓφ(t −ℓTs),
t ∈R,
(18.39)
where we assume that the discrete-time CSP

Cℓ

satisﬁes the boundedness con-
dition (18.23) and that the complex pulse shape φ(·) satisﬁes the orthogonality
condition (18.38) and the decay condition
|φ(t)| ≤
β
1 + |t/Ts|1+α ,
t ∈R,
(18.40)
for some α, β > 0.
Computing the power in

XBB(t), t ∈R

using Theorem 14.5.2, which easily
extends to the complex case, we obtain from (18.27):
Theorem 18.3.3. Let the SP

X(t), t ∈R

be given by
X(t) = 2 Re
	
A
∞

ℓ=−∞
Cℓφ(t −ℓTs) ei2πfct

,
t ∈R,
(18.41)
where A ≥0; Ts > 0; the pulse shape φ: R →C is an integrable function that is
bandlimited to W/2 Hz, is Borel measurable, satisﬁes the orthogonality condition
(18.38), and satisﬁes the decay condition (18.40); the carrier frequency fc satisﬁes
fc > W/2 > 0; and where the discrete-time CSP

Cℓ

satisﬁes the boundedness
condition (18.23). Then

X(t), t ∈R

is a measurable stochastic process, and
lim
T→∞
1
2T E
5 T
−T
X2(t) dt
6
= 2A2
Ts
lim
L→∞
1
2L + 1
L

ℓ=−L
E

|Cℓ|2
,
(18.42)
whenever the limit on the RHS exists.
18.4 The Operational PSD of QAM Signals
349
18.4
The Operational PSD of QAM Signals
We shall compute the operational PSD of the QAM signal

X(t), t ∈R

(18.24) by
relating it to the operational PSD of the complex signal

XBB(t), t ∈R

(18.25)
and by then computing the operational PSD of the latter using techniques similar
to the ones we employed in Chapter 15 in our study of the operational PSD of real
PAM signals. But ﬁrst we must deﬁne the operational PSD of complex stochastic
processes. The deﬁnition is very similar to that for real stochastic processes (Deﬁ-
nition 15.3.1), but there are two issues to note. The ﬁrst is that we do not require
that the operational PSD be a symmetric function, and the second is that we allow
for ﬁlters of complex impulse response.
Deﬁnition 18.4.1 (Operational PSD of a CSP). We say that a CSP

Z(t), t ∈R

is of operational power spectral density SZZ if

Z(t), t ∈R

is a measurable
CSP;1 the mapping SZZ : R →R is integrable; and for every integrable complex-
valued function h: R →C the power of the convolution of

Z(t), t ∈R

and h is
given by
Power in Z ⋆h =
 ∞
−∞
SZZ(f) |ˆh(f)|2 df.
(18.43)
By Lemma 15.3.5 (i) the PSD is unique:
Note 18.4.2 (The Operational PSD Is Unique). The operational PSD of a CSP
is unique in the sense that if a CSP is of two diﬀerent operational power spectral
densities, then the two must be indistinguishable.
The relationship between the operational PSD of the real QAM signal

X(t)

(18.24) and of the CSP

XBB(t)

(18.25) turns out to be very simple. Indeed,
subject to the conditions that are made precise in Theorem 18.6.6, if the baseband
CSP

XBB(t)

is of operational PSD SBB, then the real QAM SP

X(t)

is of
operational PSD SXX, where
SXX(f) = SBB

|f| −fc

,
f ∈R.
(18.44)
This result is proved in Section 18.6 and relies heavily on the fact that g is band-
limited to W/2 Hz and that fc > W/2. Here we shall only derive it heuristically
and then see how to apply it.
Recalling the deﬁnition of the operational PSD of a real SP (Deﬁnition 15.3.1), we
note that in order to derive (18.44) we need to show that its RHS is an integrable
symmetric function and that
Power in X ⋆h =
 ∞
−∞
ˆh(f)
2 SBB

|f| −fc

df,
(18.45)
whenever h: R →R is integrable. The integrability of f 	→SBB(|f| −fc) follows
directly from the integrability of SBB(·). The symmetry is obvious because the RHS
1A complex stochastic processes is said to be measurable if its real and imaginary parts are
measurable real stochastic processes.
350
Energy, Power, and PSD in QAM
of (18.44) depends on f only via |f|. Our plan for computing the power in X ⋆h
is to ﬁrst use the results of Section 7.6.7 to express the baseband representation
of X ⋆h in the form XBB ⋆h′
BB, where h′
BB is the baseband representation of the
result of passing h through a unit-gain bandpass ﬁlter of bandwidth W around
the carrier frequency fc. Using the relationship between power in passband and
baseband, this will allow us to express the power in X ⋆h as twice the power in
XBB ⋆h′
BB. Expressing the power in the latter using the operational PSD SBB(·)
of XBB will allow us to complete the calculation of the power in X ⋆h.
Before executing this plan, we pause here to heuristically argue that, loosely speak-
ing, the condition that g is bandlimited to W/2 Hz implies that we may assume
that
SBB(f) = 0,
|f| > W
2 .
(18.46)
For a precise statement of this result, see Proposition 18.6.3 in Section 18.6.2. The
intuition behind this statement is that, since g is bandlimited to W/2 Hz, in some
loose sense, all the power of the signal XBB is contained in the band |f| ≤W/2.
To heuristically justify (18.46), we shall show that if SBB(·) is an operational PSD
for

XBB(t)

, then so is the mapping f 	→SBB(f) I{|f| ≤W/2}. This follows by
noting that for every h: R →C in L1
Power in XBB ⋆h = Power in

t 	→A

ℓ∈Z
Cℓg(t −ℓTs)

⋆h
= Power in t 	→A

ℓ∈Z
Cℓ(g ⋆h)(t −ℓTs)
= Power in t 	→A

ℓ∈Z
Cℓ

(g ⋆LPFW/2) ⋆h

(t −ℓTs)
= Power in t 	→A

ℓ∈Z
Cℓ

g ⋆(h ⋆LPFW/2)

(t −ℓTs)
= Power in

t 	→A

ℓ∈Z
Cℓg(t −ℓTs)

⋆(h ⋆LPFW/2)
=
 ∞
−∞
SBB(f)
ˆh(f) I{|f| ≤W/2}
2 df
=
 ∞
−∞

SBB(f) I{|f| ≤W/2}
 ˆh(f)
2 df,
from which the result follows from the uniqueness (to within indistinguishability)
of the operational PSD (Note 18.4.2).
Here the ﬁrst equality follows from the
deﬁnition of XBB (18.25); the second because convolving a PAM signal of pulse
shape g (in our case complex) with h is tantamount to replacing the pulse shape g
with the new pulse shape g ⋆h (see the derivation of (15.19) in Section 15.4 which
extends verbatim to the complex case); the third because, by assumption, g is
bandlimited to W/2 Hz; the fourth by the associativity of convolution (see Theo-
rem 5.6.1, which, strictly speaking, is not applicable here because LPFW/2 is not
integrable); the ﬁfth because replacing the pulse shape g by g ⋆

h ⋆LPFW/2

is
tantamount to convolving the PAM signal with (h ⋆LPFW/2); the sixth from our
18.4 The Operational PSD of QAM Signals
351
assumption that SBB(·) is an operational PSD for XBB (and by ignoring the fact
that h ⋆LPFW/2 need not be integrable); and the seventh by trivial algebra.
Having established (18.46), we are now ready to compute the power in X ⋆h.
Using the results of Section 7.6.7 we obtain that for every integrable h: R →R,
the baseband representation of X ⋆h is given by XBB ⋆h′
BB where h′
BB : R →C is
the baseband representation of the result of passing h through a unit-gain bandpass
ﬁlter of bandwidth W around the carrier frequency fc:
ˆh′
BB(f) = ˆh(f + fc) I{|f| ≤W/2},
f ∈R.
(18.47)
And since the power in passband is twice the power in baseband, we conclude that
Power in X ⋆h = 2 Power in XBB ⋆h′
BB
= 2
 ∞
−∞
SBB(f)
ˆh′
BB(f)
2 df
= 2
 ∞
−∞
SBB(f)
ˆh(f + fc)
2 I{|f| ≤W/2} df
= 2
 ∞
−∞
SBB(f)
ˆh(f + fc)
2 df
= 2
 ∞
−∞
SBB( ˜f −fc)
ˆh( ˜f)
2 d ˜f
=
 ∞
−∞
SBB( ˜f −fc)
ˆh( ˜f)
2 d ˜f +
 ∞
−∞
SBB( ˜f −fc)
ˆh(−˜f)
2 d ˜f
=
 ∞
−∞
SBB( ˜f −fc)
ˆh( ˜f)
2 d ˜f +
 ∞
−∞
SBB(−f ′ −fc)
ˆh(f ′)
2 df ′
=
 ∞
−∞

SBB(f −fc) + SBB(−f −fc)
 ˆh(f)
2 df
=
 ∞
−∞
SBB(|f| −fc)
ˆh(f)
2 df,
where the ﬁrst equality follows from (18.27) because XBB ⋆h′
BB is the baseband
representation of X⋆h; the second holds because XBB is of operational PSD SBB(·);
the third by (18.47); the fourth by (18.46); the ﬁfth by changing the integration
variable to ˜f ≜f + fc; the sixth because h is real so its Fourier Transform must
be conjugate-symmetric; the seventh by changing the integration variable in the
second integral to f ′ ≜−˜f; the eighth by the linearity of integration; and the ﬁnal
equality by (18.46) and the assumption that fc > W/2. This establishes (18.45)
and thus concludes the proof of (18.44).
We next apply (18.44) to calculate the operational PSD of QAM in two scenarios:
when the complex symbols

Cℓ

form a bounded, zero-mean, WSS, CSP and when
they are generated in bi-inﬁnite block-mode.
18.4.1

Cℓ

Zero-Mean WSS and Bounded
We next use (18.44) to derive the operational PSD of QAM when the discrete-time
WSS CSP

Cℓ

is of zero mean and of autocovariance function KCC; see (18.28).
352
Energy, Power, and PSD in QAM
To use (18.44) we ﬁrst need to compute the operational PSD of the CSP XBB. This
is straightforward. As in Section 15.4.2, we note that XBB ⋆h has the same form
as (18.25) with the pulse shape g replaced by g ⋆h. Consequently, by substituting
the FT of g ⋆h for the FT of g in (18.32),2 we obtain that
Power in XBB ⋆h = A2
Ts
 ∞
−∞
∞

m=−∞
KCC(m) e−i2πfmTs |ˆg(f)|2 |ˆh(f)|2 df
(18.48)
and the operational PSD of XBB is thus
SBB(f) = A2
Ts
∞

m=−∞
KCC(m) e−i2πfmTs |ˆg(f)|2,
f ∈R.
(18.49)
This is the complex analog of (15.24). From (18.49) and (18.44) we now obtain:
Theorem 18.4.3. Under the assumptions of Theorem 18.3.1, the operational PSD
of the QAM signal

X(t), t ∈R

is given by
SXX(f) = A2
Ts
∞

m=−∞
KCC(m) e−i2π(|f|−fc)mTs ˆg

|f| −fc
2,
f ∈R.
(18.50)
Proof. The justiﬁcation of (18.44) is in Theorem 18.6.6. A formal derivation of
the operational PSD of

XBB(t), t ∈R

can be found in Section 18.6.5. We draw
the reader’s attention to the fact that the proof that we gave for the real case in
Section 15.5 is not directly applicable to the complex case because that proof relied
on Theorem 25.14.1 (Wiener-Khinchin), which we prove in Section 25.14 only for
real WSS stochastic processes.3
Figure 18.1 depicts the relationship between the pulse shape g and the operational
PSD of the QAM signal for the case where KCC(m) = I{m = 0} for every m ∈Z.
18.4.2
The Operational PSD of QAM in Bi-Inﬁnite Block-Mode
The operational PSD of QAM in bi-inﬁnite block-mode can also be computed
using (18.44).
All we need is the operational PSD of

XBB(t)

, which can be
computed from (18.36) as follows. As in Section 15.4.2, we note that XBB ⋆h has
the same form as (18.25) with the pulse shape g replaced by g ⋆h. Consequently,
by substituting the FT of g ⋆h for the FT of g in (18.36), we obtain that
Power in XBB ⋆h
=
 ∞
−∞
	 A2
NTs
N

ℓ=1
N

ℓ′=1
E[CℓC∗
ℓ′] ei2πf(ℓ′−ℓ)Ts ˆg(f)
2

 ˆh(f)
2 df,
(18.51)
2We are ignoring here the fact that g ⋆h need not satisfy the required decay condition.
3The extension to the complex case is not as trivial as one might think because the real and
imaginary parts of a WSS complex SP need not be WSS.
18.4 The Operational PSD of QAM Signals
353
f
f
f
fc
−fc
ˆg(f)
|ˆg(f)|2
A2
Ts
ˆg

|f| −fc
2
Figure 18.1: The relationship between the Fourier Transform of the pulse shape
g(·) and the operational PSD of a QAM signal. The complex symbols

Cℓ

are
assumed to be of zero mean, of unit variance, and uncorrelated.
and the operational PSD of XBB is thus
SBB(f) = A2
NTs
N

ℓ=1
N

ℓ′=1
E[CℓC∗
ℓ′] ei2πf(ℓ′−ℓ)Ts ˆg(f)
2,
f ∈R.
(18.52)
This is the complex analog of (15.26). (But note that, in our present case, SBB(·)
need not be a symmetric function.) From (18.52) and (18.44) we now obtain:
Theorem 18.4.4 (Operational PSD of QAM in Bi-Inﬁnite Block-Mode). Under
the assumptions of Theorem 18.3.2, the operational PSD SXX of the QAM signal

X(t), t ∈R

is given for every f ∈R by
SXX(f) = A2
NTs
N

ℓ=1
N

ℓ′=1
E[CℓC∗
ℓ′] e−i2π(|f|−fc)(ℓ−ℓ′)Ts ˆg

|f| −fc
2.
(18.53)
354
Energy, Power, and PSD in QAM
Proof. The justiﬁcation of (18.44) is in Theorem 18.6.6, and a formal derivation
of the operational PSD of

XBB(t)

is given in Section 18.6.5.
18.5
A Formal Account of Power in Passband and Baseband
In this section we formulate conditions under which (18.27) holds, i.e., under which
the power in passband is twice the power in baseband. We ﬁrst extend the Triangle
Inequality (4.14) to stochastic processes.
Proposition 18.5.1 (Triangle Inequality for Stochastic Processes). Let

X(t)

and

Y (t)

be (real or complex) measurable stochastic processes, and let a < b be
arbitrary real numbers. Suppose further that
E
 b
a
X(t)
2 dt

, E
 b
a
Y (t)
2 dt

< ∞.
(18.54)
Then
⎛
⎝

E
 b
a
X(t)
2 dt

−

E
 b
a
Y (t)
2 dt
⎞
⎠
2
≤E
5 b
a
X(t) + Y (t)
2 dt
6
≤
⎛
⎝

E
 b
a
X(t)
2 dt

+

E
 b
a
Y (t)
2 dt
 ⎞
⎠
2
.
(18.55)
This also holds when a is replaced with −∞and/or b is replaced with +∞.
Proof. Replace all integrals in the proof of (4.14) with expectations of integrals.
We can now state the main result of this section relating the power in passband to
the power in baseband.
Theorem 18.5.2. Let Ts, g, W, and fc be as in Theorem 18.2.1 and, addition-
ally, assume that g satisﬁes the decay condition (18.22) and that the CSP

Cℓ

is
bounded in the sense of (18.23). Then the condition
lim
T→∞
1
2T E
5 T
−T


ℓ∈Z
Cℓg(t −ℓTs)

2
dt
6
= P
(18.56)
is equivalent to the condition
lim
T→∞
1
2T E
5 T
−T
	
2 Re
	 
ℓ∈Z
Cℓg(t −ℓTs) ei2πfct


2
dt
6
= 2P.
(18.57)
18.5 A Formal Account of Power in Passband and Baseband
355
The rest of this section is dedicated to proving this theorem.
To simplify the
notation we begin by showing that it suﬃces to prove the result for the case where
Ts = 1. If Ts > 0 is not necessarily equal to 1, then we deﬁne for every t ∈R,
˜g(t) = g(tTs),
˜W = W Ts,
˜fc = fcTs,
and note that g is bandlimited to W/2 Hz if, and only if, ˜g is bandlimited to ˜W/2
Hz; that

fc ≥W/2

⇐⇒
 ˜fc ≥˜W/2

;
and that g satisﬁes the decay condition (18.22) if, and only if,
|˜g(t)| ≤
β
1 + |t|1+α ,
t ∈R
for some positive α and β.
By deﬁning τ ≜t/Ts we obtain that
1
2T
 T
−T


ℓ∈Z
Cℓg(t −ℓTs)

2
dt =
1
2(T/Ts)
 T/Ts
−T/Ts


ℓ∈Z
Cℓ˜g(τ −ℓ)

2
dτ
so the power in the mapping t 	→ Cℓg(t −ℓTs) is the same as in the mapping
τ 	→ Cℓ˜g(τ −ℓ). Similarly,
1
2T
 T
−T
	
2 Re
 
ℓ
Cℓg(t −ℓTs) ei2πfct
2
dt
=
1
2(T/Ts)
 T/Ts
−T/Ts
	
2 Re
 
ℓ
Cℓ˜g(τ −ℓ) ei2π ˜
fcτ
2
dτ
so the power in the mapping t 	→2 Re

ℓCℓg(t−ℓTs) ei2πfct
is the same as in the
mapping τ 	→2 Re

ℓCℓ˜g(τ −ℓ) ei2π ˜
fcτ
. Thus, if we establish that the inequality
˜fc > ˜W/2 implies that the power in the baseband signal τ 	→ Cℓ˜g(τ −ℓ) is equal
to half the power in τ 	→2 Re

ℓCℓ˜g(τ −ℓ) ei2π ˜
fcτ
, then it will also follow that
the inequality fc > W/2 implies that the power in t 	→ Cℓg(t −ℓTs) is equal to
half the power in t 	→2 Re

ℓCℓg(t −ℓTs) ei2πfct
.
Having established that it suﬃces to prove the theorem for Ts = 1, we assume for
the remainder of this section that Ts = 1, so the decay condition (18.22) can be
rewritten as
|g(t)| ≤
β
1 + |t|1+α ,
t ∈R.
(18.58)
As in the proof of Theorem 14.5.2, we shall simplify notation and assume that—in
calculating power as the limiting ratio of the energy in the interval [−T, T ] to the
length of the interval—T is restricted to the positive integers. The justiﬁcation is
identical to the one we gave in proving Theorem 14.5.2; see (14.52).
356
Energy, Power, and PSD in QAM
We shall ﬁnd it convenient to introduce an additional subscript “w” to indicate
“windowing.” Thus, if we deﬁne XBB(·) as
XBB(t) =

ℓ∈Z
Cℓg(t −ℓ),
t ∈R,
then its windowed version XBB,w(·) is given by
XBB,w(t) =

ℓ∈Z
Cℓg(t −ℓ) I{|t| ≤T},
t ∈R.
Similarly XPB,w(·) is the windowed version of the SP
XPB(t) = 2 Re
	 
ℓ∈Z
Cℓg(t −ℓ) ei2πfct

,
t ∈R,
and gℓ,w is the windowed version of
gℓ: t 	→g(t −ℓ),
ℓ∈Z.
(18.59)
We can now express the power in baseband as the limit, as T tends to inﬁnity, of
E
%
∥XBB,w∥2
2
&
/(2T), and the power in passband as the limit of E
%
∥XPB,w∥2
2
&
/(2T).
Note that, since the function I{·} is real-valued,
XPB,w(t) = 2 Re

XBB,w(t) ei2πfct
,
t ∈R.
(18.60)
But (18.60) notwithstanding, the energy in XPB,w need not be twice the en-
ergy in XBB,w because the signal XBB,w—unlike its unwindowed version XBB—is
not bandlimited. It is time-limited, and as such cannot be bandlimited (Theo-
rem 6.8.2).
The diﬃculty in proving the theorem is in relating the energy in XPB,w to the
energy in XBB,w and, speciﬁcally, in showing that the diﬀerence between half the
energy in XPB,w and the energy in XBB,w, when normalized by 2T, tends to zero as
T tends to inﬁnity. Aiding us in this is the following lemma relating the energy in
passband to the energy in baseband for signals that are not necessarily bandlimited.
Lemma 18.5.3. Let z be a complex energy-limited signal that is not necessarily
bandlimited, and consider the real signal x: t 	→2 Re

z(t) ei2πfct
, where fc > 0 is
arbitrary. Then,

∥z∥2 −
√
2ϵ
2
≤1
2 ∥x∥2
2 ≤

∥z∥2 +
√
2ϵ
2
,
(18.61)
where
ϵ2 =
 −fc
−∞
ˆz(f)
2 df.
(18.62)
Proof. Expressing the FT of x in terms of the FT of z, we obtain that for every
f ∈R outside a set of frequencies of Lebesgue measure zero,
ˆx(f) I{f ≥0}
= ˆz(f −fc) I{f ≥0} + ˆz∗(−f −fc) I{f ≥0}
= ˆz(f −fc) + ˆz∗(−f −fc) I{f ≥0} −ˆz(f −fc) I{f < 0}.
(18.63)
18.5 A Formal Account of Power in Passband and Baseband
357
We next consider the integral over f of the squared magnitude of the LHS and of
the RHS of (18.63). Since x is real, its FT is conjugate-symmetric so, by Parseval’s
Theorem, the integral of the squared magnitude of the LHS of (18.63) is 1
2 ∥x∥2
2.
The integral of the squared magnitude of the ﬁrst term on the RHS of (18.63) is
given by ∥z∥2
2. Finally, the integral of the squared magnitude of each of the last
two terms on the RHS of (18.63) is ϵ2 and, since they are orthogonal, the integral
of the squared magnitude of their sum is 2ϵ2. The result now follows from the
Triangle Inequality (4.14).
Applying Lemma 18.5.3 with the substitution of xBB,w for z and of xPB,w for x
we obtain upon noting that fc > W/2 that, in order to establish the theorem, it
suﬃces to show that the “out-of-band energy” term
e2 ≜

|f|≥W/2
ˆxBB,w(f)
2 df
(18.64)
satisﬁes
lim
T→∞
1
T e2 = 0,
(18.65)
with the convergence being uniform. That is, we need to show that e2/T is upper-
bounded by some function of α, β, γ, and T that converges to zero as T tends to
inﬁnity with α, β, γ held ﬁxed. Aiding us in the calculation of the out-of-band
energy is the following lemma.
Lemma 18.5.4. Let x be an energy-limited signal and let W ≥0.
(i) If u is any energy-limited signal that is bandlimited to W/2 Hz, then

|f|≥W/2
|ˆx(f)|2 df ≤∥x −u∥2
2 .
(18.66)
(ii) In particular,

|f|≥W/2
|ˆx(f)|2 df ≤∥x∥2
2 .
(18.67)
Proof. Part (ii) follows from Parseval’s Theorem. Part (i) follows by noting that
if u is an energy-limited signal that is bandlimited to W/2 Hz, then the Fourier
Transforms of x and x −u are indistinguishable for frequencies f that satisfy
|f| ≥W/2. Consequently,

|f|≥W/2
|ˆx(f)|2 df =

|f|≥W/2
|ˆx(f) −ˆu(f)|2 df
≤∥x −u∥2
2 ,
where the inequality follows by applying Part (ii) to the signal x −u.
To prove (18.65) ﬁx some integer ν ≥2 and express xBB,w as
xBB,w = s0,w + s1,w + s2,w,
(18.68)
358
Energy, Power, and PSD in QAM
where
s0,w =

0≤|ℓ|≤T−ν
cℓgℓ,w
(18.69)
s1,w =

T−ν<|ℓ|≤T+ν
cℓgℓ,w
(18.70)
s2,w =

T+ν<|ℓ|<∞
cℓgℓ,w
(18.71)
are of corresponding out-of-band energies
e2
κ =

|f|≥W/2
ˆsκ,w(f)
2 df,
κ = 0, 1, 2.
(18.72)
Note that by (18.64), (18.68), (18.72), and the Triangle Inequality
e2 ≤

e0 + e1 + e2
2.
(18.73)
Since the integer ν ≥2 is arbitrary, it follows from (18.73) that, to establish (18.65)
and to thus complete the proof of Theorem 18.5.2, it suﬃces to show that for every
ﬁxed integer ν ≥2,
lim
T→∞
1
T e2
0 = 0,
(18.74)
lim
T→∞
1
T e2
1 = 0,
(18.75)
and that
lim
ν→∞

lim
T→∞
1
T e2
2

= 0.
(18.76)
We thus conclude the theorem’s proof by establishing (18.74), (18.75), and (18.76).
We begin with the easiest, namely (18.75).
To establish (18.75) we recall the
deﬁnition of e1 (18.72) & (18.70) and use the Triangle Inequality to obtain
e1 ≤

T−ν<|ℓ|≤T+ν
	
|f|≥W/2
cℓˆgℓ,w(f)
2 df

1/2
≤γ

T−ν<|ℓ|≤T+ν
∥gℓ,w∥2
≤4γν ∥g∥2 ,
(18.77)
where the second inequality follows from (18.23) and from Lemma 18.5.4 (ii), and
where the ﬁnal inequality holds because windowing cannot increase energy, so
∥gℓ,w∥2 ≤∥gℓ∥2, with ∥gℓ∥2 being equal to ∥g∥2 by (18.59). Inequality (18.77)
establishes (18.75).
Having established (18.75), we next turn to proving (18.74). The proof is quite
similar except that, instead of using Part (ii) of Lemma 18.5.4, we use Part (i) with
the substitutions of gℓ,w for x and of gℓfor u to obtain

|f|≥W/2
ˆgℓ,w(f)
2 df ≤∥gℓ,w −gℓ∥2
2 ,
ℓ∈Z.
(18.78)
18.5 A Formal Account of Power in Passband and Baseband
359
We further upper-bound the RHS of (18.78) using the decay condition (18.58) as
∥gℓ,w −gℓ∥2
2 =
 ∞
−∞
gℓ(t)
2 I{|t| > T} dt
=
 −T
−∞
|g(t −ℓ)|2 dt +
 ∞
T
|g(t −ℓ)|2 dt
=
 −T−ℓ
−∞
|g(τ)|2 dτ +
 ∞
T−ℓ
|g(τ)|2 dτ
≤
 −T−ℓ
−∞
β2
|τ|2+2α dτ +
 ∞
T−ℓ
β2
|τ|2+2α dτ
≤2
 ∞
T−|ℓ|
β2
|τ|2+2α dτ
=
2β2
1 + 2α
1
(T −|ℓ|)1+2α ,
|ℓ| < T,
to obtain
	
|f|≥W/2
ˆgℓ,w(f)
2 df

1/2
≤
4
2β2
1 + 2α
1
(T −|ℓ|)1/2+α ,
|ℓ| < T.
(18.79)
Using (18.72), (18.69), (18.79), (18.23), and the Triangle Inequality we thus obtain
e0 ≤

|ℓ|≤T−ν
|cℓ|
	
|f|≥W/2
ˆgℓ,w(f)
2 df

1/2
≤
4
2γ2β2
1 + 2α

|ℓ|≤T−ν
1
(T −|ℓ|)1/2+α
≤2
4
2γ2β2
1 + 2α
T−ν

ℓ=0
1
(T −ℓ)1/2+α
= 2
4
2γ2β2
1 + 2α
T

˜ℓ=ν
1
˜ℓ1/2+α
≤2
4
2γ2β2
1 + 2α
 T
ν−1
1
ξ1/2+α dξ
=
3
2γ2β2
1+2α
4
1−2α

T1/2−α −(ν −1)1/2−α
if α ̸= 1/2
2γβ

ln T −ln(ν −1)

if α = 1/2
,
(18.80)
where the inequality in the ﬁrst line follows from (18.72) and from the Triangle
Inequality; the inequality in the second line from (18.79) and the boundedness
condition (18.23); the inequality in the third line by counting the term ℓ= 0 twice;
the equality in the fourth line by changing the summation variable to ˜ℓ≜T −ℓ;
the inequality in the ﬁfth line from the monotonicity of the function ξ 	→ξ−1/2−α,
which implies that
˜ℓ−1/2−α ≤
 ˜ℓ
˜ℓ−1
1
ξ1/2+α dξ;
360
Energy, Power, and PSD in QAM
and where the ﬁnal equality on the sixth line follows by direct calculation. Inequal-
ity (18.80) combines with our assumption that α is positive to prove (18.74).
We now conclude the proof of the theorem by establishing (18.76). To that end, we
begin by using Lemma 18.5.4 (ii) and the fact that s2,w is zero outside the interval
[−T, T ] to obtain
e2
2 ≤
 T
−T
s2,w(t)
2 dt.
(18.81)
We next upper-bound the RHS of (18.81) using the boundedness of the symbols
(18.23) and the decay condition (18.58):
s2,w(t)
 =


T+ν<|ℓ|<∞
cℓgℓ,w(t)

≤γ

T+ν<|ℓ|<∞
|g(t −ℓ)| I{|t| ≤T}
≤γ

T+ν<|ℓ|<∞
β
|t −ℓ|1+α I{|t| ≤T}
≤γ

T+ν<|ℓ|<∞
β
|ℓ| −|t|
1+α I{|t| ≤T}
≤γ

T+ν<|ℓ|<∞
β
(|ℓ| −T)1+α
= 2γβ
∞

ℓ=T+ν+1
1
(ℓ−T)1+α
= 2γβ
∞

˜ℓ=ν+1
1
˜ℓ1+α
≤2γβ
 ∞
ν
ξ−1−α dξ
= 2γβ
α ν−α,
(18.82)
where the equality in the ﬁrst line follows from the deﬁnition of s2,w (18.71); the
inequality in the second line from the Triangle Inequality for Complex Numbers
(2.12), the boundedness of

Cℓ

(18.23), and from the deﬁnition of gℓ(18.59); the
inequality in the third line from (18.58); the inequality in the fourth line because
|ξ −ζ| ≥
|ξ| −|ζ|
 whenever ξ, ζ ∈R; the inequality in the ﬁfth line because for
|t| > T the LHS is zero and the RHS is positive, and because for |t| ≤T we have
that |ℓ| −|t| ≥|ℓ| −T throughout the range of summation; the equality in the
sixth line from the symmetry of the summand and from the assumption that T is
an integer; the equality in the seventh line by changing the summation variable to
˜ℓ= ℓ−T; the inequality in the eighth line from the monotonicity of the function
ξ 	→ξ−1−α, which implies that
1
˜ℓ1+α ≤
 ˜ℓ
˜ℓ−1
1
ξ1+α dξ;
18.6 A Formal Account of the PSD in Baseband and Passband
361
and the ﬁnal equality in the ninth line by evaluating the integral.
It follows from (18.82) and (18.81) that
e2
2 ≤2T 4γ2β2
α2
ν−2α
(18.83)
and hence that
lim
T→∞
1
T e2
2 ≤8γ2β2
α2
ν−2α,
ν ≥2,
which proves (18.76).
18.6
A Formal Account of the PSD in Baseband and Passband
In this section we justify the derivations of Section 18.4.
18.6.1
On Limits of Convolutions
We begin with a lemma that justiﬁes the swapping of inﬁnite summation and
convolution. As a corollary we establish conditions under which feeding a (real or
complex) PAM signal of pulse shape g to a stable ﬁlter of impulse response h is
tantamount to replacing its pulse shape g with the new pulse shape g ⋆h.
Lemma 18.6.1. Let s1, s2, . . . be a sequence of measurable functions from R to C
satisfying the following two conditions:
1) The sequence is uniformly bounded in the sense that there exists some positive
number σ∞such that
sℓ(t)
 ≤σ∞,

t ∈R,
ℓ= 1, 2, . . .

.
(18.84)
2) The sequence converges to some function s uniformly over compact sets in
the sense that for every ﬁxed ξ > 0
lim
ℓ→∞sup
|t|≤ξ
s(t) −sℓ(t)
 = 0.
(18.85)
Then for every h ∈L1,
lim
ℓ→∞

sℓ⋆h

(t) =

s ⋆h

(t),
t ∈R.
(18.86)
Proof. Fix some epoch t0 ∈R and some h ∈L1. We will show that for every
ϵ > 0 there exists some L0 ∈N (depending on ϵ) such that
(sℓ⋆h

(t0) −(s ⋆h

(t0)
 < ϵ,
ℓ≥L0.
(18.87)
To that end note that our assumption that h is integrable implies that there exists
some ξ > 0 such that

|τ|≥ξ
|h(τ)| dτ <
ϵ
3σ∞
.
(18.88)
362
Energy, Power, and PSD in QAM
And when we apply our assumption that the sequence s1, s2, . . . converges to s
uniformly over compact sets to the compact interval [t0 −ξ, t0 + ξ], we obtain that
there exists some L0 (depending on ϵ, t0, and ξ) such that
∥h∥1
sup
t0−ξ≤τ≤t0+ξ
s(τ) −sℓ(τ)
 < ϵ
3,
ℓ≥L0.
(18.89)
We can now derive (18.87) as follows:
(sℓ⋆h

(t0) −(s ⋆h

(t0)

=

 ∞
−∞
sℓ(t0 −τ) h(τ) dτ −
 ∞
−∞
s(t0 −τ) h(τ) dτ

≤

 ξ
−ξ
sℓ(t0 −τ) h(τ) dτ −
 ξ
−ξ
s(t0 −τ) h(τ) dτ

+


|τ|>ξ
sℓ(t0 −τ) h(τ) dτ
 +


|τ|>ξ
s(t0 −τ) h(τ) dτ

≤
 ξ
−ξ
sℓ(t0 −τ) −s(t0 −τ)
 |h(τ)| dτ
+

|τ|>ξ
sℓ(t0 −τ) h(τ)
 dτ +

|τ|>ξ
s(t0 −τ) h(τ)
 dτ
≤∥h∥1

sup
t0−ξ≤τ≤t0+ξ
s(τ) −sℓ(τ)


+ 2σ∞

|τ|>ξ
|h(τ)| dτ
< ϵ,
where the last inequality follows from (18.88), from (18.89), and from the inequality
|s(t0 −τ)| ≤σ∞(which holds by (18.84) and (18.85)).
Corollary 18.6.2. If the sequence

Cℓ

is bounded in the sense of (18.23) and if the
measurable function g satisﬁes the decay condition (18.22), then for every h ∈L1
and every epoch t0 ∈R
	
t 	→

ℓ∈Z
Cℓg(t −ℓTs)

⋆h

(t0) =

ℓ∈Z
Cℓ(g ⋆h)(t0 −ℓTs).
(18.90)
Proof. Follows by applying Lemma 18.6.1 to the functions
sL : t 	→
L

ℓ=−L
Cℓg(t −ℓTs),
L = 1, 2, . . .
and by using the linearity of the convolution.
18.6.2
On the Support of the Operational PSD of XBB
We next prove that if the pulse shape g is bandlimited to W/2 Hz, then the
operational PSD of XBB is zero at frequencies outside the band [−W/2, W/2].
That is, we justify (18.46).
18.6 A Formal Account of the PSD in Baseband and Passband
363
Proposition 18.6.3. Assume that A, Ts, g, W, and fc are as in Theorem 18.2.1
and, additionally, that g satisﬁes the decay condition (18.22) and that the CSP

Cℓ

is bounded in the sense of (18.23). If the CSP

XBB(t), t ∈R

of (18.25) is
of operational PSD SBB(·), then SBB(f) is zero for all |f| > W/2 outside a set of
Lebesgue measure zero, and consequently
f 	→SBB(f) I
'
|f| ≤W
2
(
is also an operational PSD for

XBB(t), t ∈R

.
Proof. We shall show that the proposition’s hypotheses imply that if h ∈L1 is
such that ˆh(f) = 0 at all frequencies f satisfying |f| ≤W/2, then the power in
XBB ⋆h is zero, irrespective of the values of ˆh(f) at other frequencies. That is, we
shall show that

ˆh(f) = 0,
|f| ≤W/2

=⇒

Power in XBB ⋆h = 0

,
h ∈L1.
(18.91)
Since XBB is, by assumption, of operational PSD SBB(·), it will then follow from
(18.91) that

ˆh(f) = 0,
|f| ≤W/2

=⇒
	  ∞
−∞
SBB(f) |ˆh(f)|2 df = 0

,
h ∈L1. (18.92)
From (18.92) it is just a technicality to show that the nonnegative function SBB(·)
must be zero at all frequencies |f| > W/2 outside a set of Lebesgue measure
zero. Indeed, if, in order to reach a contradiction, we assume that SBB(·) is not
indistinguishable from the all-zero function in some interval [a, b], where a and b
are such that W/2 < a < b, then picking h as an integrable function such that
ˆh(f) is zero for |f| ≤W/2 and such that ˆh(f) = 1 for a ≤f ≤b would yield
a contradiction to (18.92). (An example of such a function h is the IFT of the
shifted-trapezoid mapping
f 	→
⎧
⎪
⎨
⎪
⎩
1
if a ≤f ≤b,
0
if f ≤W/2 or f ≥b + (a −W/2),
1 −|f−(a+b)/2|−(b−a)/2
a−W/2
otherwise,
f ∈R,
which is a frequency shifted version of the function we encountered in (7.16) and
(7.18).)
The assumption that SBB(·) is not indistinguishable from the all-zero
function in some interval [a, b] where a < b < −W/2 can be similarly contradicted.
To complete the proof we thus need to justify (18.91).
This follows from two
observations. The ﬁrst is that, by Corollary 18.6.2, for every h ∈L1
Power in XBB ⋆h = Power in t 	→A

ℓ∈Z
Cℓ

g ⋆h

(t −ℓTs).
(18.93)
The second is that, because g is an integrable function that is bandlimited to W/2
Hz, it follows from Proposition 6.5.2 that

g ⋆h

(t) =
 W/2
−W/2
ˆg(f) ˆh(f) ei2πft df,
t ∈R
364
Energy, Power, and PSD in QAM
and, in particular,

ˆh(f) = 0,
|f| ≤W/2

=⇒

g ⋆h = 0

,
h ∈L1.
(18.94)
Combining (18.93) and (18.94) establishes (18.91).
18.6.3
On the Deﬁnition of the Operational PSD
In order to demonstrate that

Z(t), t ∈R

is of operational PSD SZZ, one has
to show that (18.43) holds for every function h: R →C in L1 (Deﬁnition 18.4.1).
It turns out that it suﬃces to establish (18.43) only for functions that are in a
subset of L1, provided that the subset is suﬃciently rich. This result will allow
us to consider only functions h of compact support. To make this result precise
we need the following deﬁnition. We say that the set H is a dense subset of L1
if H is a subset of L1 such that for every h ∈L1 there exists a sequence h1, h2, . . .
of elements of H such that limν→∞∥h −hν∥1 = 0. An example of a dense subset
of L1 is the subset of functions of compact support, where a function h: R →C is
said to be of compact support if there exists some Δ > 0 such that
h(t) = 0,
|t| ≥Δ.
(18.95)
Lemma 18.6.4 (On Functions of Compact Support).
(i) The set of integrable functions of compact support is a dense subset of L1.
(ii) If h is of compact support and if g satisﬁes the decay condition (18.22) with
parameters α, β, Ts > 0, then g ⋆h also satisﬁes this decay condition with the
same parameters α and Ts but with a possibly diﬀerent parameter β′.
Proof. We begin with Part (i). Given any integrable function h (not necessarily
of compact support) we deﬁne the sequence of integrable functions of compact
support h1, h2, . . . by hν : t 	→h(t) I{|t| ≤ν} for every ν ∈N. It is then just a
technicality to show that ∥h −hν∥1 converges to zero. (This can be shown using
the Dominated Convergence Theorem, because at every t ∈R we have that hν(t)
converges to h(t) with |hν(t)| ≤|h(t)| and h being integrable.)
We next prove Part (ii). Let g satisfy the decay condition (18.22) with the positive
parameters α, β, Ts, and let Δ > 0 be such that (18.95) is satisﬁed. We shall prove
the lemma by showing that
(g ⋆h)(t)
 ≤
β′
1 + (|t|/Ts)1+α ,
t ∈R,
(18.96)
where
β′ = β ∥h∥1 21+α
1 + (2Δ/Ts)1+α
.
(18.97)
To that end we shall ﬁrst show that
(g ⋆h)(t)
 ≤β ∥h∥1 ,
t ∈R
(18.98)
18.6 A Formal Account of the PSD in Baseband and Passband
365
and
(g ⋆h)(t)
 ≤β ∥h∥1 21+α
1
1 + (|t|/Ts)1+α ,
|t| ≥2Δ.
(18.99)
We shall then proceed to show that the RHS of (18.96) is larger than the RHS of
(18.98) for |t| ≤2Δ and that it is larger than the RHS of (18.99) for |t| > 2Δ.
Both (18.98) and (18.99) follow from the bound
|(g ⋆h)(t)| =

 t+Δ
t−Δ
g(τ) h(t −τ) dτ

≤
 t+Δ
t−Δ
|g(τ)| |h(t −τ)| dτ
≤
 t+Δ
t−Δ

sup
t−Δ≤σ≤t+Δ
|g(σ)|

|h(t −τ)| dτ
= ∥h∥1
sup
t−Δ≤σ≤t+Δ
|g(σ)|
as follows. Bound (18.98) simply follows by using (18.22) to upper-bound |g(t)|
by β. And Bound (18.99) follows by using (18.22) to upper-bound |g(t)| for |t| ≥Δ
by β/

1 + ((|t| −Δ)/Ts)1+α
, and by then upper-bounding this latter expression
in the range |t| ≥2Δ by β21+α/

1 + (|t|/Ts)1+α
because in this range
1 +

(|t| −Δ)/Ts
1+α = 1 +
|t|
Ts
1+α|t| −Δ
|t|
1+α
≥1 +
|t|
Ts
1+α1
2
1+α
≥2−(1+α) + 2−(1+α)|t|
Ts
1+α
,
|t| ≥2Δ.
Having established (18.98) and (18.99), we now complete the proof by showing
that the RHS of (18.96) upper-bounds the RHS of (18.98) whenever |t| ≤2Δ, and
that it upper-bounds the RHS of (18.99) for |t| > 2Δ. That the RHS of (18.96)
upper-bounds the RHS of (18.98) whenever |t| ≤2Δ follows because
β ∥h∥1 21+α
1 + (2Δ/Ts)1+α
1 + (|t|/Ts)1+α
≥β ∥h∥1 21+α ≥β ∥h∥1 ,
|t| ≤2Δ.
And that the RHS of (18.96) upper-bounds the RHS of (18.99) whenever |t| > 2Δ
follows because the term 1 + (2Δ/Ts)1+α is larger than one.
Proposition 18.6.5. Assume that H is a dense subset of L1 and that the (real or
complex) measurable stochastic process

Z(t), t ∈R

is bounded in the sense that
for some σ∞
|Z(t)| ≤σ∞,
t ∈R.
(18.100)
If S(·) is a nonnegative integrable function such that the relation
Power in Z ⋆h =
 ∞
−∞
S(f)
ˆh(f)
2 df
(18.101)
holds for every h ∈H, then it holds for all h ∈L1.
366
Energy, Power, and PSD in QAM
Proof. Let h be an element of L1 (but not necessarily of H) for which we would
like to prove (18.101). Since H is a dense subset of L1, there exists a sequence
h1, h2, . . . of elements of H
hν ∈H,
ν = 1, 2, . . .
(18.102)
such that
lim
ν→∞∥h −hν∥1 = 0.
(18.103)
We shall prove that (18.101) holds for h by justifying the calculation
Power in Z ⋆h = lim
ν→∞Power in Z ⋆hν
(18.104)
= lim
ν→∞
 ∞
−∞
S(f)
ˆhν(f)
2 df
(18.105)
=
 ∞
−∞
S(f)
ˆh(f)
2 df.
(18.106)
The justiﬁcation of (18.105) is that, by (18.102), each of the functions hν is in H,
and the proposition’s hypothesis guarantees that (18.101) holds for such functions.
The justiﬁcation of (18.106) is a bit technical. It is based on noting that (18.103)
implies (by Theorem 6.2.11 (i) with the substitution of h −hν for x) that
lim
ν→∞
ˆhν(f) = ˆh(f),
f ∈R
(18.107)
and by then using the Dominated Convergence Theorem to justify the swapping of
the limit and integral. Indeed, (by Theorem 6.2.11 (i)) for every ν ∈N, the function
f 	→S(f) ˆhν(f) is bounded by the function f 	→

supν ∥hν∥1

S(f), which is
integrable because S(·) is integrable (by the proposition’s hypothesis) and because
the integrability of h and (18.103) imply that the supremum is ﬁnite as can be
veriﬁed using the Triangle Inequality by writing hν as h −(h −hν).
We now complete the proof by justifying (18.104). Since Z⋆hν = Z⋆h−Z⋆(h−hν),
it follows from the Triangle Inequality for Stochastic Processes (Proposition 18.5.1)
that for every T > 0


E
 T
−T
Z ⋆hν(t)
2 dt

−

E
 T
−T
Z ⋆h(t)
2 dt

≤

E
 T
−T

Z ⋆(h −hν)

(t)
2 dt

≤
√
2T σ∞∥h −hν∥1 ,
(18.108)
where the second inequality follows from (18.100) using (5.8c). Upon dividing by
√
2T and taking the limit of T →∞, it now follows from (18.108) that


Power in Z ⋆hν −
√
Power in Z ⋆h
 ≤σ∞∥h −hν∥1 ,
from which (18.104) follows by (18.103).
18.6 A Formal Account of the PSD in Baseband and Passband
367
18.6.4
Relating the Operational PSD in Passband and Baseband
We next make the relationship (18.44) between the operational PSD of X and the
operational PSD of XBB formal.
Theorem 18.6.6. Under the assumptions of Proposition 18.6.3, if the complex
stochastic process

XBB(t), t ∈R

of (18.25) is of operational PSD SBB(·) in
the sense that SBB(·) is an integrable function satisfying that for every complex
hc ∈L1,
lim
T→∞
1
2T E
5 T
−T


XBB ⋆hc

(t)

2
dt
6
=
 ∞
−∞
SBB(f)
ˆhc(f)
2 df,
(18.109)
then the QAM real SP

X(t), t ∈R

of (18.24) is of operational PSD
SPB(f) ≜SBB

f −fc

+ SBB

−f −fc

,
f ∈R
(18.110)
in the sense that SPB(·) is an integrable symmetric function such that for every
real hr ∈L1
lim
T→∞
1
2T E
 T
−T


X ⋆hr

(t)

2
dt

=
 ∞
−∞
SPB(f)
ˆhr(f)
2 df.
(18.111)
Proof. The hypothesis that SBB(·) is integrable clearly implies that SPB(·), as
deﬁned in (18.110), is integrable and symmetric. It remains to show that if (18.109)
holds for every complex hc ∈L1, then (18.111) must hold for every real hr ∈L1.
Since the set of integrable functions of compact support is a dense subset of L1
(Lemma 18.6.4 (i)), it follows from Proposition 18.6.5 that it suﬃces to establish
(18.111) for real functions hr that are of compact support.
Let hr be such a
function. The following calculation demonstrates that passing the QAM signal X
through a ﬁlter of impulse response hr is tantamount to replacing its pulse shape g
with the pulse shape consisting of the convolution of g with the complex signal
τ 	→e−i2πfcτhr(τ):

X ⋆hr

(t) =
	
τ 	→2 Re

XBB(τ) ei2πfcτ
⋆hr

(t)
= 2 Re
	
τ 	→XBB(τ) ei2πfcτ
⋆hr

(t)

= 2 Re
	
ei2πfct
XBB ⋆

τ 	→e−i2πfcτ hr(τ)

(t)

= 2 Re
	
ei2πfct A
∞

ℓ=−∞
Cℓ

g ⋆

τ 	→e−i2πfcτ hr(τ)

(t −ℓTs)

= 2 Re
	
A
∞

ℓ=−∞
Cℓ

g ⋆hc

(t −ℓTs) ei2πfct

,
(18.112)
where the ﬁrst equality follows from the deﬁnition of X in terms of XBB; the second
because hr is real (see (7.39) on the convolution between a real and a complex
368
Energy, Power, and PSD in QAM
signal); the third from Proposition 7.8.1; the fourth from Corollary 18.6.2; and
where the ﬁfth equality follows by deﬁning the mapping
hc : t 	→e−i2πfct hr(t).
(18.113)
Note that by (18.113)
ˆhc(f) = ˆhr(f + fc),
f ∈R.
(18.114)
It follows from (18.112) that X⋆hr has the form of a QAM signal with pulse shape
g⋆hc. We note that, because g (by hypothesis) satisﬁes the decay condition (18.22)
and because the fact that hr is of compact support implies by (18.113) that hc is
also of compact support, it follows from Lemma 18.6.4 (ii) that the pulse shape
g ⋆hc satisﬁes the decay condition
(g ⋆hc)(t)
 ≤
β′
1 + (|t|/Ts)1+α ,
t ∈R
(18.115)
for some positive β′. Consequently, we can apply Theorem 18.5.2 to obtain that
the power of X ⋆hr is given by
Power in X ⋆hr = 2 Power in t 	→A
∞

ℓ=−∞
Cℓ(g ⋆hc)(t −ℓTs)
= 2 Power in

t 	→A
∞

ℓ=−∞
Cℓg(t −ℓTs)

⋆hc
= 2 Power in (XBB ⋆hc)
= 2
 ∞
−∞
SBB(f)
ˆhc(f)
2 df
= 2
 ∞
−∞
SBB(f)
ˆhr(f + fc)
2 df
= 2
 ∞
−∞
SBB( ˜f −fc)
ˆhr( ˜f)
2 d ˜f
=
 ∞
−∞

SBB( ˜f −fc) + SBB(−˜f −fc)
 ˆhr( ˜f)
2 d ˜f,
(18.116)
where the second equality follows from Corollary 18.6.2; the third by the deﬁnition
of XBB; the fourth because, by hypothesis, XBB is of operational PSD SBB(·); the
ﬁfth from (18.114); the sixth by changing the integration variable to ˜f ≜f + fc;
and the seventh from the conjugate symmetry of ˆhr(·).
Since hr was an arbitrary integrable real function of compact support, (18.116)
establishes (18.111) for all such functions.
Corollary 18.6.7. Under the assumptions of Theorem 18.6.6, the QAM signal

X(t), t ∈R

is of operational PSD
SXX(f) = SBB

|f| −fc

,
f ∈R.
(18.117)
18.6 A Formal Account of the PSD in Baseband and Passband
369
Proof. Follows from the theorem by noting that, by Proposition 18.6.3 and by the
assumption that fc > W/2,
SBB

f −fc

+ SBB

−f −fc

= SBB

|f| −fc

at all frequencies f outside a set of frequencies of Lebesgue measure zero.
18.6.5
On the Operational PSD in Baseband
In the calculation of the operational PSD of the QAM signal

X(t)

via (18.44)
(which is formally stated as Corollary 18.6.7) we needed the operational PSD of
the CSP

XBB(t)

of (18.25). In this section we justify the calculations of this
operational PSD that lead to Theorems 18.4.3 and 18.4.4. Speciﬁcally, we show:
Proposition 18.6.8 (Operational PSD of a Complex PAM Signal). Let the CSP

XBB(t), t ∈R

be given by (18.25), where A ≥0, Ts > 0, and where g is a
complex Borel measurable function satisfying the decay condition (18.22) for some
constants α, β > 0.
(i) If

Cℓ

is a bounded, zero-mean, WSS CSP of autocovariance function KCC,
i.e., if it satisﬁes (18.23) and (18.28), then the CSP

XBB(t), t ∈R

is of
operational PSD SBB(·) as given in (18.49).
(ii) If

Cℓ

is produced in bi-inﬁnite block-mode from IID random bits using an
encoder enc: {0, 1}K →CN that produces zero-mean symbols from IID ran-
dom bits, then

XBB(t), t ∈R

is of operational PSD SBB(·) as given in
(18.52).
Proof. We have all the ingredients that are needed to justify our derivations of
(18.49) and (18.52). All that remains is to piece them together. Let h be any
complex integrable function of compact support. Then
Power in XBB ⋆h = Power in
	
t 	→A

ℓ∈Z
Cℓg(t −ℓTs)

⋆h

= Power in
	
t 	→A

ℓ∈Z
Cℓ(g ⋆h)(t −ℓTs)

,
(18.118)
where the ﬁrst equality follows from the deﬁnition of XBB (18.25), and where the
second equality follows from Corollary 18.6.2. Note that by Lemma 18.6.4 (ii) the
function g ⋆h satisﬁes the decay condition (18.96) for some β′ > 0.
To prove Part (i) we substitute g ⋆h for g in (18.32) to obtain from (18.118) that
Power in XBB ⋆h = A2
Ts
 ∞
−∞
∞

m=−∞
KCC(m) e−i2πfmTs |ˆg(f)|2 |ˆh(f)|2 df,
(18.119)
for every integrable complex h of compact support.
It follows from the fact
that the set of integrable functions of compact support is a dense subset of L1
370
Energy, Power, and PSD in QAM
(Lemma 18.6.4 (i)) and from Proposition 18.6.5 that (18.119) must hold for ev-
ery h that is integrable. Recalling the deﬁnition of the operational PSD (Deﬁni-
tion 18.4.1), it follows that

XBB(t), t ∈R

is of operational PSD SBB(·) as given
in (18.49).
The proof of Part (ii) is very similar except that we compute the RHS of (18.118)
using (18.36) with the substitution of g ⋆h for the pulse shape.
18.7
Exercises
Exercise 18.1 (The Second Moment of the Square QAM Constellation).
(i) Show that picking X and Y IID uniformly over the set in (10.20) results in X + iY
being uniformly distributed over the set in (16.19).
(ii) Compute the second moment of the square 2ν × 2ν QAM constellation (16.19).
Exercise 18.2 (Optimal Constellations). Let C denote a QAM constellation, and let z ∈C
be arbitrary. Deﬁne the constellation C′ = {c −z : c ∈C}.
(i) Relate the minimum distance of C′ to that of C.
(ii) Relate the second moment of C′ to that of C.
(iii) How would you choose z to minimize the second moment of C′?
Exercise 18.3 (The Power in Baseband Is Real). Show that the RHS of (18.29) is real.
Which properties of the autocovariance function KCC and of the self-similarity func-
tion Rgg are you exploiting?
Exercise 18.4 (The Power in the In-Phase and Quadrature Components). Consider the
setup of Theorem 18.3.1 with the additional assumptions that the real part (and hence,
by Exercise 17.17, also the imaginary part) of

Cℓ, ℓ∈Z

is WSS and that the pulse
shape g is real. Compute the power in each of the signals
t →2A
∞

ℓ=−∞
Re(Cℓ) g(t −ℓTs) cos(2πfct),
t →−2A
∞

ℓ=−∞
Im(Cℓ) g(t −ℓTs) sin(2πfct),
and show that these powers add up to the power in X(·). Give an intuitive explanation
for this result. Do you expect a similar result for the operational PSD?
Hint: To compute their power, express the signals as QAM signals and use Theorem 18.3.1.
Exercise 18.5 (π/4-QPSK). In QPSK or 4-QAM the data bits are mapped to complex
symbols

Cℓ

which take value in the set {±1±i} and which are then transmitted using the
signal

X(t)

deﬁned in (18.24). Consider now π/4-QPSK where, prior to transmission,
the complex symbols

Cℓ

are rotated to form the complex symbols
˜Cℓ= αℓCℓ,
ℓ∈Z,
18.7 Exercises
371
where α = eiπ/4. The transmitted signal is then
2A Re
	
∞

ℓ=−∞
˜Cℓg(t −ℓTs) ei2πfct
,
t ∈R.
Compute the power and the operational PSD of the π/4-QPSK signal when

Cℓ

is a zero-
mean WSS CSP of autocovariance function KCC. Compare the power and operational
PSD of π/4-QPSK with those of QPSK. How do they compare when the symbols

Cℓ

are IID?
Hint: See Exercise 17.22.
Exercise 18.6 (The Bandwidth of the QAM Signal). Formulate and prove a result anal-
ogous to Theorem 15.4.1 for QAM.
Exercise 18.7 (Bandwidth and Power in PAM and QAM). Data bits

Dj

are generated
at rate Rb bits per second.
(i) The bits are mapped to real symbols using a (K, N) binary-to-reals block-encoder
of rate K/N bits per real symbol. The symbols are mapped to a PAM signal of
pulse shape φ whose time shifts by integer multiples of Ts are orthonormal and
whose excess bandwidth is η. Find the bandwidth of the transmitted signal (Deﬁ-
nition 15.3.7).
(ii) Repeat for the bandwidth around the carrier frequency fc in QAM when the bits
are mapped to complex symbols using a (K, N) binary-to-complex block-encoder of
rate K/N bits per complex symbol. (As in Part (i), the pulse shape is of excess
bandwidth η.)
(iii) Show that if we express the rate ρ of the block-encoder in both cases in bits per
complex symbol, then in the former case ρ = 2K/N; in the latter case ρ = K/N;
and in both cases the bandwidth can be expressed as the same function of Rb, ρ,
and η.
(iv) Show that for both PAM and QAM the transmitted power is given by
P = EsRb
ρ
provided that the energy per symbol Es and the rate ρ are computed in both cases
per complex symbol.
Hint: Exercise 18.6 is useful for Part (ii).
Exercise 18.8 (Operational PSD of Diﬀerential PSK). Let the bi-inﬁnite sequence of IID
random bits

Dj, j ∈Z

be mapped to the complex symbols

Cℓ, ℓ∈Z

as follows:
Cℓ+1 = Cℓexp
	
i2π
8 (4D3ℓ+ 2D3ℓ+1 + D3ℓ+2)

,
ℓ= 0, 1, 2, . . .
Cℓ= Cℓ+1 exp
	
−i2π
8 (4D3ℓ+ 2D3ℓ+1 + D3ℓ+2)

,
ℓ= . . . , −2, −1,
where C0 is independent of

Dj

and uniformly distributed over the set
C =
 
1, ei 2π
8 , e2i 2π
8 , e3i 2π
8 , . . . , e7i 2π
8
!
.
Find the operational PSD of the QAM signal under the assumptions of Section 18.3 on
the pulse shape.
372
Energy, Power, and PSD in QAM
Exercise 18.9 (PAM/QAM). Let D1, . . . , Dk be IID random bits. These bits are mapped
by a mapping ϕQAM : {0, 1}k →Cn to the complex symbols C1, . . . , Cn, which are then
mapped to the QAM signal
XQAM(t; D1, . . . , Dk) = 2A Re

n

ℓ=1
CℓφQAM

t −ℓTs,QAM

ei2πfct

,
t ∈R,
where the time shifts of φQAM by integer multiples of Ts,QAM are orthonormal.
Deﬁne the real symbols X1, . . . , X2n by
X2ℓ−1 = Re(Cℓ),
X2ℓ= Im(Cℓ),
ℓ∈{1, . . . , n}
and the corresponding PAM signal
XPAM(t; D1, . . . , Dk) = A
2n

ℓ=1
XℓφPAM

t −ℓTs,PAM

,
t ∈R,
where φPAM is real and its time shifts by integer multiples of Ts,PAM are orthonormal.
(i) Relate the expected energy in XQAM to that in XPAM.
(ii) Relate the minimum squared distance
min
(d1,...,dk)̸=(d′
1,...,d′
k)
 ∞
−∞
	
XQAM

t; d1, . . . , dk

−XQAM

t; d′
1, . . . , d′
k

2
dt
to
min
(d1,...,dk)̸=(d′
1,...,d′
k)
 ∞
−∞
	
XPAM

t; d1, . . . , dk

−XPAM

t; d′
1, . . . , d′
k

2
dt.
Exercise 18.10 (A Heuristic Derivation of the Operational PSD). Throwing mathematical
caution to the wind, derive the OPSD of QAM (18.50) using Proposition 16.9.1 on ﬁltered
QAM and Theorem 18.3.1 on the power in QAM.
Hint: Ignore the fact that Proposition 16.9.1 deals with the transmission of a ﬁnite number
of symbols.
Exercise 18.11 (The Operational PSD Is Nonnegative). Show that if the CSP

Z(t)

is
of operational PSD SZZ, then SZZ(f) must be nonnegative outside a set of frequencies of
Lebesgue measure zero.
Hint: Recall Lemma 15.3.2.
Exercise 18.12 (The Operational PSD of the Real Part of a CSP). Show that the
operational PSD of a CSP does not uniquely specify the operational PSD of its real part.
Hint: How does multiplying a CSP by i aﬀect its operational PSD?
Chapter 19
The Univariate Gaussian Distribution
19.1
Introduction
In many communication scenarios the noise is modeled as a Gaussian stochastic
process. This is sometimes justiﬁed by invoking a Central Limit Theorem, which
demonstrates that many small independent disturbances add up to a stochastic
process that is approximately Gaussian.
Another justiﬁcation is mathematical
convenience: while Gaussian processes may seem daunting at ﬁrst, they are actually
well understood and often amenable to analysis. Finally, particularly in wireline
communications, the Gaussian model is justiﬁed because it leads to robust results
and to good engineering design.
For other scenarios, e.g., fast-moving wireless
mobile communications, more intricate models are needed.
Rather than starting immediately with the deﬁnition and analysis of Gaussian
stochastic processes, we shall take the more moderate approach and start by ﬁrst
discussing Gaussian random variables.
Building on that, we shall later discuss
Gaussian random vectors in Chapter 23, and only then introduce continuous-time
Gaussian stochastic processes in Chapter 25.
19.2
Standard Gaussian Random Variables
We begin with a special kind of Gaussian: the standard Gaussian.
Deﬁnition 19.2.1 (Standard Gaussian). We say that the random variable W is a
standard Gaussian or that it has a standard Gaussian distribution, if its
density function fW (·) is given by
fW (w) =
1
√
2π e−w2
2 ,
w ∈R.
(19.1)
This density is depicted in Figure 19.1. For this deﬁnition to be meaningful, the
RHS of (19.1) had better be a valid density function, i.e., be nonnegative and
integrate to one. This is indeed the case. In fact, the RHS of (19.1) is positive,
and it integrates to one because, as we next show,
 ∞
−∞
e−w2/2 dw =
√
2π.
(19.2)
373
374
The Univariate Gaussian Distribution
w
fW (w)
Figure 19.1: The standard Gaussian density function.
This integral can be veriﬁed by computing its square as follows:
	 ∞
−∞
e−w2
2 dw

2
=
 ∞
−∞
e−w2
2 dw
 ∞
−∞
e−v2
2 dv
=
 ∞
−∞
 ∞
−∞
e−w2+v2
2
dw dv
=
 ∞
0
 π
−π
r e−r2
2 dϕ dr
= 2π
 ∞
0
r e−r2
2 dr
= 2π

−e−r2/2
∞
0
= 2π,
where the ﬁrst equality follows by writing a2 as a times a; the second by writing
the product of the integrals as a double integral over R2; the third by changing
from Cartesian to polar coordinates:
w = r cos ϕ,
v = r sin ϕ,
r ≥0,
−π ≤ϕ < π,
dw dv = r dr dϕ;
the fourth because the integrand does not depend on ϕ; the ﬁfth because the
derivative of −e−r2/2 is r e−r2/2; and where the ﬁnal equality follows by direct
evaluation.
Note that the density of a standard Gaussian random variable is symmetric (19.1).
Consequently, if W is a standard Gaussian, then so is −W. This symmetry also
establishes that the expectation of a standard Gaussian is zero. The variance of a
19.3 Gaussian Random Variables
375
standard Gaussian can be computed using integration by parts:
 ∞
−∞
w2
1
√
2π e−w2
2 dw =
1
√
2π
 ∞
−∞
w
	
−d
dw e−w2
2

dw
=
1
√
2π
	
−w e−w2
2

∞
−∞+
 ∞
−∞
e−w2
2 dw

=
1
√
2π
 ∞
−∞
e−w2
2 dw
= 1,
where the last equality follows from (19.2).
19.3
Gaussian Random Variables
We next deﬁne a Gaussian (not necessarily standard) random variable as the result
of applying an aﬃne transformation to a standard Gaussian.
Deﬁnition 19.3.1 (Centered Gaussians and Gaussians). We say that a random
variable X is a centered Gaussian or that it has a centered Gaussian distri-
bution if it can be written in the form
X = aW
(19.3)
for some deterministic a ∈R and for some standard Gaussian W. We say that
the random variable X is Gaussian or that it has a Gaussian distribution if
X = aW + b
(19.4)
for some deterministic a, b ∈R and for some standard Gaussian W.
Note 19.3.2. We do not preclude a from being zero. The case a = 0 leads to X
being deterministically equal to b. We thus include the deterministic random vari-
ables in the family of Gaussian random variables.
Note 19.3.3. The family of Gaussian random variables is closed with respect to
aﬃne transformations: if X is Gaussian and α, β ∈R are deterministic, then
αX + β is also Gaussian.
Proof. Since X is Gaussian, it can be written as X = aW + b, where W is a
standard Gaussian. Consequently
αX + β = α(aW + b) + β
= (αa)W + (αb + β),
which has the form a′W + b′ for some deterministic a′, b′ ∈R.
If (19.4) holds, then the random variables on its RHS and LHS must have the same
mean. The mean of a standard Gaussian is zero, so the mean of the RHS of (19.4)
is b. The LHS is of mean E[X], and we thus conclude that in the representation
376
The Univariate Gaussian Distribution
(19.4) the deterministic constant b is uniquely determined by the mean of X, and
in fact,
b = E[X] .
Similarly, since the variance of a standard Gaussian is one, the variance of the RHS
of (19.4) is a2. And since the variance of the LHS is Var[X], we conclude that
a2 = Var[X] .
Up to its sign, the deterministic constant a in the representation (19.4) is thus also
unique.
Based on the above, one might mistakenly think that for any given mean μ and
variance σ2 there are two diﬀerent Gaussian distributions corresponding to
σW + μ,
and
−σW + μ,
(19.5)
where W is a standard Gaussian. This, however, is not the case:
Note 19.3.4. There is only one Gaussian distribution of a given mean and variance.
Proof. This can be seen in two diﬀerent ways. The ﬁrst is to note that the two
representations in (19.5) lead to the same distribution, because the standard Gaus-
sian W has a symmetric distribution, so σW and −σW have the same distribution.
The second is based on computing the density of σW + μ and showing that it is a
symmetric function of σ; see (19.6) ahead.
Having established that there is only one Gaussian distribution of a given mean μ
and variance σ2, we denote it by
N

μ, σ2
and set out to study its density. Since the distribution does not depend on the
sign of σ, it is customary to require that σ be nonnegative and to refer to it as the
standard deviation. Thus, σ2 is the variance and σ is the standard deviation.
If σ2 = 0, then the Gaussian distribution is deterministic with mean μ and has
no density.1
If σ2 > 0, then the density can be computed from the density of
the standard Gaussian distribution as follows. If X ∼N

μ, σ2
, then X has the
same distribution as μ + σW, where W is a standard Gaussian, because both X
and μ + σW are of mean μ and variance σ2 (W is zero-mean and unit-variance);
both are Gaussian (Note 19.3.3); and Gaussians of identical means and variances
have identical distributions (Note 19.3.4). The density of X is thus identical to the
density of μ + σW. The density of the latter can be computed from the density
of W (19.1) to obtain that the density of a N

μ, σ2
Gaussian random variable of
positive variance is
1
√
2πσ2 e−(x−μ)2
2σ2 ,
x ∈R.
(19.6)
This density is depicted in Figure 19.2. To derive the density of μ + σW from
1Some would say that the density of a deterministic random variable is given by Dirac’s Delta,
but we prefer not to use generalized functions in this book.
19.3 Gaussian Random Variables
377
μ
2σ
μ + σ
μ −σ
e−1/2
√
2πσ2
1
√
2πσ2
Figure 19.2: The Gaussian density function with mean μ and variance σ2.
that of W, we have used the fact that if X = g(W), where g(·) is a deterministic
continuously diﬀerentiable function whose derivative never vanishes (in our case
g(w) = μ + σw) and where W is of density fW (·) (in our case (19.1)), then the
density fX(·) of X is given by:
fX(x) =

0
if for no ξ is x = g(ξ),
1
|g′(ξ)|fW

ξ

if ξ satisﬁes x = g(ξ),
(19.7)
where g′(ξ) denotes the derivative of g(·) at ξ. (For a more formal multivariate
version of this fact see Theorem 17.3.4.)
Since the family of Gaussian random variables is closed under deterministic aﬃne
transformations (Note 19.3.3), it follows that if X ∼N

μ, σ2
with σ2 > 0, then
(X −μ)/σ is also a Gaussian random variable. Since it is of zero mean and of
unit variance, it follows that it must be a standard Gaussian, because there is only
one Gaussian distribution of zero mean and unit variance (Note 19.3.4). We thus
conclude that for σ2 > 0 and arbitrary μ ∈R,
	
X ∼N

μ, σ2
=⇒
	X −μ
σ
∼N(0, 1)

.
(19.8)
Recall that the Cumulative Distribution Function FX(·) of a RV X is deﬁned
for x ∈R as
FX(x) = Pr[X ≤x],
=
 x
−∞
fX(ξ) dξ,
where the second equality holds if X has a density function fX(·).
If W is a
standard Gaussian, then its CDF is thus given by
FW (w) =
 w
−∞
1
√
2π e−ξ2
2 dξ,
w ∈R.
378
The Univariate Gaussian Distribution
α
Q(α)
Figure 19.3: Q(α) is the area to the right of α under the standard Gaussian density
plot. Here it is represented by the shaded area.
There is, alas, no closed-form expression for this integral. To handle such expres-
sions we next introduce the Q-function.
19.4
The Q-Function
The Q-function maps every α ∈R to the probability that a standard Gaussian
exceeds it:
Deﬁnition 19.4.1 (The Q-Function). The Q-function is deﬁned by
Q(α) ≜
1
√
2π
 ∞
α
e−ξ2/2 dξ,
α ∈R.
(19.9)
For a graphical interpretation of this integral see Figure 19.3.
Since the Q-function is a well-tabulated function, we are usually happy when we can
express answers to various questions using this function. The CDF of a standard
Gaussian W can be expressed using the Q-function as follows:
FW (w) = Pr[W ≤w]
= 1 −Pr[W ≥w]
= 1 −Q(w),
w ∈R,
(19.10)
where the second equality follows because the standard Gaussian has a density,
so Pr[W = w] = 0. Similarly, with the aid of the Q-function we can express the
probability that a standard Gaussian W lies in some given interval [a, b]:
Pr[a ≤W ≤b] = Pr[W ≥a] −Pr[W ≥b]
= Q(a) −Q(b),
a ≤b.
19.4 The Q-Function
379
More generally, if X ∼N

μ, σ2
with σ > 0, then
Pr[a ≤X ≤b] = Pr[X ≥a] −Pr[X ≥b],
a ≤b
= Pr
X −μ
σ
≥a −μ
σ

−Pr
X −μ
σ
≥b −μ
σ

,
σ > 0
= Q
a −μ
σ

−Q
b −μ
σ

,

a ≤b, σ > 0

,
(19.11)
where the last equality follows because (X −μ)/σ is a standard Gaussian; see
(19.8). Letting b tend to +∞in (19.11), we obtain the probability of a half ray:
Pr[X ≥a] = Q
a −μ
σ

,
σ > 0.
(19.12a)
And letting a tend to −∞we obtain
Pr[X ≤b] = 1 −Q
b −μ
σ

,
σ > 0.
(19.12b)
The Q-function is usually only tabulated for nonnegative arguments, because the
standard Gaussian density (19.1) is symmetric: if W ∼N(0, 1) then, by the sym-
metry of its density,
Pr[W ≥−α] = Pr[W ≤α]
= 1 −Pr[W ≥α],
α ∈R.
Consequently, as illustrated in Figure 19.4,
Q(α) + Q(−α) = 1,
α ∈R,
(19.13)
and it suﬃces to tabulate the Q-function for nonnegative arguments. Note that,
by (19.13),
Q(0) = 1
2.
(19.14)
An alternative expression for the Q-function as an integral with ﬁxed integration
limits is known as Craig’s formula:
Q(α) = 1
π
 π/2
0
e−
α2
2 sin2 ϕ dϕ,
α ≥0.
(19.15)
This expression can be derived by computing a two-dimensional integral in two
diﬀerent ways as follows.
Let X ∼N(0, 1) and Y ∼N(0, 1) be independent.
Consider the probability of the event “X ≥0 and Y ≥α” where α ≥0. Since the
two random variables are independent, it follows that
Pr[X ≥0 and Y ≥α] = Pr[X ≥0] Pr[Y ≥α]
= 1
2 Q(α),
(19.16)
380
The Univariate Gaussian Distribution
α
−α
−α
−α
Q(α)
Q(α)
Q(−α)
Q(−α)
Q(α)
Figure 19.4: The identity Q(α) + Q(−α) = 1.
19.4 The Q-Function
381
y
x
α
ϕ
α
sin ϕ
area of integration
Figure 19.5: Use of polar coordinates to compute 1
2Q(α).
where the second equality follows from (19.14). We now proceed to compute the
LHS of the above in polar coordinates centered at the origin (Figure 19.5):
Pr[X ≥0 and Y ≥α] =
 ∞
0
 ∞
α
1
2π e−x2+y2
2
dy dx
=
 π/2
0
 ∞
α
sin ϕ
1
2π e−r2/2 r dr dϕ,
α ≥0
= 1
2π
 π/2
0
 ∞
α2
2 sin2 ϕ
e−t dt dϕ
= 1
2π
 π/2
0
e−
α2
2 sin2 ϕ dϕ,
α ≥0,
(19.17)
where we have performed the change of variable t ≜r2/2. The integral represen-
tation (19.15) now follows from (19.16) & (19.17).
We next describe various approximations for the Q-function. We are particularly
interested in its value for large arguments.2
Since Q(α) is the probability that
a standard Gaussian exceeds α, it follows that limα→∞Q(α) = 0. Thus, large
arguments to the Q-function correspond to small values of the Q-function. The
following bounds justify the approximation
Q(α) ≈
1
√
2πα2 e−α2
2 ,
α ≫1.
(19.18)
Proposition 19.4.2 (Estimates for the Q-Function). The Q-function is bounded
by
1
√
2πα2 e−α2/2
	
1 −1
α2

< Q(α) <
1
√
2πα2 e−α2/2,
α > 0
(19.19)
2In Digital Communications this corresponds to scenarios with low probability of error.
382
The Univariate Gaussian Distribution
and
Q(α) ≤1
2 e−α2/2,
α ≥0.
(19.20)
Proof. The proof of (19.19) is omitted (but see Exercise 19.3). Inequality (19.20)
is proved by replacing the integrand in (19.15) with its maximal value, namely, its
value at ϕ = π/2. We shall see an alternative proof in Section 20.10.
19.5
Integrals of Exponentiated Quadratics
The fact that (19.6) is a density and hence integrates to one, i.e.,
 ∞
−∞
1
√
2πσ2 e−(x−μ)2
2σ2
dx = 1,
(19.21)
can be used to compute seemingly complicated integrals. Here we shall show how
(19.21) can be used to derive the identity
 ∞
−∞
e−αx2±βx dx =
4π
α e
β2
4α ,
β ∈R, α > 0.
(19.22)
Note that this identity is meaningless when α ≤0, because in this case the inte-
grand is not integrable. For example, if α < 0, then the integrand tends to inﬁnity
as |x| tends to ∞. If α = 0 and β ̸= 0, then the integrand tends to inﬁnity either
as x tends to +∞or as x tends to −∞(depending on the sign of β). Finally, if
both α and β are zero, then the integrand is 1, which is not integrable. Note also
that, by considering the change of variable u ≜−x, one can verify that the sign
of β on the LHS of this identity is immaterial.
The trick to deriving (19.22) is to complete the exponent to a square and to then
apply (19.21):
 ∞
−∞
e−αx2+βx dx =
 ∞
−∞
exp
	
−x2 −β
αx
2(1/
√
2α)2

dx
=
 ∞
−∞
exp
-
−

x −β
2α
2
2(1/
√
2α)2 + β2
4α
.
dx
= e
β2
4α
 ∞
−∞
exp
-
−

x −β
2α
2
2(1/
√
2α)2
.
dx
= e
β2
4α
3
2π

1/
√
2α
2 ∞
−∞
1
3
2π

1/
√
2α
2 exp
-
−

x −β
2α
2
2(1/
√
2α)2
.
dx
= e
β2
4α
3
2π

1/
√
2α
2
=
4π
α e
β2
4α ,
19.6 The Moment Generating Function
383
where the ﬁrst equality follows by rewriting the integrand so that the term x2 in
the numerator is of coeﬃcient one and so that the denominator has the form 2σ2
for σ which turns out here to be given by σ ≜1/
√
2α; the second follows by
completing the square; the third by taking the multiplicative constant out of the
integral; the fourth by multiplying and dividing the integral by
√
2πσ2 so as to
bring the integrand to the form of the density of a Gaussian; the ﬁfth by (19.21);
and the sixth equality by trivial algebra.
19.6
The Moment Generating Function
As an application of (19.22) we next derive the Moment Generating Function
(MGF) of a Gaussian RV. Recall that the MGF of a RV X is denoted by MX(·)
and is given by
MX(θ) ≜E

eθX
(19.23)
for all θ ∈R for which this expectation is ﬁnite. If X has density fX(·), then its
MGF can be written as
MX(θ) =
 ∞
−∞
fX(x) eθx dx,
(19.24)
thus highlighting the connection between the MGF of X and the double-sided
Laplace Transform of its density.
If X ∼N

μ, σ2
where σ2 > 0, then
MX(θ) =
 ∞
−∞
fX(x) eθx dx
=
 ∞
−∞
1
√
2πσ2 e−(x−μ)2
2σ2
eθx dx
=
 ∞
−∞
1
√
2πσ2 e−ξ2
2σ2 eθ(ξ+μ) dξ
= eθμ
1
√
2πσ2
 ∞
−∞
e−ξ2
2σ2 +θξ dξ
= eθμ
1
√
2πσ2
4
π
1/(2σ2) e
θ2
4/(2σ2)
= eθμ+ 1
2 θ2σ2,
θ ∈R,
where the ﬁrst equality follows from (19.24); the second from (19.6); the third by
changing the integration variable to ξ ≜x −μ; the fourth by rearranging terms;
the ﬁfth from (19.22) with the substitution of 1/(2σ2) for α and of θ for β; and the
ﬁnal by simple algebra. This can be veriﬁed to hold also when σ2 = 0. Thus,

X ∼N

μ, σ2
=⇒

MX(θ) = eθμ+ 1
2 θ2σ2,
θ ∈R

.
(19.25)
384
The Univariate Gaussian Distribution
19.7
The Characteristic Function of Gaussians
19.7.1
The Characteristic Function
Recall that the Characteristic Function ΦX(·) of a random variable X is deﬁned
for every ϖ ∈R by
ΦX(ϖ) = E

eiϖX
=
 ∞
−∞
fX(x) eiϖx dx,
where the second equality holds if X has density fX(·). The second equality demon-
strates that the characteristic function is related to the Fourier Transform of the
density function but, by convention, there are no 2π’s, and the complex exponential
is not conjugated. If we allow for complex arguments to the MGF (by performing
an analytic continuation), then the characteristic function can be viewed as the
MGF evaluated on the imaginary axis:
ΦX(ϖ) = MX(iϖ),
ϖ ∈R.
(19.26)
Some of the properties of the characteristic function are summarized next.
Proposition 19.7.1 (On the Characteristic Function). Let X be a random variable
of characteristic function ΦX(·).
(i) If E[Xn] < ∞for some n ∈N, then ΦX(·) is diﬀerentiable n times and the
ν-th moment of X is related to the ν-th derivative of ΦX(·) at zero via the
relation
E[Xν] = 1
iν
dνΦX(ϖ)
dϖν

ϖ=0
,
ν = 1, . . . , n.
(19.27)
(ii) Two random variables of identical characteristic functions must have the
same distribution.
(iii) If X and Y are independent random variables of characteristic functions
ΦX(·) and ΦY (·), then the characteristic function ΦX+Y (·) of their sum is
given by the product of their characteristic functions:

X & Y independent

=⇒

ΦX+Y (ϖ) = ΦX(ϖ) ΦY (ϖ),
ϖ ∈R

.
(19.28)
Proof. For a proof of Part (i) see (Shiryaev, 1996, Chapter II, § 12.3, Theorem 1).
For Part (ii) see (Shiryaev, 1996, Chapter II, § 12.4, Theorem 2). For Part (iii) see
(Shiryaev, 1996, Chapter II, § 12.5, Theorem 4).
Recalling the MGF of a N(μ, σ2) RV (19.25), the relationship between the charac-
teristic function and the MGF (19.26), and the fact that the characteristic function
19.7 The Characteristic Function of Gaussians
385
uniquely speciﬁes the distribution (Proposition 19.7.1 (ii)), we obtain that3

X ∼N

μ, σ2
⇐⇒

ΦX(ϖ) = eiϖμ−1
2 ϖ2σ2,
ϖ ∈R

.
(19.29)
19.7.2
Moments
Since the standard Gaussian density decays faster than exponentially, it possesses
moments of all orders. Those can be computed from the characteristic function
(19.29) using Proposition 19.7.1 (i) by repeated diﬀerentiation. Using this approach
we obtain that the moments of a standard Gaussian are
E

W ν
=

1 × 3 × · · · × (ν −1)
if ν is even,
0
if ν is odd,
W ∼N(0, 1) .
(19.30)
We mention here in passing that4
E

|W|ν
=

1 × 3 × · · · × (ν −1)
if ν is even,
3
2
π 2(ν−1)/2  ν−1
2

!
if ν is odd,
W ∼N(0, 1)
(19.31)
(Johnson, Kotz, and Balakrishnan, 1994, Chapter 18, Section 3, Equation (18.13)).
19.7.3
Sums of Independent Gaussians
Using the characteristic function we next show:
Proposition 19.7.2 (The Sum of Two Independent Gaussians Is Gaussian). The
sum of two independent Gaussian random variables is a Gaussian RV.5
Proof. Let X ∼N

μx, σ2
x

and Y ∼N

μy, σ2
y

be independent. By (19.29),
ΦX(ϖ) = eiϖμx−1
2 ϖ2σ2
x,
ϖ ∈R,
ΦY (ϖ) = eiϖμy−1
2 ϖ2σ2
y,
ϖ ∈R.
Since the characteristic function of the sum of two independent random variables
is equal to the product of their characteristic functions (19.28),
ΦX+Y (ϖ) = ΦX(ϖ) ΦY (ϖ)
= eiϖμx−1
2 ϖ2σ2
x eiϖμy−1
2 ϖ2σ2
y
= eiϖ(μx+μy)−1
2 ϖ2(σ2
x+σ2
y),
ϖ ∈R.
3It does require a (small) leap of faith to accept that (19.25) also holds for complex θ. This can
be justiﬁed using analytic continuation. But there are also direct ways of deriving (19.29); see, for
example, (Williams, 1991, Chapter E, Exercise E16.4) or (Shiryaev, 1996, Chapter II, Section 12,
Paragraph 2, Example 2). Another approach is to express dΦX(ϖ)/ dϖ as E

iX eiϖX
and to
use integration by parts to verify that the latter’s expectation is equal to −ϖΦX(ϖ) and to then
solve the diﬀerential equation dΦX(ϖ)/ dϖ = −ϖΦX(ϖ) with the condition ΦX(0) = 1 to obtain
that ln ΦX(ϖ) = −1
2 ϖ2.
4The distribution of |W| is sometimes called half-normal. It is the positive square root of
the central chi-squared distribution with one degree of freedom.
5More generally, as we shall see in Chapter 23, X + Y is Gaussian whenever X and Y are
jointly Gaussian. And independent Gaussians are jointly Gaussian.
386
The Univariate Gaussian Distribution
By (19.29), this is also the characteristic function of a N

μx + μy, σ2
x + σ2
y

RV.
Since the characteristic function of a random variable fully determines its law
(Proposition 19.7.1 (ii)), X + Y must be N

μx + μy, σ2
x + σ2
y

.
Using induction one can generalize this proposition to any ﬁnite number of ran-
dom variables: if X1, . . . , Xn are independent Gaussian random variables, then
their sum is Gaussian. Applying this to α1X1, . . . , αnXn, which are independent
Gaussians whenever X1, . . . , Xn are independent Gaussians, we obtain:
Proposition 19.7.3 (Linear Combinations of Independent Gaussians). If the ran-
dom variables X1, . . . , Xn are independent Gaussians, and if α1, . . . , αn ∈R are
deterministic, then the RV Y = n
ℓ=1 αℓXℓis Gaussian with mean and variance
E[Y ] =
n

ℓ=1
αℓE[Xℓ] ,
Var[Y ] =
n

ℓ=1
α2
ℓVar[Xℓ] .
19.8
Central and Noncentral Chi-Square Random Variables
We summarize here some of the deﬁnitions and main properties of the central and
noncentral χ2 distributions and of some related distributions. We shall only use
three results from this section: that the sum of the squares of two independent
N(0, 1) random variables has a mean-2 exponential distribution; that the distri-
bution of the sum of the squares of n independent Gaussian random variables of
unit-variance and possibly diﬀerent means depends only on n and on the sum of
the squared means; and that the MGF of this latter sum has a simple explicit form.
These results can be derived quite easily from the MGF of a squared Gaussian RV,
an MGF which, using (19.22), can be shown to be given by

X ∼N

μ, σ2
=⇒

MX2(θ) =
1
√
1 −2σ2θ
e−μ2
2σ2 e
μ2
2σ2(1−2σ2θ) , θ <
1
2σ2

.
(19.32)
With a small leap of faith we can assume that (19.32) also holds for complex
arguments whose real part is smaller than 1/(2σ2) so that upon substituting iϖ
for θ we can obtain the characteristic function

X ∼N

μ, σ2
=⇒

ΦX2(ϖ) =
1
√
1 −i2σ2ϖ
e−μ2
2σ2 e
μ2
2σ2(1−i2σ2ϖ) , ϖ ∈R

.
(19.33)
19.8.1
The Central χ2 Distribution and Related Distributions
The central χ2 distribution with n degrees of freedom is denoted by χ2
n
and is deﬁned as the distribution of the sum of the squares of n IID zero-mean
19.8 Central and Noncentral Chi-Square Random Variables
387
unit-variance Gaussian random variables:
	
X1, . . . , Xn ∼IID N(0, 1)

=⇒
	
n

j=1
X2
j ∼χ2
n

.
(19.34)
Using the fact that the MGF of the sum of independent random variables is the
product of their MGFs and using (19.32) with μ = 0 and σ2 = 1, we obtain that
the MGF of the central χ2 distribution with n degrees of freedom is given by
E
%
eθχ2
n
&
=
1
(1 −2θ)n/2 ,
θ < 1
2.
(19.35)
Similarly, by (19.33) and the fact that the characteristic function of the sum of
independent random variables is the product of their characteristic functions, (or
by substituting iϖ for θ in (19.35)), we obtain that the characteristic function of
the central χ2 distribution with n degrees of freedom is given by
E
%
eiϖχ2
n
&
=
1
(1 −2iϖ)n/2 ,
ϖ ∈R.
(19.36)
Notice that for n = 2 this characteristic function is given by ϖ 	→1/(1 −i2ϖ),
which is the characteristic function of the mean-2 exponential density
1
2 e−x/2 I{x > 0},
x ∈R.
Since two random variables of identical characteristic functions must be of equal
law (Proposition 19.7.1 (ii)), we conclude:
Note 19.8.1. The central χ2 distribution with two degrees of freedom χ2
2 is the
mean-2 exponential distribution.
From (19.36) and the relationship between the moments of a distribution and the
derivatives at zero of its characteristic function (19.27), one can verify that the
ν-th moment of a χ2
n RV is given by
E

χ2
n
ν
= n × (n + 2) × · · · ×

n + 2(ν −1)

,
ν ∈N,
(19.37)
so the mean is n; the second moment is n(n + 2); and the variance is 2n.
Since the sum of the squares of random variables must be nonnegative, the density
of the χ2
n distribution is zero on the negative numbers. It is given by
fχ2
n(x) =
1
2n/2 Γ(n/2) e−x/2 x(n/2)−1 I{x > 0},
(19.38)
where Γ(·) is the Gamma function, which is deﬁned by
Γ(ξ) ≜
 ∞
0
e−t tξ−1 dt,
ξ > 0.
(19.39)
If the number of degrees of freedom is even, then the density has a particularly
simple form:
fχ2
2k(x) =
1
2k(k −1)! e−x/2 xk−1 I{x > 0},
k ∈N,
(19.40)
388
The Univariate Gaussian Distribution
thus demonstrating again that when the number of degrees of freedom is two, the
central χ2 distribution is the mean-2 exponential distribution (Note 19.8.1).
A related distribution is the generalized Rayleigh distribution, which is the
distribution of the square root of a random variable having a χ2
n distribution. The
density of the generalized Rayleigh distribution is given by
f√
χ2
n(x) =
2
2n/2 Γ(n/2)xn−1 e−x2/2 I{x > 0},
(19.41)
and its moments are given by
E
%
χ2n
ν&
= 2ν/2 Γ

(n + ν)/2

Γ(n/2)
,
ν ∈N.
(19.42)
The Rayleigh distribution is the distribution of the square root of a χ2
2 random
variable, i.e., the distribution of the square root of a mean-2 exponential random
variable. The density of the Rayleigh distribution is obtained by setting n = 2 in
(19.41):
f√
χ2
2(x) = x e−x2/2 I{x > 0}.
(19.43)
The CDF of the Rayleigh distribution (which is only nonzero for positive argu-
ments) can be computed from its density (19.43) by substituting u for ξ2/2:
F√
χ2
2(x) =
 x
0
ξ e−ξ2/2 dξ
= 1 −exp

−x2
2

,
x ≥0.
(19.44)
19.8.2
The Noncentral χ2 Distribution and Related Distributions
Using (19.32) and the fact that the MGF of the sum of independent random vari-
ables is the product of their MGFs, we obtain that if X1, . . . , Xn are independent
with Xj ∼N

μj, σ2
, then the MGF of 
j X2
j is given by
	
1
√
1 −2σ2 θ

n
e−
n
j=1 μ2
j
2σ2
e
n
j=1 μ2
j
2σ2(1−2σ2θ) ,
θ <
1
2σ2 .
(19.45)
Noting that this MGF depends on the individual means μ1, . . . , μn only via the
sum of their squares  μ2
j, we obtain:
Note 19.8.2. The distribution of the sum of the squares of independent equivari-
ance Gaussians is determined by their number, their common variance, and by the
sum of the squares of their means.
The distribution of the sum of the squares of n independent unit-variance Gaussians
whose squared means sum to λ is called the noncentral χ2 distribution with
n degrees of freedom and noncentrality parameter λ. This distribution is
denoted by χ2
n,λ. Substituting 1 for σ2 and λ for n
j=1 μ2
j in (19.45), we obtain
that the MGF of the χ2
n,λ distribution is
E
%
eθχ2
n,λ
&
=
	
1
√
1 −2θ

n
e−λ
2 e
λ
2(1−2θ) ,
θ < 1
2.
(19.46)
19.8 Central and Noncentral Chi-Square Random Variables
389
A special case of this distribution is the central χ2 distribution, which corresponds
to the case where the noncentrality parameter λ is zero.
Explicit expressions for the density of the noncentral χ2 distribution can be found
in (Johnson, Kotz, and Balakrishnan, 1995, Chapter 29, Equation (29.4)) and in
(Simon, 2002, Chapter 2). An interesting representation of this density in terms
of the density fχ2
ν,0 of the central χ2 distribution is:
fχ2
n,λ(x) =
∞

j=0
	( 1
2λ)j
j!
e−λ/2

fχ2
n+2j,0(x),
x ∈R.
(19.47)
It demonstrates that a χ2
n,λ random variable X can be generated by picking a
random integer j according to the Poisson distribution of parameter λ/2 and by
then generating a central χ2 random variable of n + 2j degrees of freedom. That
is, to generate a χ2
n,λ random variable X, generate some random variable J taking
values in the nonnegative integers according to the law
Pr[J = j] = e−λ/2 (λ/2)j
j!
,
j = 0, 1, . . .
(19.48)
and then generate X according the central χ2 distribution with n + 2j degrees of
freedom, where j is the outcome of J.
The density of the χ2
2,λ distribution is
fχ2
2,λ(x) = 1
2 e−(λ+x)/2 I0
√
λx

I{x > 0},
(19.49)
where I0(·) is the modiﬁed zeroth-order Bessel function, which is deﬁned in (27.34)
ahead.
The generalized Rice distribution corresponds to the distribution of the square
root of a noncentral χ2 distribution with n degrees of freedom and noncentrality pa-
rameter λ. The case n = 2 is called the Rice distribution. The Rice distribution
is thus the distribution of the square root of a random variable having the noncen-
tral χ2 distribution with 2 degrees of freedom and noncentrality parameter λ. The
density of the Rice distribution is
f√
χ2
2,λ(x) = x e−(x2+λ)/2 I0

x
√
λ

I{x > 0}.
(19.50)
The cumulative distribution function of the Rice distribution is, of course, zero
for negative arguments. For nonnegative arguments it can be expressed using the
ﬁrst-order Marcum Q-function, as
F√
χ2
2,λ(x) = 1 −Q1
√
λ, x

,
x ≥0,
(19.51a)
where the ﬁrst-order Marcum Q-function is deﬁned as
Q1

μ, α

=
 ∞
α
ξ e−(ξ2+μ2)/2 I0

μ ξ

dξ.
(19.51b)
390
The Univariate Gaussian Distribution
The following property of the noncentral χ2 is useful in detection theory. In the sta-
tistics literature this property is called the Monotone Likelihood Ratio property
(Lehmann and Romano, 2005, Section 3.4). Alternatively, it is called the Total
Positivity of Order 2 of the function (x, λ) 	→fχ2
n,λ(x).
Proposition 19.8.3 (The Noncentral χ2 Family Has Monotone Likelihood Ratio).
Let fχ2
n,λ(ξ) denote the density at ξ of the noncentral χ2 distribution with n degrees
of freedom and noncentrality parameter λ ≥0; see (19.47). Then for ξ0, ξ1 > 0
and λ0, λ1 ≥0 we have

ξ0 < ξ1 and λ0 < λ1

=⇒

fχ2
n,λ1 (ξ0) fχ2
n,λ0 (ξ1) ≤fχ2
n,λ0 (ξ0) fχ2
n,λ1 (ξ1)

,
(19.52)
i.e.,

λ1 > λ0

=⇒
	
ξ 	→
fχ2
n,λ1 (ξ)
fχ2
n,λ0 (ξ)
is nondecreasing in ξ > 0

.
(19.53)
Proof. See, for example, (Finner and Roters, 1997, Proposition 3.8).
19.9
The Limit of Gaussians Is Gaussian
There are a number of useful deﬁnitions of convergence for sequences of random
variables. Here we brieﬂy mention a few and show that, under each of these deﬁ-
nitions, the convergence of a sequence of Gaussian random variables to a random
variable X implies that X is Gaussian.
Let the random variables X, X1, X2, . . . be deﬁned over a common probability space
(Ω, F, P). We say that the sequence X1, X2, . . . converges to X with probability
one or almost surely if
Pr
'
ω ∈Ω : lim
n→∞Xn(ω) = X(ω)
(
= 1.
(19.54)
Thus, the sequence X1, X2, . . . converges to X almost surely if there exists an event
N ∈F of probability zero such that for every ω /∈N the sequence of real numbers
X1(ω), X2(ω), . . . converges to the real number X(ω).
The sequence X1, X2, . . . converges to X in probability if
lim
n→∞Pr

|Xn −X| ≥ϵ

= 0,
ϵ > 0.
(19.55)
The sequence X1, X2, . . . converges to X in mean square if
lim
n→∞E
%
Xn −X
2&
= 0.
(19.56)
We refer the reader to (Shiryaev, 1996, Ch. II, Section 10, Theorem 2) for a proof
that convergence in mean-square implies convergence in probability and for a proof
that almost-sure convergence implies convergence in probability. Also, if a sequence
converges in probability to X, then it has a subsequence that converges to X with
probability one (Shiryaev, 1996, Ch. II, Section 10, Theorem 5).
19.9 The Limit of Gaussians Is Gaussian
391
Theorem 19.9.1. Let the random variables X, X1, X2, . . . be deﬁned over a common
probability space (Ω, F, P). Assume that each of the random variables X1, X2, . . .
is Gaussian. If the sequence X1, X2, . . . converges to X in the sense of (19.54) or
(19.55) or (19.56), then X must also be Gaussian.
Proof. Since both mean-square convergence and almost-sure convergence imply
convergence in probability, it suﬃces to prove the theorem in the case where the
sequence X1, X2, . . . converges to X in probability. And since every sequence con-
verging to X in probability has a subsequence converging to X almost surely, it
suﬃces to prove the theorem for almost sure convergence. Our proof for this case
follows (Shiryaev, 1996, Ch. II, Section 13, Paragraph 5).
Since the random variables X1, X2, . . . are all Gaussian, it follows from (19.29) that
E

eiϖXn
= eiϖμn−1
2 ϖ2σ2
n,
ϖ ∈R,
(19.57)
where μn and σ2
n are the mean and variance of Xn. By the Dominated Convergence
Theorem it follows that the almost sure convergence of X1, X2, . . . to X implies
that
lim
n→∞E

eiϖXn
= E

eiϖX
,
ϖ ∈R.
(19.58)
It follows from (19.57) and (19.58) that
lim
n→∞eiϖμn−1
2 ϖ2σ2
n = E

eiϖX
,
ϖ ∈R.
(19.59)
The limit in (19.59) can exist for every ϖ ∈R only if there exist μ, σ2 such that
μn →μ and σ2
n →σ2. And in this case, by (19.59),
E

eiϖX
= eiϖμ−1
2 ϖ2σ2,
ϖ ∈R,
so, by Proposition 19.7.1 (ii) and by (19.29), X is N

μ, σ2
.
As we have noted, convergence in mean-square implies convergence in probability
(Shiryaev, 1996, Ch. II, Section 10, Theorem 2). The reverse is not true. But it is
true for Gaussian sequences:
Theorem 19.9.2. Let X1, X2, . . . be a sequence of Gaussian random variables that
converges in probability to the random variable X. Then X is Gaussian, and
lim
n→∞E

(Xn −X)p
= 0,
1 ≤p ≤∞,
where for p = ∞the above should be interpreted as indicating that for every ϵ > 0
the absolute diﬀerence |Xn −X| is upper-bounded by ϵ with probability one when-
ever n is suﬃciently large.
Proof. See (Neveu, 1968, Ch. I, Lemma 1.5).
Another type of convergence is convergence in distribution or weak conver-
gence, which is deﬁned as follows. Let F1, F2, . . . denote the cumulative distri-
bution functions of the sequence of random variables X1, X2, . . . We say that the
392
The Univariate Gaussian Distribution
sequence F1, F2, . . . (or sometimes X1, X2, . . .) converges in distribution to the cu-
mulative distribution function F(·) if Fn(ξ) converges to F(ξ) at every point ξ ∈R
at which F(·) is continuous. That is,

Fn(ξ) →F(ξ)

,

F(·) is continuous at ξ

.
(19.60)
Theorem 19.9.3. Let the sequence of random variables X1, X2, . . . be such that
Xn ∼N

μn, σ2
n

, for every n ∈N. Then the sequence converges in distribution to
some limiting distribution if, and only if, there exist some μ and σ2 such that
μn →μ and σ2
n →σ2.
(19.61)
And if the sequence does converge in distribution, then it converges to the mean-μ
variance-σ2 Gaussian distribution.
Proof. See (Gikhman and Skorokhod, 1996, Chapter I, Section 3, Theorem 4)
where this statement is proved in the multivariate case.
For extensions of Theorems 19.9.1 and 19.9.3 to random vectors, see Theorems 23.9.1
and 23.9.2 in Chapter 23 ahead.
19.10
Additional Reading
The Gaussian distribution, its characteristic function, and its moment generating
function appear in almost every basic book on Probability Theory. For more on
the Q-function see (Verd´u, 1998, Section 3.3) and (Simon, 2002). For more on
distributions related to the Gaussian distribution see (Simon, 2002), (Johnson,
Kotz, and Balakrishnan, 1994), and (Johnson, Kotz, and Balakrishnan, 1995).
For more on the central χ2 distribution see (Johnson, Kotz, and Balakrishnan,
1994, Chapter 18) and (Simon, 2002, Chapter 2). For more on the noncentral χ2
distribution see (Johnson, Kotz, and Balakrishnan, 1995, Chapter 29) and (Simon,
2002, Chapter 2). Various characterizations of the Gaussian distribution can be
found in (Bryc, 1995) and (Bogachev, 1998).
19.11
Exercises
Exercise 19.1 (Sums of Independent Gaussians). Let X1 ∼N

0, σ2
1

and X2 ∼N

0, σ2
2

be independent. Convolve their densities to show that X1 + X2 is Gaussian.
Exercise 19.2 (Computing Probabilities). Let X ∼N(1, 3) and Y ∼N(−2, 4) be inde-
pendent. Express the probabilities Pr[X ≤2] and Pr[2X +3Y > −2] using the Q-function
with nonnegative arguments.
Exercise 19.3 (Bounds on the Q-Function). Prove (19.19).
We suggest changing the
integration variable in (19.9) to ζ ≜ξ −α and then proving (19.19) using the inequality
1 −ζ2
2 ≤exp
	
−ζ2
2

≤1,
ξ ∈R.
19.11 Exercises
393
Exercise 19.4 (An Application of Craig’s Formula). Let the random variables Z ∼N(0, 1)
and A be independent, where A2 is of MGF MA2(·). Show that
Pr
$
Z ≥|A|
%
= 1
π
 π/2
0
MA2
	
−
1
2 sin2 ϕ

dϕ.
Exercise 19.5 (An Expression for Q2(α)). In analogy to (19.15), derive the identity
Q2(α) = 1
π
 π/4
0
e
−
α2
2 sin2 ϕ dϕ,
α ≥0.
Exercise 19.6 (Expectation of Q(X)). Show that for any RV X
E
$
Q(X)
%
=
1
√
2π
 ∞
−∞
Pr[X ≤ξ] e−ξ2/2 dξ.
(See (Verd´u, 1998, Chapter 3, Section 3.3, Eq. (3.57)).)
Exercise 19.7 (Generating Gaussians from Uniform RVs).
(i) Let W1 and W2 be IID N(0, 1), and let R =

W 2
1 + W 2
2 .
Show that R has
a Rayleigh distribution, i.e., that its density fR(r) is given for every r ∈R by
re−r2
2 I{r ≥0}. What is the CDF FR(·) of R?
(ii) Prove that if a RV X is of density fX(·) and of CDF FX(·), then FX(X) ∼U (0, 1).
(iii) Show that if U1 and U2 are IID U (0, 1) and if we deﬁne R =
-
ln
1
U1 and Θ = 2πU2,
then R cos Θ and R sin Θ are IID N(0, 1/2). (This is the Box-Muller transform. See
also Exercise 19.8.)
Exercise 19.8 (More on Generating Gaussians). Let U1 and U2 be IID U (0, 1). Deﬁne
Sν = 1 −2Uν for ν = 1, 2, and deﬁne S =

S2
1 + S2
2. Show that, conditional on S being
smaller than 1, the random variables
S1
S

−2 ln S2
and
S2
S

−2 ln S2
are independent standard Gaussians (Grimmett and Stirzaker, 2001, Chapter 4, Sec-
tion 4.11, Exercise 7). This is a variation on the Box-Muller transform (Exercise 19.7).
Exercise 19.9 (Inﬁnite Divisibility). Show that for any μ ∈R and σ2 ≥0 there exist IID
random variables X and Y such that X + Y ∼N

μ, σ2
.
Exercise 19.10 (Gaussian Mixtures of Gaussians).
(i) A RV X is drawn N

μx, σ2
x

. Conditional on X = x, a RV Y is drawn N

x, σ2
y

.
Find the distribution of Y .
(ii) Prove that
Q(α) =
 ∞
−∞
1
√π e−ξ2Q
	√
2(α −ξ)

dξ,
α ∈R.
Hint: The sum of independent Gaussians is Gaussian (Proposition 19.7.2).
394
The Univariate Gaussian Distribution
Exercise 19.11 (The Sum Is Gaussian). Show that if at least one of two independent
random variables is Gaussian and if their sum is Gaussian, then both are Gaussian.
Exercise 19.12 (MGF of the Square of a Gaussian). Derive (19.32).
Exercise 19.13 (The Distribution of the Magnitude). Show that if a random variable X
is of density fX(·) and if Y = |X|, then the density fY (·) of Y is
fY (y) =

fX(y) + fX(−y)

I{y ≥0},
y ∈R.
Exercise 19.14 (Uniformly Distributed Random Variables). Suppose that X ∼U

[0, 1]

.
(i) Find the characteristic function ΦX(·) of X.
(ii) Show that if X and Y are independent with X as above, then X+Y is not Gaussian.
Exercise 19.15 (More on the χ2 Distribution). For X ∼χ2
2m, where m is a positive
integer, and for γ > 0 arbitrary, show that
E
+
Q

γX
,
≤1
2
1
(1 + γ)m .
Hint: Use (19.20) and (19.35).
Exercise 19.16 (Stein’s Characterization of Standard Gaussians).
(i) Let W be a standard Gaussian. Show that for every continuously diﬀerentiable
function h: R →R such that h and its derivative h′ grow at most polynomially in
|x| as |x| →∞
E
$
h′(W)
%
= E
$
W h(W)
%
.
(ii) Argue heuristically that the above characterizes the standard Gaussian distribution.
Hint: For Part (ii) you might want to consider functions that closely approximate the
step-function x →I{x ≥ξ}.
Chapter 20
Binary Hypothesis Testing
20.1
Introduction
In Digital Communications the task of the receiver is to observe the channel out-
puts and to use these observations to accurately guess the data bits that were sent
by the transmitter, i.e., the data bits that were fed to the modulator. Ideally, the
guessing would be perfect, i.e., the receiver would make no errors. This, alas, is
typically impossible because of the distortions and noise that the channel intro-
duces. Indeed, while one can usually recover the data bits from the transmitted
waveform (provided that the modulator is a one-to-one mapping), the receiver has
no access to the transmitted waveform but only to the received waveform. And
since the latter is typically a noisy version of the former, some errors are usually
unavoidable.
In this chapter we shall begin our study of how to guess intelligently, i.e., how,
given the channel output, one should guess the data bits with as low a probability
of error as possible. This study will help us not only in the design of receivers but
also in the design of modulators that allow for reliable decoding from the channel’s
output.
In the engineering literature the process of guessing the data bits based on the
channel output is called “decoding.”
In the statistics literature this process is
called “hypothesis testing.” We like “guessing” because it demystiﬁes the process.
In most applications the channel output is a continuous-time waveform and we seek
to decode a large number of bits. Nevertheless, for pedagogical reasons, we shall
begin our study with the simpler case where we wish to decode only a single data
bit. This corresponds in the statistics literature to “binary hypothesis testing,”
where the term “binary” reminds us that in this guessing problem there are only
two alternatives.
Moreover, we shall assume that the observation, rather than
being a continuous-time waveform, is a vector or a scalar. In fact, we shall begin
our study with the simplest case where there are no observations at all.
20.2
Problem Formulation
In choosing a guessing strategy to minimize the probability of error, the labels
of the two alternatives are immaterial. The principles that guide us in guessing
395
396
Binary Hypothesis Testing
the outcome of a fair coin toss (where the labels are “heads” or “tails”) are the
same as for guessing the value of a random variable that takes on the values +1
and −1 equiprobably. (These are, of course, extremely simple cases that can be
handled with common sense.) Statisticians typically denote the two alternatives
by H0 and H1 and call them “hypotheses.” We shall denote the two alternatives
by 0 and 1. We thus envision guessing the value of a random variable H taking
values in the set {0, 1} with probabilities
π0 = Pr[H = 0],
π1 = Pr[H = 1].
(20.1)
The prior is the distribution of H or the pair (π0, π1). It reﬂects the state of our
knowledge about H before having made any observations. We say that the prior
is nondegenerate if
π0, π1 > 0.
(20.2)
(If the prior is degenerate, then H is deterministic and we can determine its value
without any observation. For example if π0 = 0 we always guess 1 and never err.)
The prior is uniform if π0 = π1 = 1/2.
Aiding us in the guess work is the observation Y, which is a random vector
taking values in the observation space Rd.
(When d = 1 the observation is a
random variable and we denote it by Y .) We assume that Y is a column vector,
so, using the notation of Section 17.2,
Y =

Y (1), . . . , Y (d)T.
Typically there is some statistical dependence between Y and H; otherwise, Y
would be useless. If the dependence is so strong that from Y one can deduce H,
then our guess work is very easy: we simply compute from Y the value of H and
declare the result as our guess; we never err. The cases of most interest to us
are therefore those where Y neither determines H nor is statistically independent
of H. Unless otherwise speciﬁed, we shall assume that, conditional on H = 0,
the observation Y is of density fY|H=0(·) and that, conditional on H = 1, it is of
density fY|H=1(·). Here fY|H=0(·) and fY|H=1(·) are nonnegative Borel measurable
functions from Rd to R that integrate to one.1
Our problem is how to use the observation Y to intelligently guess the value of H.
At ﬁrst we shall limit ourselves to deterministic guessing rules. Later we shall show
that no randomized guessing rule can outperform an optimal deterministic rule. A
deterministic guessing rule (or decision rule, or decoding rule) for guessing H
based on Y is a (Borel measurable) mapping from the set of possible observations
Rd to the set {0, 1}. We denote such a mapping by
φGuess : Rd →{0, 1}
(20.3)
and say that φGuess(yobs) is the guess we make after having observed that Y = yobs.
1Readers who are familiar with Measure Theory should note that these are densities with
respect to the Lebesgue measure on Rd, but that the reference measure is inessential to our
analysis. We could have also chosen as our reference measure the sum of the probability measures
on Rd corresponding to H = 0 and to H = 1. This would have guaranteed the existence of the
densities.
20.3 Guessing in the Absence of Observables
397
The probability of error associated with the guessing rule φGuess(·) is
Pr(error) ≜Pr[φGuess(Y) ̸= H].
(20.4)
Note that two sources of randomness determine whether the guessing rule φGuess(·)
errs or not: the realization of H and the generation of Y conditional on that
realization.
We say that a guessing rule is optimal if no other guessing rule
attains a smaller probability of error. (We shall later see that there always exists
an optimal guessing rule.2) In general, there may be a number of diﬀerent optimal
guessing rules.
We shall therefore try to refrain from speaking of the optimal
guessing rule. We apologize if this results in cumbersome writing. The probability
of error associated with optimal guessing rules is the optimal probability of
error and is denoted throughout by
p∗(error).
20.3
Guessing in the Absence of Observables
We begin with the simplest case where there are no observables. Common sense
dictates that in this case we should base our guess on the prior (π0, π1) as follows.
If π0 > π1, then we should guess that the value of H is 0; if π0 < π1, then we
should guess the value 1; and if π0 = π1 = 1/2, then it does not really matter what
we guess: the probability of error will be either way 1/2.
To verify that this intuition is correct note that, since there are no observables,
there are only two guessing rules: the rule “guess 0” and the rule “guess 1.” The
former results in the probability of error π1 (it is in error whenever H = 1, which
happens with probability π1), and the latter results in the probability of error π0.
Hence the former rule is optimal if π0 ≥π1 and the latter is optimal when π1 ≥π0.
When π0 = π1 both rules are optimal and we can use either one.
We summarize that, in the absence of observations, an optimal guessing rule is:
φ∗
Guess =

0
if Pr[H = 0] ≥Pr[H = 1],
1
otherwise.
(20.5)
(Here we guess 0 also when Pr[H = 0] = Pr[H = 1]. An equally good rule would
guess 1 in this case.)
As we next show, the error probability p∗(error) of this rule is
p∗(error) = min

Pr[H = 0], Pr[H = 1]

.
(20.6)
This can be veriﬁed by considering the case where Pr[H = 0] ≥Pr[H = 1] and the
case where Pr[H = 0] < Pr[H = 1] separately. By (20.5), in the former case our
guess is 0 with the associated probability of error Pr[H = 1], whereas in the latter
case our guess is 1 with the associated probability of error Pr[H = 0]. In either
case the probability of error is given by the RHS of (20.6).
2Thus, while there is no such thing as “smallest strictly positive number,” i.e., a positive
number that is smaller-or-equal to any other positive number, we shall see that there always
exists a guessing rule that no other guessing rule can outperform. Mathematicians paraphrase
this by saying that “the inﬁmum of the probability of error over all the guessing rules is achievable,
i.e., is a minimum.”
398
Binary Hypothesis Testing
20.4
The Joint Law of H and Y
Before we can extend the results of Section 20.3 to the more interesting case where
we guess H after observing Y, we pause to discuss the joint distribution of H
and Y. This joint distribution is needed in order to derive an optimal decision rule
and in order to analyze its performance. Some care must be exercised in describing
this law because H is discrete (binary) and Y has a density. It is usually simplest
to describe the joint law by describing the prior (the distribution of H), and by
then describing the conditional law of Y given H = 0 and the conditional law of Y
given H = 1.
If, conditional on H = 0, the distribution of Y has the density fY|H=0(·) and if,
conditional on H = 1, the distribution of Y has the density fY|H=1(·), then the
joint distribution of H and Y can be described using the prior (π0, π1) (20.1) and
the conditional densities
fY|H=0(·)
and
fY|H=1(·).
(20.7)
From the prior (π0, π1) and the conditional densities fY|H=0(·), fY|H=1(·) we can
compute the (unconditional) density of Y:
fY(y) = π0fY|H=0(y) + π1fY|H=1(y),
y ∈Rd.
(20.8)
The conditional distribution of H given Y = yobs is a bit more tricky because
the probability of Y taking on the value yobs (exactly) is zero. There are two
approaches to deﬁning Pr[H = 0|Y = yobs] in this case: the heuristic one that is
usually used in a ﬁrst course on probability theory and the measure-theoretic one
that was pioneered by Kolmogorov. Our approach is to deﬁne this quantity in a
way that will be palatable to both mathematicians and engineers and to then give
a heuristic justiﬁcation for our deﬁnition.
We deﬁne the conditional probability that H = 0 given Y = yobs as
Pr

H = 0
 Y = yobs

≜
 π0fY|H=0(yobs)
fY(yobs)
if fY(yobs) > 0,
1
2
otherwise,
(20.9a)
where fY(·) is given in (20.8), and analogously
Pr

H = 1
 Y = yobs

≜
 π1fY|H=1(yobs)
fY(yobs)
if fY(yobs) > 0,
1
2
otherwise.
(20.9b)
Notice that our deﬁnition is meaningful in the sense that the values we assign to
Pr[H = 0|Y = yobs] and Pr[H = 1|Y = yobs] are nonnegative and sum to one:
Pr

H = 0
 Y = yobs

+ Pr

H = 1
 Y = yobs

= 1,
yobs ∈Rd.
(20.10)
Also note that our deﬁnition of Pr[H = 0|Y = yobs] and Pr[H = 1|Y = yobs]
for those yobs ∈Rd for which fY(yobs) = 0 is quite arbitrary; we chose 1/2 just
20.4 The Joint Law of H and Y
399
for concreteness.3 Indeed, it is not diﬃcult to verify that the probability that yobs
satisﬁes π0fY|H=0(yobs)+π1fY|H=1(yobs) = 0 is zero, and hence our deﬁnitions in
this eventuality are not important; see (20.12) ahead.
If d = 1, then the observation is a random variable Y and a heuristic way to
motivate (20.9a) is to consider the limit
lim
δ↓0
Pr

H = 0, Y ∈

yobs −δ, yobs + δ

Pr

Y ∈

yobs −δ, yobs + δ

.
(20.11)
Assuming some regularity of the conditional densities (e.g., continuity) we can use
the approximations
Pr

H = 0, Y ∈(yobs −δ, yobs + δ)

= π0
 yobs+δ
yobs−δ
fY |H=0(y) dy
≈2π0δfY |H=0(yobs),
δ ≪1,
Pr

Y ∈(yobs −δ, yobs + δ)

=
 yobs+δ
yobs−δ
fY (y) dy
≈2δfY (yobs),
δ ≪1,
to argue that, under suitable regularity conditions, (20.11) agrees with the RHS of
(20.9a) when fY (yobs) > 0. A similar calculation can be carried out in the vector
case where d > 1.
We next remark on observations yobs at which the density of Y is zero. Accounting
for such observations makes the writing a bit cumbersome as in (20.9). Fortunately,
the probability of such observations is zero:
Note 20.4.1. Let H be drawn according to the prior (π0, π1), and let the con-
ditional densities of Y given H be fY|H=0(·) and fY|H=1(·) with fY(·) given in
(20.8). Then
Pr

Y ∈
˜y ∈Rd : fY(˜y) = 0

= 0.
(20.12)
Proof.
Pr

Y ∈
˜y ∈Rd : fY(˜y) = 0

=

{˜y∈Rd:fY(˜y)=0}
fY(y) dy
=

{˜y∈Rd:fY(˜y)=0}
0 dy
= 0,
where the second equality follows because the integrand is zero over the range of
integration.
3In the measure-theoretic probability literature our deﬁnition is just a “version” (among many
others) of the conditional probabilities of the event H = 0 (respectively H = 1), conditional on
the σ-algebra generated by the random vector Y (Billingsley, 1995, Section 33), (Williams, 1991,
Chapter 9).
400
Binary Hypothesis Testing
We conclude this section with two technical remarks which are trivial if you ignore
observations where fY(·) is zero:
Note 20.4.2. Consider the setup of Note 20.4.1.
(i) For every y ∈Rd
min

π0fY|H=0(y), π1fY|H=1(y)

= min

Pr[H = 0|Y = y], Pr[H = 1|Y = y]

fY(y).
(20.13)
(ii) For every y ∈Rd

π0fY|H=0(y) ≥π1fY|H=1(y)

⇐⇒

Pr[H = 0|Y = y] ≥Pr[H = 1|Y = y]

.
(20.14)
Proof. Identity (20.13) can be proved using (20.9) and (20.8) by separately con-
sidering the case fY(y) > 0 and the case fY(y) = 0 (where the latter is equivalent,
by (20.8), to π0fY|H=0(y) and π1fY|H=1(y) both being zero).
To prove (20.14) we also separately consider the case fY(y) > 0 and the case
fY(y) = 0. In the former case we note that for c > 0 the condition a ≥b is
equivalent to the condition a/c ≥b/c so for fY(yobs) > 0

π0fY|H=0(y) ≥π1fY|H=1(y)

⇐⇒
	π0fY|H=0(y)
fY(y)



Pr[H=0|Y=y]
≥π1fY|H=1(y)
fY(y)



Pr[H=1|Y=y]

.
In the latter case where fY(y) = 0 we note that, by (20.8), both π0fY|H=0(y)
and π1fY|H=1(y) are zero, so the condition on the LHS of (20.14) is true (0 ≥0).
Fortunately, when fY(y) = 0 the condition on the RHS of (20.14) is also true,
because in this case (20.9) implies that Pr[H = 0|Y = y] and Pr[H = 1|Y = y]
are both equal to 1/2 (and 1/2 ≥1/2).
20.5
Guessing after Observing Y
We next derive an optimal rule for guessing H after observing that Y = yobs.
We begin with a heuristic argument. Having observed that Y = yobs, there are
only two possible decision rules: to guess 0 or guess 1. Which should we choose?
The answer now depends on the a posteriori distribution of H. Once it has been
revealed to us that Y = yobs, our outlook changes and we now assign the event
H = 0 the a posteriori probability Pr[H = 0|Y = yobs] and the event H = 1 the
complementary probability Pr[H = 1|Y = yobs]. If the former is greater than the
latter, then we should guess 0, and otherwise we should guess 1. Thus, after it has
been revealed to us that Y = yobs the situation is equivalent to one in which we
need to guess H without any observables and where our distribution on H is not
20.5 Guessing after Observing Y
401
its a priori distribution (prior) but its a posteriori distribution. Using our analysis
from Section 20.3 we conclude that the guessing rule
φ∗
Guess(yobs) =

0
if Pr[H = 0|Y = yobs] ≥Pr[H = 1|Y = yobs],
1
otherwise,
(20.15)
is optimal. Once again, the way we resolve ties is arbitrary: if the observation
Y = yobs results in the a posteriori distribution of H being uniform, that is, if
Pr[H = 0|Y = yobs] = Pr[H = 1|Y = yobs] = 1/2, then either guess is optimal.
Using Note 20.4.2 (ii) we can also express the decision rule (20.15) as
φ∗
Guess(yobs) =

0
if π0fY|H=0(yobs) ≥π1fY|H=1(yobs),
1
otherwise.
(20.16)
Conditional on Y = yobs, the probability of error of the optimal decision rule is,
in analogy to (20.6), given by
p∗(error|Y = yobs) = min

Pr[H = 0|Y = yobs], Pr[H = 1|Y = yobs]

,
(20.17)
as can be seen by treating the case Pr[H = 0|Y = yobs] ≥Pr[H = 1|Y = yobs] and
the complementary case Pr[H = 0|Y = yobs] < Pr[H = 1|Y = yobs] separately.
The unconditional probability of error associated with the rule (20.15) is thus
p∗(error) = E

min

Pr[H = 0|Y], Pr[H = 1|Y]

(20.18)
=

Rd min

Pr[H = 0|Y = y], Pr[H = 1|Y = y]

fY(y) dy
(20.19)
=

Rd min

π0fY|H=0(y), π1fY|H=1(y)

dy,
(20.20)
where the last equality follows from Note 20.4.2 (i).
Before summarizing these conclusions in a theorem, we present the following simple
lemma on the probabilities of error associated with general decision rules.
Lemma 20.5.1. Consider the setup of Note 20.4.1.
Let φGuess(·) be an arbi-
trary guessing rule as in (20.3). Then the probabilities of error p(error|H = 0),
p(error|H = 1), and p(error) associated with φGuess(·) are given by
p(error|H = 0) =

y/∈D
fY|H=0(y) dy,
(20.21)
p(error|H = 1) =

y∈D
fY|H=1(y) dy,
(20.22)
and
p(error) =

Rd

π0fY|H=0(y) I{y /∈D} + π1fY|H=1(y) I{y ∈D}

dy,
(20.23)
where
D = {y ∈Rd : φGuess(y) = 0}.
(20.24)
402
Binary Hypothesis Testing
Proof. Conditional on H = 0 the guessing rule makes an error only if Y does not
fall in the set of observations for which φGuess(·) produces the guess “H = 0.” This
establishes (20.21). A similar argument proves (20.22). Finally, (20.23) follows
from (20.21) & (20.22) using the identity
p(error) = π0 p(error|H = 0) + π1 p(error|H = 1).
We next state the key result about binary hypothesis testing. The statement is a
bit cumbersome because, in general, there may be many observations that result
in H being a posteriori uniformly distributed, and an optimal decision rule can
map each such observation to a diﬀerent guess and still be optimal.
Theorem 20.5.2 (Optimal Binary Hypothesis Testing). Suppose that a guessing
rule φ∗
Guess : Rd →{0, 1} produces the guess “H = 0” only when yobs is such that
π0fY|H=0(yobs) ≥π1fY|H=1(yobs), i.e.,

φ∗
Guess(yobs) = 0

=⇒

π0fY|H=0(yobs) ≥π1fY|H=1(yobs)

,
(20.25a)
and produces the guess “H = 1” only when π1fY|H=1(yobs) ≥π0fY|H=0(yobs),
i.e.,

φ∗
Guess(yobs) = 1

=⇒

π1fY|H=1(yobs) ≥π0fY|H=0(yobs)

.
(20.25b)
Then no other guessing rule has a smaller probability of error, and
Pr

φ∗
Guess(Y) ̸= H

=

Rd min

π0fY|H=0(y), π1fY|H=1(y)

dy.
(20.26)
Proof. Let φGuess : Rd →{0, 1} be any guessing rule, and let
D = {y ∈Rd : φGuess(y) = 0}
(20.27)
be the set of observations that result in φGuess(·) producing the guess “H = 0.”
Then the probability of error associated with φGuess(·) can be lower-bounded by
Pr

φGuess(Y) ̸= H

=

Rd

π0fY|H=0(y) I{y /∈D} + π1fY|H=1(y) I{y ∈D}

dy
≥

Rd min

π0fY|H=0(y), π1fY|H=1(y)

dy,
(20.28)
where the equality follows from Lemma 20.5.1 and where the inequality follows
because for every value of y ∈Rd
π0fY|H=0(y) I{y /∈D} + π1fY|H=1(y) I{y ∈D}
≥min

π0fY|H=0(y), π1fY|H=1(y)

,
(20.29)
as can be veriﬁed by noting that, irrespective of the set D, one of the two terms
I{y ∈D} and I{y /∈D} is equal to one and the other is equal to zero, so the LHS of
20.6 Randomized Decision Rules
403
(20.29) is either equal to π0fY|H=0(y) or to π1fY|H=1(y) and hence lower-bounded
by min{π0fY|H=0(y), π1fY|H=1(y)}.
We prove the optimality of φ∗
Guess(·) by next showing that the probability of error
associated with φ∗
Guess(·) is equal to the RHS of (20.28). To this end we deﬁne
D∗= {y ∈Rd : φ∗
Guess(y) = 0}
(20.30)
and note that if both (20.25a) and (20.25b) hold, then
π0fY|H=0(y) I{y /∈D∗} + π1fY|H=1(y) I{y ∈D∗}
= min

π0fY|H=0(y), π1fY|H=1(y)

,
y ∈Rd.
(20.31)
Applying Lemma 20.5.1 to the decoder φ∗
Guess(·) we obtain
Pr

φ∗
Guess(Y) ̸= H

=

Rd

π0fY|H=0(y) I{y /∈D∗} + π1fY|H=1(y) I{y ∈D∗}

dy
=

Rd min

π0fY|H=0(y), π1fY|H=1(y)

dy,
(20.32)
where the second equality follows from (20.31). The theorem now follows from
(20.28) and (20.32).
Referring to a situation where the observation results in the a posteriori distribu-
tion of H being uniform as a tie we have:
Note 20.5.3. The fact that both conditional on H = 0 and conditional on H = 1
the observation Y has a density does not imply that the probability of a tie is zero.
For example, if H takes values in {0, 1} equiprobably, and if the observation Y is
given by Y = H + U, where U is uniformly distributed over the interval [−2, 2]
independently of H, then the a posteriori distribution of H is uniform whenever
Y ∈[−1, 2], and this occurs with probability 3/4.
20.6
Randomized Decision Rules
So far we have restricted ourselves to deterministic decision rules, where the guess
is a deterministic function of the observation. We next remove this restriction and
allow for some randomization in the decision rule. As we shall see in this section
and in greater generality in Section 20.11, when properly deﬁned, randomization
does not help: the lowest probability of error that is achievable with randomized
decision rules can also be achieved with deterministic decision rules.
By a randomized decision rule we mean that, after observing that Y = yobs, the
guesser chooses some bias b(yobs) ∈[0, 1] and then tosses a coin of that bias.
If the result is “heads” it guesses 0 and otherwise it guesses 1.
Note that the
deterministic rules we have considered before are special cases of the randomized
ones: any deterministic decision rule can be viewed as a randomized decision rule
where, depending on yobs, the bias b(yobs) is either zero or one.
Some care must be exercised in deﬁning the joint distribution of the coin toss with
the other variables (H, Y). We do not want to allow for “telepathic coins.” That is,
404
Binary Hypothesis Testing
yobs
Bias
Calculator
b(yobs)
Guess
Θ ∼U ([0, 1])
Θ < b(yobs) ⇒“H = 0”
Θ ≥b(yobs) ⇒“H = 1”
Random Number
Generator
Figure 20.1: A block diagram of a randomized decision rule.
we want to make sure that once Y = yobs has been observed and the bias b(yobs)
has been accordingly computed, the outcome of the coin toss is random, i.e., has
nothing to do with H. Probabilists would say that we require that, conditional on
Y = yobs, the outcome of the coin toss be independent of H. (We shall discuss
conditional independence in Section 20.11.) We can clarify the setting as follows.
Upon observing the outcome Y = yobs, the guesser computes the bias b(yobs).
Using a local random number generator the guesser then draws a random variable Θ
uniformly over the interval [0, 1], independently of the pair (H, Y). If the outcome θ
is smaller than b(yobs), then it guesses “H = 0,” and otherwise it guesses “H = 1.”
A randomized decision rule is depicted in Figure 20.1.
We oﬀer two proofs that randomized decision rules cannot outperform the best
deterministic ones.
The ﬁrst is by straightforward calculation.
Conditional on
Y = yobs, the randomized guesser makes an error either if Θ ≤b(yobs) (resulting
in the guess “H = 0”) while H = 1, or if Θ > b(yobs) (resulting in the guess
“H = 1”) while H = 0. Consequently,
Pr

error
 Y = yobs

= b(yobs) Pr

H = 1
 Y = yobs

+

1 −b(yobs)

Pr

H = 0
 Y = yobs

.
(20.33)
Thus, Pr(error|Y = yobs) is a weighted average of Pr[H = 0|Y = yobs] and
Pr[H = 1|Y = yobs]. As such, irrespective of the weights, it cannot be smaller
than the minimum of the two. But, by (20.17), the optimal deterministic decision
rule (20.15) achieves just this minimum. We conclude that, irrespective of the bias,
for each outcome Y = yobs the conditional probability of error of the randomized
decoder is lower-bounded by that of the optimal deterministic decoder (20.15).
Since this is the case for every outcome, it must also be the case when we average
over the outcomes. This concludes the ﬁrst proof.
20.7 The MAP Decision Rule
405
In the second proof we view the outcome of the local random number generator Θ
as an additional observation.
Since it is independent of (H, Y) and since it is
uniform over [0, 1],
fY,Θ|H=0(y, θ) = fY|H=0(y) fΘ|Y=y,H=0(θ)
= fY|H=0(y) fΘ(θ)
= fY|H=0(y) I{0 ≤θ ≤1},
(20.34a)
and similarly
fY,Θ|H=1(y, θ) = fY|H=1(y) I{0 ≤θ ≤1}.
(20.34b)
Since the randomized decision rule can be viewed as a deterministic decision
rule that is based on the pair (Y, Θ), it cannot outperform any optimal de-
terministic guessing rule based on (Y, Θ).
But by Theorem 20.5.2 and (20.34)
it follows that the deterministic decision rule that guesses “H = 0” whenever
π0fY|H=0(y) ≥π1fY|H=1(y) is optimal not only for guessing H based on Y but
also for guessing H based on (Y, Θ), because it produces the guess “H = 0” only
when π0fY,Θ|H=0(y, θ) ≥π1fY,Θ|H=1(y, θ) and it produces the guess “H = 1”
only when π1fY,Θ|H=1(y, θ) ≥π0fY,Θ|H=0(y, θ). This concludes the second proof.
Even though randomized decision rules cannot outperform the best deterministic
rules, they may have other advantages. For example, they allow for more symmetric
ways of resolving ties. Suppose, for example, that we have no observations and that
the prior is uniform. In this case guessing “H = 0” will give rise to a probability of
error of 1/2, with an error occurring whenever H = 1. Similarly guessing “H = 1”
will also result in a probability of error of 1/2, this time with an error occurring
whenever H = 0. If we think about H as being an information bit, then the former
rule makes sending 0 less error prone than sending 1. A randomized test that ﬂips
a fair coin and guesses 0 if “heads” and 1 if “tails” gives rise to the same average
probability of error (i.e., 1/2) and makes sending 0 and sending 1 equally (highly)
error prone.
If Y = yobs results in a tie, i.e., if it yields a uniform a posteriori distribution
on H,
Pr

H = 0
 Y = yobs

= Pr

H = 1
 Y = yobs

= 1
2,
then the probability of error of the randomized decoder (20.33) does not depend on
the bias. In this case there is thus no loss of optimality in choosing b(yobs) = 1/2,
i.e., by employing a fair coin. This makes for a symmetric way of resolving the tie
in the a posteriori distribution of H.
20.7
The MAP Decision Rule
In Section 20.5 we presented an optimal decision rule (20.15). A slight variation
on that decoder is the Maximum A Posteriori (MAP) decision rule. The MAP
rule is identical to (20.15) except in how it resolves ties. Unlike (20.15), which
resolves ties by guessing “H = 0,” the MAP rule resolves ties by ﬂipping a fair
406
Binary Hypothesis Testing
coin. It can thus be summarized as follows:
φMAP(yobs) ≜
⎧
⎪
⎨
⎪
⎩
0
if Pr[H = 0|Y = yobs] > Pr[H = 1|Y = yobs],
1
if Pr[H = 0|Y = yobs] < Pr[H = 1|Y = yobs],
U

{0, 1}

if Pr[H = 0|Y = yobs] = Pr[H = 1|Y = yobs],
(20.35)
where we use “U

{0, 1}

” to indicate that we guess the outcome uniformly at
random.
Note that, like the rule in (20.15), the MAP rule is optimal. This follows because
the way ties are resolved does not inﬂuence the probability of error, and because
the MAP rule agrees with the rule (20.15) for all observations which do not result
in a tie.
Theorem 20.7.1 (The MAP Rule Is Optimal). The Maximum A Posteriori deci-
sion rule (20.35) is optimal.
Since the MAP decoder is optimal,
p∗(error) = π0 pMAP(error|H = 0) + π1 pMAP(error|H = 1),
(20.36)
where pMAP(error|H = 0) and pMAP(error|H = 1) denote the conditional prob-
abilities of error for the MAP decoder.
Note that one can easily ﬁnd guessing
rules (such as the rule “always guess 0”) that yield a conditional probability of
error smaller than pMAP(error|H = 0), but one cannot ﬁnd a rule whose average
probability of error outperforms the RHS of (20.36).
Using Note 20.4.2 (ii) we can express the MAP rule in terms of the densities and
the prior as
φMAP(yobs) =
⎧
⎪
⎨
⎪
⎩
0
if π0fY|H=0(yobs) > π1fY|H=1(yobs),
1
if π0fY|H=0(yobs) < π1fY|H=1(yobs),
U

{0, 1}

if π0fY|H=0(yobs) = π1fY|H=1(yobs).
(20.37)
Alternatively, the MAP decision rule can be described using the likelihood-ratio
function LR(·), which is deﬁned by
LR(y) ≜fY|H=0(y)
fY|H=1(y),
y ∈Rd
(20.38)
using the convention
α
0 = ∞,
α > 0

and
0
0 = 1.
(20.39)
Since densities are nonnegative, and since we are deﬁning the likelihood-ratio func-
tion using the convention (20.39), the codomain of LR(·) is the set [0, ∞] consisting
of the nonnegative reals and the special symbol ∞:
LR: Rd →[0, ∞].
20.8 The ML Decision Rule
407
Using the likelihood-ratio function and (20.37), we can rewrite the MAP rule for
the case where the prior is nondegenerate (20.2) and where the observation yobs is
such that fY(yobs) > 0 as
φMAP(yobs) =
⎧
⎪
⎨
⎪
⎩
0
if LR(yobs) > π1
π0 ,
1
if LR(yobs) < π1
π0 ,
U

{0, 1}

if LR(yobs) = π1
π0 ,

π0, π1 > 0,
fY(yobs) > 0

.
(20.40)
Since many of the densities that are of interest to us have an exponential form, it
is sometimes more convenient to describe the MAP rule using the log likelihood-
ratio function LLR: Rd →[−∞, ∞], which is deﬁned by
LLR(y) ≜ln fY|H=0(y)
fY|H=1(y),
y ∈Rd,
(20.41)
using the convention

ln α
0 = +∞, ln 0
α = −∞, α > 0

and
ln 0
0 = 0,
(20.42)
where ln(·) denotes natural logarithm.
Using the log likelihood-ratio function LLR(·) and the monotonicity of the loga-
rithmic function

a > b

⇐⇒

ln a > ln b

,
a, b > 0,
(20.43)
we can express the MAP rule (20.40) as
φMAP(yobs) =
⎧
⎪
⎨
⎪
⎩
0
if LLR(yobs) > ln π1
π0 ,
1
if LLR(yobs) < ln π1
π0 ,
U

{0, 1}

if LLR(yobs) = ln π1
π0 ,

π0, π1 > 0,
fY(yobs) > 0

.
(20.44)
20.8
The ML Decision Rule
A diﬀerent decision rule, which is typically suboptimal unless H is a priori uniform,
is the Maximum-Likelihood (ML) decision rule. Its structure is similar to that
of the MAP rule except that it ignores the prior. In fact, if π0 = π1, then the two
rules are identical. The ML rule is thus given by
φML(yobs) ≜
⎧
⎪
⎨
⎪
⎩
0
if fY|H=0(yobs) > fY|H=1(yobs),
1
if fY|H=0(yobs) < fY|H=1(yobs),
U

{0, 1}

if fY|H=0(yobs) = fY|H=1(yobs).
(20.45)
The ML decision rule can be alternatively described using the likelihood-ratio func-
tion LR(·) (20.38) as
φML(yobs) =
⎧
⎪
⎨
⎪
⎩
0
if LR(yobs) > 1,
1
if LR(yobs) < 1,
U

{0, 1}

if LR(yobs) = 1.
(20.46)
408
Binary Hypothesis Testing
Alternatively, using the log likelihood-ratio function LLR(·) (20.41):
φML(yobs) =
⎧
⎪
⎨
⎪
⎩
0
if LLR(yobs) > 0,
1
if LLR(yobs) < 0,
U

{0, 1}

if LLR(yobs) = 0.
(20.47)
20.9
Performance Analysis: the Bhattacharyya Bound
We next derive the Bhattacharyya Bound, which is a useful upper bound on
the optimal probability of error p∗(error).
Starting with the exact expression (20.20) we obtain:
p∗(error) =

Rd min

π0fY|H=0(y), π1fY|H=1(y)

dy
≤

Rd
3
π0fY|H=0(y)π1fY|H=1(y) dy
= √π0π1

Rd
3
fY|H=0(y)fY|H=1(y) dy
≤1
2

Rd
3
fY|H=0(y)fY|H=1(y) dy,
where the equality in the ﬁrst line follows from (20.20); the inequality in the second
line from the inequality
min{a, b} ≤
√
ab,
a, b ≥0,
(20.48)
(which can be easily veriﬁed by treating the case a ≥b and the case a < b sepa-
rately); the equality in the third line by trivial algebra; and where the inequality
in the fourth line follows by noting that if c, d ≥0, then their geometric mean
√
cd
cannot exceed their arithmetic mean (c + d)/2, i.e.,
√
cd ≤c + d
2
,
c, d ≥0,
(20.49)
and because in our case c = π0 and d = π1, so c + d = 1.
We have thus established the bound
p∗(error) ≤1
2

y∈Rd
3
fY|H=0(y)fY|H=1(y) dy,
(20.50)
which is known as the Bhattacharyya Bound.
20.10
Example
Consider the problem of guessing H based on the observation Y , where H takes
on the values 0 and 1 equiprobably and where the conditional densities of Y given
H = 0 and H = 1 are
fY |H=0(y) =
1
√
2πσ2 e−(y−A)2/(2σ2),
y ∈R,
(20.51a)
20.10 Example
409
fY |H=1(y) =
1
√
2πσ2 e−(y+A)2/(2σ2),
y ∈R
(20.51b)
for some deterministic A, σ > 0. Here the observable is a RV, so d = 1.
For these conditional densities the likelihood-ratio function (20.38) is given by:
LR(y) = fY |H=0(y)
fY |H=1(y)
=
1
√
2πσ2 e−(y−A)2/(2σ2)
1
√
2πσ2 e−(y+A)2/(2σ2)
= e4yA/(2σ2),
y ∈R.
Since the two hypotheses are a priori equally likely, the MAP rule is equivalent to
the ML rule and both rules guess “H = 0” or “H = 1” depending on whether the
likelihood-ratio LR(yobs) is greater or smaller than one. And since
LR(yobs) > 1 ⇐⇒e4yobsA/(2σ2) > 1
⇐⇒ln

e4yobsA/(2σ2)
> ln 1
⇐⇒4yobsA/(2σ2) > 0
⇐⇒yobs > 0,
and
LR(yobs) < 1 ⇐⇒e4yobsA/(2σ2) < 1
⇐⇒ln

e4yobsA/(2σ2)
< ln 1
⇐⇒4yobsA/(2σ2) < 0
⇐⇒yobs < 0,
it follows that the MAP decision rule guesses “H = 0,” if yobs > 0; guesses “H = 1,”
if yobs < 0; and guesses “H = 0” or “H = 1” equiprobably, if yobs = 0 (i.e., in the
case of a tie).
Note that in this example the probability of a tie is zero.
Indeed, under both
hypotheses, the probability that the observed variable Y is exactly equal to zero is
zero:
Pr

Y = 0
 H = 0

= Pr

Y = 0
 H = 1

= Pr

Y = 0

= 0.
(20.52)
Consequently, the way ties are resolved is immaterial.
We next compute the probability of error of the MAP decoder. To this end, let
pMAP(error|H = 0) and pMAP(error|H = 1) denote its conditional probabilities of
error. Its (unconditional) probability of error, which is also the optimal probability
of error, can be expressed as
p∗(error) = π0 pMAP(error|H = 0) + π1 pMAP(error|H = 1).
(20.53)
We proceed to compute the required terms on the RHS. Starting with the term
pMAP(error|H = 0), we note that pMAP(error|H = 0) corresponds to the condi-
tional probability that Y is negative or that Y is equal to zero and the coin toss
410
Binary Hypothesis Testing
that the MAP decoder uses to resolve the tie causes the guess to be “H = 1.” By
(20.52), the conditional probability of a tie is zero, so pMAP(error|H = 0) is, in
fact, just the conditional probability that Y is negative:
pMAP(error|H = 0) = Pr[Y < 0|H = 0]
= Q
A
σ

,
(20.54)
where the second equality follows because, conditional on H = 0, the random
variable Y is N

A, σ2
, and the probability that it is smaller than zero can be thus
computed using the Q-function; see (19.12b) and (19.13). Similarly, using (19.12a),
pMAP(error|H = 1) = Pr[Y > 0|H = 1]
= Q
A
σ

.
(20.55)
Note that in this example the MAP rule is “fair” in the sense that the conditional
probability of error given H = 0 is the same as given H = 1. This is a coincidence
(that results from the symmetry in the problem). In general, the MAP rule need
not be fair.
We conclude from (20.53), (20.54), and (20.55) that
p∗(error) = Q
A
σ

.
(20.56)
Figure 20.2 depicts the conditional densities of y given H = 0 and given H = 1
and the decision regions of the MAP decision rule φMAP(·). The area of the shaded
region is the probability of an error conditioned on H = 0.
Note that the optimal decision rule for this example is not unique. Another optimal
decision rule is to guess “H = 0” if yobs is positive but not equal to 17, and to
guess “H = 1” otherwise.
Even though we have an exact expression for the probability of error (20.56) it is
instructive to compute the Bhattacharyya Bound too:
p∗(error) ≤1
2
 ∞
−∞
3
fY |H=0(y)fY |H=1(y) dy
= 1
2
 ∞
−∞

1
√
2πσ2 e−(y−A)2/(2σ2)
1
√
2πσ2 e−(y+A)2/(2σ2) dy
= 1
2 e−A2/(2σ2)
 ∞
−∞
1
√
2πσ2 e−y2/(2σ2) dy
= 1
2 e−A2/(2σ2),
(20.57)
where the ﬁrst line follows from (20.50); the second from (20.51); the third by simple
algebra; and the ﬁnal equality because the Gaussian density (like all densities)
integrates to one.
As an aside, we have from (20.57) and (20.56) the bound
Q(α) ≤1
2 e−α2/2,
α ≥0,
(20.58)
20.11 (Nontelepathic) Processing
411
y
A
−A
Guess “H = 0”
Guess “H = 1”
fY |H=0(y)
fY |H=1(y)
fY (y)
pMAP(error|H = 0)
Figure 20.2: Binary hypothesis testing with a uniform prior. Conditional on H = 0
the observable Y is N

A, σ2
and conditional on H = 1 it is N

−A, σ2
. The area
of the shaded region is the probability of error of the MAP rule conditional on
H = 0.
which we encountered in Proposition 19.4.2.
20.11
(Nontelepathic) Processing
To further emphasize the optimality of the Maximum A Posteriori decision rule,
and for ulterior motives that have to do with the introduction of conditional inde-
pendence, we shall next show that no processing of the observables can reduce the
probability of a guessing error. To that end we shall have to properly deﬁne what
we mean by “processing.”
The ﬁrst thing that comes to mind is to consider processing as the application of
some deterministic mapping. I.e., we think of mapping the observation yobs using
some deterministic function g(·) to g(yobs) and then guessing H based on g(yobs).
That this cannot reduce the probability of error is clear from Figure 20.3, which
demonstrates that mapping yobs to g(yobs) and then guessing H based on g(yobs)
can be viewed as a special case of guessing H based on yobs and, as such, cannot
outperform the MAP decision rule, which is optimal among all decision rules based
on yobs.
A more general kind of processing involves randomization, or “dithering.” Here we
envision the processor as using a local random number generator to generate a ran-
412
Binary Hypothesis Testing
yobs
g(·)
g(yobs)
Guess
Guess H based
on g(yobs)
Figure 20.3: No decision rule based on g(yobs) can outperform an optimal decision
rule based on yobs, because computing g(yobs) and then forming the decision based
on the answer can be viewed as a special case of guessing based on yobs.
dom variable Θ and then producing an output of the form g(yobs, θobs), where θobs
is the outcome of Θ, and where g(·) is some deterministic function. Here Θ is
assumed to be independent of the pair (H, Y), so the processor can generate it
using a local random number generator.
An argument very similar to the one we used in Section 20.6 (in the second proof of
the claim that randomized decision rules cannot outperform optimal deterministic
rules) can be used to show that this type of processing cannot improve our guessing.
The argument is as follows. We view the application of the function g(·) to the
pair (Y, Θ) as deterministic processing of the pair (Y, Θ), so no decision rule based
on g(Y, Θ) can outperform a decision rule that is optimal for guessing H based
on (Y, Θ).
It thus remains to show that the decision rule ‘Guess “H = 0” if
π0fY|H=0(yobs) ≥π1fY|H=1(yobs)’ is also optimal when observing (Y, Θ) and not
only Y. This follows from Theorem 20.5.2 by noting that the independence of Θ
and (H, Y), implies that
fY,Θ|H=0(yobs, θobs) = fY|H=0(yobs) fΘ(θobs),
fY,Θ|H=1(yobs, θobs) = fY|H=1(yobs) fΘ(θobs),
and hence that this rule guesses “H = 0” only when yobs and θobs are such that
π0fY,Θ|H=0(yobs, θobs) ≥π1fY,Θ|H=1(yobs, θobs) and guesses “H = 1” only when
π1fY,Θ|H=1(yobs, θobs) ≥π0fY,Θ|H=0(yobs, θobs).
Fearless readers who are not afraid to divide by zero should note that
LR(yobs, θobs) = fY,Θ|H=0(yobs, θobs)
fY,Θ|H=1(yobs, θobs)
= fY|H=0(yobs) fΘ(θobs)
fY|H=1(yobs) fΘ(θobs)
= fY|H=0(yobs)
fY|H=1(yobs),
fΘ(θobs) ̸= 0
= LR(yobs),
fΘ(θobs) ̸= 0,
20.11 (Nontelepathic) Processing
413
so (ignoring some technical issues) the MAP detector based on (yobs, θobs) ig-
nores θobs and is identical to the MAP detector based on yobs only.4
Ostensibly more general is processing Y by mapping it to g(Y, Θ), where the
distribution of Θ is allowed to depend on yobs. This motivates us to further extend
the notion of processing. The cleanest way to deﬁne processing is to deﬁne its
outcome rather than the way it is generated.
Before deﬁning processing we remind the reader of the notion of conditional inde-
pendence. But ﬁrst we recall the deﬁnition of (unconditional) independence. We
do so for discrete random variables using their Probability Mass Function (PMF).
The extension to random variables with a joint density is straightforward. For the
deﬁnition of independence in more general scenarios see, for example, (Billingsley,
1995, Section 20) or (Lo`eve, 1963, Section 15) or (Williams, 1991, Chapter 4).
Deﬁnition 20.11.1 (Independent Discrete Random Variables). We say that the
discrete random variables X and Y of joint PMF PX,Y (·, ·) and marginal PMFs
PX(·) and PY (·) are independent if PX,Y (·, ·) factors as
PX,Y (x, y) = PX(x) PY (y).
(20.59)
Equivalently, X and Y are independent if, for every outcome y such that PY (y) > 0,
the conditional distribution of X given Y = y is the same as its unconditional
distribution:
PX|Y (x|y) = PX(x),
PY (y) > 0.
(20.60)
Equivalently, X and Y are independent if, for every outcome x such that PX(x) > 0,
the conditional distribution of Y given X = x is the same as its unconditional
distribution:
PY |X(y|x) = PY (y),
PX(x) > 0.
(20.61)
The equivalence of (20.59) and (20.60) follows because, by the deﬁnition of the
conditional probability mass function,
PX|Y (x|y) = PX,Y (x, y)
PY (y)
,
PY (y) > 0.
Similarly, the equivalence of (20.59) and (20.61) follows from
PY |X(y|x) = PX,Y (x, y)
PX(x)
,
PX(x) > 0.
The beauty of (20.59) is that it is symmetric in X, Y . It makes it clear that X
and Y are independent if, and only if, Y and X are independent. This is less
obvious from (20.60) or (20.61).
The deﬁnition of the conditional independence of X and Y given Z is similar, except
that we condition everywhere on Z. Again we only consider the discrete case and
refer the reader to (Lo`eve, 1963, Section 25.3) or (Chung, 2001, Section 9.2) or
(Kallenberg, 2002, Chapter 6) for the general case.
4Technical issues arise when the outcome of Θ, namely θobs, is such that fΘ(θobs) = 0.
414
Binary Hypothesis Testing
Deﬁnition 20.11.2 (Conditionally Independent Discrete Random Variables). Let
the discrete random variables X, Y, Z have a joint PMF PX,Y,Z(·, ·, ·). We say that
X and Y are conditionally independent given Z and write
X⊸−Z⊸−Y
if
PX,Y |Z(x, y|z) = PX|Z(x|z) PY |Z(y|z),
PZ(z) > 0.
(20.62)
Equivalently, X and Y are conditionally independent given Z if, for any outcome
y, z with PY,Z(y, z) > 0, the conditional distribution of X given that Y = y and
Z = z is the same as the distribution of X when conditioned on Z = z only:
PX|Y,Z(x|y, z) = PX|Z(x|z),
PY,Z(y, z) > 0.
(20.63)
Or, equivalently, X and Y are conditionally independent given Z if
PY |X,Z(y|x, z) = PY |Z(y|z),
PX,Z(x, z) > 0.
(20.64)
The equivalence of (20.62) and (20.63) follows because, by the deﬁnition of the
conditional probability mass function,
PX|Y,Z(x|y, z) = PX,Y,Z(x, y, z)
PY,Z(y, z)
= PX,Y |Z(x, y|z)PZ(z)
PY |Z(y|z)PZ(z)
= PX,Y |Z(x, y|z)
PY |Z(y|z)
,
PY,Z(y, z) > 0,
and similarly the equivalence of (20.62) and (20.64) follows from
PY |X,Z(y|x, z) = PX,Y |Z(x, y|z)
PX|Z(x|z)
,
PX,Z(x, z) > 0.
Again, the beauty of (20.62) is that it is symmetric in X, Y . Thus X⊸−Z⊸−Y if,
and only if, Y ⊸−Z⊸−X. When X and Y are conditionally independent given Z
we sometimes say that X⊸−Z⊸−Y forms a Markov chain.
The equivalence between the diﬀerent deﬁnitions of conditional independence con-
tinues to hold in the general case where the random variables are not necessarily
discrete. We only reluctantly state this as a theorem, because we never deﬁned
conditional independence in nondiscrete settings.
Theorem 20.11.3 (Equivalent Deﬁnition for Conditional Independence). Let X,
Y, and Z be random vectors. Then the following statements are equivalent:
(a) X and Y are conditionally independent given Z.
(b) The conditional distribution of Y given (X, Z) is equal to its conditional
distribution given Z.
20.11 (Nontelepathic) Processing
415
(c) The conditional distribution of X given (Z, Y) is equal to its conditional
distribution given Z.
Proof. For a precise deﬁnition of concepts appearing in this theorem and for a
proof of the equivalence between the statements see (Kallenberg, 2002, Chapter 6)
or (Lo`eve, 1963, Section 25.3) and particularly Theorem 25.3A therein.
We are now ready to deﬁne the processing of the observation Y with respect to
the hypothesis H.
Deﬁnition 20.11.4 (Processing). We say that Z is the result of processing Y
with respect to H if H and Z are conditionally independent given Y.
As we next show, this deﬁnition of processing extends the previous ones. We ﬁrst
show that if Z = g(Y) for some deterministic Borel measurable function g(·) then
H⊸−Y⊸−g(Y).
This follows by noting that, conditional on Y, the random
variable g(Y) is deterministic and hence independent of everything and a fortiori
of H.
We next show that if Θ is independent of (H, Y), then H⊸−Y⊸−g(Y, Θ). In-
deed, if Z = g(Y, Θ) with Θ being independent of (Y, H), then, conditionally on
Y = y, the distribution of Z is simply the distribution of g(y, Θ) so (under this
conditioning) Z is independent of H.
We next show that processing the observables cannot help decrease the probability
of error. The proof is conceptually very simple; the neat part is in the deﬁnition.
Theorem 20.11.5 (Processing Is Futile). If Z is the result of processing Y with
respect to H, then no rule for guessing H based on Z can outperform an optimal
guessing rule based on Y.
Proof. Surely no decision rule that guesses H based on Z can outperform an
optimal decision rule based on Z, let alone outperform a decision rule that is
optimal for guessing H based on Z and Y. But an optimal decision rule based on
the pair (Z, Y) is the MAP rule, which compares
Pr[H = 0|Y = y, Z = z]
and
Pr[H = 1|Y = y, Z = z].
And, because H⊸−Y⊸−Z, it follows from Theorem 20.11.3 that this is equivalent
to comparing
Pr[H = 0|Y = y]
and
Pr[H = 1|Y = y]
i.e., to an optimal (MAP) decision rule based on Y only.
The above theorem is more powerful than it seems. To demonstrate its strength,
we next use it to show that in testing for a signal in Gaussian noise—irrespective of
the prior—the optimal probability of error is monotonically nondecreasing in the
noise variance. The setup we consider is one where H is of prior (π0, π1) and aiding
us in guessing H is the observable Y , which, conditional on H = m, is N

αm, σ2
for m ∈{0, 1}. We shall argue that, irrespective of the prior (π0, π1), the optimal
probability of error is monotonically nondecreasing in σ2.
416
Binary Hypothesis Testing
The beauty of the argument is that it allows us to prove the monotonicity result
without having to calculate the optimal probability of error explicitly (as we did
in Section 20.10 for the case of a uniform prior with α0 = A and α1 = −A). While
we could also compute the optimal probability of error for this more general setup
and then use calculus to derive the monotonicity result, the argument we present
instead has the advantage of also being applicable to multi-dimensional multi-
hypothesis testing scenarios, where there is typically no closed-form expression for
the optimal probability of error.
To prove this result, let p∗
e(σ2) denote the optimal probability of error as a function
of σ2. We need to show that p∗
e(σ2) ≤p∗
e(σ2 + δ2), for all δ ∈R. Consider the
low-noise case where the conditional law of Y given H is N

αm, σ2
. Suppose that
the receiver generates W ∼N

0, δ2
independently of (H, Y ) and adds W to Y
to form Z = Y + W. Since Z is the result of processing Y with respect to H, it
follows that the optimal probability of error based on Y , namely p∗
e(σ2), is at least
as good as the optimal probability of error based on Z (Theorem 20.11.5). We
now complete the argument by showing that the optimal probability of error based
on Z is p∗
e(σ2 + δ2). This follows because, by Proposition 19.7.2, the conditional
law of Z given H is N

αm, σ2 + δ2
.
Stated diﬀerently, since using a local random number generator the receiver can
produce from an observation Y of conditional law N

αm, σ2
a random variable Z
whose conditional law is N

αm, σ2 + δ2
, the minimal probability of error based
on an observation having conditional law N

αm, σ2
cannot be larger than the
optimal probability of error achievable based on an observation having conditional
law N

αm, σ2 + δ2
. See Figure 20.4 for an illustration of this argument.
20.12
Suﬃcient Statistics
This section aﬀords a ﬁrst glance at the notion of suﬃcient statistics, which will be
studied in greater depth and generality in Chapter 22. We begin with the following
example. Consider the hypothesis testing problem with a uniform prior, where the
observation is a tuple of real numbers (Y1, Y2). Conditional on H = 0, the random
variables Y1, Y2 are IID N

0, σ2
0

, whereas conditional on H = 1 they are IID
N

0, σ2
1

, where
σ0 > σ1 > 0.
(20.65)
(If σ2
0 = σ2
1, then the problem is boring in that the conditional law of the observable
given H = 0 is the same as given H = 1, so the two hypotheses cannot be diﬀer-
entiated. For σ2
0 ̸= σ2
1 there is no loss in generality in assuming σ0 > σ1 because
we can always relabel the hypotheses. And if σ0 > σ1 = 0, then the problem is
trivial: we guess “H = 1” only if Y1 = Y2 = 0.) Thus, the observation space is the
two-dimensional Euclidean space R2 and, using the explicit form of the Gaussian
density (19.6),
fY1,Y2|H=0(y1, y2) =
1
2πσ2
0
exp

−1
2σ2
0
(y2
1 + y2
2)

,
y1, y2 ∈R,
(20.66a)
fY1,Y2|H=1(y1, y2) =
1
2πσ2
1
exp

−1
2σ2
1
(y2
1 + y2
2)

,
y1, y2 ∈R.
(20.66b)
20.12 Suﬃcient Statistics
417
yobs
yobs + W
MAP for testing
N

α0, σ2 + δ2
vs. N

α1, σ2 + δ2
with prior (π0, π1)
Gaussian RV
Generator
W ∼N

0, δ2
W independent of (Y, H).
+
Guess
Local
Randomness
Figure 20.4:
A suboptimal guessing rule (with randomization) for testing
N

α0, σ2
vs. N

α1, σ2
with the given prior (π0, π1).
It attains the optimal
probability of error for guessing N

α0, σ2 + δ2
vs. N

α1, σ2 + δ2
(with the given
prior).
Since we assumed a uniform prior, the ML decoding rule for guessing H based on
the tuple (Y1, Y2) is optimal. To derive the ML rule explicitly, we compute the
likelihood-ratio function
LR(y1, y2) = fY1,Y2|H=0(y1, y2)
fY1,Y2|H=1(y1, y2)
=
1
2πσ2
0 exp

−
1
2σ2
0 (y2
1 + y2
2)

1
2πσ2
1 exp

−
1
2σ2
1 (y2
1 + y2
2)

= σ2
1
σ2
0
exp
	1
2
 1
σ2
1
−1
σ2
0

(y2
1 + y2
2)

,
y1, y2 ∈R.
(20.67)
Thus,
LR(y1, y2) > 1 ⇐⇒exp
	1
2
 1
σ2
1
−1
σ2
0

(y2
1 + y2
2)

> σ2
0
σ2
1
⇐⇒1
2
 1
σ2
1
−1
σ2
0

(y2
1 + y2
2) > ln σ2
0
σ2
1
⇐⇒σ2
0 −σ2
1
2σ2
0σ2
1
(y2
1 + y2
2) > ln σ2
0
σ2
1
⇐⇒y2
1 + y2
2 > 2σ2
0σ2
1
σ2
0 −σ2
1
ln σ2
0
σ2
1
,
(20.68)
where the second equivalence follows from the monotonicity of the logarithm func-
tion (20.43); and where the last equivalence follows by multiplying both sides of
418
Binary Hypothesis Testing
the inequality by the constant 2σ2
0σ2
1/(σ2
0 −σ2
1) (without the need to change the
inequality direction because this constant is by (20.65) positive).
It follows from (20.68) that the ML decision rule for guessing H based on (Y1, Y2)
computes Y 2
1 + Y 2
2 and then compares the result to a threshold. It is interesting to
note that to implement this decision rule one need not observe Y1 and Y2 directly;
it suﬃces to observe the sum of their squares
T ≜Y 2
1 + Y 2
2 .
(20.69)
Of course, being the result of processing (Y1, Y2) with respect to H, no guess of H
based on T can outperform an optimal guess based on (Y1, Y2) (Section 20.11).
But what is interesting about this example is that, even though one cannot recover
(Y1, Y2) from T (so there are some decision rules based on (Y1, Y2) that cannot
be implemented if one only knows T), the ML rule based on (Y1, Y2) only requires
knowledge of T. Thus, in this example, even though pre-processing the observations
to produce T = Y 2
1 +Y 2
2 is not reversible, basing one’s decision on T incurs no loss
of optimality. An optimal decision rule based on T is just as good as an optimal
rule based on (Y1, Y2).
The reason for this can be traced to the fact that, in this example, to compute the
likelihood-ratio LR(y1, y2) one need not know the pair (y1, y2); it suﬃces that one
know the sum of their squares y2
1 + y2
2; see (20.67). In this sense T = Y 2
1 + Y 2
2
forms a suﬃcient statistic for guessing H from (Y1, Y2), as we next deﬁne.
We would like to deﬁne a mapping T(·) from the observation space Rd to Rd′ as
being suﬃcient for the densities fY|H=0(·) and fY|H=1(·) if the likelihood-ratio
LR(yobs) can be computed from T(yobs) for every yobs in Rd. However, for techni-
cal reasons, we require slightly less: we only require that LR(yobs) be computable
from T(yobs) for those observations yobs for which at least one of the densities is
positive (so the likelihood-ratio is not of the form 0/0) and that additionally lie
outside some prespeciﬁed set Y0 ⊂Rd of Lebesgue measure zero.5 Thus, we shall
require that there exist a set Y0 ⊂Rd of Lebesgue measure zero and a function
ζ : Rd′ →[0, ∞] such that ζ

T(yobs)

is equal to LR(yobs) whenever
yobs /∈Y0
and
fY|H=0(yobs) + fY|H=1(yobs) > 0.
(20.70)
Note that the fact that Y0 is of Lebesgue measure zero implies that
Pr[Y ∈Y0 |H = 0] = Pr[Y ∈Y0 |H = 1] = 0.
(20.71)
To convince the reader that this really is only “slightly” less, we note:
Note 20.12.1. Both conditional on H = 0 and conditional on H = 1, the proba-
bility that the observable violates (20.70) is zero.
5We allow this exception set so that the question of whether T(·) forms a suﬃcient statistic
or not will not depend on our choice of the density function of the conditional distribution of the
observable. (Recall that if a RV has a probability density function, then it has inﬁnitely many
diﬀerent probability density functions, every two of which diﬀer on a set of Lebesgue measure
zero.)
20.12 Suﬃcient Statistics
419
Proof. We shall show that conditional on H = 0, the probability that the ob-
servable violates (20.70) is zero. The conditional probability given H = 1 can be
analogously shown to be zero. The condition that (20.70) is violated is equivalent
to the condition that either yobs ∈Y0 or fY|H=0(yobs) + fY|H=1(yobs) = 0. By
(20.71), Pr[Y ∈Y0 |H = 0] = 0. And, by the nonnegativity of the densities,
Pr

fY|H=0(Y) + fY|H=1(Y) = 0
 H = 0

≤Pr

fY|H=0(Y) = 0
 H = 0

=

{˜y∈Rd:fY|H=0(˜y)=0}
fY|H=0(y) dy
=

{˜y∈Rd:fY|H=0(˜y)=0}
0 dy
= 0.
Conditionally on H = 0, the probability of the observable violating (20.70) is thus
the probability of the union of two events, each of which is of zero probability, and
is thus of zero probability; see Corollary 21.5.2 ahead.
Deﬁnition 20.12.2 (Suﬃcient Statistic for Two Densities). We say that a map-
ping T : Rd →Rd′ forms a suﬃcient statistic for the density functions fY|H=0(·)
and fY|H=1(·) on Rd if it is Borel measurable6 and if there exist a set Y0 ⊂Rd of
Lebesgue measure zero and a Borel measurable function ζ : Rd′ →[0, ∞] such that
for all yobs ∈Rd satisfying (20.70)
fY|H=0(yobs)
fY|H=1(yobs) = ζ

T(yobs)

,
(20.72)
where on the LHS of (20.72) we deﬁne a/0 to be +∞whenever a > 0.
In our example the observation (Y1, Y2) takes values in R2 so d = 2; the mapping
T : (y1, y2) 	→y2
1 + y2
2 is a mapping from R2 to R so d′ = 1; and by, (20.67),
ζ : t 	→σ2
1
σ2
0
exp
	1
2
 1
σ2
1
−1
σ2
0

t

.
Here we can take Y0 to be the empty set.7
We next show that if T(·) is a suﬃcient statistic, then there is no loss of opti-
mality in considering decision rules that base their decision on T(Y). This result
6The technical condition that T(·) is Borel measurable guarantees that T(Y) is a random
vector. See for example (Billingsley, 1995, Theorem 13.1(ii)) for a discussion of this technical
issue.
The issue is best seen in the scalar case.
Suppose that Y is a RV deﬁned over the
probability space (Ω, F, P). If T(·) is any function, then T(Y ) is a mapping from Ω to the R, but
we are not guaranteed that it be a RV, because for T(Y ) to be a RV we must have that, for every
ξ ∈R, the set {ω ∈Ω : T(Y (ω)) ≤ξ} be in F, and this is, in general, not true. However, if T(·) is
Borel measurable, then the above cited theorem guarantees that T(X) is, indeed, a RV. Note that
any continuous function is Borel measurable (Billingsley, 1995, Theorem 13.2). In practice, one
never encounters functions that are not Borel measurable. In fact, it is hard work to construct
one.
7We would have needed to choose a nontrivial set Y0 if we had changed the densities (20.66)
at a ﬁnite (or countable) number of points.
420
Binary Hypothesis Testing
is almost obvious, because the MAP decision rule is optimal (Theorem 20.7.1);
because it can be expressed in terms of the likelihood-ratio function (20.40); and
because the suﬃciency of T(·) implies that the likelihood-ratio function LR(yobs)
is computable from T(yobs). Nevertheless, we provide a formal proof because the
result is important.
Proposition 20.12.3. If T : Rd →Rd′ is a suﬃcient statistic for the densities
fY|H=0(·) and fY|H=1(·), then, irrespective of the prior of H, there exists a decision
rule that guesses H based on T(Y) and which is as good as any optimal guessing
rule based on Y.
Proof. We need to show that if φ∗
Guess(·) is an optimal decision rule for guessing H
based on Y, then there exists a guessing rule based on T(Y) that has the same
probability of error. We note that it is enough to prove this result for a nondegen-
erate prior (20.2), because for degenerate priors one can achieve zero probability
of error even without looking at T(Y): if Pr[H = 0] = 1 guess “H = 0,” and if
Pr[H = 1] = 1 guess “H = 1.” We thus proceed to assume a nondegenerate prior
(20.2).
Let φMAP(·) be the MAP rule for guessing H based on Y. Since this rule is optimal,
it suﬃces to exhibit a decoding rule φT (·) based on T(Y) of equal performance.
Since T(·) is suﬃcient, it follows that there exists a set of Lebesgue measure zero Y0
and a Borel measurable function ζ(·) such that ζ

T(yobs)

= LR(yobs), whenever
(20.70) holds. Based upon the observation T(Y) = T(yobs), the desired rule is to
guess
φT

T(yobs)

=
⎧
⎪
⎨
⎪
⎩
0
if ζ

T(yobs)

> π1
π0 ,
1
if ζ

T(yobs)

< π1
π0 ,
U

{0, 1}

if ζ

T(yobs)

= π1
π0 .
(20.73)
That φT (·) has the same performance as φMAP(·) now follows by noting that,
by (20.72), the two decoding rules are in agreement except perhaps for observa-
tions yobs violating (20.70), but those, by Note 20.12.1, occur with probability zero.
The performance of φMAP(·) (which is optimal based on Y) and of φT (·) (which is
based on T(Y)) are thus identical.
Deﬁnition 20.12.2 is intuitive in that it demonstrates how one typically goes about
identifying a suﬃcient statistic: one computes the likelihood-ratio and checks what
it depends on.
This deﬁnition, however, becomes a bit cumbersome in multi-
hypothesis testing, which we shall discuss in Chapter 21. A deﬁnition that is more
appropriate for that setting is given in Chapter 22 in terms of the computability
of the a posteriori probabilities from T(yobs) (Deﬁnition 22.2.1). The purpose of
the next proposition is to show that the two deﬁnitions coincide in the binary case:
ignoring sets of Lebesgue measure zero, the likelihood-ratio can be computed from
T(yobs) (whenever the ratio is not 0/0), if, and only if, for any prior (π0, π1) one can
compute the a posteriori distribution of H from T(yobs) (whenever fY(yobs) > 0).
We draw the reader’s attention to the following subtle issue. Deﬁnition 20.12.2
makes it clear that the suﬃciency of T(·) has nothing to do with the prior; it only
depends on the densities fY|H=0(·) and fY|H=1(·). The equivalent deﬁnition of
20.12 Suﬃcient Statistics
421
suﬃcient statistics in terms of the computability of the a posteriori distribution
ostensibly depends also on the prior, because it is only meaningful to discuss the a
posteriori distribution if H has a prior. Nevertheless, the deﬁnitions are equivalent
because in the latter deﬁnition we require that the a posteriori distribution be
computable from T(Y) for every prior, and not just for the prior given in the
problem’s formulation.
Proposition 20.12.4 (Computability of the a Posteriori Distribution). Let the
mapping T : Rd →Rd′ be Borel measurable, and let fY|H=0(·) and fY|H=1(·) be
densities on Rd. Then the following two conditions are equivalent:
(a) T(·) forms a suﬃcient statistic for the densities fY|H=0(·) and fY|H=1(·).
(b) For some set Y0 ⊂Rd of Lebesgue measure zero we have that for every prior
(π0, π1) there exist Borel measurable functions from Rd′ to [0, 1]
t 	→ψm

π0, π1, t

,
m = 0, 1,
such that the vector

ψ0

π0, π1, T(yobs)

, ψ1

π0, π1, T(yobs)
T
is a probability vector, and this probability vector is equal to the vector

Pr[H = 0|Y = yobs], Pr[H = 1|Y = yobs]
T
,
(20.74)
whenever both the condition yobs /∈Y0, and the condition
π0fY|H=0(yobs) + π1fY|H=1(yobs) > 0
(20.75)
are satisﬁed. Here (20.74) is computed for H having the prior (π0, π1) and
for the conditional densities fY|H=0(·) and fY|H=1(·).
Proof. We begin by proving that (a) implies (b). That is, we assume that T(·)
forms a suﬃcient statistic and proceed to prove the existence of the set Y0 and
of the functions ψ0(·), ψ1(·). Let Y0 and ζ : Rd′ →[0, ∞] be as guaranteed by the
deﬁnition of suﬃcient statistics (Deﬁnition 20.12.2) so
fY|H=0(yobs)
fY|H=1(yobs) = ζ

T(yobs)

,
(20.76)
whenever yobs satisﬁes (20.70).
We next show how to construct for every pair
(π0, π1) the functions ψ0(·), ψ1(·). We consider three cases separately: the case
π0 = 1 −π1 = 1, the case π0 = 1 −π1 = 0, and the case where both π0 and π1 are
strictly positive.
In the ﬁrst case H is deterministically zero, and the functions ψ0(1, 0, t) = 1 and
ψ1(1, 0, t) = 0 meet our requirements. In the second case H is deterministically
one, and the functions ψ0(0, 1, t) = 1 −ψ1(0, 1, t) = 0 meet our requirements.
422
Binary Hypothesis Testing
It remains to treat the case where π0, π1 > 0. We shall show that in this case the
functions
ψ0

π0, π1, t

≜
π0 ζ(t)
π0 ζ(t) + π1
,
ψ1(π0, π1, t) ≜1 −ψ0

π0, π1, t

,
(20.77)
(where ∞/(∞+ a) is deﬁned as one for all ﬁnite a) meet our requirements. To
that end we ﬁrst note that ψ0(π0, π1, t) and ψ1(π0, π1, t) are nonnegative and sum
to one.
We next note that, for π0, π1 > 0, the condition (20.75) implies that
fY|H=0(yobs) and fY|H=1(yobs) are not both zero. Consequently, if yobs satisﬁes
(20.75) and also yobs /∈Y0, then it satisﬁes (20.70) and LR(yobs) = ζ

T(yobs)

.
Thus, in the case π0, π1 > 0, we have that, whenever (20.75) and yobs /∈Y0 hold,
we have from (20.77)
ψ0

π0, π1, T(yobs)

=
π0 ζ

T(yobs)

π0 ζ

T(yobs)

+ π1
=
π0 LR(yobs)
π0 LR(yobs) + π1
=
π0fY|H=0(yobs)/fY|H=1(yobs)
π0fY|H=0(yobs)/fY|H=1(yobs) + π1
=
π0fY|H=0(yobs)
π0fY|H=0(yobs) + π1fY|H=1(yobs)
= Pr[H = 0|Y = yobs]
as required. This implies that, whenever (20.75) and yobs /∈Y0 hold, we also have
ψ1

π0, π1, T(yobs)

= Pr[H = 1|Y = yobs], since ψ1(π0, π1, t) = 1 −ψ0

π0, π1, t

and since Pr[H = 1|Y = yobs] = 1 −Pr[H = 0|Y = yobs]; see (20.77) and (20.10).
We now prove that (b) implies (a), i.e., that the existence of the set Y0 and of the
functions ψ0(·), ψ1(·) imply the existence of the function ζ(·) of Deﬁnition 20.12.2.
In fact, we shall prove a stronger statement that if for some nondegenerate prior
the a posteriori distribution of H given Y = yobs is computable from T(yobs)
(whenever (20.75) and yobs /∈Y0 hold), then there exists a function ζ : Rd′ →[0, ∞]
such that LR(yobs) = ζ(T(yobs)), whenever yobs satisﬁes (20.70).
To construct ζ(·) from ψ0(·) and ψ1(·), pick some arbitrary strictly positive ˜π0, ˜π1
summing to one (e.g., ˜π0, ˜π1 = 1/2), and deﬁne ζ(·) by
ζ

T(yobs)

= ˜π1 ψ0

˜π0, ˜π1, T(yobs)

˜π0 ψ1

˜π0, ˜π1, T(yobs)
,
(20.78)
using the convention that a/0 = ∞for all a > 0; see (20.39).
We next verify that if yobs satisﬁes (20.70) then ζ(T(yobs)) = LR(yobs).
To
this end, deﬁne H to have the law Pr[H = 0] = ˜π0 and Pr[H = 1] = ˜π1,
and let the conditional law of Y given H be as speciﬁed by the given densities.
Since ˜π0 and ˜π1 are strictly positive, it follows that whenever fY|H=0(yobs) and
fY|H=1(yobs) are not both zero, we also have ˜π0fY|H=0(yobs)+˜π1fY|H=1(yobs) > 0.
Consequently, for strictly positive ˜π0, ˜π1 we have that (20.70) implies that yobs /∈Y0
20.12 Suﬃcient Statistics
423
and ˜π0fY|H=0(yobs)+˜π1fY|H=1(yobs) > 0 and thus, for observations yobs satisfying
(20.70),
ζ

T(yobs)

= ˜π1 ψ0

˜π0, ˜π1, T(yobs)

˜π0 ψ1

˜π0, ˜π1, T(yobs)

= Pr[H = 1] Pr[H = 0|Y = yobs]
Pr[H = 0] Pr[H = 1|Y = yobs]
= LR(yobs),
where the last equality follows by dividing the equation
Pr[H = 0|Y = yobs] =
Pr[H = 0] fY|H=0(yobs)
Pr[H = 0] fY|H=0(yobs) + Pr[H = 1] fY|H=1(yobs)
(which is a restatement of (20.9a) for our case) by
Pr[H = 1|Y = yobs] =
Pr[H = 1] fY|H=1(yobs)
Pr[H = 0] fY|H=0(yobs) + Pr[H = 1] fY|H=1(yobs)
(which is a restatement of (20.9b) for our case).
Once we have identiﬁed a suﬃcient statistic T(Y), we can proceed to derive an
optimal guessing rule using two methods that we describe next. Again, we focus
on nondegenerate priors.
Method 1:
We ignore the fact that T(Y) forms a suﬃcient statistic and simply
use the MAP rule (20.40):
φMAP(yobs) =
⎧
⎪
⎨
⎪
⎩
0
if LR(yobs) > π1
π0 ,
1
if LR(yobs) < π1
π0 ,
U

{0, 1}

if LR(yobs) = π1
π0 .
(20.79)
(Because T(Y) is a suﬃcient statistic, the likelihood-ratio function LR(yobs) will
be computable from T(yobs) whenever LR(yobs) does not have the pathological
form 0/0 and does not lie in the exception set Y0. Such pathological observations
occur with probability zero (20.12), so we need not worry about them.)
Method 2:
By Proposition 20.12.3, there is no loss of optimality in forming our
guess based on T(Y). So we can use any optimal rule, e.g., the MAP rule, for
guessing H based on the new d′-dimensional observations tobs = T(yobs). This
method requires computing the conditional distribution of the random d′-vector
T = T(Y) conditional on H = 0 and conditional on H = 1 and deciding according
to the rule:
φGuess(T(yobs)) =
⎧
⎪
⎨
⎪
⎩
0
if π0 fT|H=0

T(yobs)

> π1 fT|H=1

T(yobs)

,
1
if π0 fT|H=0

T(yobs)

< π1 fT|H=1

T(yobs)

,
U

{0, 1}

if π0 fT|H=0

T(yobs)

= π1 fT|H=1

T(yobs)

.
(20.80)
424
Binary Hypothesis Testing
Why would we want to use Method 2 when we have already computed the likelihood-
ratio function to establish the suﬃciency of the statistic? The answer is that some-
times one can demonstrate that T(Y) forms a suﬃcient statistic by methods that
are not based on the computation of the likelihood-ratio. In such cases, Method 2
may be advantageous. Also, sometimes the analysis of the probability of error in
Method 2 is easier. The choice is ours.
Returning to the example of (20.66), we demonstrate Method 2 by calculating
the law of the suﬃcient statistic T = Y 2
1 + Y 2
2 under each of the hypotheses.
Recalling that the sum of the squares of two IID zero-mean Gaussians is exponential
(Note 19.8.1) we obtain:
fT |H=0(t) =
1
2σ2
0
exp

−t
2σ2
0

,
t ≥0,
(20.81a)
fT |H=1(t) =
1
2σ2
1
exp

−t
2σ2
1

,
t ≥0.
(20.81b)
Consequently, the likelihood-ratio is given by
fT |H=0(t)
fT |H=1(t) = σ2
1
σ2
0
exp
	
t
 1
2σ2
1
−
1
2σ2
0

,
t ≥0,
and the log likelihood-ratio by
ln fT |H=0(t)
fT |H=1(t) = ln σ2
1
σ2
0
+ t
 1
2σ2
1
−
1
2σ2
0

,
t ≥0.
We thus guess “H = 0” if the log likelihood-ratio is positive,
t ≥2σ2
0σ2
1
σ2
0 −σ2
1
ln σ2
0
σ2
1
,
i.e., if
y2
1 + y2
2 ≥2σ2
0σ2
1
σ2
0 −σ2
1
ln σ2
0
σ2
1
.
We similarly guess “H = 1” if the log likelihood-ratio is negative, and ﬂip a coin if
it is zero. This is the same law we obtained in (20.68) based on Method 1.
20.13
Implications of Optimality
Consider the problem of guessing an a priori uniformly distributed binary ran-
dom variable H based on the observable Y whose conditional law given H = 0
is N

0, σ2
and whose conditional distribution given H = 1 is N

1, σ2
. To de-
rive an optimal guessing rule we could derive the MAP rule by computing the
likelihood-ratio function as we did in Section 20.10. But having already carried
out the calculations in Section 20.10 for testing whether an observation was drawn
N

A, σ2
or N

−A, σ2
, there is a better way. Let
T = Y −1
2.
(20.82)
20.14 Multi-Dimensional Binary Gaussian Hypothesis Testing
425
Because there is a one-to-one relationship between Y and T, there is no loss of
optimality in subtracting 1/2 from Y to obtain T and in then applying an optimal
decision rule to T. Indeed, since Y = T + 1/2, it follows that Y is the result of
processing T with respect to H, so no decision rule based on Y can outperform an
optimal decision rule based on T (Theorem 20.11.5). (Of course, no decision rule
based on T can outperform an optimal one based on Y , because T is the result of
processing Y with respect to H.) In fact, using the terminology of Section 20.12,
T : y 	→y −1/2 forms a suﬃcient statistic for guessing H based on Y , because the
likelihood-ratio function LR(yobs) = fY |H=0(yobs)/fY |H=1(yobs) can be expressed
as ζ

T(yobs)

for the mapping ζ : t 	→LR(t + 1/2). Consequently, our assertion
that there is no loss of optimality in forming our guess based on T(Y ) is just a
consequence of Proposition 20.12.3.
Conditional on H = 0, the random variable T(Y ) is N

−0.5, σ2
, and, conditional
on H = 1, it is N

+0.5, σ2
. Consequently, using the results of Section 20.10 (with
the substitution of 1/2 for A), we obtain that an optimal rule based on T is to guess
“H = 0” if T is negative, and to guess “H = 1” if T is positive. To summarize, the
decision rule we derived is to guess “H = 0” if Y −1/2 < 0 and to guess “H = 1”
if Y −1/2 > 0.
In the terminology of Section 20.12, we used the fact that the transformation in
(20.82) is one-to-one to conclude that T(·) forms a suﬃcient statistic, and we then
used Method 2 from that section to derive an optimal decision rule.
20.14
Multi-Dimensional Binary Gaussian Hypothesis Testing
We now come closer to the receiver front end.
The kind of problem we would
eventually like to address is the hypothesis testing problem in which, conditional
on H = 0, the observable is a continuous-time waveform of the form s0(t) + N(t)
whereas, conditional on H = 1, it is of the form s1(t) + N(t), where

N(t), t ∈R

is some continuous-time stochastic process modeling the noise. This problem will
be addressed in Chapter 26. For now we only address the discrete-time version of
this problem.
20.14.1
The Setup
We consider the problem of guessing the random variable H that takes on the
values 0 and 1 with positive probabilities π0 and π1. The observable Y ∈RJ is
a random vector with J components Y (1), . . . , Y (J).8 Conditional on H = 0, the
components of Y are independent Gaussians with Y (j) ∼N

s(j)
0 , σ2
, where s0 is
some deterministic vector of J components s(1)
0 , . . . , s(J)
0 , and where σ2 > 0. Con-
ditional on H = 1, the components of Y are independent with Y (j) ∼N

s(j)
1 , σ2
,
for some other deterministic vector s1 of J components s(1)
1 , . . . , s(J)
1 . We assume
that s0 and s1 diﬀer in at least one coordinate. The setup can be described as
H = 0: Y (j) = s(j)
0
+ Z(j),
j = 1, 2, . . . , J,
8We use J rather than d in order to comply with the notation of Section 21.6 ahead.
426
Binary Hypothesis Testing
H = 1: Y (j) = s(j)
1
+ Z(j),
j = 1, 2, . . . , J,
where Z(1), Z(2), . . . , Z(J) are IID N

0, σ2
.
For typographical reasons, instead of denoting the observed vector by yobs, we now
denote it by y and its J components by y(1), . . . , y(J).
20.14.2
An Optimal Decision Rule
To ﬁnd an optimal guessing rule we compute the likelihood-ratio function:
LR(y) = fY|H=0(y)
fY|H=1(y)
=
?J
j=1
-
1
√
2πσ2 exp
	
−

y(j)−s(j)
0
2
2σ2

.
?J
j=1
-
1
√
2πσ2 exp
	
−

y(j)−s(j)
1
2
2σ2

.
=
J
@
j=1
-
exp
	
−

y(j) −s(j)
0
2
2σ2
+

y(j) −s(j)
1
2
2σ2

.
,
y ∈RJ.
The log likelihood-ratio function is thus given by
LLR(y) = ln LR(y)
=
1
2σ2
J

j=1

y(j) −s(j)
1
2 −

y(j) −s(j)
0
2
= 1
σ2
	
⟨y, s0 −s1⟩E + ∥s1∥2 −∥s0∥2
2

= 1
σ2
	
⟨y, s0 −s1⟩E −⟨s0, s0 −s1⟩E + ⟨s1, s0 −s1⟩E
2

= ∥s0 −s1∥
σ2
⎛
⎝

y, s0 −s1
∥s0 −s1∥

E
−
#
s0,
s0−s1
∥s0−s1∥
$
E +
#
s1,
s0−s1
∥s0−s1∥
$
E
2
⎞
⎠
= ∥s0 −s1∥
σ2

⟨y, φ⟩E −1
2

⟨s0, φ⟩E + ⟨s1, φ⟩E

,
y ∈RJ,
(20.83)
where for real vectors u = (u(1), . . . , u(J))T and v = (v(1), . . . , v(J))T taking values
in RJ we deﬁne9
⟨u, v⟩E ≜
J

j=1
u(j)v(j),
(20.84)
∥u∥≜
3
⟨u, u⟩E =
A
B
B
C
J

j=1

u(j)2,
(20.85)
9This is sometimes called the standard inner product on RJ or the inner product between
J-tuples. The subscript “E” stands here for “Euclidean.”
20.14 Multi-Dimensional Binary Gaussian Hypothesis Testing
427
s0
s0
s0
s1
s1
s1
φ
φ
φ
guess 0
guess 1
guess 0
guess 1
guess 0
guess 1
π0 < π1
π0 = π1
π0 > π1
Figure 20.5: Eﬀect of the ratio π0/π1 on the decision rule.
and where
φ =
s0 −s1
∥s0 −s1∥
(20.86)
is a unit-norm vector pointing from s1 to s0.
An optimal decision rule is to guess “H = 0” when LLR(y) ≥ln π1
π0 , i.e.,
Guess “H = 0” if
⟨y, φ⟩E ≥⟨s0, φ⟩E + ⟨s1, φ⟩E
2
+
σ2
∥s0 −s1∥ln π1
π0
,
(20.87)
and to guess “H = 1” otherwise. This decision rule is illustrated in Figure 20.5.
Depicted are the cases where π1/π0 is smaller than one, equal to one, and larger
than one.
It is interesting to note that the projection ⟨y, φ⟩E φ of y onto the normalized
vector φ = (s0 −s1)/ ∥s0 −s1∥forms a suﬃcient statistic for this problem. Indeed,
by (20.83), the log likelihood-ratio (and hence the likelihood-ratio) function is
computable from ⟨y, φ⟩E. The projection is depicted in Figure 20.6.
The rule (20.87) simpliﬁes if H has a uniform prior. In this case the rule is
Guess “H = 0” if
⟨y, φ⟩E ≥⟨s0, φ⟩E + ⟨s1, φ⟩E
2
.
(20.88)
Note that in this case the guessing rule can be implemented even if σ2 is unknown.
20.14.3
Error Probability Analysis
We next ﬁnd the error probability associated with our guessing rule. Denote the
conditional error probabilities associated with our guessing rule by p(error|H = 0)
and p(error|H = 1). Since our rule is optimal, its unconditional probability of error
is p∗(error), and thus
p∗(error) = π0 p(error|H = 0) + π1 p(error|H = 1).
(20.89)
Because in (20.87) we resolved ties by guessing “H = 0”, it follows that to evaluate
p(error|H = 0) we need to evaluate the probability that a random vector Y drawn
428
Binary Hypothesis Testing
φ
φ
s1
s0
y
Figure 20.6: The projection of y onto the normalized vector φ = (s0−s1)/∥s0−s1∥.
according to the density fY|H=0(·) is such that the a posteriori probability of H = 0
is strictly smaller than the a posteriori probability of H = 1. Thus, if ties in the a
posteriori distribution of H are resolved in favor of guessing “H = 0”, then
p(error|H = 0) = Pr

π0fY|H=0(Y) < π1fY|H=1(Y)
 H = 0

.
(20.90)
This may seem self-referential, but it is not. Another way to state this is
p(error|H = 0) =

y/∈B1,0
fY|H=0(y) dy,
(20.91)
where
B1,0 =
'
y ∈RJ : π0fY|H=0(y) ≥π1fY|H=1(y)
(
.
(20.92)
To compute this probability we need the following lemma:
Lemma 20.14.1. Let π0 and π1 be strictly positive but not necessarily sum to one.
Let the vectors s0, s1 ∈RJ diﬀer in at least one component, i.e., ∥s0 −s1∥> 0. Let
f0(y) =
	
1
√
2πσ2

J
exp
	
−1
2σ2
J

j=1

y(j) −s(j)
0
2

,
y ∈RJ,
f1(y) =
	
1
√
2πσ2

J
exp
	
−1
2σ2
J

j=1

y(j) −s(j)
1
2

,
y ∈RJ,
where σ2 > 0. Deﬁne
B1,0 ≜
'
y ∈RJ : π0f0(y) ≥π1f1(y)
(
.
Then

y/∈B1,0
f0(y) dy = Q
	∥s0 −s1∥
2σ
+
σ
∥s0 −s1∥ln π0
π1

.
(20.93)
20.14 Multi-Dimensional Binary Gaussian Hypothesis Testing
429
This equality continues to hold if we replace the weak inequality (≥) in the deﬁnition
of B1,0 with a strict inequality (>).
Proof. Using a calculation identical to the one leading to (20.83) we obtain that
the set B1,0 can also be expressed as
B1,0 =
1
y ∈RJ : ⟨y, φ⟩E ≥⟨s0, φ⟩E + ⟨s1, φ⟩E
2
+
σ2
∥s0 −s1∥ln π1
π0
2
,
(20.94)
where φ is deﬁned in (20.86).
The density f0(·) is the same as the density of the vector s0 + Z, where the com-
ponents Z(1), . . . , Z(J) of Z are IID N

0, σ2
. Thus, the LHS of (20.93) can be
expressed as

y/∈B1,0
f0(y) dy = Pr

⟨s0 + Z, φ⟩E < ⟨s0, φ⟩E + ⟨s1, φ⟩E
2
+
σ2
∥s0 −s1∥ln π1
π0

= Pr

⟨Z, φ⟩E < ⟨s1, φ⟩E −⟨s0, φ⟩E
2
+
σ2
∥s0 −s1∥ln π1
π0

= Pr

−⟨Z, φ⟩E > ⟨s0, φ⟩E −⟨s1, φ⟩E
2
+
σ2
∥s0 −s1∥ln π0
π1

= Pr

−⟨Z, φ⟩E > ⟨s0 −s1, φ⟩E
2
+
σ2
∥s0 −s1∥ln π0
π1

= Pr

⟨Z, −φ⟩E > ∥s0 −s1∥
2
+
σ2
∥s0 −s1∥ln π0
π1

= Q
	∥s0 −s1∥
2σ
+
σ
∥s0 −s1∥ln π0
π1

,
where the ﬁrst equality follows from (20.94) and from the observation that the
density f0(·) is the density of s0 + Z; the second because ⟨·, ·⟩E is linear in the ﬁrst
argument, so ⟨s0 + Z, φ⟩E = ⟨s0, φ⟩E+⟨Z, φ⟩E; the third by noting that multiplying
both sides of an inequality by (−1) requires changing the direction of the inequality;
the fourth by the linear relationship ⟨s1, φ⟩E−⟨s0, φ⟩E = ⟨s1 −s0, φ⟩E; the ﬁfth by
(20.86); and the ﬁnal equality because, as we next argue, ⟨Z, −φ⟩E ∼N

0, σ2
, so
we can employ (19.12a). To see that ⟨Z, −φ⟩E ∼N

0, σ2
, note that, by (20.86),
∥−φ∥= 1 and then employ Proposition 19.7.3.
This establishes the ﬁrst part of the lemma. The result where the weak inequality
is replaced with a strict inequality follows by replacing all the weak inequalities
in the proof with the corresponding strict inequalities and vice versa. (If X has a
density, then Pr[X < ξ] = Pr[X ≤ξ].)
By applying Lemma 20.14.1 to our problem we obtain
p(error|H = 0) = Q
	∥s0 −s1∥
2σ
+
σ
∥s0 −s1∥ln π0
π1

.
(20.95)
Similarly, one can show that
p(error|H = 1) = Q
	∥s0 −s1∥
2σ
+
σ
∥s0 −s1∥ln π1
π0

.
(20.96)
430
Binary Hypothesis Testing
Consequently, by (20.89)
p∗(error) = π0 Q
	∥s0 −s1∥
2σ
+
σ
∥s0 −s1∥ln π0
π1

+ π1 Q
	∥s0 −s1∥
2σ
+
σ
∥s0 −s1∥ln π1
π0

.
(20.97)
In the special case where the prior is uniform we obtain from (20.95), (20.96), and
(20.97)
p∗(error) = p(error|H = 0) = p(error|H = 1) = Q
	∥s0 −s1∥
2σ

.
(20.98)
This has a nice geometric interpretation. It is the probability that a N

0, σ2
RV
exceeds half the distance between the vectors s0 and s1. Stated diﬀerently, since
∥s0 −s1∥/σ is the number of standard deviations that separate s0 and s1, we can
express the probability of error as the probability that a standard Gaussian exceeds
half the distance between the vectors as measured in standard deviations of the
noise.
20.14.4
The Bhattacharyya Bound
Finally, we compute the Bhattacharyya Bound for this problem. From (20.50) we
obtain that, irrespective of the values of π0, π1,
p∗(error)
≤1
2

y∈RJ
3
fY|H=0(y)fY|H=1(y) dy
= 1
2

y
A
B
B
C
J
@
j=1
-
1
√
2πσ2 e−(y(j)−s(j)
0 )
2
2σ2
.
J
@
j=1
-
1
√
2πσ2 e−(y(j)−s(j)
1 )
2
2σ2
.
dy
= 1
2

y
A
B
B
C
J
@
j=1
-
1
√
2πσ2 e−(y(j)−s(j)
0 )
2
2σ2
1
√
2πσ2 e−(y(j)−s(j)
1 )
2
2σ2
.
dy
= 1
2

y
J
@
j=1
-
1
√
2πσ2 exp
	
−

y(j) −s(j)
0
2 +

y(j) −s(j)
1
2
4σ2

.
dy
= 1
2
J
@
j=1

y(j)∈R
1
√
2πσ2 e−
2(y(j))
2−2y(j)(s(j)
0
+s(j)
1 )+(s(j)
0 )
2
+(s(j)
1 )
2
4σ2
dy(j)
= 1
2
J
@
j=1
 ∞
−∞
1
√
2πσ2 exp
-
−
y2 −y

s(j)
0
+ s(j)
1

+ 1
2

s(j)
0
2 +

s(j)
1
2
2σ2
.
dy
= 1
2
J
@
j=1
 ∞
−∞
1
√
2πσ2 exp
⎛
⎜
⎜
⎜
⎝−
	
y −s(j)
0
+s(j)
1
2

2
+

s(j)
0
−s(j)
1
2
4
2σ2
⎞
⎟
⎟
⎟
⎠dy
20.15 Guessing in the Presence of a Random Parameter
431
= 1
2
J
@
j=1
exp
-
−

s(j)
0
−s(j)
1
2
8σ2
.
= 1
2 exp
-
−1
8σ2
J

j=1

s(j)
0
−s(j)
1
2
.
= 1
2 exp
-
−∥s0 −s1∥2
8σ2
.
,
(20.99)
where the last integral is evaluated using (19.21).
20.15
Guessing in the Presence of a Random Parameter
We now consider the guessing problem when the distribution of the observable Y
depends not only on the hypothesis H but also on a random parameter Θ, which
is independent of H. Based on the conditional densities fY|Θ,H=0(·), fY|Θ,H=1(·),
the nondegenerate prior π0, π1 > 0, and on the law of Θ, we seek an optimal rule
for guessing H. We distinguish between two cases depending on whether we must
base our guess on the observed value yobs of Y alone—random parameter not
observed—or whether we also observe the value θobs of Θ—random parameter
observed. The analysis of both cases is conceptually straightforward.
20.15.1
Random Parameter Not Observed
The guessing problem when the random parameter is not observed is sometimes
called “testing in the presence of a nuisance parameter.” Conceptually, the situ-
ation is quite simple. We have only one observation, Y = yobs, and an optimal
decision rule is the MAP rule (Theorem 20.7.1). The MAP rule entails computing
the likelihood-ratio function
LR(yobs) = fY|H=0(yobs)
fY|H=1(yobs),
(20.100)
and comparing the result to the threshold π1/π0; see (20.40).
Often, however, the densities fY|H=0(yobs) and fY|H=1(yobs) appearing in (20.100)
are not given directly. Instead we are given the density of Θ and the conditional
density of Y given (H, Θ). (We shall encounter such a situation in Chapter 27
when we discuss noncoherent communications.) In such cases we can compute the
conditional density fY|H=0(yobs) as follows:
fY|H=0(yobs) =

θ
fY,Θ|H=0(yobs, θ) dθ
=

θ
fY|Θ=θ,H=0(yobs) fΘ|H=0(θ) dθ
=

θ
fY|Θ=θ,H=0(yobs) fΘ(θ) dθ,
(20.101)
432
Binary Hypothesis Testing
where the ﬁrst equality follows because from the joint density one obtains the
marginal density by integrating out the variable in which we are not interested;
the second by the deﬁnition of the conditional density; and the ﬁnal equality from
our assumption that Θ and H are independent. (In computations such as these
it is best to think about the conditioning on H = 0 as deﬁning a new law on
(Y, Θ)—a new law to which all the regular probabilistic manipulations, such as
marginalization and computation of conditional densities, continue to apply. We
thus simply think of the conditioning on H = 0 as specifying the joint law of (Y, Θ)
that we have in mind.)
Repeating the calculation under H = 1 we obtain that the likelihood-ratio function
is given by
LR(yobs) =

θ fY|Θ=θ,H=0(yobs) fΘ(θ) dθ

θ fY|Θ=θ,H=1(yobs) fΘ(θ) dθ.
(20.102)
The case where Θ is discrete can be similarly addressed. An optimal decision rule
can now be derived based on this expression for the likelihood-ratio function and
on the MAP rule (20.40).
20.15.2
Random Parameter Observed
When the random parameter is observed to be Θ = θobs, we merely view the
problem as a standard hypothesis testing problem with the observation consisting
of Y and Θ. That is, we base our decision on the likelihood-ratio function
LR(yobs, θobs) = fY,Θ|H=0(yobs, θobs)
fY,Θ|H=1(yobs, θobs).
(20.103)
The additional twist is that because Θ is independent of H we have
fY,Θ|H=0(yobs, θobs) = fΘ|H=0(θobs) fY|Θ=θobs,H=0(yobs)
= fΘ(θobs) fY|Θ=θobs,H=0(yobs),
(20.104)
where the second equality follows from the independence of Θ and H. Repeating
for the conditional law of the pair (Y, Θ) given H = 1 we have
fY,Θ|H=1(yobs, θobs) = fΘ(θobs) fY|Θ=θobs,H=1(yobs).
(20.105)
Consequently, by (20.103), (20.104), and (20.105), we obtain that for θobs satisfying
fΘ(θobs) ̸= 0
LR(yobs, θobs) = fY|H=0,Θ=θobs(yobs)
fY|H=1,Θ=θobs(yobs).
(20.106)
An optimal decision rule can be again derived based on this expression for the
likelihood-ratio and on the MAP rule (20.40).
20.16 Mathematical Notes
433
20.16
Mathematical Notes
A standard reference on hypothesis testing is (Lehmann and Romano, 2005). It
also contains a measure-theoretic treatment of the subject. For a precise math-
ematical deﬁnition of the condition X⊸−Y ⊸−Z we refer the reader to (Lo`eve,
1963, Section 25.3). For a measure-theoretic treatment of suﬃcient statistic see
(Lo`eve, 1963, Section 24.4), (Billingsley, 1995, Section 34), (Romano and Siegel,
1986, pp. 154–156), and (Halmos and Savage, 1949). For a measure-theoretic treat-
ment of the notion of conditional distribution see, for example, (Billingsley, 1995,
Chapter 6), (Williams, 1991, Chapter 9), or (Lehmann and Romano, 2005, Chap-
ter 2).
20.17
Exercises
Exercise 20.1 (Hypothesis Testing). Let H take on the values 0 and 1 equiprobably.
Conditional on H = 0, the observable Y is equal to a + Z, where Z is independent of H
and has the Laplace distribution
fZ(z) = 1
2 e−|z|, z ∈R,
and a > 0 is a given constant. Conditional on H = 1, the observable Y is given by −a+Z.
(i) Find and draw the densities fY |H=0(·) and fY |H=1(·).
(ii) Find an optimal rule for guessing H based on Y .
(iii) Compute the optimal probability of error.
(iv) Compute the Bhattacharyya Bound.
Exercise 20.2 (A Discrete Multi-Dimensional Problem). Let H take on the values 0
and 1 according to the prior (π0, π1). Let the observation Y = (Y1, . . . , Yn)T be an n-
dimensional binary vector. Conditional on H = 0, the components of the vector Y are
IID with
Pr
$
Yℓ= 1
 H = 0
%
= 1 −Pr
$
Yℓ= 0
 H = 0
%
= 1
4,
ℓ= 1, . . . , n.
Conditional on H = 1, the components are IID with
Pr
$
Yℓ= 1
 H = 1
%
= 1 −Pr
$
Yℓ= 0
 H = 1
%
= 3
4,
ℓ= 1, . . . , n.
(i) Find an optimal rule for guessing H based on Y.
(ii) Compute the optimal probability of error.
(iii) Compute the Bhattacharyya Bound.
Hint: You may need to treat the cases of n even and n odd separately.
Exercise 20.3 (A Multi-Antenna Receiver). Let H take on the values 0 and 1 equiprob-
ably. We wish to guess H based on the random variables Y1 and Y2. Conditional on
H = 0,
Y1 = A + Z1,
Y2 = A + Z2,
434
Binary Hypothesis Testing
and conditional on H = 1,
Y1 = −A + Z1,
Y2 = −A + Z2.
Here A is a positive constant, and Z1 ∼N

0, σ2
1

, Z2 ∼N

0, σ2
2

, and H are independent.
(i) Find an optimal rule for guessing H based on (Y1, Y2).
(ii) Draw the decision regions in the (Y1, Y2)-plane for the special case where σ1 = 2σ2.
(iii) Returning to the general case, ﬁnd a one-dimensional suﬃcient statistic.
(iv) Find the optimal probability of error in terms of σ2
1, σ2
2, and A.
(v) Consider a suboptimal receiver that declares “H = 0” if Y1 +Y2 > 0, and otherwise
declares “H = 1.” Evaluate the probability of error for this decoder as a function
of σ2
1, σ2
2, and A.
Exercise 20.4 (Binary Hypothesis Testing with General Costs). Let H take on the values 0
and 1 according to the prior (π0, π1). The observable Y has conditional densities fY|H=0(·)
and fY|H=1(·). Based on Y, we wish to guess the value of H. Let the guess associated
with Y = yobs be denoted by φGuess(yobs). Guessing “H = η” when H = ν costs c(η, ν),
where c(·, ·) is a given function from {0, 1} × {0, 1} to the nonnegative reals.
Find a
decision rule that minimizes the expected cost
E
+
c

φGuess(Y), H
,
=
1

ν=0
πν
1

η=0
c(η, ν) Pr
$
φGuess(Y) = η
 H = ν
%
.
Exercise 20.5 (Binary Hypothesis Testing). Let H take on the values 0 and 1 according
to the prior (π0, π1), and let the observation consist of the RV Y . Conditional on H, the
densities of Y are given for every y ∈R by
fY |H=0(y) = e−y I{y ≥0},
fY |H=1(y) = β e−y2
2 I{y ≥0},
where β > 0 is some constant.
(i) Determine β.
(ii) Find a decision rule that minimizes the probability of error.
(iii) For the rule that you have found, compute Pr(error|H = 0).
Hint: Diﬀerent priors can lead to dramatically diﬀerent decision rules.
Exercise 20.6 (On the Bhattacharyya Bound).
(i) Show that the Bhattacharyya Bound never exceeds 1/2.
(ii) When is it equal to 1/2?
Hint: You may ﬁnd the Cauchy-Schwarz Inequality useful.
Exercise 20.7 (When Is the Bhattacharyya Bound Tight?). Under what conditions on the
prior (π0, π1) and the conditional densities fY|H=0(·), fY|H=1(·) does the Bhattacharyya
Bound coincide with the optimal probability of error?
20.17 Exercises
435
Exercise 20.8 (The Bhattacharyya Bound for Conditionally IID Observations). Consider
a binary hypothesis testing problem where, conditional on H = 0, the J components of
the observed random vector Y are IID with each component of density f0(·). Conditional
on H = 1 the components of Y are IID with each component of density f1(·). Express
the Bhattacharyya Bound in terms of J and

R

f0(y) f1(y) dy.
Exercise 20.9 (Another Bound on the Optimal Probability of Error).
(i) Prove that if α and β are nonnegative and sum to one, then min{α, β} is upper-
bounded by 2αβ.
(ii) Prove that
p∗(error) ≤2

Rd Pr
$
H = 0
 Y = y
%
Pr
$
H = 1
 Y = y
%
fY(y) dy.
Exercise 20.10 (Conditional Independence and Factorizations of the PMF). For (dis-
crete) random variables X, Y , and Z show that X and Y are conditionally indepen-
dent given Z if, and only if, the joint PMF PX,Y,Z(x, y, z) can be written as a product
g1(x, z) g2(y, z) for some functions g1 and g2.
Exercise 20.11 (Another Characterization of Conditional Independence). Let the random
variables X, Y , and Z take values in ﬁnite subsets of the reals. Show that X and Y are
conditionally independent given Z if, and only if, for all functions g, h: R2 →R
E[g(X, Z) h(Y, Z)] = E
$
E[g(X, Z)|Z] E[h(Y, Z)|Z]
%
.
Exercise 20.12 (Independence and Conditional Independence). Let the random variables
X, Y , and Z be of joint PMF PX,Y,Z(·, ·, ·). Show that if X and Z are independent, Y
and Z are independent, and X and Y are conditionally independent given Z, then X, Y ,
and Z are independent.
Exercise 20.13 (Error Probability and L1-Distance). In the setting of Theorem 20.5.2
with a uniform prior, show that (20.26) can also be written as
Pr
$
φ∗
Guess(Y) ̸= H
%
= 1
2 −1
4

Rd
fY |H=0(y) −fY |H=1(y)
 dy.
436
Binary Hypothesis Testing
Exercise 20.14 (The Probability of Y Being in a Set). Consider binary hypothesis testing
with a uniform prior.
(i) Suppose that the (Lebesgue measurable) subset A of Rd is such that
Pr[Y ∈A|H = 1] −Pr[Y ∈A|H = 0] = Δ,
where Δ is some positive number.
Prove that the optimal probability of error
p∗(error) for guessing H based on Y is upper-bounded by (1 −Δ)/2.
(ii) Prove that for any (Lebesgue measurable) A ⊆Rd
Pr[Y ∈A|H = 1] −Pr[Y ∈A|H = 0]
 ≤1 −2p∗(error),
A ⊆Rd.
(iii) Find a set A for which this holds with equality.
Thus, if the optimal probability of error is very close to 1/2, then for every (Lebesgue
measurable) subset A ⊆Rd the probability that Y is in A hardly depends on H.
Hint: For Part (i) compare an optimal guessing rule to a (possibly suboptimal) rule that
guesses “H = 1” if Y ∈A and “H = 0” otherwise.
Exercise 20.15 (Multi-Dimensional Binary Hypothesis Testing). The random 2-vector
Z = (Z(1), Z(2)) is uniformly distributed on the unit disc

(x, y) ∈R2 : x2 + y2 ≤1

,
independently of H, which takes on the values 0 and 1 equiprobably. Conditional on
H = 0, we observe Y = αZ, and conditional on H = 1, we observe Y = βZ, where α and
β are real numbers.
(i) Derive an optimal decision rule for guessing H based on Y.
(ii) Find a one-dimensional suﬃcient statistic for guessing H based on Y.
(iii) Compute the optimal probability of error and the Bhattacharyya Bound.
Exercise 20.16 (Optimality and Suﬃciency). Let U1 and U2 be independent and uniformly
distributed on the unit interval [0, 1]. The binary random variable H takes on the values
0 and 1 equiprobably. Conditional on H = 0, we observe the random 2-vector
Y =
αU1
βU2

,
and conditional on H = 1, we observe
Y =
βU1
αU2

,
where α and β are deterministic and β > α > 0.
(i) Derive an optimal decision rule for guessing H based on Y.
(ii) Compute the optimal probability of error and the Bhattacharyya Bound.
(iii) Exhibit an optimal rule for guessing H based on Y that bases its guess on Y (1).
(iv) Show that, notwithstanding Part (iii), T

y(1), y(2)
= y(1) is not a suﬃcient statistic
for the densities fY|H=0(·) and fY|H=1(·).
20.17 Exercises
437
Exercise 20.17 (Conditionally Poisson Observations). A RV X is said to have a Poisson
distribution of parameter (“intensity”) λ, where λ is some nonnegative real number, if X
takes value in the nonnegative integers and
Pr
$
X = n
%
= e−λ λn
n! ,
n = 0, 1, 2, . . .
(i) Find the Moment Generating Function of a Poisson RV of intensity λ.
(ii) Show that if X and Y are independent Poisson random variables of intensities λx
and λy, then their sum X + Y is Poisson with parameter λx + λy.
(iii) Let H take on the values 0 and 1 according to the prior (π0, π1).
We wish to
guess H based on the RV Y . Conditional on H = 0, the observation Y is Poisson
of intensity α + λ, whereas conditional on H = 1 it is Poisson of intensity β + λ.
Here α, β, λ are known nonnegative constants. Show that the optimal probability
of error is monotonically nondecreasing in λ.
Hint: For Part (iii) recall Part (ii) and that no randomized decision rule can outperform
an optimal deterministic rule.
Exercise 20.18 (Optical Communication). Consider an optical communication system
that uses binary on/oﬀkeying at a rate of 108 bits per second. At the beginning of each
time interval of duration 10−8 seconds a new data bit D enters the transmitter. If D = 0,
the laser is turned oﬀfor the duration of the interval; otherwise, if D = 1, the laser is
turned on. The receiver counts the number Y of photons received during the interval.
Assume that, conditional on D, the observation Y is a Poisson RV whose conditional
PMF is
Pr
$
Y = y
 D = 0
%
= e−μ μy
y!
,
y = 0, 1, 2, . . . ,
(20.107)
Pr
$
Y = y
 D = 1
%
= e−λ λy
y!
,
y = 0, 1, 2, . . . ,
(20.108)
where λ > μ ≥0. Further assume that Pr[D = 0] = Pr[D = 1] = 1/2.
(i) Find an optimal guessing rule for guessing D based on Y .
(ii) Compute the optimal probability of error. (Not necessarily in closed-form.)
(iii) Suppose that we now transmit each data bit over two time intervals, each of duration
10−8 seconds. (The system now supports a data rate of 0.5 × 108 bits per second.)
The receiver produces the photon counts Y1 and Y2 over the two intervals. Assume
that, conditional on D = 0, the counts Y1 & Y2 are IID with the PMF (20.107)
and that, conditional on D = 1, they are IID with the PMF (20.108).
Find a
one-dimensional suﬃcient statistic for the problem and use it to ﬁnd an optimal
decision rule.
Hint: For Part (iii), recall Part (ii) of Exercise 20.17.
Exercise 20.19 (Fun with Randomization). Two random variables U and V are drawn
IID according to some unknown probability density function f(·). After observing that
U = uobs, we wish to guess whether it is smaller or larger than V .
(i) What is the probability of success of the rule that, irrespective of uobs, guesses that
U is the larger of the two?
438
Binary Hypothesis Testing
(ii) What is the probability of success of the rule that for some a ∈R guesses that U
is the larger of the two whenever uobs > a? Express your answer in terms of a and
f(·). Under what conditions is this probability of success strictly larger than 1/2 ?
(iii) Can you ﬁnd a randomized guessing rule whose probability of success is strictly
larger than 1/2 irrespective of f(·) ?
Exercise 20.20 (Monotone Likelihood Ratio and Log-Concavity). Let H take on the
values 0 and 1 according to the nondegenerate prior (π0, π1). Conditional on H = 0, the
observation Y is given by
Y = ξ0 + Z,
where ξ0 ∈R is some deterministic number and Z is a RV of PDF fZ(·). Conditional on
H = 1, the observation Y is given by
Y = ξ1 + Z,
where ξ1 > ξ0.
(i) Show that if the PDF fZ(·) is positive and is such that
fZ(y1 −ξ0) fZ(y0 −ξ1) ≤fZ(y1 −ξ1) fZ(y0 −ξ0),
	
y1 > y0, ξ1 > ξ0

, (20.109)
then an optimal decision rule is to guess “H = 0” if Y ≤y⋆and to guess “H = 1”
if Y > y⋆for some real number y⋆.
(ii) Show that if z →log fZ(z) is a concave function, then (20.109) is satisﬁed.
Mathematicians state this result by saying that if g: R →R is positive, then the mapping
(x, y) →g(x −y) has the Total Positivity property of Order 2 if, and only if, g is log-
concave (Marshall, Olkin, and Arnold, 2011, Chapter 18, Section A, Example A.10).
Statisticians state this result by saying that a location family generated by a positive
PDF f(·) has monotone likelihood ratios if, and only if, f(·) is log-concave. For more on
distributions with monotone likelihood ratios see (Lehmann and Romano, 2005, Chapter
3, Section 3.4).
Hint: For Part (ii) recall that a function g: R →R is concave if for any a < b and
0 < α < 1 we have g

αa + (1 −α)b

≥α g(a) + (1 −α) g(b). You may like to proceed as
follows. Show that if g is concave then
g(a −Δ2) + g(a + Δ2) ≤g(a −Δ1) + g(a + Δ1),
|Δ1| ≤|Δ2|.
Deﬁning g(z) = log fZ(z), show that the logarithm of the LHS of (20.109) can be written
as
g
	
¯y −¯ξ + 1
2Δy + 1
2Δξ

+ g
	
¯y −¯ξ −1
2Δy −1
2Δξ

,
where
¯y = (y0 + y1)/2,
¯ξ = (ξ0 + ξ1)/2,
Δy = y1 −y0,
Δξ = ξ1 −ξ0.
Show that the logarithm of the RHS of (20.109) is given by
g
	
¯y −¯ξ + 1
2Δy −1
2Δξ

+ g
	
¯y −¯ξ + 1
2Δξ −1
2Δy

.
Exercise 20.21 (Worst Prior). Given a guessing rule φGuess : Rd →{0, 1}, which prior
maximizes the probability of error?
Hint: You may need to consider several cases separately.
20.17 Exercises
439
Exercise 20.22 (Is a Uniform Prior the Worst Prior?). Based on an observation Y , we
wish to guess the value of a RV H taking on the values 0 and 1 according to the prior
(π0, π1). Conditional on H = 0, the observation Y is uniform over the interval [0, 1], and,
conditional on H = 1, it is uniform over the interval [0, 1/2].
(i) Find an optimal rule for guessing H based on the observation Y . Note that the
rule may depend on π0.
(ii) Let p∗(error; π0) denote the optimal probability of error. Find p∗(error; π0) and
plot it as a function of π0 in the range 0 ≤π0 ≤1.
(iii) Which value of π0 maximizes p∗(error; π0)?
Consider now the general problem where the RV Y is of conditional densities fY |H=0(·),
fY |H=1(·), and H is of prior (π0, π1). Let p∗(error; π0) denote the optimal probability of
error for guessing H based on Y .
(iv) Prove that
p∗	
error; 1
2

≥1
2 p∗(error; π0) + 1
2 p∗(error; 1 −π0),
π0 ∈[0, 1].
(20.110a)
(v) Show that if the densities fY |H=0(·) and fY |H=1(·) satisfy
fY |H=0(y) = fY |H=1(−y),
y ∈R,
(20.110b)
then
p∗(error; π0) = p∗(error; 1 −π0),
π0 ∈[0, 1].
(20.110c)
(vi) Show that if (20.110b) holds, then the uniform prior is the worst prior:
p∗(error; π0) ≤p∗(error; 1/2),
π0 ∈[0, 1].
(20.110d)
Hint: For Part (iv) you might like to consider a new setup. In the new setup ˜H = M ⊕S,
where ⊕denotes the exclusive-or operation and where the binary random variables M
and S are independent with S taking value in {0, 1} equiprobably and with Pr[M = 0] =
1 −Pr[M = 1] = π0.
Assume that in the new setup (M, S)⊸−˜H⊸−˜Y and that the
conditional density of ˜Y given ˜H = 0 is fY |H=0(·) and given ˜H = 1 it is fY |H=1(·).
Compare now the performance of an optimal decision rule for guessing ˜H based on ˜Y
with the performance of an optimal decision rule for guessing ˜H based on the pair ( ˜Y , S).
Express these probabilities of error in terms of the parameters of the original problem.
Exercise 20.23 (Hypothesis Testing with a Random Parameter). Let Y = X + AZ,
where X, A, and Z are independent random variables with X taking on the values ±1
equiprobably, A taking on the values 2 and 3 equiprobably, and Z ∼N

0, σ2
.
(i) Find an optimal rule for guessing X based on the pair (Y, A).
(ii) Repeat when you observe only Y .
Exercise 20.24 (Bounding the Conditional Probability of Error). Show that when the
prior is uniform
pMAP(error|H = 0) ≤
 -
fY|H=0(y) fY|H=1(y) dy.
440
Binary Hypothesis Testing
Exercise 20.25 (Upper Bounds on the Conditional Probability of Error).
(i) Let H take on the values 0 and 1 according to the nondegenerate prior (π0, π1). Let
the observation Y have the conditional densities fY|H=0(·) and fY|H=1(·). Show
that for every ρ > 0
pMAP(error|H = 0) ≤
	π1
π0

ρ 
f ρ
Y|H=1(y) f 1−ρ
Y|H=0(y) dy.
(ii) A suboptimal decoder guesses “H = 0” if q0(y) > q1(y); guesses “H = 1” if
q0(y) < q1(y); and otherwise tosses a coin.
Here q0(·) and q1(·) are arbitrary
positive functions. Show that for this decoder
p(error|H = 0) ≤
 q1(y)
q0(y)
ρ
fY|H=0(y) dy,
ρ > 0.
Hint: In Part (i) show that you can upper-bound I{π1 fY|H=1(y)/(π0 fY|H=0(y)) ≥1} by

π1 fY|H=1(y)/(π0 fY|H=0(y))
ρ.
Exercise 20.26 (The Hellinger Distance). The Hellinger distance between the densities
f(·) and g(·) is deﬁned as the square root of
1
2
 	
f(ξ) −

g(ξ)

2
dξ
(though some authors drop the one-half).
(i) Show that the Hellinger distance between f(·) and h(·) is upper-bounded by the
sum of the Hellinger distances between f(·) and g(·) and between g(·) and h(·).
(ii) Relate the Hellinger distance to the Bhattacharyya Bound.
(iii) Show that the Hellinger distance is upper-bounded by one.
Exercise 20.27 (Artifacts of Suboptimality). Let H take on the values 0 and 1 equiprob-
ably. Conditional on H = 0, the observation Y is N

1, σ2
, and, conditional on H = 1,
it is N

−1, σ2
. Alice guesses “H = 0” if Y > 2 and guesses “H = 1” otherwise.
(i) Compute the probability that Alice errs as a function of σ2.
(ii) Show that this probability is not monotonically nondecreasing in σ2.
(iii) Does her guessing rule minimize the probability of error?
(iv) Show that if you are obliged to use her rule, then adding noise to Y prior to feeding
it to her detector may be beneﬁcial.
Exercise 20.28 (The Bhattacharyya Bound and a Random Parameter). Let Θ be inde-
pendent of H and of density fΘ(·). Express the Bhattacharyya Bound on the probability
of guessing H incorrectly in terms of fΘ(·), fY|Θ=θ,H=0(·) and fY|Θ=θ,H=1(·). Treat the
case where Θ is not observed and the case where it is observed separately. Show that the
Bhattacharyya Bound in the former case is always at least as large as in the latter case.
Chapter 21
Multi-Hypothesis Testing
21.1
Introduction
In Chapter 20 we discussed how to guess the outcome of a binary random variable.
We now extend the discussion to random variables that take on more than two—but
still a ﬁnite—number of values. Statisticians call this problem “multi-hypothesis
testing” to indicate that there may be more than two hypotheses. Rather than
using H, we now denote the random variable whose outcome we wish to guess
by M. (In Chapter 20 we used H for “hypothesis;” now we use M for “message.”)
We denote the number of possible values that M can take by M and assume that
M ≥2. (The case M = 2 corresponds to binary hypothesis testing.) As before the
“labels” are not important and there is no loss in generality in assuming that M
takes values in the set M = {1, . . . , M}. (In the binary case we used the traditional
labels 0 and 1 but now we prefer 1, 2, . . . , M.)
21.2
The Setup
A random variable M takes values in the set M = {1, . . . , M}, where M ≥2,
according to the prior
πm = Pr[M = m],
m ∈M,
(21.1)
where
πm ≥0,
m ∈M,
(21.2)
and where

m∈M
πm = 1.
(21.3)
We say that the prior is nondegenerate if
πm > 0,
m ∈M,
(21.4)
with the inequalities being strict, so M can take on any value in M with positive
probability. We say that the prior is uniform if
π1 = · · · = πM = 1
M.
(21.5)
441
442
Multi-Hypothesis Testing
The observation is a random vector Y taking values in Rd. We assume that for
each m ∈M the distribution of Y conditional on M = m has the density1
fY|M=m(·),
m ∈M,
(21.6)
where fY|M=m(·) is a nonnegative Borel measurable function that integrates to
one over Rd.
A guessing rule is a Borel measurable function φGuess : Rd →M from the space
of possible observations Rd to the set of possible messages M. We think about
φGuess(yobs) as the guess we form after observing that Y = yobs.
The error
probability associated with the guessing rule φGuess(·) is given by
Pr

φGuess(Y) ̸= M

.
(21.7)
Note that two sources of randomness determine whether we err or not: the real-
ization of M and the generation of Y conditional on that realization. A guessing
rule is said to be optimal if no other guessing rule achieves a lower probability
of error.2 The optimal error probability p∗(error) is the probability of error
associated with an optimal decision rule. In this chapter we shall derive optimal
decision rules and study the optimal probability of error.
21.3
Optimal Guessing
Having observed that Y = yobs, we would like to guess M. An optimal guessing
rule can be derived, as in the binary case, by ﬁrst considering the scenario where
there are no observables.
Its extension to the more interesting case where we
observe Y is straightforward.
21.3.1
Guessing in the Absence of Observables
In this scenario there are only M deterministic decision rules to choose from: the
decision rule “guess 1”, the decision rule “guess 2”, etc. If we employ the “guess 1”
rule, then we are correct if M is indeed equal to 1 and thus with probability of
success π1 and corresponding probability of error of 1−π1. In general, if we employ
the “guess m” rule for some m ∈M, then our probability of success is πm. Thus,
of the M diﬀerent rules at our disposal, the one that has the highest probability
of success is the “guess ˜m” rule, where ˜m is the outcome that is a priori the most
likely. If this ˜m is not unique, then guessing any one of the outcomes that have
the highest a priori probability is optimal.
1We feel no remorse for limiting ourselves to conditional distributions possessing a density.
The reason is that, while the reader is encouraged to assume that the densities are with respect to
the Lebesgue measure, this assumption is never used in the text. And using the Radon-Nikodym
Theorem (Billingsley, 1995, Section 32), one can show that even in the most general case there
exists a measure on Rd with respect to which the conditional laws of Y given each of the possible
values of M are absolutely continuous. That measure can be taken, for example, as the sum of
the conditional laws corresponding to each of the possible values that M can take.
2As in the case of binary hypothesis testing, an optimal guessing rule always exists; see
Footnote 2 on Page 397.
21.3 Optimal Guessing
443
We conclude that in the absence of observables, the guessing rule “guess ˜m” is
optimal if, and only if,
π ˜m = max
m′∈M πm′.
(21.8)
For an optimal guessing rule the probability of success is
p∗(correct) = max
m′∈M πm′
(21.9)
and the optimal error probability is thus
p∗(error) = 1 −max
m′∈M πm′.
(21.10)
21.3.2
The Joint Law of M and Y
Using the prior {πm} and the conditional densities {fY|M=m(·)}, we can express
the unconditional density of Y as
fY(y) =

m∈M
πm fY|M=m(y),
y ∈Rd.
(21.11)
As in Section 20.4, we deﬁne for every m ∈M and for every yobs ∈Rd the
conditional probability that M equals m given Y = yobs by
Pr[M = m|Y = yobs] ≜
⎧
⎪
⎨
⎪
⎩
πm fY|M=m(yobs)
fY(yobs)
if fY(yobs) > 0,
1
M
otherwise.
(21.12)
By an argument similar to the one proving (20.12) we have
Pr

Y ∈
˜y ∈Rd : fY(˜y) = 0

= 0,
(21.13)
which can also be written as
Pr

fY(Y) = 0

= 0.
21.3.3
Guessing in the Presence of Observables
The problem of guessing in the presence of an observable is very similar to the
one without observables. The intuition is that after observing that Y = yobs, we
associate with each m ∈M the a posteriori probability Pr[M = m|Y = yobs] and
then guess M as though there were no observables. Thus, rather than choosing
the message that has the highest a priori probability as we do in the absence of
observables, we should now choose the message that has the highest a posteriori
probability.
After having observed that Y = yobs we should thus guess “ ˜m” where ˜m is the out-
come in M that has the highest a posteriori probability. If more than one outcome
attains the highest a posteriori probability, then we say that a tie has occurred
and we need to resolve this tie by picking one (it does not matter which) of the
444
Multi-Hypothesis Testing
outcomes that attains the maximum a posteriori probability. We thus guess “ ˜m,”
in analogy to (21.8), only if
Pr[M = ˜m|Y = yobs] = max
m′∈M

Pr[M = m′ |Y = yobs]

.
(We shall later deﬁne the Maximum A Posteriori guessing rule as a randomized
decision rule that picks uniformly at random from the outcomes that have the
highest a posteriori probability; see Deﬁnition 21.3.2 ahead.)
In analogy with (21.9), for this optimal rule
p∗(correct|Y = yobs) = max
m′∈M

Pr[M = m′ |Y = yobs]

,
and, in analogy with (21.10),
p∗(error|Y = yobs) = 1 −max
m′∈M

Pr[M = m′ |Y = yobs]

.
Consequently, the unconditional optimal probability of error can be expressed as
p∗(error) =

Rd

1 −max
m′∈M

Pr[M = m′ |Y = y]

fY(y) dy,
(21.14)
where fY(·) is the unconditional density function of Y and is given in (21.11).
We next proceed to make the above intuitive discussion more rigorous. We begin
by deﬁning for every possible observation yobs ∈Rd the set of outcomes of maximal
a posteriori probability:
˜
M(yobs) ≜
'
˜m ∈M : Pr[M = ˜m|Y = yobs] = max
m′∈M Pr[M = m′ |Y = yobs]
(
.
(21.15)
As we next argue, this set can also be expressed as
˜
M(yobs) =
'
˜m ∈M : π ˜m fY|M= ˜m(yobs) = max
m′∈M πm′ fY|M=m′(yobs)
(
.
(21.16)
This can be shown by treating the case fY(yobs) > 0 and the case fY(yobs) = 0
separately. In the former case, (21.16) is veriﬁed by noting that in this case we
have, by (21.12), that Pr[M = m′ |Y = yobs] = πm′ fY|M=m′(yobs)/fY(yobs), so
the result follows because scaling the scores of all the elements of a set by a positive
number that is common to them all (1/fY(yobs)) does not change the subset of
the elements with the highest score. In the latter case we note that, by (21.12),
we have for all m′ ∈M that Pr[M = m′ |Y = yobs] = 1/M, so the RHS of (21.15)
is M and we also have by (21.11) for all m′ ∈M that πm′ fY|M=m′(yobs) = 0 so
the RHS of (21.16) is also M.
Using the above deﬁnition of ˜
M(yobs) we can now state the main theorem regarding
optimal guessing rules.
Theorem 21.3.1 (Optimal Multi-Hypothesis Testing). Let M take values in the
set M = {1, . . . , M} with the prior (21.1), and let the observation Y be a random
21.3 Optimal Guessing
445
vector taking values in Rd with conditional densities fY|M=1(·), . . . , fY|M=M(·).
Any guessing rule φ∗
Guess : Rd →M that satisﬁes
φ∗
Guess(yobs) ∈˜
M(yobs),
yobs ∈Rd
(21.17)
is optimal. Here
˜
M(yobs) is the set deﬁned in (21.15) or (21.16).
Proof. Every (deterministic) guessing rule induces a partitioning of the space of
possible outcomes Rd into M disjoint sets D1, . . . , DM:
M

m=1
Dm = Rd,
(21.18a)
Dm ∩Dm′ = ∅,
m ̸= m′,
(21.18b)
where Dm is the set of observations that result in the guessing rule producing
the guess “M = m.” Conversely, every partition D1, . . . , DM of Rd corresponds
to some deterministic guessing rule that guesses “M = m” whenever yobs ∈Dm.
Searching for an optimal decision rule is thus equivalent to searching for an optimal
way to partition Rd. For every partition D1, . . . , DM the probability of success of
the guessing rule associated with it is given by
Pr(correct) =

m∈M
πm Pr(correct|M = m)
=

m∈M
πm

Dm
fY|M=m(y) dy
=

m∈M
πm

Rd fY|M=m(y) I{y ∈Dm} dy
=

Rd
	 
m∈M
πm fY|M=m(y) I{y ∈Dm}

dy.
To minimize the probability of error we maximize the probability of correct deci-
sion. We thus need to ﬁnd a partition D1, . . . , DM that maximizes the last integral.
To maximize the integral we shall maximize the integrand

m∈M
πm fY|M=m(y) I{y ∈Dm}.
For a ﬁxed value of y, the value of the integrand depends on the set to which we
have assigned y. If y was assigned to D1 (i.e., if y ∈D1), then all the terms in the
sum except for the ﬁrst are zero, and the value of the integrand is π1 fY|M=1(y).
More generally, if y was assigned to Dm, then all the terms in the sum except for
the m-th term are zero, and the value of the integrand is πm fY|M=m(y). For a
ﬁxed value of y, the integrand will thus be maximized if we assign y to the set D ˜m
(and correspondingly guess ˜m), only if
π ˜m fY|M= ˜m(y) = max
m′∈M

πm′ fY|M=m′(y)

.
Thus, if φ∗
Guess(·) satisﬁes the theorem’s hypotheses, then it maximizes the in-
tegrand for every y ∈Rd and thus also maximizes the probability of guessing
correctly.
446
Multi-Hypothesis Testing
21.3.4
The MAP and ML Rules
As in the binary hypothesis testing case, we can also consider randomized decision
rules. Extending the deﬁnition of a randomized decision rule to our setting, one
can show using arguments very similar to those of Section 20.6 that randomization
does not help: no randomized decision rule can yield a smaller probability of error
than an optimal deterministic rule. But randomized decision rules can yield more
symmetric or more “fair” rules.
Indeed, we shall deﬁne the MAP rule as the
randomized rule that resolves ties by choosing one of the messages that achieves
the highest a posteriori probability uniformly at random:
Deﬁnition 21.3.2 (The M-ary MAP Decision Rule). The Maximum A Poste-
riori decision rule is the guessing rule that, after observing that Y = yobs, forms
a guess by picking uniformly at random an element of the set
˜
M(yobs), which is
deﬁned in (21.15) or (21.16).
Theorem 21.3.3 (The MAP Rule Is Optimal). For the setting of Theorem 21.3.1
the MAP decision rule is optimal in the sense that it achieves the smallest proba-
bility of error among all deterministic or randomized decision rules. Thus,
p∗(error) =

m∈M
πm pMAP(error|M = m),
(21.19)
where p∗(error) denotes the optimal probability of error and pMAP(error|M = m)
denotes the conditional probability of error of the MAP rule.
Proof. Irrespective of the realization of the randomization that is used to pick
an element of
˜
M(yobs), the resulting decision rule is optimal (Theorem 21.3.1).
Consequently, the average probability of error that results when we average over
this source of randomness must also be optimal.
The Maximum-Likelihood (ML) rule ignores the prior. It is identical to the
MAP rule when the prior is uniform. Having observed that Y = yobs, the ML
decoder produces as its guess a member of the set
'
˜m ∈M : fY|M= ˜m(yobs) = max
m′∈M fY|M=m′(yobs)
(
that is drawn uniformly at random.
The ML decoder thus guesses “M = ˜m” only if
fY|M= ˜m(yobs) = max
m′∈M fY|M=m′(yobs).
(21.20)
(If more than one outcome achieves this maximum, it chooses uniformly at random
one of the outcomes that achieves the maximum.)
21.3.5
Processing
As in Section 20.11, we say that Z is the result of processing Y with respect to M
if
M⊸−Y⊸−Z
21.4 Example: Multi-Hypothesis Testing for 2D Signals
447
(a1, b1)
(a2, b2)
(a3, b3)
(a4, b4)
(a5, b5)
(a6, b6)
(a7, b7)
(a8, b8)
A
Figure 21.1: Eight equiprobable hypotheses; the situation corresponds to 8-PSK.
forms a Markov chain. In analogy to Theorem 20.11.5, one can prove that if Z is
the result of processing Y with respect to M, then no decision rule based on Z can
outperform an optimal decision rule based on Y.
21.4
Example: Multi-Hypothesis Testing for 2D Signals
21.4.1
The Setup
Consider the case where M is uniformly distributed over the set M = {1, . . . , M}
and where we would like to guess the outcome of M based on an observation
consisting of a two-dimensional random vector Y of components Y (1) and Y (2).
Conditional on M = m, the random variables Y (1) and Y (2) are independent
with Y (1) ∼N

am, σ2
and Y (2) ∼N

bm, σ2
. We assume that σ2 > 0, so the
conditional densities can be written for every m ∈M and every y(1), y(2) ∈R as
fY (1),Y (2)|M=m

y(1), y(2)
=
1
2πσ2 exp
	
−(y(1) −am)2 + (y(2) −bm)2
2σ2

.
(21.21)
This hypothesis testing problem is related to QAM communication over an additive
white Gaussian noise channel with a pulse shape that is orthogonal to its time shifts
by integer multiples of the baud period. The setup is demonstrated in Figure 21.1
for the special case of M = 8 with
am = A cos
2πm
8

,
bm = A sin
2πm
8

,
m = 1, . . . , 8.
(21.22)
This special case is related to 8-PSK communication, where M-PSK stands for
M-ary Phase Shift Keying.
21.4.2
The “Nearest-Neighbor” Decoding Rule
We shall next derive an optimal decision rule. For typographical reasons we shall
use y rather than yobs to denote the observed vector. To ﬁnd an optimal decoding
448
Multi-Hypothesis Testing
rule we note that, since M has a uniform prior, the Maximum-Likelihood rule
(21.20) is optimal. Now ˜m maximizes the likelihood function if, and only if,
	
fY (1),Y (2)|M= ˜m(y(1), y(2)) = max
m′∈M

fY (1),Y (2)|M=m′(y(1), y(2))

⇐⇒
-
1
2πσ2 e−(y(1)−a ˜
m)
2+(y(2)−b ˜
m)
2
2σ2
= max
m′∈M

1
2πσ2 e−(y(1)−am′)
2+(y(2)−bm′)
2
2σ2
!.
⇐⇒
-
e−(y(1)−a ˜
m)
2+(y(2)−b ˜
m)
2
2σ2
= max
m′∈M

e−(y(1)−am′)
2+(y(2)−bm′)
2
2σ2
!.
⇐⇒
-
−

y(1) −a ˜m
2 +

y(2) −b ˜m
2
2σ2
= max
m′∈M

−

y(1) −am′2 +

y(2) −bm′2
2σ2
!.
⇐⇒
-
y(1) −a ˜m
2 +

y(2) −b ˜m
2
2σ2
= min
m′∈M

y(1) −am′2 +

y(2) −bm′2
2σ2
!.
⇐⇒

y(1) −a ˜m
2 +

y(2) −b ˜m
2 = min
m′∈M
'
y(1) −am′2 +

y(2) −bm′2(
⇐⇒
	
∥y −s ˜m∥= min
m′∈M
'
∥y −sm′∥
(
,
where y = (y(1), y(2))T, sm ≜(am, bm)T for m ∈M, and ∥·∥denotes the Euclidean
distance (20.85). It is thus seen that the ML rule (which is equivalent to the MAP
rule because the prior is uniform) is equivalent to a “nearest-neighbor” decoding
rule, which chooses the hypothesis under which the mean vector is closest to the
observed vector (with ties being resolved at random).
Figure 21.2 depicts the
nearest-neighbor decoding rule for 8-PSK. The shaded region corresponds to the
set of observables that result in the guess “M = 1,” i.e., the set of points that are
nearest to

A cos(2π/8), A sin(2π/8)

.
21.4.3
Exact Error Analysis for 8-PSK
The analysis of the probability of error can be a bit tricky. Here we only present
the analysis for 8-PSK. If nothing else, it will motivate us to seek more easily
computable bounds.
We shall compute the probability of error conditional on M = 4. But there is
nothing special about this choice; the rotational symmetry of the problem implies
that the probability of error does not depend on the hypothesis.
Conditional on M = 4, the observables (Y (1), Y (2))T can be expressed as

Y (1), Y (2)T = (−A, 0)T +

Z(1), Z(2)T,
where Z(1) and Z(2) are independent N

0, σ2
random variables:
fZ(1),Z(2)(z(1), z(2)) =
1
2πσ2 exp
	
−(z(1))2 + (z(2))2
2σ2

,
z(1), z(2) ∈R.
21.4 Example: Multi-Hypothesis Testing for 2D Signals
449
m = 1
y(1)
y(2)
guess 1
Figure 21.2: The shaded region corresponds to observations leading the ML rule
to guess “M = 1.”
y(1)
y(2)
Figure 21.3: Contour lines of the density fY1,Y2|M=4(·). The shaded region corre-
sponds to guessing “M = 4”.
Figure 21.3 depicts the contour lines of the density fY (1),Y (2)|M=4(·), which are
centered on the mean (a4, b4) = (−A, 0). Note that fY (1),Y (2)|M=4(·) is symmetric
about the horizontal axis:
fY (1),Y (2)|M=4

y(1), −y(2)
= fY (1),Y (2)|M=4

y(1), y(2)
,
y(1), y(2) ∈R.
(21.23)
The shaded region in the ﬁgure is the set of pairs (y(1), y(2)) that cause the nearest-
neighbor decoder to guess “M = 4.”3 Conditional on M = 4 an error results if
(Y (1), Y (2)) is outside the shaded region.
Referring now to Figure 21.4 we need to compute the probability that the noise
(Z(1), Z(2)) causes the received signal to lie in the union of the shaded areas. The
symmetry of fY (1),Y (2)|M=4(·) about the horizontal axis (21.23) implies that the
probability that the received vector lies in the darkly-shaded region is the same as
3It can be shown that the probability that the observation lies exactly on the boundary of
the region is zero; see Proposition 21.6.2 ahead. We shall thus ignore this possibility.
450
Multi-Hypothesis Testing
4
5
3
ψ
θ
A
ρ(θ) =
A sin ψ
sin(θ+ψ)
Figure 21.4: Error analysis for 8-PSK.
the probability that it lies in the lightly-shaded region. We shall thus compute the
probability of the latter and double the result.
Let ψ = π/8 denote half the angle between the constellation points. To carry out
the integration we shall use polar coordinates (r, θ) centered on the constellation
point (−A, 0) corresponding to Message 4:
pMAP(error|M = 4) = 2
 π−ψ
0
 ∞
ρ(θ)
1
2πσ2 e−r2
2σ2 r dr dθ
= 1
π
 π−ψ
0
 ∞
ρ2(θ)/(2σ2)
e−u du dθ
= 1
π
 π−ψ
0
e−ρ2(θ)
2σ2 dθ,
(21.24)
where ρ(θ) is the distance we travel from the point (−A, 0) at angle θ until we
reach the lightly-shaded region, and where the second equality follows using the
substitution u ≜r2/(2σ2). Using the Law of Sines and the identity sin(π −θ) =
sin(θ), we have
ρ(θ) =
A sin ψ
sin(θ + ψ).
(21.25)
Since the symmetry of the problem implies that the conditional probability of error
conditioned on M = m does not depend on m, it follows from (21.24), (21.25), and
21.5 The Union-of-Events Bound
451
(21.19) that
p∗(error) = 1
π
 π−ψ
0
e
−
A2 sin2 ψ
2 sin2(θ+ψ)σ2 dθ,
ψ = π
8 .
(21.26)
21.5
The Union-of-Events Bound
Although simple, the Union-of-Events Bound, or Union Bound for short, is an
extremely powerful and useful bound.4 To derive it, recall that one of the axioms
of probability is that the probability of the union of two disjoint events is the sum
of their probabilities.5 Given two not necessarily disjoint events V and W, we can
express the set V as in Figure 21.5 as the union of those elements of V that are not
in W and those that are both in V and in W:
V = (V \ W) ∪(V ∩W).
(21.27)
Because the sets V \ W and V ∩W are disjoint, and because their union is V, it
follows that Pr(V) = Pr(V \ W) + Pr(V ∩W), which can also be written as
Pr(V \ W) = Pr(V) −Pr(V ∩W).
(21.28)
Writing the union V ∪W as the union of two disjoint sets
V ∪W = W ∪(V \ W)
(21.29)
as in Figure 21.6, we conclude that
Pr(V ∪W) = Pr(W) + Pr(V \ W),
(21.30)
which combines with (21.28) to prove that
Pr(V ∪W) = Pr(V) + Pr(W) −Pr(V ∩W).
(21.31)
Since probabilities are nonnegative, it follows from (21.31) that
Pr(V ∪W) ≤Pr(V) + Pr(W),
(21.32)
which is the Union Bound. This bound can also be extended to derive an upper
bound on the union of more sets. For example, we can show that for three events
U, V, W we have Pr(U ∪V ∪W) ≤Pr(U)+Pr(V)+Pr(W). Indeed, by ﬁrst applying
the Union Bound to the two sets U and (V ∪W) we obtain
Pr(U ∪V ∪W) = Pr

U ∪(V ∪W)

≤Pr(U) + Pr(V ∪W)
≤Pr(U) + Pr(V) + Pr(W),
4It is also sometimes called Boole’s Inequality.
5Actually the axiom is stronger; it states that this holds also for a countably inﬁnite number
of sets.
452
Multi-Hypothesis Testing
V
V \ W
V ∩W
=
D
Figure 21.5: Diagram of two nondisjoint sets.
V ∪W
W
V \ W
=
D
Figure 21.6: Diagram of the union of two nondisjoint sets.
where the last inequality follows by applying the Union Bound (21.32) to the two
sets V and W. One can continue the argument by induction for a ﬁnite6 collection
of events to obtain:
Theorem 21.5.1 (Union-of-Events Bound). If V1, V2, . . . , is a ﬁnite or countably
inﬁnite collection of events then
Pr
	
j
Vj

≤

j
Pr(Vj).
(21.33)
We can think about the LHS of (21.33) as the probability that at least one of
the events V1, V2, . . . occurs and of its RHS as the expected number of events that
occur. Indeed, if for each j we deﬁne the random variables Xj(ω) = I{ω ∈Vj} for
all ω ∈Ω, then the LHS of (21.33) is equal to Pr
 
j Xj > 0

, and the RHS is

j E[Xj], which can also be expressed as E
 
j Xj

.
After the trivial bound that the probability of any event cannot exceed one, the
Union Bound is probably the most important bound in Probability Theory. What
makes it so useful is the fact that the RHS of (21.33) can be computed without
regard to any dependencies between the events.
Corollary 21.5.2.
(i) If each of a ﬁnite (or countably inﬁnite) collection of events occurs with prob-
ability zero, then their union also occurs with probability zero.
6In fact, this claim holds for a countably inﬁnite number of events.
21.5 The Union-of-Events Bound
453
(ii) If each of a ﬁnite (or countably inﬁnite) collection of events occurs with prob-
ability one, then their intersection also occurs with probability one.
Proof. To prove Part (i) we assume that each of the events V1, V2, . . . is of zero
probability and compute
Pr
	 
j
Vj

≤

j
Pr(Vj)
=

j
0
= 0,
where the ﬁrst inequality follows from the Union Bound, and where the subsequent
equality follows from our assumption that Pr(Vj) = 0, for all j.
To prove Part (ii) we assume that each of the events W1, W2, . . . occurs with
probability one and apply Part (i) to the sets V1, V2, . . ., where Vj is the set-
complement of Wj, i.e., Vj = Ω \ Wj:
Pr
	 E
j
Wj

= 1 −Pr
	 E
j
Wj
c
= 1 −Pr
	 
j
Vj

= 1,
where the ﬁrst equality follows because the probabilities of an event and its com-
plement sum to one; the second because the complement of an intersection is the
union of the complements; and the ﬁnal equality follows from Part (i) because
the events Wj are, by assumption, of probability one so their complements are of
probability zero.
21.5.1
Applications to Hypothesis Testing
We shall now use the Union Bound to derive an upper bound on the conditional
probability of error pMAP(error|M = m) of the MAP decoding rule. The bound
we derive is applicable to any decision rule that satisﬁes the hypothesis of Theo-
rem 21.3.1 as expressed in (21.17).
Deﬁne for every m′ ̸= m the set Bm,m′ ⊂Rd by
Bm,m′ =

y ∈Rd : πm′ fY|M=m′(y) ≥πm fY|M=m(y)

.
(21.34)
Notice that y ∈Bm,m′ does not imply that the MAP rule will guess m′: there may
be a third hypothesis that is a posteriori even more likely than either m or m′.
Also, since the inequality in (21.34) is not strict, y ∈Bm,m′ does not imply that
the MAP rule will not guess m: there may be a tie, which may be resolved in favor
of m. As we next argue, what is true is that if m was not guessed by the MAP rule,
454
Multi-Hypothesis Testing
then some m′ which is not equal to m must have had an a posteriori probability
that is at least as high as that of m:

m was not guessed

=⇒

Y ∈

m′̸=m
Bm,m′

.
(21.35)
Indeed, if m was not guessed by the MAP rule, then some other message was.
Denoting that other message by m′, we note that πm′ fY|M=m′(y) must be at least
as large as πm fY|M=m(y) (because otherwise m′ would not have been guessed),
so y ∈Bm,m′.
Continuing from (21.35), we note that if the occurrence of an event E1 implies the
occurrence of an event E2, then Pr(E1) ≤Pr(E2). Consequently, by (21.35),
pMAP(error|M = m) ≤Pr

Y ∈

m′̸=m
Bm,m′
 M = m

= Pr
	 
m′̸=m
'
ω ∈Ω : Y(ω) ∈Bm,m′
(  M = m

≤

m′̸=m
Pr

{ω ∈Ω : Y(ω) ∈Bm,m′}
 M = m

=

m′̸=m
Pr

Y ∈Bm,m′  M = m

=

m′̸=m

Bm,m′
fY|M=m(y) dy.
We have thus derived:
Proposition 21.5.3. For the setup of Theorem 21.3.1 let pMAP(error|M = m)
denote the conditional probability of error conditional on M = m of the MAP rule
for guessing M based on Y. Then,
pMAP(error|M = m) ≤

m′̸=m
Pr

Y ∈Bm,m′  M = m

(21.36)
=

m′̸=m

Bm,m′
fY|M=m(y) dy,
(21.37)
where
Bm,m′ =

y ∈Rd : πm′ fY|M=m′(y) ≥πm fY|M=m(y)

.
(21.38)
This bound is applicable to any decision rule satisfying the hypothesis of Theo-
rem 21.3.1 as expressed in (21.17).
The term Pr[Y ∈Bm,m′ |M = m] has an interesting interpretation. If ties occur
with probability zero, then it corresponds to the conditional probability of error
(given that M = m) incurred by a MAP decoder designed for the binary hypothesis
21.5 The Union-of-Events Bound
455
4
4
4
5
5
5
3
3
3
B4,3
B4,5
B4,3 ∪B4,5
∪
=
Figure 21.7: Error events for 8-PSK conditional on M = 4.
testing problem of guessing whether M = m or M = m′ when the prior probability
that M = m is πm/(πm + πm′) and that M = m′ is πm′/(πm + πm′).
Alternatively, we can write (21.36) as
pMAP(error|M = m) ≤

m′̸=m
Pr

πm′ fY|M=m′(Y) ≥πm fY|M=m(Y)
 M = m

.
(21.39)
21.5.2
Example: The Union Bound for 8-PSK
We next apply the Union Bound to upper-bound the probability of error associated
with maximum-likelihood decoding of 8-PSK. For concreteness we focus on the
conditional probability of error, conditional on M = 4. We shall see that in this
case the RHS of (21.36) is still an upper bound on the probability of error even if
we do not sum over all m′ that diﬀer from m. Indeed, as we next argue, in upper-
bounding the conditional probability of error of the ML decoder given M = 4, it
suﬃces to sum over m′ ∈{3, 5} only.
To show this we ﬁrst note that for this problem the set Bm,m′ of (21.34) corresponds
to the set of vectors that are at least as close to (am′, bm′) as to (am, bm):
Bm,m′ =
'
y ∈R2 :

y(1) −am′2 +

y(2) −bm′2 ≤

y(1) −am
2 +

y(2) −bm
2(
.
As seen in Figure 21.7, given M = 4, an error will occur only if the observed
vector Y is at least as close to (a3, b3) as to (a4, b4), or if it is at least as close
to (a5, b5) as to (a4, b4). Thus, conditional on M = 4, an error can occur only if
Y ∈B4,3 ∪B4,5. (If Y /∈B4,3 ∪B4,5, then an error will certainly not occur. If
Y ∈B4,3 ∪B4,5, then an error may or may not occur. It will not occur in the case
of a tie—corresponding to Y being on the boundary of B4,3 ∪B4,5—provided that
the tie is resolved in favor of M = 4.)
Note that the events Y ∈B4,5 and Y ∈B4,3 are not mutually exclusive, but,
nevertheless, by the Union-of-Events Bound
pMAP(error|M = 4) ≤Pr[Y ∈B4,3 ∪B4,5 |M = 4]
≤Pr[Y ∈B4,3 |M = 4] + Pr[Y ∈B4,5 |M = 4],
(21.40)
456
Multi-Hypothesis Testing
where the ﬁrst inequality follows because, conditional on M = 4, an error can
occur only if y ∈B4,3 ∪B4,5; and where the second inequality follows from the
Union-of-Events Bound. In fact, the ﬁrst inequality holds with equality because,
for this problem, the probability of a tie is zero; see Proposition 21.6.2 ahead.
From our analysis of multi-dimensional binary hypothesis testing (Lemma 20.14.1)
we obtain that
Pr

Y ∈B4,3
 M = 4

= Q
	
(a4 −a3)2 + (b4 −b3)2
2σ

= Q
	A
σ sin
π
8

(21.41)
and
Pr

Y ∈B4,5
 M = 4

= Q
	
(a4 −a5)2 + (b4 −b5)2
2σ

= Q
	A
σ sin
π
8

.
(21.42)
Combining (21.40), (21.41), and (21.42) we obtain
pMAP(error|M = 4) ≤2Q
	A
σ sin
π
8

.
(21.43)
This is only an upper bound and not the exact error probability because the sets
B4,3 and B4,5 are not disjoint so the events Y ∈B4,3 and Y ∈B4,5 are not
mutually exclusive and the Union-Bound is not tight; see Figure 21.7. Nevertheless,
a moment’s reﬂection will convince the reader that the bound is oﬀby no more
than a factor of two; see also (21.66) ahead.
For this symmetric problem the conditional probability of error conditional on
M = m does not depend on the message m, and we thus also have by (21.19)
p∗(error) ≤2Q
	A
σ sin
π
8

.
(21.44)
21.5.3
Union-Bhattacharyya Bound
We next derive a bound which is looser than the Union Bound but which is of-
ten easier to evaluate in non-Gaussian settings. It is the multi-hypothesis testing
version of the Bhattacharyya Bound (20.50).
Recall that, by Theorem 21.3.1, any guessing rule whose guess after observing that
Y = yobs is in the set
˜
M(yobs) =
'
˜m ∈M : π ˜m fY|M= ˜m(yobs) = max
m′

πm′ fY|M=m′(yobs)
(
is optimal. To analyze the optimal probability of error p∗(error), we shall analyze
one particular optimal decision rule. This rule is not the MAP rule, but it diﬀers
21.5 The Union-of-Events Bound
457
from the MAP rule only in the way it resolves ties. Rather than resolving ties at
random, this rule resolves ties according to the index of the hypothesis: it chooses
the message in
˜
M(yobs) of smallest index. For example, if the messages of highest
a posteriori probability are Messages 7, 9, and 17, i.e., if ˜
M(yobs) = {7, 9, 17}, then
it guesses “7.” This decision rule may not appeal to the reader’s sense of fairness
but, by Theorem 21.3.1, it is nonetheless optimal. Consequently, if we denote the
conditional probability of error of this decoder by p(error|M = m), then
p∗(error) =

m∈M
πm p(error|M = m).
(21.45)
We next analyze the performance of this decision rule. For every m′ ̸= m let
˜Bm,m′ =

y ∈Rd : πm′ fY|M=m′(y) ≥πm fY|M=m(y)

if m′ < m,

y ∈Rd : πm′ fY|M=m′(y) > πm fY|M=m(y)

if m′ > m.
(21.46)
Notice that
˜Bm,m′ = ˜Bc
m′,m,
m ̸= m′.
(21.47)
Conditional on M = m, our detector will err if, and only if, yobs ∈∪m′̸=m ˜Bm,m′.
Thus
p(error|M = m) = Pr

Y ∈

m′̸=m
˜Bm,m′
 M = m

= Pr
	 
m′̸=m
'
ω ∈Ω : Y(ω) ∈˜Bm,m′
(  M = m

≤

m′̸=m
Pr

ω ∈Ω : Y(ω) ∈˜Bm,m′  M = m

=

m′̸=m
Pr

Y ∈˜Bm,m′  M = m

=

m′̸=m

˜
Bm,m′
fY|M=m(y) dy,
(21.48)
where the inequality follows from the Union Bound. To upper-bound p∗(error) we
use (21.45) and (21.48) to obtain
p∗(error) =
M

m=1
πm p(error|M = m)
≤
M

m=1
πm

m′̸=m

˜
Bm,m′
fY|M=m(y) dy
=
M

m=1

m′>m
	
πm

˜
Bm,m′
fY|M=m(y) dy + πm′

˜
Bm′,m
fY|M=m′(y) dy

=
M

m=1

m′>m
	
˜
Bm,m′
πm fY|M=m(y) dy +

˜
Bc
m,m′
πm′ fY|M=m′(y) dy

458
Multi-Hypothesis Testing
=
M

m=1

m′>m

Rd min

πm fY|M=m(y), πm′ fY|M=m′(y)

dy
≤
M

m=1

m′>m
√πmπm′

Rd
3
fY|M=m(y)fY|M=m′(y) dy
≤
M

m=1

m′>m
πm + πm′
2

Rd
3
fY|M=m(y)fY|M=m′(y) dy
= 1
2

m∈M

m′̸=m
πm + πm′
2

Rd
3
fY|M=m(y)fY|M=m′(y) dy,
where the equality in the ﬁrst line follows from (21.45); the inequality in the second
line from (21.48); the equality in the third line by rearranging the sum; the equality
in the fourth line from (21.47); the equality in the ﬁfth line from the deﬁnition of
the set ˜Bm,m′; the inequality in the sixth line from the inequality min{a, b} ≤
√
ab,
which holds for all nonnegative a, b ∈R (see (20.48)); the inequality in the seventh
line from the Arithmetic-Geometric Inequality
√
cd ≤(c + d)/2, which holds for
all c, d ≥0 (see (20.49)); and the ﬁnal equality by the symmetry of the summand.
We have thus obtained the Union-Bhattacharyya Bound:
p∗(error) ≤1
4

m∈M

m′̸=m
(πm + πm′)

Rd
3
fY|M=m(y)fY|M=m′(y) dy.
(21.49)
For a priori equally likely hypotheses it takes the form
p∗(error) ≤
1
2M

m∈M

m′̸=m

Rd
3
fY|M=m(y)fY|M=m′(y) dy,
(21.50)
which is the Union-Bhattacharyya Bound for M-ary hypothesis testing with a
uniform prior.
21.6
Multi-Dimensional M-ary Gaussian Hypothesis Testing
We next use Theorem 21.3.3 to study the multi-hypothesis testing version of the
problem we addressed in Section 20.14. We begin with the problem setup and then
proceed to derive the MAP decision rule. We then assess the performance of this
rule by deriving an upper bound and a lower bound on its probability of error.
21.6.1
Problem Setup
A random variable M takes values in the set M = {1, . . . , M} with a nondegen-
erate prior (21.4).
We wish to guess M based on an observation consisting of
a random column-vector Y taking value in RJ whose components are given by
Y (1), . . . , Y (J).7 For typographical reasons we denote the observed realization of
7Our observation now takes values in RJ and not as before in Rd. My excuse for using J
instead of d is that later, when we refer to this section, d will have a diﬀerent meaning and
choosing J here reduces the chance of confusion later on.
21.6 Multi-Dimensional M-ary Gaussian Hypothesis Testing
459
Y by y, instead of yobs. For every m ∈M we have that, conditional on M = m,
the components of Y are independent Gaussians, with Y (j) ∼N(s(j)
m , σ2), where
sm is some deterministic vector of J components s(1)
m , . . . , s(J)
m , and where σ2 > 0.
Recalling the density of the univariate Gaussian distribution (19.6) and using the
conditional independence of the components of Y given M = m, we can express the
conditional density fY|M=m(y) of the vector Y at every point y = (y(1), . . . , y(J))T
in RJ as
fY|M=m(y) =
J
@
j=1
-
1
√
2πσ2 exp
	
−

y(j) −s(j)
m
2
2σ2

.
.
(21.51)
21.6.2
Optimal Guessing Rule
Using Theorem 21.3.3 we obtain that, having observed y = (y(1), . . . , y(J))T ∈RJ,
an optimal decision rule is the MAP rule, which picks uniformly at random an
element from the set
˜
M(y) =
1
˜m ∈M : π ˜m fY|M= ˜m(y) = max
m′∈M
'
πm′ fY|M=m′(y)
(2
=
1
˜m ∈M : ln

π ˜m fY|M= ˜m(y)

= max
m′∈M
'
ln

πm′ fY|M=m′(y)
(2
, (21.52)
where the second equality follows from the strict monotonicity of the logarithm.
We next obtain a more explicit description of
˜
M(y) for our setup. By (21.51),
ln

πm fY|M=m(y)

= ln πm −J
2 ln(2πσ2) −
1
2σ2
J

j=1

y(j) −s(j)
m
2.
(21.53)
The term (J/2) ln(2πσ2) is a constant term that does not depend on the hypothesis.
Consequently, it does not inﬂuence the set of messages that attain the highest score.
(The tallest student in the class is the same irrespective of whether the height of all
the students is measured when they are barefoot or when they are all wearing the
one-inch heel school uniform shoes. The heel can only make a diﬀerence if diﬀerent
students wear shoes of diﬀerent heel height.) Thus,
˜
M(y)=

˜m ∈M: ln π ˜m−
J

j=1

y(j) −s(j)
˜m
2
2σ2
= max
m′∈M
1
ln πm′−
J

j=1

y(j) −s(j)
m′
2
2σ2
2!
.
The expression for
˜
M(y) can be further simpliﬁed if M is a priori uniformly
distributed. In this case we have
˜
M(y) =

˜m ∈M : −
J

j=1

y(j) −s(j)
˜m
2
2σ2
= max
m′∈M
1
−
J

j=1

y(j) −s(j)
m′
2
2σ2
2!
=

˜m ∈M :
J

j=1

y(j) −s(j)
˜m
2 = min
m′∈M
1
J

j=1

y(j) −s(j)
m′
2
2!
,
M uniform,
460
Multi-Hypothesis Testing
where the ﬁrst equality follows because when M is uniform the additive term ln πm
is given by ln(1/M) and hence does not depend on the hypothesis; and where the
second equality follows because changing the sign of all the elements of a set changes
the largest ones to the smallest ones, and by noting that scaling the score by 2σ2
does not change the highest scoring messages (because we assumed that σ2 > 0).
If we interpret the quantity
∥y −sm∥=
A
B
B
C
J

j=1

y(j) −s(j)
m
2
as the Euclidean distance between the vector y and the vector sm, then we see that,
for a uniform prior on M, it is optimal to guess the message m whose corresponding
mean vector sm is closest to the observed vector y. Notice that to implement this
“nearest-neighbor” decision rule we do not need to know the value of σ2.
We next show that if, in addition to assuming a uniform prior on M, we also
assume that the vectors s1, . . . , sM all have the same norm, i.e.,
∥s1∥= ∥s2∥= · · · = ∥sM∥,
(21.54)
then
˜
M(y) =

˜m ∈M :
J

j=1
y(j)s(j)
˜m = max
m′∈M
1
J

j=1
y(j)s(j)
m′
2!
,
so the MAP decision rules guesses the message m whose mean vector sm has
the “highest correlation” with the received vector y. To see this, we note that
because M has a uniform prior the “nearest-neighbor” decoding rule is optimal,
and we then expand
∥y −sm∥2 =
J

j=1

y(j) −s(j)
m
2
=
J

j=1

y(j)2 −2
J

j=1
y(j)s(j)
m +
J

j=1

s(j)
m
2,
where the ﬁrst term does not depend on the hypothesis and where, by (21.54), the
third term also does not depend on the hypothesis.
We summarize our ﬁndings in the following proposition.
Proposition 21.6.1. Consider the problem described in Section 21.6.1 of guess-
ing M based on the observation y.
(i) It is optimal to form the guess based on y = (y(1), . . . , y(J))T by choosing
uniformly at random from the set

˜m ∈M: ln π ˜m −
J

j=1

y(j) −s(j)
˜m
2
2σ2
= max
m′∈M
1
ln πm′ −
J

j=1

y(j) −s(j)
m′
2
2σ2
2!
.
21.6 Multi-Dimensional M-ary Gaussian Hypothesis Testing
461
(ii) If M is uniformly distributed, then this rule is equivalent to the “nearest-
neighbor” decoding rule of picking uniformly at random an element of the
set
'
˜m ∈M : ∥y −s ˜m∥= min
m′∈M

∥y −sm′∥
(
.
(iii) If, in addition to M being uniform, we also assume that the mean vectors
satisfy (21.54), then this rule is equivalent to the “maximum-correlation” rule
of picking at random an element of the set

˜m ∈M :
J

j=1
y(j)s(j)
˜m = max
m′∈M
1
J

j=1
y(j)s(j)
m′
2!
.
We next show that if the mean vectors s1, . . . , sM are distinct in the sense that for
every pair m′ ̸= m′′ in M there exists at least one component where the vectors
sm′ and sm′′ diﬀer, i.e.,
∥sm′ −sm′′∥> 0,
m′ ̸= m′′,
then the probability of ties is zero. That is, we will show that the probability of
observing a vector y for which the set
˜
M(y) (21.52) has more than one element
is zero. Stated in yet another way, the probability that the observable Y will be
such that the MAP will require randomization is zero. Stated one last time:
Proposition 21.6.2. If the mean vectors s1, . . . , sM in our setup are distinct, then
with probability one the observed vector y is such that there is a unique message of
highest a posteriori probability.
Proof. Conditional on Y = y, associate with each message m ∈M the score
ln

πm fY|M=m(y)

. We need to show that the probability of the observation y
being such that at least two messages attain the highest score is zero. Instead, we
shall prove the stronger statement that the probability of two messages attaining
the same score (be it maximal or not) is zero.
We ﬁrst show that it suﬃces to prove that for every m ∈M and for every pair of
messages m′ ̸= m′′, we have that, conditional on M = m, the probability that m′
and m′′ attain the same score is zero, i.e.,
Pr

score of Message m′ = score of Message m′′  M = m

= 0,
m′ ̸= m′′.
(21.55)
Indeed, once we show (21.55), it will follow that the unconditional probability that
Message m′ attains the same score as Message m′′ is zero, i.e.,
Pr

score of Message m′ = score of Message m′′
= 0,
m′ ̸= m′′,
(21.56)
because
Pr

score of Message m′ = score of Message m′′
=

m∈M
πm Pr

score of Message m′ = score of Message m′′  M = m

.
462
Multi-Hypothesis Testing
But (21.56) implies that the probability that any two or more messages attain the
highest score is zero because
Pr(two or more messages attain the highest score)
= Pr
-

m′,m′′∈M
m′̸=m′′

m′ and m′′ attain the highest score

.
≤

m′,m′′∈M
m′̸=m′′
Pr

m′ and m′′ attain the highest score

≤

m′,m′′∈M
m′̸=m′′
Pr

m′ and m′′ attain the same score

,
where the ﬁrst equality follows because more than one message attains the high-
est score if, and only if, there exist two distinct messages m′ and m′′ that attain
the highest score; the subsequent inequality follows from the Union Bound (Theo-
rem 21.5.1); and the ﬁnal inequality by noting that if m′ and m′′ both attain the
highest score, then they both achieve the same score.
Having established that in order to complete the proof it suﬃces to establish
(21.55), we proceed to do so. By (21.53) we obtain, upon opening the square,
that the observation Y results in Messages m′ and m′′ obtaining the same score if,
and only if,
1
σ2
J

j=1
Y (j)
s(j)
m′ −s(j)
m′′

= ln πm′′
πm′ +
1
2σ2

∥sm′∥2 −∥sm′′∥2
.
(21.57)
We next show that, conditional on M = m, the probability that Y satisﬁes (21.57)
is zero. To that end we note that, conditional on M = m, the random variables
Y (1), . . . , Y (J) are independent random variables with Y (j) being Gaussian with
mean s(j)
m and variance σ2; see (21.51). Consequently, by Proposition 19.7.3, we
have that, conditional on M = m, the LHS of (21.57) is a Gaussian random variable
of variance
1
σ2 ∥sm′ −sm′′∥2,
which is positive because m′ ̸= m′′ and because we assumed that the mean vectors
are distinct.
It follows that, conditional on M = m, the LHS of (21.57) is a
Gaussian random variable of positive variance, and hence has zero probability of
being equal to the deterministic number on the RHS of (21.57). This proves (21.55),
and hence concludes the proof.
21.6.3
The Union Bound
We next use the Union Bound to upper-bound the optimal probability of error
p∗(error). By (21.39)
pMAP(error|M = m) ≤

m′̸=m
Pr

πm′ fY|M=m′(Y) ≥πm fY|M=m(Y)
 M = m

21.6 Multi-Dimensional M-ary Gaussian Hypothesis Testing
463
=

m′̸=m
Q
	∥sm −sm′∥
2σ
+
σ
∥sm −sm′∥ln πm
πm′

,
(21.58)
where the equality follows from Lemma 20.14.1. From this and from the optimality
of the MAP rule (21.19) we thus obtain
p∗(error) ≤

m∈M

m′̸=m
πmQ
	∥sm −sm′∥
2σ
+
σ
∥sm −sm′∥ln πm
πm′

.
(21.59)
If M is uniform, these bounds simplify to:
pMAP(error|M = m) ≤

m′̸=m
Q
	∥sm −sm′∥
2σ

,
M uniform,
(21.60)
p∗(error) ≤1
M

m∈M

m′̸=m
Q
	∥sm −sm′∥
2σ

,
M uniform.
(21.61)
21.6.4
A Lower Bound
We next derive a lower bound on the optimal error probability p∗(error). We do so
by lower-bounding the conditional probability of error pMAP(error|M = m) of the
MAP rule and by then using this lower bound to derive a lower bound on p∗(error)
via (21.19).
We note that if Message m′ attains a score that is strictly higher than the one
attained by Message m, then the MAP decoder will surely not guess “M = m.”
(The MAP may or may not guess “M = m′” depending on the score associated
with messages other than m and m′.) Thus, for each message m′ ̸= m we have
pMAP(error|M = m) ≥Pr

πm′ fY|M=m′(Y) > πm fY|M=m(Y)
 M = m

(21.62)
= Q
	∥sm −sm′∥
2σ
+
σ
∥sm −sm′∥ln πm
πm′

,
(21.63)
where the equality follows from Lemma 20.14.1.
Noting that (21.63) holds for all m′ ̸= m, we can choose m′ to get the tightest
bound. This yields the lower bound
pMAP(error|M = m) ≥
max
m′∈M\{m} Q
	∥sm −sm′∥
2σ
+
σ
∥sm −sm′∥ln πm
πm′

(21.64)
and hence, by (21.19),
p∗(error) ≥

m∈M
πm
max
m′∈M\{m} Q
	∥sm −sm′∥
2σ
+
σ
∥sm −sm′∥ln πm
πm′

.
(21.65)
For uniform M this expression can be simpliﬁed by noting that the Q-function is
strictly decreasing:
pMAP(error|M = m) ≥Q
	
min
m′∈M\{m}
∥sm −sm′∥
2σ

,
M uniform,
(21.66)
464
Multi-Hypothesis Testing
p∗(error) ≥1
M

m∈M
Q
	
min
m′∈M\{m}
∥sm −sm′∥
2σ

,
M uniform.
(21.67)
21.7
Additional Reading
For additional reading on multi-hypothesis testing see the recommended reading
for Chapter 20. The problem of assessing the optimal probability of error for the
multi-dimensional M-ary Gaussian hypothesis testing problem of Section 21.6 has
received extensive attention in the coding literature. For a survey of these results
see (Sason and Shamai, 2006).
21.8
Exercises
Exercise 21.1 (Ternary Gaussian Detection). Consider the following special case of the
problem discussed in Section 21.6. Here M is uniformly distributed over the set {1, 2, 3},
and the mean vectors s1, s2, s3 are given by
s1 = 0,
s2 = s,
s3 = −s,
where s is some deterministic nonzero vector in RJ. Find the conditional probability of
error of the MAP rule conditional on each hypothesis.
Exercise 21.2 (4-PSK Detection). Consider the setup of Section 21.4 with M = 4 and
(a1, b1) = (0, A), (a2, b2) = (−A, 0), (a3, b3) = (0, −A), (a4, b4) = (A, 0).
(i) Sketch the decision regions of the MAP decision rule.
(ii) Using the Q-function, express the conditional probabilities of error of this rule
conditional on each hypothesis.
(iii) Compute an upper bound on pMAP(error|M = 1) using Proposition 21.5.3. Indicate
on the ﬁgure which events are summed two or three times. Can you improve the
bound by summing only over a subset of the alternative hypotheses?
Hint: In Part (ii) ﬁrst ﬁnd the probability of correct detection.
Exercise 21.3 (A 7-ary QAM problem). Consider the problem addressed in Section 21.4
in the special case where M = 7 and
am = A cos
	2πm
6

,
bm = A sin
	2πm
6

,
m = 1, . . . , 6,
a7 = 0,
b7 = 0.
(i) Illustrate the decision regions of the MAP (nearest-neighbor) guessing rule.
(ii) Let Z = (Z(1), Z(2))T be a random vector whose components are IID N

0, σ2
.
Show that for every message m ∈{1, . . . , 7} the conditional probability of error
pMAP(error|M = m) can be upper-bounded by the probability that the Euclidean
norm of Z exceeds A/2. Calculate this probability.
(iii) What is the upper bound on pMAP(error|M = m) that Proposition 21.5.3 yields in
this case? Can you improve it by including fewer terms?
21.8 Exercises
465
(iv) Compare the diﬀerent bounds.
See (Viterbi and Omura, 1979, Chapter 2, Problem 2.2).
Exercise 21.4 (Orthogonal Mean Vectors). Let M be uniformly distributed over the set
M = {1, . . . , M}. Let the observable Y be a random J-vector. Conditional on M = m,
the observable Y is given by
Y =
√
Es φm + Z,
where Z is a random J-vector whose components are IID N

0, σ2
, and where φ1, . . . , φM
are orthonormal in the sense that
⟨φm′, φm′′⟩E = I{m′ = m′′},
m′, m′′ ∈M.
Show that
pMAP(error|M = m) = 1 −
1
√
2π
 ∞
−∞

1 −Q(ξ)
M−1 e−(ξ−α)2
2
dξ,
(21.68)
where α = √Es/σ.
Hint: Requires familiarity with Gaussian vectors (Chapter 23).
Exercise 21.5 (Equi-Energy Constellations). Consider the setup of Section 21.6.1 with a
uniform prior and with ∥s1∥2 = · · · = ∥sM∥2 = Es. Show that the optimal probability of
correct decoding is given by
p∗(correct) = 1
M exp
	
−Es
2σ2

E
'
exp
	 1
σ2 max
m ⟨V, sm⟩E

(
,
(21.69)
where V is a random J-vector whose components are IID N

0, σ2
. We recommend the
following approach. Let D1, . . . , DM be a partition of RJ such that for every m ∈M,
	
y ∈Dm

=⇒
	
⟨y, sm⟩E = max
m′ ⟨y, sm′⟩E

.
(i) Show that
p∗(correct) = 1
M

m∈M
Pr
$
Y ∈Dm
 M = m
%
.
(ii) Show that the RHS of the above can be written as
1
M exp
	
−Es
2σ2

·

RJ
1
(2πσ2)J/2 exp
	
−∥y∥2
2σ2

 
m∈M
I{y ∈Dm} exp
	 1
σ2 ⟨y, sm⟩E


dy.
(iii) Finally show that

m∈M
I{y ∈Dm} exp
	 1
σ2 ⟨y, sm⟩E

= exp
	 1
σ2 max
m∈M ⟨y, sm⟩E

,
y ∈RJ.
See also Problem 23.13.
Exercise 21.6 (When Is the Union Bound Tight?). Under what conditions on the events
V1, V2, . . . is the Union Bound (21.33) tight?
466
Multi-Hypothesis Testing
Exercise 21.7 (The Union of Independent Events). Show that if the events V1, V2, . . . , Vn
are independent then
Pr

n
.
j=1
Vj

= 1 −
n
/
j=1

1 −Pr(Vj)

.
Exercise 21.8 (A Lower Bound on the Probability of a Union). Show that the probability
of the union of n events V1, . . . , Vn can be lower-bounded by
Pr

n
.
j=1
Vj

≥
n

j=1
Pr

Vj

−
n−1

j=1
n

ℓ=j+1
Pr

Vj ∩Vℓ

.
Inequalities of this nature are sometimes called Bonferroni-Type Inequalities.
Exercise 21.9 (The Inclusion-Exclusion Formula). Show that the probability of the union
of n events V1, V2, . . . , Vn is given by
Pr

n
.
j=1
Vj

=

j
Pr

Vj

−
 
j<k
Pr

Vj ∩Vk

+
  
j<k<ℓ
Pr

Vj ∩Vk ∩Vℓ

−· · · + (−1)n+1 Pr

V1 ∩V2 ∩· · · ∩Vn

and that (21.31) is a special case of this formula.
Hint: Show that for every ω ∈Ω we have I{ω ∈V} = 1 −0n
j=1

1 −I{ω ∈Vj}

, where
V = ∪jVj.
Expand the product and take expectations.
Alternatively, use (21.31) and
induction.
Exercise 21.10 (de Caen’s Inequality). Let X be a RV taking values in the ﬁnite set X,
and let {Ai}i∈I be a ﬁnite family of subsets (not necessarily disjoint) of X:
Ai ⊆X,
i ∈I.
Deﬁne
Pr(Ai) ≜Pr[X ∈Ai],
i ∈I,
deg(x) ≜#{i ∈I : x ∈Ai},
x ∈X,
where # B denotes the cardinality of a set B.
(i) Show that
Pr
 .
i∈I
Ai

=

i∈I

x∈Ai
Pr[X = x]
deg(x)
.
(ii) Use the Cauchy-Schwarz Inequality to show that for every i ∈I,
) 
x∈Ai
Pr[X = x]
deg(x)
*) 
x∈Ai
Pr[X = x] deg(x)
*
≥
) 
x∈Ai
Pr[X = x]
*2
.
(iii) Use Parts (i) and (ii) to show that
Pr
 .
i∈I
Ai

≥

i∈I
	
x∈Ai Pr[X = x]

2

j∈I

x′∈Ai∩Aj Pr[X = x′].
21.8 Exercises
467
(iv) Conclude that
Pr
 .
i∈I
Ai

≥

i∈I
Pr(Ai)2

j∈I Pr(Ai ∩Aj).
This is de Caen’s Bound (de Caen, 1997).
Exercise 21.11 (Asymptotic Tightness of the Union Bound). Consider the hypothesis
testing problem of Section 21.6 when the prior is uniform and the mean vectors s1, . . . , sM
are distinct. Show that the Union Bound of (21.60) is asymptotically tight in the sense
that the limiting ratio of the RHS of (21.60) to the LHS tends to one as σ tends to zero.
Hint: Use Exercise 21.8.
Chapter 22
Suﬃcient Statistics
22.1
Introduction
In layman’s terms, a suﬃcient statistic for guessing M based on the observable Y
is a random variable or a collection of random variables that contains all the infor-
mation in Y that is relevant for guessing M. This is a particularly useful concept
when the suﬃcient statistic is more concise than the observables. For example, if
we observe the results of a thousand coin tosses Y1, . . . , Y1000 and we wish to test
whether the coin is fair or has a bias of 1/4, then a suﬃcient statistic turns out
to be the number of “heads”among the outcomes Y1, . . . , Y1000.1 Another example
was encountered in Section 20.12. There the observable was a two-dimensional
random vector, and the suﬃcient statistic summarized the information that was
relevant for guessing H in a scalar random variable; see (20.69).
In this chapter we provide a formal deﬁnition of suﬃcient statistics in the multi-
hypothesis setting and explore the concept in some detail. We shall see that our
deﬁnition is compatible with Deﬁnition 20.12.2, which we gave for the binary case.
We only address the case where the observations take value in the d-dimensional
Euclidean space Rd. Also, we only treat the case of guessing among a ﬁnite number
of alternatives. We thus consider a ﬁnite set of messages
M = {1, . . . , M},
(22.1)
where M ≥2, and we assume that associated with each message m ∈M is a density
fY|M=m(·) on Rd, i.e., a nonnegative Borel measurable function that integrates to
one.
The concept of suﬃcient statistics is deﬁned for the family of densities
fY|M=m(·),
m ∈M;
(22.2)
it is unrelated to a prior. But when we wish to use it in the context of hypothesis
testing we need to introduce a probabilistic setting. If, in addition to the family
{fY|M=m(·)}m∈M, we introduce a prior {πm}m∈M, then we can discuss the pair
1Testing whether a coin is fair or not is a more complicated hypothesis testing problem of a
kind that we shall not address. It falls under the category of “composite hypothesis testing.”
468
22.2 Deﬁnition and Main Consequence
469
(M, Y), where Pr[M = m] = πm, and where, conditionally on M = m, the dis-
tribution of Y is of density fY|M=m(·). Thus, once we have introduced a prior
{πm}m∈M we can, for example, discuss the density fY(·) of Y as in (21.11)
fY(y) =

m∈M
πm fY|M=m(y),
y ∈Rd,
(22.3)
and the conditional distribution of M conditional on Y = y as in (21.12)
Pr[M = m|Y = y] ≜
⎧
⎪
⎨
⎪
⎩
πm fY|M=m(y)
fY(y)
if fY(y) > 0,
1
M
otherwise,
m ∈M, y ∈Rd.
(22.4)
22.2
Deﬁnition and Main Consequence
In this section we shall deﬁne suﬃcient statistics for a family of densities (22.2).
We shall then state the main result about this notion, namely, that there is no loss
of optimality in basing one’s guess on a suﬃcient statistic.
Very roughly, T(·) (or sometimes T(Y)) forms a suﬃcient statistic for guessing M
based on Y if there exists a black box that, when fed T(yobs) (but not yobs) and
any prior {πm} on M produces the a posteriori distribution of M given Y = yobs.
For technical reasons we make two exceptions. While the black box must always
produce a probability vector, we only require that this vector be the a posteriori
distribution of M given Y = yobs for observations yobs that satisfy

m∈M
πm fY|M=m(yobs) > 0
(22.5)
and that lie outside some prespeciﬁed set Y0 ⊂Rd of Lebesgue measure zero. Thus,
if yobs is in Y0 or if (22.5) is violated, then the output of the black box can be any
probability vector. The exception set Y0 is not allowed to depend on {πm}. Since
it is of Lebesgue measure zero, the conditional probability that the observation Y
lies in Y0 is zero:
Pr

Y ∈Y0
 M = m

= 0,
m ∈M.
(22.6)
Note that the black box need not indicate whether yobs is in Y0 and/or whether
(22.5) holds. Figure 22.1 depicts such a black box.
Deﬁnition 22.2.1 (Suﬃcient Statistics for M Densities). We say that a mapping
T : Rd →Rd′ forms a suﬃcient statistic for the densities fY|M=1(·), . . . , fY|M=M(·)
on Rd if it is Borel measurable and if for some Y0 ⊂Rd of Lebesgue measure zero we
have that for every prior {πm} there exist M Borel measurable functions from Rd′
to [0, 1]
T(yobs) 	→ψm

{πm}, T(yobs)

,
m ∈M,
such that the vector

ψ1

{πm}, T(yobs)

, . . . , ψM

{πm}, T(yobs)
T
470
Suﬃcient Statistics
yobs
T(·)
T(yobs)

πm}m∈M
Black Box
	
ψ1

{πm} , T(yobs)

, . . . , ψM

{πm} , T(yobs)

T
Figure 22.1:
A black box that when fed any prior {πm} and T(yobs) (but
not the observation yobs directly) produces a probability vector that is equal to
(Pr[M = 1|Y = yobs], . . . , Pr[M = M|Y = yobs])T whenever both the condition

m∈M πm fY |M=m(yobs) > 0 and the condition yobs /∈Y0 are satisﬁed.
is a probability vector and such that this probability vector is equal to

Pr[M = 1|Y = yobs], . . . , Pr[M = M|Y = yobs]
T
(22.7)
whenever both the condition yobs /∈Y0 and the condition
M

m=1
πm fY|M=m(yobs) > 0
(22.8)
are satisﬁed. Here (22.7) is computed for M having the prior {πm} and for the
conditional law of Y given M corresponding to the given densities.
The main result regarding suﬃcient statistics is that if T(·) forms a suﬃcient
statistic, then—even if the transformation T(·) is not reversible—there is no loss
of optimality in basing one’s guess on T(Y).
Proposition 22.2.2 (Guessing Based on T(Y) Is Optimal). If T : Rd →Rd′
is a suﬃcient statistic for the M densities {fY|M=m(·)}m∈M, then, given any
prior {πm}, there exists an optimal decision rule that bases its decision on T(Y).
Proof. To prove the proposition we shall exhibit a decision rule that is based
on T(Y) and that mimics the MAP rule based on Y. Since the latter is optimal
(Theorem 21.3.3), our proposed rule must also be optimal. Let {ψm(·)} be as in
Deﬁnition 22.2.1. Given Y = yobs, the proposed decoder considers the set of all
messages ˜m satisfying
ψ ˜m

{πm}, T(yobs)

= max
m′∈M ψm′
{πm}, T(yobs)

(22.9)
and picks uniformly at random from this set.
We next argue that this decision rule is optimal. To that end we shall show that,
with probability one, this guessing rule is the same as the MAP rule for guessing M
based on Y. Indeed, the guess produced by this rule is identical to the one produced
22.3 Equivalent Conditions
471
by the MAP rule whenever yobs satisﬁes (22.8) and lies outside Y0.
Since the
probability that Y satisﬁes (22.8) is, by (21.13), one, and since the probability
that Y is outside Y0 is, by (22.6), also one, it follows from Corollary 21.5.2 that
the probability that Y satisﬁes both (22.8) and the condition Y /∈Y0 is also one.
Thus, the proposed guessing rule, which bases its decision only on T(yobs) and
on the prior has the same performance as the (optimal) MAP decision rule for
guessing M based on Y.
22.3
Equivalent Conditions
In this section we derive a number of important equivalent deﬁnitions for suﬃcient
statistics. These will further clarify the concept and will also be useful in identifying
suﬃcient statistics. We shall try to state the theorems rigorously, but our proofs
will be mostly heuristic. Rigorous proofs require some Measure Theory that we
do not wish to assume. For a rigorous measure-theoretic treatment of this topic
see (Halmos and Savage, 1949), (Lehmann and Romano, 2005, Section 2.6), or
(Billingsley, 1995, Section 34).2
22.3.1
The Factorization Theorem
The following characterization is useful because it is purely algebraic. It explores
the form that the densities {fY|M=m(·)} must have for T(Y) to form a suﬃcient
statistic. Roughly speaking, T(·) is suﬃcient if the densities in the family all have
the form of a product of two functions, where the ﬁrst function depends on the
message and on T(y), and where the second function does not depend on the
message but may depend on y. We allow, however, an exception set Y0 ⊂Rd of
Lebesgue measure zero, so we only require that for every m ∈M
fY|M=m(y) = gm

T(y)

h(y),
y /∈Y0.
(22.10)
Note that if such a factorization exists, then it also exists with the additional
requirement that the functions be nonnegative. Indeed, if (22.10) holds, then by
the nonnegativity of the densities
fY|M=m(y) =
fY|M=m(y)

=
gm

T(y)

h(y)
,
y /∈Y0
=
gm

T(y)
 |h(y)|,
y /∈Y0,
thus yielding a factorization with the nonnegative functions

y 	→
gm

T(y)

m∈M
and
y 	→|h(y)|.
Limiting ourselves to nonnegative factorizations, as we henceforth shall, is helpful
in manipulating inequalities where multiplication by negative numbers requires
2Our setting is technically easier because we only consider the case where M is ﬁnite and
because we restrict the observation space to Rd.
472
Suﬃcient Statistics
changing the direction of the inequality. For our setting the Factorization Theorem
can be stated as follows.3
Theorem 22.3.1 (The Factorization Theorem). A Borel measurable function
T : Rd →Rd′ forms a suﬃcient statistic for the M densities {fY|M=m(·)}m∈M
on Rd if, and only if, there exists a set Y0 ⊂Rd of Lebesgue measure zero and non-
negative Borel measurable functions g1, . . . , gM : Rd′ →[0, ∞) and h: Rd →[0, ∞)
such that for every m ∈M
fY|M=m(y) = gm

T(y)

h(y),
y ∈Rd \ Y0.
(22.11)
Proof. We begin by showing that if T(·) is a suﬃcient statistic then there exists a
factorization of the form (22.11). Let the set Y0 and the functions {ψm(·)} be as in
Deﬁnition 22.2.1. Pick some ˜π1, . . . , ˜πM > 0 that sum to one, e.g., ˜πm = 1/M for
all m ∈M, and let M be of the prior {˜πm}, so Pr[M = m] = ˜πm for all m ∈M.
Let the conditional law of Y given M be as speciﬁed by the given densities so, in
particular,
fY(y) =

m∈M
˜πm fY|M=m(y),
y ∈Rd.
(22.12)
Since {˜πm} are strictly positive, it follows from (22.12) that

fY(y) = 0

=⇒

fY|M=m(y) = 0,
m ∈M

.
(22.13)
(The only way the sum of nonnegative numbers can be zero is if they are all zero.
Thus, fY(y) = 0 always implies that all the terms {˜πm fY|M=m(y)} are zero. But
if {˜πm} are strictly positive, then this implies that all the terms {fY|M=m(y)} are
zero.)
By the deﬁnition of the functions {ψm(·)} and of the conditional probability (22.4),
we have for every m ∈M
ψm

˜π1, . . . , ˜πM, T(yobs)

= ˜πm fY|M=m(yobs)
fY(yobs)
,

yobs /∈Y0 and fY(yobs) > 0

.
(22.14)
We next argue that the densities factorize as
fY|M=m(y) =
1
˜πm
ψm

˜π1, . . . , ˜πM, T(y)




gm(T (y))
fY(y)
  
h(y)
,
y ∈Rd \ Y0.
(22.15)
This can be argued as follows. If fY(y) is greater than zero, then (22.15) follows
directly from (22.14). And if fY(y) is equal to zero, then the RHS of (22.15) is
equal to zero and, by (22.13), the LHS is also equal to zero.
3A diﬀerent, perhaps more elegant, way to state the theorem is in terms of probability dis-
tributions. Let Pm be the probability distribution on Rd corresponding to M = m, where m
is in the ﬁnite set M. Assume that {Pm} are dominated by the σ-ﬁnite measure μ. Then the
Borel measurable mapping T : Rd →Rd′ forms a suﬃcient statistic for the family {Pm} if, and
only if, there exists a Borel measurable nonnegative function h(·) from Rd to R, and M nonneg-
ative, Borel measurable functions gm(·) from Rd′ to R such that for each m ∈M the function
y →gm(T(y)) h(y) is a version of the Radon-Nikodym derivative dPm/ dμ of Pm with respect
to μ; see (Billingsley, 1995, Theorem 34.6) and (Lehmann and Romano, 2005, Corollary 2.6.1).
22.3 Equivalent Conditions
473
We next prove that if the densities factorize as in (22.11), then T(·) forms a suﬃ-
cient statistic. That is, we show how using the factorization (22.11) we can design
the desired black box. The inputs to the black box are the prior {πm} and T(y).
The black box considers the vector

π1 g1

T(y)

, . . . , πM gM

T(y)
T
.
(22.16)
If all its components are zero, then the black box produces the uniform distribution
(or any other distribution of the reader’s choice). Otherwise, it produces the above
vector but normalized to sum to one. Thus, if we denote by ψm(π1, . . . , πM, T(y))
the probability that the black box assigns to m when fed π1, . . . , πM and T(y),
then
ψm

π1, . . . , πM, T(y)

≜
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
M
if
M

m′=1
πm′ gm′
T(y)

= 0,
πm gm

T(y)


m′∈M πm′ gm′
T(y)

otherwise.
(22.17)
To verify that ψm(π1, . . . , πM, T(y)) = Pr[M = m|Y = y] whenever y is such that
y /∈Y0 and (22.8) holds, we ﬁrst note that, by the factorization (22.11),

fY(y) > 0 and y /∈Y0

=⇒
	
h(y)
M

m′=1
πm′ gm′
T(y)

> 0

,
so

fY(y) > 0 and y /∈Y0

=⇒
	
h(y) > 0
and
M

m′=1
πm′ gm′
T(y)

> 0

.
(22.18)
Consequently, if y /∈Y0 and if (22.8) holds, then by (22.18) & (22.17)

ψ1

π1, . . . , πM, T(y)

, . . . , ψM

π1, . . . , πM, T(y)
T
is equal to the vector in (22.16) but scaled so that its components add to one. But
the a posteriori probability vector is also a scaled version of (22.16) (scaled by
h(y)/fY(y)) that sums to one. Thus, if y /∈Y0 and (22.8) holds, then the vector
produced by the black box is identical to the a posteriori distribution vector.
22.3.2
Pairwise suﬃciency
We next clarify the connection between suﬃcient statistics for binary hypothesis
testing and for multi-hypothesis testing. We show that T(Y) forms a suﬃcient
statistic for the family of densities {fY|M=m(·)}m∈M if, and only if, for every pair
of messages m′ ̸= m′′ in M we have that T(Y) forms a suﬃcient statistic for the
densities fY|M=m′(·) and fY|M=m′′(·).
474
Suﬃcient Statistics
One part of this statement is trivial, namely, that if T(·) is suﬃcient for the family
{fY|M=m(·)}m∈M then it is also suﬃcient for any pair. Indeed, by the Factoriza-
tion Theorem (Theorem 22.3.1), the suﬃciency of T(·) for the family implies the
existence of a set of Lebesgue measure zero Y0 ⊂Rd and functions {gm}m∈M, h
such that for all y ∈Rd \ Y0
fY|M=m(y) = gm

T(y)

h(y),
m ∈M.
(22.19)
In particular, if we limit ourselves to m′, m′′ ∈M then for y /∈Y0
fY|M=m′(y) = gm′
T(y)

h(y),
fY|M=m′′(y) = gm′′
T(y)

h(y),
which, by the Factorization Theorem, implies the suﬃciency of T(·) for the pair of
densities fY|M=m′(·), fY|M=m′′(·).
The nontrivial part of the proposition is that pairwise suﬃciency implies suﬃciency.
Even this is quite easy when the densities are all strictly positive. It is a bit more
tricky without this assumption.4
Proposition 22.3.2 (Pairwise Suﬃciency Implies Suﬃciency). Consider M den-
sities {fY|M=m(·)}m∈M on Rd, and assume that T : Rd →Rd′ forms a suﬃcient
statistic for every pair of densities fY|M=m′(·), fY|M=m′′(·), where m′ ̸= m′′ are
both in M. Then T(·) is a suﬃcient statistic for the M densities {fY|M=m(·)}m∈M.
Proof. To prove that T(·) forms a suﬃcient statistic for {fY|M=m(·)}M
m=1 we shall
describe an algorithm (black box) that when fed any prior {πm} and T(yobs) (but
not yobs) produces an M-dimensional probability vector that is equal to the a
posteriori probability vector

Pr

M = 1
 Y = yobs

, . . . , Pr

M = M
 Y = yobs
T
(22.20)
whenever yobs ∈Rd is such that
yobs /∈Y0
and
M

m=1
πmfY|M=m(yobs) > 0,
(22.21)
where Y0 is a subset of Rd that does not depend on the prior {πm} and that is of
Lebesgue measure zero.
To describe the algorithm we ﬁrst use the Factorization Theorem (Theorem 22.3.1)
to recast the proposition’s hypothesis as saying that for every pair m′ ̸= m′′ in M
there exists a set Y(m′,m′′)
0
⊂Rd of Lebesgue measure zero and there exist non-
negative functions g(m′,m′′)
m′
, g(m′,m′′)
m′′
: Rd′ →R and h(m′,m′′) : Rd →R such that
fY|M=m′(y) = g(m′,m′′)
m′

T(y)

h(m′,m′′)(y),
y ∈Rd \ Y(m′,m′′)
0
,
(22.22a)
4This result does not extend to the case where the random variable M can take on inﬁnitely
many values.
22.3 Equivalent Conditions
475
fY|M=m′′(y) = g(m′,m′′)
m′′

T(y)

h(m′,m′′)(y),
y ∈Rd \ Y(m′,m′′)
0
.
(22.22b)
Let
Y0 =

m′,m′′∈M
m′̸=m′′
Y(m′,m′′)
0
,
(22.23)
and note that, being the union of a ﬁnite number of sets of Lebesgue measure zero,
Y0 is of Lebesgue measure zero.
We now use the above functions g(m′,m′′)
m′
, g(m′,m′′)
m′′
to describe the algorithm. Note
that yobs is never fed directly to the algorithm; only T(yobs) is used. Let the prior
πm = Pr[M = m],
m ∈M
(22.24)
be given, and assume without loss of generality that it is nondegenerate in the
sense that
πm > 0,
m ∈M.
(22.25)
(If that is not the case, we can set the black box to produce 0 in the coordinates
of the output vector corresponding to messages of prior probability zero and then
proceed to ignore such messages.) Let yobs ∈Rd be arbitrary.
There are two phases to the algorithm. In the ﬁrst phase the algorithm produces
some m∗∈M whose a posteriori probability is guaranteed to be positive when-
ever (22.21) holds. In fact, if (22.21) holds, then no message has an a posteriori
probability higher than that of m∗(but this is immaterial to us because we are
not content with showing that from T(yobs) we can compute the message that a
posteriori has the highest probability; we want to be able to compute the entire
a posteriori probability vector). In the second phase the algorithm uses m∗to
compute the desired a posteriori probability vector.
The ﬁrst phase of the algorithm runs in M steps. In Step 1 we set m[1] = 1. In
Step 2 we set
m[2] =
⎧
⎪
⎨
⎪
⎩
1
if π1 g(1,2)
1

T(yobs)

π2 g(1,2)
2

T(yobs)
 > 1,
2
otherwise.
And in Step ν for ν ∈{2, . . . , M} we set
m[ν] =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
m[ν −1]
if
πm[ν−1] g(m[ν−1],ν)
m[ν−1]

T(yobs)

πν g(m[ν−1],ν)
ν

T(yobs)

> 1,
ν
otherwise.
(22.26)
Here we use the convention that a/0 = +∞whenever a > 0 and that 0/0 = 1. We
complete the ﬁrst phase by setting
m∗= m[M].
(22.27)
In the second phase we compute the vector
α[m] = πm g(m,m∗)
m

T(yobs)

πm∗g(m,m∗)
m∗

T(yobs)
,
m ∈M.
(22.28)
476
Suﬃcient Statistics
If at least one of the components of α[·] is +∞, then we produce as the algorithm’s
output the uniform distribution on M. (The output corresponding to this case is
immaterial because it will turn out that this case is only possible if yobs is such
that either yobs ∈Y0 or 
m πmfY|M=m(yobs) = 0, in which case the algorithm’s
output is not required to be equal to the a posteriori distribution.) Otherwise, the
algorithm’s output is the vector
-
α[1]
M
ν=1 α[ν]
, . . . ,
α[M]
M
ν=1 α[ν]
.T
.
(22.29)
Having described the algorithm, we now proceed to prove that it produces the
a posteriori probability vector whenever (22.21) holds. We need to show that if
(22.21) holds then
Pr[M = m|Y = yobs] =
α[m]
M
ν=1 α[ν]
,
m ∈M.
(22.30)
Since there is nothing to prove if (22.21) does not hold, we shall henceforth assume
for the rest of the proof that it does. In this case we have by (22.4)
Pr[M = m|Y = yobs] = πm fY|M=m(yobs)
fY(yobs)
,
m ∈M.
(22.31)
We shall prove (22.30) in two steps. In the ﬁrst step we show that the result m∗
of the algorithm’s ﬁrst phase satisﬁes
Pr[M = m∗|Y = yobs] > 0.
(22.32)
To establish (22.32) we shall prove the stronger statement that
Pr

M = m∗ Y = yobs

= max
m∈M Pr

M = m
 Y = yobs

.
(22.33)
This latter statement follows from the more general claim that for any ν ∈M (and
not only for ν = M) we have, subject to (22.21),
Pr

M = m[ν]
 Y = yobs

= max
1≤m≤ν Pr

M = m
 Y = yobs

.
(22.34)
For ν = 1, Statement (22.34) is trivial. For 2 ≤ν ≤M, (22.34) follows from
Pr

M = m[ν]
 Y = yobs

=
max
'
Pr

M = ν
 Y = yobs

, Pr

M = m[ν −1]
 Y = yobs
(
,
(22.35)
which we now prove. We prove (22.35) by considering two cases separately depend-
ing on whether Pr[M = ν |Y = yobs] and Pr[M = m[ν−1]|Y = yobs] are both zero
or not. In the former case there is nothing to prove because (22.35) holds irrespec-
tive of whether (22.26) results in m[ν] being set to ν or to m[ν −1]. In the latter
case we have by (22.31) and (22.25) that fY|M=ν(yobs) and fY|M=m[ν−1](yobs)
22.3 Equivalent Conditions
477
are not both zero. Consequently, by (22.22), in this case h(m[ν−1],ν)(yobs) is not
only nonnegative but strictly positive. It follows that the choice (22.26) guarantees
(22.35) because
πm[ν−1] g(m[ν−1],ν)
m[ν−1]

T(yobs)

πν g(m[ν−1],ν)
ν

T(yobs)

=
πm[ν−1] g(m[ν−1],ν)
m[ν−1]

T(yobs)

h(m[ν−1],ν)(yobs)
πν g(m[ν−1],ν)
ν

T(yobs)

h(m[ν−1],ν)(yobs)
=
πm[ν−1] g(m[ν−1],ν)
m[ν−1]

T(yobs)

h(m[ν−1],ν)(yobs)/fY(yobs)
πν g(m[ν−1],ν)
ν

T(yobs)

h(m[ν−1],ν)(yobs)/fY(yobs)
= Pr

M = m[ν −1]
 Y = yobs

Pr[M = ν]|Y = yobs]
,
where the ﬁrst equality follows because h(m[ν−1],ν)(yobs) is strictly positive; the
second because in this part of the proof we are assuming (22.21); and where the
last equality follows from (22.22) and (22.31).
This establishes (22.35), which
implies (22.34), which in turn implies (22.33), which in turn implies (22.32), and
thus concludes the proof of the ﬁrst step.
In the second step of the proof we use (22.32) to establish (22.30).
This is
straightforward because, in view of (22.31), we have that (22.32) implies that
fY|M=m∗(yobs) > 0 so, by (22.22b), we have that
h(m,m∗)(yobs) > 0,
m ∈M,
g(m,m∗)
m∗
(yobs) > 0,
m ∈M.
Consequently
α[m] = πm g(m,m∗)
m

T(yobs)

πm∗g(m,m∗)
m∗

T(yobs)

= πm g(m,m∗)
m

T(yobs)

h(m,m∗)(yobs)
πm∗g(m,m∗)
m∗

T(yobs)

h(m,m∗)(yobs)
= πm g(m,m∗)
m

T(yobs)

h(m,m∗)(yobs)/fY(yobs)
πm∗g(m,m∗)
m∗

T(yobs)

h(m,m∗)(yobs)/fY(yobs)
= Pr[M = m|Y = yobs]
Pr[M = m∗|Y = yobs],
from which (22.30) follows by (22.32).
22.3.3
Markov Condition
We now characterize suﬃcient statistics using Markov chains and conditional in-
dependence. These concepts were introduced in Section 20.11. The key result we
ask the reader to recall is Theorem 20.11.3. We rephrase it for our present setting
as follows.
478
Suﬃcient Statistics
Proposition 22.3.3. The statement that M⊸−T(Y)⊸−Y forms a Markov chain
is equivalent to each of the following statements:
(a) The conditional distribution of M given

T(Y), Y

is the same as given T(Y).
(b) M and Y are conditionally independent given T(Y).
(c) The conditional distribution of Y given

M,T(Y)

is the same as given T(Y).
Statement (a) can also be written as:
(a’) The conditional distribution of M given Y is the same as given T(Y).
Indeed, the conditional distribution of any random variable—in particular M—
given

T(Y), Y

is the same as given Y only, because T(Y) carries no information
that is not in Y.
Statement (a’) can be rephrased as saying that the conditional distribution of M
given Y can be computed from T(Y). Since this is the key requirement of suﬃcient
statistics, we obtain:
Proposition 22.3.4. A Borel measurable function T : Rd →Rd′ forms a suﬃcient
statistic for the M densities {fY|M=m(·)}m∈M if, and only if, for any prior {πm}
M⊸−T(Y)⊸−Y
(22.36)
forms a Markov chain.
Proof. The proof of this proposition is omitted. It is not diﬃcult, but it requires
some measure-theoretic tools.5
Using Proposition 22.3.4 and Proposition 22.3.3 (cf. (b)) we obtain that a Borel
measurable function T(·) forms a suﬃcient statistic for guessing M based on Y if,
and only if, for any prior {πm} on M, the message M and the observation Y are
conditionally independent given T(Y).
We next explore the implications of Proposition 22.3.4 and the equivalence of the
Markovity M⊸−T(Y)⊸−Y and Statement (c) in Proposition 22.3.3. These imply
that a Borel measurable function T(·) forms a suﬃcient statistic if, and only if, the
conditional distribution of Y given

T(Y), M = m

is the same for all m ∈M. Or,
in other words, a Borel measurable function T(·) forms a suﬃcient statistic if, and
only if, the conditional distribution of Y given T(Y) does not depend on which
of the densities in {fY|M=m(·)} governs the law of Y. This characterization has
interesting implications regarding the possibility of simulating observables. These
implications are explored next.
5If T(·) forms a suﬃcient statistic, then by Deﬁnition 22.2.1 ψm {πm}, T(Y)

is a version of
the conditional probability that M = m conditional on the σ-algebra generated by Y, and it is
also measurable with respect to the σ-algebra generated by T(Y). The reverse direction follows
from (Lehmann and Romano, 2005, Lemma 2.3.1).
22.3 Equivalent Conditions
479
T(yobs)
PY|T (Y)=T (yobs)
˜Y

T(yobs), Θ

Given Rule for Guessing
M based on Y
Random Number
Generator
Guess
Θ
Figure 22.2: If T(Y) forms a suﬃcient statistic for guessing M based on Y, then—
even though Y cannot typically be recovered from T(Y)—the performance of any
given detector based on Y can be achieved based on T(Y) and a local random
number generator as follows. Using T(yobs) and local randomness Θ, one produces
a ˜Y whose conditional law given M = m is the same as that of Y, for each m ∈M.
One then feeds ˜Y to the given detector.
22.3.4
Simulating Observables
For T(Y) to form a suﬃcient statistic, we do not require that T(·) be invertible, i.e.,
that Y be recoverable from T(Y). Indeed, the notion of suﬃcient statistics is most
useful when this transformation is not invertible, in which case T(·) “summarizes”
the information in the observation Y that is needed for guessing M. Nevertheless,
as we shall next show, if T(Y) forms a suﬃcient statistic, then from T(Y) we
can produce (using a local random number generator) a vector ˜Y that appears
statistically like Y in the sense that the conditional law of ˜Y given M is identical
to the conditional law of Y given M.
To expand on this, we ﬁrst explain what we mean by “we can produce . . . ˜Y” and
then elaborate on the consequences of the vector ˜Y having the same conditional
law given M = m as Y. By “producing” ˜Y from T(Y) we mean that ˜Y is the
result of processing T(Y) with respect to M. Stated diﬀerently, for every t ∈Rd′
there corresponds a probability distribution P ˜Y|t (not dependent on m) that can
be used to generate ˜Y as follows: having observed T(yobs), we use a local random
number generator to generate the vector ˜Y according to the distribution P ˜Y|t,
where t = T(yobs); see Figure 22.2.
By ˜Y appearing statistically the same as Y we mean that the conditional law of ˜Y
given M = m is the same as that of Y, i.e., is of density fY|M=m(·). Consequently,
anything that can be learned about M from Y can also be learned about M from ˜Y.
Also, any guessing device that was designed to guess M based on the input Y will
yield the same probability of error when, instead of being fed Y, it is fed ˜Y. Thus,
if p(error|M = m) is the conditional error probability associated with a guessing
device that is fed Y, then it is also the conditional probability of error that will be
incurred by this device if, rather than Y, it is fed ˜Y; see Figure 22.2.
480
Suﬃcient Statistics
Before stating this as a theorem, let us consider the following simple example.
Suppose that our observation consists of d random variables Y1, . . . , Yd and that,
conditional on H = 0, these random variables are IID Bernoulli(p0), i.e., they
each take on the value 1 with probability p0 and the value 0 with probability
1 −p0. Conditional on H = 1, these d random variables are IID Bernoulli(p1).
Here 0 < p0, p1 < 1 and p0 ̸= p1. Consequently, the conditional probability mass
functions are
PY1,...,Yd|H=0(y1, . . . , yd) =
d
@
j=1

pyj
0 (1 −p0)1−yj
= p
d
j=1 yj
0
(1 −p0)d−d
j=1 yj,
and
PY1,...,Yd|H=1(y1, . . . , yd) = p
d
j=1 yj
1
(1 −p1)d−d
j=1 yj,
so T(Y1, . . . , Yd) ≜d
j=1 Yj forms a suﬃcient statistic by the Factorization The-
orem.6
From T(y1, . . . , yd) one cannot recover the sequence y1, . . . , yd. Indeed,
specifying that T(y1, . . . , yd) = t does not determine which of the random vari-
ables is one; it only determines how many of them are one. There are thus
d
t

possible outcomes (y1, . . . yd) that are consistent with T(y1, . . . , yd) being equal to t.
We leave it to the reader to verify that if we use a local random number genera-
tor to pick one of these outcomes uniformly at random then the result ( ˜Y1, . . . ˜Yd)
will have the same conditional law given H as (Y1, . . . , Yd). We do not, of course,
guarantee that ( ˜Y1, . . . ˜Yd) be identical to (Y1, . . . , Yd). (The transformation T(·)
is, after all, not reversible.)
For additional insight let us consider our example of (20.66). For T(y1, y2) = y2
1+y2
2
we can generate ˜Y from a uniform random variable Θ ∼U ([0, 1)) as
˜Y1 =

T(Y) cos

2πΘ

˜Y2 =

T(Y) sin

2πΘ

.
That is, after observing T(yobs) = t, we generate
 ˜Y1, ˜Y2

uniformly over the tuples
that are at radius
√
t from the origin.
This last example also demonstrates the diﬃculty of stating the result. The random
vector Y in this example has a density, both when conditioned on H = 0 and when
conditioned on H = 1. The same applies to the random variable T(Y). However,
the distribution that is used to generate ˜Y from T(Y) is neither discrete nor has
a density. All its mass is concentrated on the circle of radius
√
t, so it cannot have
a density, and it is uniformly distributed over that circle, so it cannot be discrete.
Theorem 22.3.5 (Simulating the Observables from the Suﬃcient Statistic). Let
T : Rd →Rd′ be Borel measurable and let fY|M=1(·), . . . , fY|M=M(·) be M densities
on Rd. Then the following two statements are equivalent:
(a) T(·) forms a suﬃcient statistic for the given densities.
6For illustration purposes we are extending the discussion here to discrete distributions.
22.4 Identifying Suﬃcient Statistics
481
(b) To every t in Rd′ there corresponds a distribution on Rd such that the fol-
lowing holds: for every m ∈{1, . . . , M}, if Y = yobs is generated according
to the density fY|M=m(·) and if the random vector ˜Y is then generated ac-
cording to the distribution corresponding to t, where t = T(yobs), then ˜Y is
of density fY|M=m(·).
Proof. For a measure-theoretic statement and proof see (Lehmann and Romano,
2005, Theorem 2.6.1). Here we only present some intuition. Ignoring some of the
technical details, the proof is very simple. The suﬃciency of T(·) is equivalent
to M⊸−T(Y)⊸−Y forming a Markov chain for every prior on M. This latter
condition is equivalent by Proposition 22.3.3 (cf. (c)) to the conditional distribution
of Y given

T(Y), M

being the same as given T(Y) only. This latter condition
is equivalent to the conditional distribution of Y given T(Y) not depending on
which density in the family {fY|M=m(·)}m∈M was used to generate Y, i.e., to the
existence of a conditional distribution of Y given T(Y) that does not depend on
m ∈M.
22.4
Identifying Suﬃcient Statistics
Often a suﬃcient statistic can be identiﬁed without having to compute and factorize
the conditional densities of the observation. A number of such cases are described
in this section.
22.4.1
Invertible Transformation
We begin by showing that, ignoring some technical details, any invertible transfor-
mation forms a suﬃcient statistic. It may not be a particularly helpful suﬃcient
statistic because it does not “summarize” the observation, but it is a suﬃcient
statistic nonetheless.
Proposition 22.4.1 (Reversible Transformations Yield Suﬃcient Statistics). If
T : Rd →Rd′ is Borel measurable with a Borel measurable inverse, then T(·) forms
a suﬃcient statistic for guessing M based on Y.
Proof. We provide two proofs. The ﬁrst uses the deﬁnition. We need to verify that
from T(yobs) one can compute the conditional distribution of M given Y = yobs.
This is obvious because if t = T(yobs), then one can compute Pr[M = m|Y = yobs]
from t by ﬁrst applying the inverse T −1(t) to recover yobs and by then substituting
the result in the expression for Pr[M = m|Y = yobs] (22.4).
A second proof can be based on Proposition 22.3.4. We need to verify that for any
prior {πm}
M⊸−T(Y)⊸−Y
forms a Markov chain. To this end we note that, by Proposition 22.3.3, it suﬃces
to verify that M and Y are conditionally independent given T(Y). This is clear
because the invertibility of T(·) guarantees that, conditional on T(Y), the random
vector Y is deterministic and hence independent of any random variable and a
fortiori of M.
482
Suﬃcient Statistics
22.4.2
A Suﬃcient Statistic Is Computable from the Statistic
Intuitively, we think about T(·) as forming a suﬃcient statistic if T(Y) contains
all the information about Y that is relevant to guessing M. For this intuition to
make sense it had better be the case that if T(·) forms a suﬃcient statistic for
guessing M based on Y, and if T(Y) is computable from S(Y), then S(·) also
forms a suﬃcient statistic. Fortunately, this is so:
Proposition 22.4.2. Suppose that a Borel measurable mapping T : Rd →Rd′ forms
a suﬃcient statistic for the M densities {fY|M=m(·)}m∈M on Rd. Let the mapping
S : Rd →Rd′′ be Borel measurable. If T(·) can be written as the composition ψ ◦S
of S with some Borel measurable function ψ: Rd′′ →Rd′, then S(·) also forms a
suﬃcient statistic for these densities.
Proof. We need to show that Pr[M = m|Y = yobs] is computable from S(yobs).
This follows because, by assumption, T(yobs) is computable from S(yobs) and
because the suﬃciency of T(·) implies that Pr[M = m|Y = yobs] is computable
from T(yobs).
22.4.3
Establishing Suﬃciency in Two Steps
It is sometimes convenient to establish suﬃciency in two steps: in the ﬁrst step
we establish that T(Y) is suﬃcient for guessing M based on Y, and in the second
step we establish that S(T) is suﬃcient for guessing M based on T(Y).
The
next proposition demonstrates that it then follows that S(T(Y)) forms a suﬃcient
statistic for guessing M based on Y.
Proposition 22.4.3. If T : Rd →Rd′ forms a suﬃcient statistic for the M densities
{fY|M=m(·)}m∈M and if S : Rd′ →Rd′′ forms a suﬃcient statistic for the corre-
sponding family of densities of T(Y), then the composition S ◦T forms a suﬃcient
statistic for the densities {fY|M=m(·)}m∈M.
Proof. We shall establish the suﬃciency of S◦T by proving that for any prior {πm}
M⊸−S

T(Y)

⊸−Y.
This follows because for every m ∈M and every yobs ∈Rd
Pr

M = m
 S

T(Y)

= S

T(yobs)

= Pr

M = m
 T(Y) = T(yobs)

= Pr

M = m
 Y = yobs

,
where the ﬁrst equality follows from the suﬃciency of S(T(Y)) for guessing M
based on T(Y), and where the second equality follows from the suﬃciency of T(Y)
for guessing M based on Y.
22.4.4
Guessing whether M Lies in a Given Subset of M
We motivate the next result with the following example, which arises in the detec-
tion of PAM signals in white Gaussian noise (Section 28.3 ahead). Suppose that
22.4 Identifying Suﬃcient Statistics
483
the distribution of the observable Y is determined by the value of a k-tuple of bits
(D1, . . . , Dk). Thus, to each outcome (d1, . . . , dk) ∈{0, 1}k among the 2k diﬀer-
ent outcomes of (D1, . . . , Dk) there corresponds a distribution on Y of some given
density fY|D1=d1,...,Dk=dk(·). Suppose now that T(·) forms a suﬃcient statistic for
this family of M = 2k densities. The result we next describe guarantees that T(·)
is also suﬃcient for the binary hypothesis testing problem of guessing whether a
speciﬁc bit Dj is zero or one. More precisely, we shall show that if {π(d1,...,dk)}
is any nondegenerate prior on the 2k diﬀerent k-tuples of bits, then T(·) forms a
suﬃcient statistic for the two densities
γ0

(d1,...,dk)
dj=0
π(d1,...,dk) fY|D1=d1,...,Dk=dk(y), γ1

(d1,...,dk)
dj=1
π(d1,...,dk) fY|D1=d1,...,Dk=dk(y)
γ0 =
-

(d1,...,dk)
dj=0
π(d1,...,dk)
.−1
,
γ1 =
-

(d1,...,dk)
dj=1
π(d1,...,dk)
.−1
.
Proposition 22.4.4 (Guessing whether M Is in K). Let T : Rd →Rd′ form a
suﬃcient statistic for the M densities {fY|M=m(·)}m∈M. Let the set K ⊂M be a
nonempty strict subset of M. Let {πm} be a prior on M satisfying
0 <

m∈K
πm < 1.
Then T(·) forms a suﬃcient statistic for the two densities
y 	→γ0

m∈K
πm fY|M=m(y)
and
y 	→γ1

m/∈K
πm fY|M=m(y)
(22.37a)
γ0 =
	 
m∈K
πm

−1
γ1 =
	 
m/∈K
πm

−1
.
(22.37b)
Proof. From the Factorization Theorem it follows that the suﬃciency of T(·) for
the family {fY|M=m(·)}m∈M is equivalent to the condition that for every m ∈M
and for every y /∈Y0
fY|M=m(y) = gm

T(y)

h(y),
(22.38)
where the set Y0 ⊂Rd is of Lebesgue measure zero; where {gm(·)}m∈M are non-
negative Borel measurable functions from Rd′; and where h(·) is a nonnegative
Borel measurable function from Rd. Consequently,
γ0

m∈K
πm fY|M=m(y) = γ0

m∈K
πm gm

T(y)

h(y)
=
	
γ0

m∈K
πm gm

T(y)

h(y),
y /∈Y0,
(22.39a)
484
Suﬃcient Statistics
and
γ1

m/∈K
πm fY|M=m(y) = γ1

m/∈K
πm gm

T(y)

h(y)
=
	
γ1

m/∈K
πm gm

T(y)

h(y),
y /∈Y0.
(22.39b)
The factorization (22.39) of the densities in (22.37) proves that T(·) is also suﬃcient
for these two densities.
Note 22.4.5. The proposition also extends to more general partitions as follows.
Suppose that T(·) is suﬃcient for the family {fY|M=m(·)}m∈M. Let K1, . . . , Kκ be
disjoint nonempty subsets of M whose union is equal to M, and let the prior {πm}
be such that

m∈Kj
πm > 0,
j ∈{1, . . . , κ}.
Then T(·) is suﬃcient for the κ densities
y 	→γ1

m∈K1
πm fY|M=m(y), . . . , y 	→γκ

m∈Kκ
πm fY|M=m(y)
γj =
	 
m∈Kj
πm

−1
,
j ∈{1, . . . , κ}.
22.4.5
Conditionally Independent Observations
Our next result deals with a situation where we need to guess M based on two
observations: Y1 and Y2. We assume that T1(Y1) forms a suﬃcient statistic for
guessing M when only Y1 is observed, and that T2(Y2) forms a suﬃcient statistic
for guessing M when only Y2 is observed. It is tempting to conjecture that in
this case the pair (T1(Y1), T2(Y2)) must form a suﬃcient statistic for guessing M
when both Y1 and Y2 are observed. But, without additional assumptions, this is
not the case. An example where this fails can be constructed as follows. Let M
and Z be independent with M taking on the values 0 and 1 equiprobably and with
Z ∼N(0, 1). Suppose that Y1 = M +Z and that Y2 = Z. In this case the invertible
mapping T1(Y1) = Y1 forms a suﬃcient statistic for guessing M based on Y1 alone,
and the mapping T2(Y2) = 17 forms a suﬃcient statistic for guessing M based
on Y2 alone (because M and Z are independent). Nevertheless, the pair (Y1, 17) is
not suﬃcient for guessing M based on the pair (Y1, Y2). Basing one’s guess of M on
(Y1, 17) is not as good as basing it on the pair (Y1, Y2). (The reader is encouraged
to verify that Y1 −Y2 is suﬃcient for guessing M based on (Y1, Y2) and that M
can be guessed error-free from Y1 −Y2.)
The additional assumption we need is that Y1 and Y2 be conditionally independent
given M. (It would make no sense to assume that they are independent, because
they are presumably both related to M.) This assumption is valid in many appli-
cations. For example, it occurs when a signal is received at two diﬀerent antennas
with the additive noises in the two antennas being independent.
22.5 Suﬃcient Statistics for the M-ary Gaussian Problem
485
Proposition 22.4.6 (Conditionally Independent Observations). Let the mapping
T1 : Rd1 →Rd′
1 form a suﬃcient statistic for guessing M based on the observation
Y1 ∈Rd1, and let T2 : Rd2 →Rd′
2 form a suﬃcient statistic for guessing M based on
the observation Y2 ∈Rd2. If Y1 and Y2 are conditionally independent given M,
then the pair (T1(Y1), T2(Y2)) forms a suﬃcient statistic for guessing M based on
the pair (Y1, Y2).
Proof. The proof we oﬀer is based on the Factorization Theorem. The hypothesis
that T1 : Rd1 →Rd′
1 forms a suﬃcient statistic for guessing M based on the obser-
vation Y1 implies the existence of nonnegative functions

g(1)
m

m∈M and h(1) and
a subset Y(1)
0
⊂Rd1 of Lebesgue measure zero such that
fY1|M=m(y1) = g(1)
m

T1(y1)

h(1)(y1),
m ∈M, y1 /∈Y(1)
0 .
(22.40)
Similarly, the hypothesis that T2(·) is suﬃcient for guessing M based on Y2 im-
plies the existence of nonnegative functions

g(2)
m

m∈M and h(2) and a subset of
Lebesgue measure zero Y(2)
0
⊂Rd2 such that
fY2|M=m(y2) = g(2)
m

T2(y2)

h(2)(y2),
m ∈M, y2 /∈Y(2)
0 .
(22.41)
The conditional independence of Y1 and Y2 given M implies7
fY1,Y2|M=m(y1, y2) = fY1|M=m(y1) fY2|M=m(y2),
m ∈M, y1 ∈Rd1, y2 ∈Rd2.
(22.42)
Combining (22.40), (22.41), and (22.42), we obtain
fY1,Y2|M=m(y1, y2) = g(1)
m

T1(y1)

g(2)
m

T2(y2)




gm

T1(y1),T2(y2)

h(1)(y1) h(2)(y2)



h(y1,y2)
,
m ∈M, y1 /∈Y(1)
0 , y2 /∈Y(2)
0 .
(22.43)
The set of pairs (y1, y2) ∈Rd1 × Rd2 for which y1 is in Y(1)
0
and/or y2 is in Y(2)
0
is of Lebesgue measure zero, and consequently, the factorization (22.43) implies
that the pair

T1(Y1), T2(Y2)

forms a suﬃcient statistic for guessing M based on
(Y1, Y2).
22.5
Suﬃcient Statistics for the M-ary Gaussian Problem
We next revisit the multi-dimensional M-ary Gaussian hypothesis testing problem
of Section 21.6 with an eye to ﬁnding suﬃcient statistics. Starting from (21.51) we
7Technically speaking, this must only hold outside a set of Lebesgue measure zero, but we do
not want to make things even more cumbersome.
486
Suﬃcient Statistics
can express the conditional densities at every y ∈RJ as
fY|M=m(y)
=
J
@
j=1
1
√
2πσ2 exp
-
−

y(j) −s(j)
m
2
2σ2
.
= (2πσ2)−J
2 exp
-
−1
2σ2
J

j=1

y(j) −s(j)
m
2
.
= (2πσ2)−J
2 exp
	
−1
2σ2

∥y∥2 −2 ⟨y, sm⟩E + ∥sm∥2
= exp
	 1
σ2

⟨y, sm⟩E −1
2 ∥sm∥2
· (2πσ2)−J
2 exp
	
−1
2σ2 ∥y∥2

.
(22.44)
If we now deﬁne the mapping T : RJ →RM so that T(Y) will be the M-dimensional
random vector
T(Y) =

⟨Y, s1⟩E , . . . , ⟨Y, sM⟩E
T
(22.45)
whose m-th component T (m)(Y) is
T (m)(Y) = ⟨Y, sm⟩E ,
m ∈M,
then we can use (22.44) and the Factorization Theorem (Theorem 22.3.1) to es-
tablish that T(·) forms a suﬃcient statistic for guessing M based on Y. Indeed,
we can rewrite (22.44) as
fY|M=m(y) = gm

T(y)

h(y)
where
gm

T(y)

= exp
	 1
σ2

T (m)(y) −1
2 ∥sm∥2
and
h(y) = (2πσ2)−J
2 exp
	
−1
2σ2 ∥y∥2

.
(The suﬃciency of T(·) can also be demonstrated by ﬁrst showing pairwise suﬃ-
ciency and then invoking Proposition 22.3.2. To establish pairwise suﬃciency we
can use the expression for the LLR (20.83) to show that the LLR is computable
from T(y).)
If the vectors {sm} are linearly dependent, then we can ﬁnd a more succinct suf-
ﬁcient statistic. Indeed, if ˜s1, . . . ,˜sn are vectors in RJ whose span includes all the
vectors s1, . . . , sM, then for each m ∈M we can ﬁnd coeﬃcients α(1)
m , . . . , α(n)
m such
that
sm = α(1)
m ˜s1 + · · · + α(n)
m ˜sn,
m ∈M.
22.6 Irrelevant Data
487
Consequently,
⟨Y, sm⟩E = α(1)
m ⟨Y,˜s1⟩E + · · · + α(n)
m ⟨Y,˜sn⟩E ,
m ∈M,
and T(Y) is computable from
⟨Y,˜s1⟩E , . . . , ⟨Y,˜sn⟩E .
It thus follows from Proposition 22.4.2 that
y 	→

⟨y,˜s1⟩E , . . . , ⟨y,˜sn⟩E
T
forms a suﬃcient statistic for guessing M based on Y. We have thus established:
Proposition 22.5.1 (Multi-Dimensional M-ary Gaussian Problem: Suﬃciency).
Consider the setup of Section 21.6.1. If ˜s1, . . . ,˜sn ∈RJ are such that
sm ∈span (˜s1, . . . ,˜sn) ,
m ∈M,
then
y 	→

⟨y,˜s1⟩E , . . . , ⟨y,˜sn⟩E
T
forms a suﬃcient statistic for guessing M based on Y.
22.6
Irrelevant Data
Closely related to the notion of suﬃcient statistics is the notion of irrelevant data.
This notion is particularly useful when we think about the data as consisting of
two parts.
Heuristically speaking, we say that the second part of the data is
irrelevant for guessing M given the ﬁrst, if it adds no information about M that
is not already contained in the ﬁrst part. In such cases the second part of the
data can be ignored. It should be emphasized that the question whether a part
of the observation is irrelevant depends not only on its dependence on the random
variable to be guessed but also on the other part of the observation.
Deﬁnition 22.6.1 (Irrelevant Data). We say that R is irrelevant for guessing M
given Y, if Y forms a suﬃcient statistic for guessing M based on (Y, R).
Equivalently, R is irrelevant for guessing M given Y, if for any prior {πm} on M
M⊸−Y⊸−(Y, R),
(22.46)
i.e.,
M⊸−Y⊸−R.
(22.47)
Example 22.6.2. Let H take on the values 0 and 1, and assume that, conditional
on H = 0, the observation Y is N

0, σ2
0

and that, conditional on H = 1, it is
N

0, σ2
1

. Rather than thinking of this problem as a decision problem with a single
observation, let us think of it as a decision problem with two observations (Y1, Y2),
where Y1 is the absolute value of Y , and where Y2 is the sign of Y . Thus Y = Y1 Y2,
488
Suﬃcient Statistics
where Y1 ≥0 and Y2 ∈{+1, −1}. (The probability that Y = 0 is zero under each
hypothesis, so we need not deﬁne the sign of zero.) We now show that Y2 (= the
sign of Y ) is irrelevant data for guessing H given Y1 (= the magnitude of Y ). Or,
in other words, the magnitude of Y is a suﬃcient statistic for guessing H based on
(Y1, Y2). Indeed the likelihood-ratio function
LR(y1, y2) = fY1,Y2|H=0(y1, y2)
fY1,Y2|H=1(y1, y2)
=
1
√
2πσ2
0 exp

−(y1y2)2
2σ2
0

1
√
2πσ2
1 exp

−(y1y2)2
2σ2
1

= σ1
σ0
exp
 y2
1
2σ2
1
−y2
1
2σ2
0

can be computed from the magnitude y1 only, so Y1 is a suﬃcient statistic for
guessing H based on (Y1, Y2).
The following two notes clarify that the notion of irrelevance is diﬀerent from that
of statistical independence. Neither implies the other.
Note 22.6.3. A RV can be independent of the RV that we wish to guess and yet
not be irrelevant.
Proof. We provide an example of a RV R that is independent of the RV H that
we wish to guess and that is nonetheless not irrelevant. Suppose that H takes on
the values 0 and 1, and assume that under both hypotheses Y ∼Bernoulli(1/2):
Pr

Y = 1
 H = 0

= Pr

Y = 1
 H = 1

= 1
2.
Further assume that under H = 0 the RV R is given by 0 ⊕Y = Y , whereas under
H = 1 it is given by 1 ⊕Y . Here ⊕denotes the exclusive-or operation or mod-2
addition.
The distribution of R does not depend on the hypothesis; it is Bernoulli(1/2) both
conditional on H = 0 and conditional on H = 1.
But R is not irrelevant for
guessing H given Y . In fact, if we had to guess H based on Y only, our probability
of error would be 1/2. But if we base our decision on Y and R, then our probability
of error is zero because
H = Y ⊕R.
Note 22.6.4. A RV can be irrelevant even if it is statistically dependent on the
RV that we wish to guess.
Proof. As an example, consider the case where R is equal to Y with probability one
and that Y (and hence also R) is statistically dependent on the RV M that we wish
to guess. Since R is deterministically equal to Y , it follows that, conditional on Y ,
the random variable R is deterministic. Consequently, since a deterministic RV is
independent of every RV, it follows that M and R are conditionally independent
22.7 Testing with Random Parameters
489
given Y , i.e., that (22.47) holds. Thus, even though in this example R is statistically
dependent on M, it is irrelevant for guessing M given Y . The intuitive explanation
is that, in this example, R is irrelevant for guessing M given Y not because it
conveys no information about M (it does!) but because it conveys no information
about M that is not already conveyed by Y .
Condition (22.46) is often diﬃcult to establish directly, especially when the distri-
bution of the pair (R, Y) is speciﬁed in terms of its conditional density given M,
because in this case the conditional law of (M, R) given Y can be unwieldy. In
some cases the following proposition can be used to establish that R is irrelevant.
Proposition 22.6.5 (A Condition that Implies Irrelevance). Suppose that the con-
ditional law of R given M = m does not depend on m and that, for each m ∈M,
we have that, conditionally on M = m, the observations Y and R are independent.
Then R is irrelevant for guessing M given Y.
Proof. We provide the proof for the case where the pair (Y, R) has a conditional
density given M. The discrete case or the mixed case (where one has a conditional
density and the other a conditional PMF) can be treated with the same approach.
To prove this proposition we shall demonstrate that Y is a suﬃcient statistic for
guessing H based on (Y, R) using the Factorization Theorem. To that end, we
express the conditional density of (Y, R) as
fY,R|M=m(y, r) = fY|M=m(y) fR|M=m(r)
= fY|M=m(y) fR(r)
= gm(y) h(y, r),
(22.48)
where the ﬁrst equality follows from the conditional independence of Y and R
given M; the second from the hypothesis that the conditional density of R given
M = m does not depend on m and by denoting this density by fR(·); and the
ﬁnal equality follows by deﬁning gm(y) ≜fY|M=m(y) and h(y, r) ≜fR(r). The
factorization (22.48) demonstrates that Y forms a suﬃcient statistic for guessing M
based on (Y, R), i.e., that R is irrelevant for guessing M given Y.
22.7
Testing with Random Parameters
The notions of suﬃcient statistics and irrelevance also apply when testing in the
presence of a random parameter.
If the random parameter Θ is not observed,
then T(Y) is suﬃcient if, and only if, for any prior {πm} on M
M⊸−T(Y)⊸−Y.
(22.49)
If Θ is of density fΘ(·) and independent of M, then, as in (20.101), we can express
the conditional density of Y given M = m as
fY|M=m(y) =

θ
fY|Θ=θ,M=m(y) fΘ(θ) dθ,
490
Suﬃcient Statistics
so T(·) forms a suﬃcient statistic if, and only if, it forms a suﬃcient statistic for
the M densities
1
y 	→

θ
fY|Θ=θ,M=m(y) fΘ(θ) dθ
2
m∈M
.
Similarly, R is irrelevant for guessing M given Y if, and only if,
M⊸−Y⊸−R
forms a Markov chain for every prior {πm} on M.
If the parameter Θ is observed, then T(Y, Θ) is a suﬃcient statistic if, and only if,
for any prior {πm} on M
M⊸−T(Y, Θ)⊸−(Y, Θ).
If Θ is independent of M and is of density fΘ(·), then the density fY,Θ|M=m(·) can
be expressed, as in (20.104), as
fY,Θ|M=m(y, θ) = fΘ(θ) fY|Θ=θ,M=m(y),
so T(·) forms a suﬃcient statistic if, and only if, it forms a suﬃcient statistic for
the M densities
'
(θ, y) 	→fΘ(θ) fY|Θ=θ,M=m(y)
(
m∈M.
Similarly, R is irrelevant for guessing M given (Y, Θ) if, and only if, for every prior
on M
M⊸−(Y, Θ)⊸−R.
The following lemma provides an easily-veriﬁable condition that guarantees that R
is irrelevant for guessing H based on Y, irrespective of whether the random pa-
rameter is observed or not.
Lemma 22.7.1. If for any prior {πm} on M we have that R is independent of the
triple (M, Θ, Y),8 then R is irrelevant for guessing M given (Θ, Y) and also for
guessing M given Y.
Proof. To prove the lemma when Θ is observed, we need to show that the inde-
pendence of R and the triple (M, Θ, Y) implies
M⊸−(Y, Θ)⊸−R,
i.e., that the conditional distribution of R given (Y, Θ) is the same as given
(M, Y, Θ).
This is indeed the case because R is independent of (M, Y, Θ) so
the two conditional distributions are equal to the unconditional distribution of R.
To prove the lemma in the case where Θ is unobserved, we need to show that the
independence of R and the triple (M, Θ, Y) implies that
M⊸−Y⊸−R.
8Note that being independent of the triple is a stronger condition than being independent of
each of the members of the triple!
22.8 Additional Reading
491
Again, one can do so by noting that the conditional distribution of R given Y is
equal to the conditional distribution of R given (Y, M) because both are equal to
the unconditional distribution of R.
22.8
Additional Reading
The classical deﬁnition of suﬃcient statistic as a mapping T(·) such that the dis-
tribution of Y given

T(Y), M = m

does not depend on m is due to R. A. Fisher.
A. N. Kolmogorov deﬁned T(·) to be suﬃcient if for every prior {πm} the a pos-
teriori distribution of M given Y can be computed from T(Y). In our setting
where M takes on a ﬁnite number of values the two deﬁnitions are equivalent. For
an example where the deﬁnitions diﬀer, see (Blackwell and Ramamoorthi, 1982).
For a discussion of pairwise suﬃciency and its relation to suﬃciency, see (Halmos
and Savage, 1949).
22.9
Exercises
Exercise 22.1 (Another Proof of Proposition 22.4.6). Give an alternative proof of Propo-
sition 22.4.6 using Theorem 22.3.5.
Exercise 22.2 (Hypothesis Testing with Two Observations). Let H take on the values 0
and 1 equiprobably. Let Y1 be a random vector taking values in R2, and let Y2 be a
random variable. Conditional on H = 0,
Y1 = μ + Z1,
Y2 = α + Z2,
and, conditional on H = 1,
Y1 = −μ + Z1,
Y2 = −α + Z2.
Here H, Z1, and Z2 are independent with the components of Z1 being IID N(0, 1),
with Z2 having the mean-one exponential distribution, and with μ ∈R2 and α ∈R being
deterministic.
(i) Find an optimal rule for guessing H based on Y1. Find a one-dimensional suﬃcient
statistic.
(ii) Find an optimal rule for guessing H based on Y2.
(iii) Find a two-dimensional suﬃcient statistic (T1, T2) for guessing H based on (Y1, Y2).
(iv) Find an optimal rule for guessing H based on the pair (T1, T2).
Exercise 22.3 (Suﬃcient Statistics and the Bhattacharyya Bound). Show that if the
mapping T : Rd →Rd′ is a suﬃcient statistic for the densities fY|H=0(·) & fY|H=1(·), and
if T = T(Y) is of conditional densities fT|H=0(·) and fT|H=1(·), then
1
2

Rd
-
fY|H=0(y) fY|H=1(y) dy = 1
2

Rd′
-
fT|H=0(t) fT|H=1(t) dt.
492
Suﬃcient Statistics
Hint: You may want to ﬁrst derive the identity

Rd
-
fY|H=0(y) fY|H=1(y) dy = E
1fY|H=0(Y)
fY|H=1(Y)
1/2  H = 1
2
.
Exercise 22.4 (Suﬃcient Statistics and Irrelevant Data).
(i) Show that if the hypotheses of Proposition 22.6.5 are satisﬁed, then the random
variables Y and R must be independent also when one does not condition on M.
(ii) Show that the conditions for irrelevance in that proposition are not necessary.
Exercise 22.5 (Two More Characterizations of Suﬃcient Statistics). Let PY |H=0(·) and
PY |H=1(·) be probability mass functions on the ﬁnite set Y. We say that T(Y ) forms a
suﬃcient statistic for guessing H based on Y if H⊸−T(Y )⊸−Y for every prior on H.
Show that each of the following conditions is equivalent to T(Y ) forming a suﬃcient
statistic for guessing H based on Y :
(a) For every y ∈Y satisfying PY |H=0(y) + PY |H=1(y) > 0 we have
PY |H=0(y)
PY |H=1(y) = PT |H=0

T(y)

PT |H=1

T(y)
,
where we adopt the convention (20.39).
(b) For every prior (π0, π1) on H there exists a decision rule that bases its decision on
π0, π1, and T(Y ) and that is optimal for guessing H based on Y .
Exercise 22.6 (Pairwise Suﬃciency Implies Suﬃciency). Prove Proposition 22.3.2 in the
case where the conditional densities of the observable given each of the hypotheses are
positive.
Exercise 22.7 (Simulating the Observable). In all the examples we gave in Section 22.3.4
the random vector ˜Y was generated from T(yobs) uniformly over the set of vectors ξ in Rd
satisfying T(ξ) = T(yobs). Provide an example where this is not the case.
Hint: The setup of Proposition 22.6.5 might be useful.
Exercise 22.8 (Densities with Zeros). Conditional on H = 0, the d components of Y are
IID and uniformly distributed over the interval [α0, β0]. Conditional on H = 1, they are
IID and uniformly distributed over the interval [α1, β1]. Show that the tuple
	
max

Y (1), . . . , Y (d)
, min

Y (1), . . . , Y (d)
forms a suﬃcient statistic for guessing H based on Y.
Exercise 22.9 (Optimality Does Not Imply Suﬃciency). Let H take value in the set
{0, 1}, and let d = 2. Suppose that
Yj = (1 −2H) + ΘZj,
j = 1, . . . , d,
where H, Θ, Z1, . . . , Zd are independent with Θ taking on the distinct positive values σ0
and σ1 with probability ρ0 and ρ1 respectively, and with Z1, . . . , Zd being IID N(0, 1).
Let T = 
j Yj.
22.9 Exercises
493
(i) Show that (T, Θ) forms a suﬃcient statistic for guessing H based on Y1, . . . , Yd
when Θ is observed.
(ii) Show that T does not form a suﬃcient statistic for guessing H based on Y1, . . . , Yd
when Θ is not observed.
(iii) Show that notwithstanding Part (ii), if H has a uniform prior, then the decision
rule that guesses “H = 0” whenever T ≥0 is optimal both when Θ is observed and
when it is not observed.
Exercise 22.10 (Markovity Implies Markovity). Suppose that for every prior on M
(M, A)⊸−T(Y)⊸−Y
forms a Markov chain, where M takes value in the set M = {1, . . . , M}, where A and Y
are random vectors, and where T(·) is Borel measurable. Does this imply that T(·) forms
a suﬃcient statistic for guessing M based on Y?
Exercise 22.11 (Ancillary Statistic). A (Borel measurable) function S(·) of the observ-
able Y is said to be an ancillary statistic if the distribution of S(Y) when Y is of density
fY|M=m(·) is identical for all m ∈M. Must every ancillary statistic be irrelevant?
Exercise 22.12 (Minimal Suﬃcient). A suﬃcient statistic T(·) for guessing M based on Y
is said to be minimal suﬃcient if T(Y) can be computed from any suﬃcient statistic
for guessing M based on Y. That is, for every suﬃcient statistic S(·) there exists some
function g(·) such that
Pr
$
g

S(Y)

= T(Y)
 M = m
%
= 1,
m ∈M.
Show that in binary hypothesis testing the mapping yobs →LR(yobs) is minimal suﬃcient.
(If you are bothered by the fact that LR(yobs) may be inﬁnite, consider the inverse tangent
yobs →tan−1(LR(yobs)), which takes value in the interval [0, π/2].)
Chapter 23
The Multivariate Gaussian Distribution
23.1
Introduction
The multivariate Gaussian distribution is arguably the most important multi-
variate distribution in Digital Communications. It is the extension of the univariate
Gaussian distribution from scalars to vectors. A random vector of this distribu-
tion is said to be a Gaussian vector, and its components are said to be jointly
Gaussian. In this chapter we shall deﬁne this distribution, provide some useful
characterizations, and study some of its key properties.
To emphasize its con-
nection to the univariate distribution, we shall derive it along the same lines we
followed in deriving the univariate Gaussian distribution in Chapter 19.
There are a number of equivalent ways to deﬁne the multivariate Gaussian distri-
bution, and authors typically pick one deﬁnition and then proceed over the course
of numerous pages to derive alternate characterizations. We shall also proceed in
this way, but to satisfy the impatient reader’s curiosity we shall state the various
equivalent deﬁnitions in this section. The proof of their equivalence will be spread
over the whole chapter.
In the following deﬁnition we use the notation introduced in Section 17.2.
In
particular, all vectors are column vectors, and we denote the components of the
vector a ∈Rn by a(1), . . . , a(n).
Deﬁnition 23.1.1 (Standard Gaussians, Centered Gaussians, and Gaussians).
(i) A random vector W taking values in Rn is said to be a standard Gaussian
if its n components W (1), . . . , W (n) are independent and each is a zero-mean
unit-variance univariate Gaussian.
(ii) A random vector X taking values in Rn is said to be a centered Gaussian
if there exists some deterministic n × m matrix A such that the distribution
of X is the same as the distribution of AW, i.e.,
X
L= AW,
(23.1)
where W is a standard Gaussian with m components.
494
23.2 Notation and Preliminaries
495
(iii) A random vector X taking values in Rn is said to be Gaussian if there exists
some deterministic n × m matrix A and some deterministic vector μ ∈Rn
such that the distribution of X is equal to the distribution of AW + μ, i.e., if
X
L= AW + μ,
(23.2)
where W is a standard Gaussian with m components.
The random vectors AW + μ and X can have identical laws only if they have
identical mean vectors. As we shall see, the linearity of expectation and the fact
that a standard Gaussian is of zero mean imply that the mean vector of AW + μ
is equal to μ. Thus, AW + μ and X can have identical laws only if μ = E[X].
Consequently, X is a Gaussian random vector if, and only if, for some A and W
as above X
L= AW + E[X]. Stated diﬀerently, X is a Gaussian random vector if,
and only if, X −E[X] is a centered Gaussian.
While Deﬁnition 23.1.1 allows for the matrix A to be rectangular, we shall see in
Corollary 23.6.13 that every centered Gaussian can be generated from a standard
Gaussian by multiplication by a square matrix. That is, if X is an n-dimensional
centered Gaussian, then there exists an n×n square matrix A such that X
L= AW,
where W is a standard Gaussian.
In fact, we shall see in Theorem 23.6.14 that we can even limit ourselves to square
matrices that are the product of an orthogonal matrix by a diagonal matrix. Since
multiplying W by a diagonal matrix merely scales its components while leaving
them independent and Gaussian, it follows that X is a centered Gaussian if, and
only if, its law is the same as the law of the result of applying an orthogonal
transformation to a random vector whose components are independent zero-mean
univariate Gaussians (not necessarily of equal variance).
In view of Deﬁnition 23.1.1, it is not surprising that applying a linear transfor-
mation to a Gaussian vector results in a Gaussian vector ((23.49) ahead). The
reverse is perhaps more surprising: X is a Gaussian vector if, and only if, the re-
sult of applying any deterministic linear functional to X has a univariate Gaussian
distribution (Theorem 23.6.17 ahead).
We conclude this section with the following pact with the reader.
(i) Unless preceded by the word “random” or “Gaussian,” all scalars, vectors,
and matrices in this chapter are deterministic.
(ii) Unless preceded by the word “complex,” all scalars, vectors, and matrices in
this chapter are real.
But, without violating this pact, we shall sometimes get excited and throw in the
words “real” and “deterministic” even when unnecessary.
23.2
Notation and Preliminaries
Our notation in this chapter expands upon the one introduced in Section 17.2. To
minimize page ﬂipping, we repeat here parts of that section.
496
The Multivariate Gaussian Distribution
Deterministic vectors are denoted by boldface lowercase letters such as w, whereas
random vectors are denoted by boldface uppercase letters such as W. When we
deal with deterministic matrices we make an exception to our rule of trying to
denote deterministic quantities by lowercase letters.1 Thus, deterministic matrices
are denoted by uppercase letters. But to make it clear that we are dealing with
a deterministic matrix and not a scalar random variable, we use special fonts to
distinguish the two. Thus A denotes a deterministic matrix, whereas A denotes a
random variable. Random matrices, which only appear brieﬂy in this book, are
denoted by uppercase letters of yet another font, e.g., H.
An n × m deterministic real matrix A is an array of real numbers having n rows
and m columns
A =
⎛
⎜
⎜
⎜
⎝
a(1,1)
a(1,2)
. . .
a(1,m)
a(2,1)
a(2,2)
. . .
a(2,m)
...
...
...
...
a(n,1)
a(n,2)
. . .
a(n,m)
⎞
⎟
⎟
⎟
⎠.
The Row-j Column-ℓelement of the matrix A is denoted
a(j,ℓ)
or
[A]j,ℓ.
The transpose of an n×m matrix A is the m×n matrix AT whose Row-j Column-ℓ
entry is equal to the Row-ℓColumn-j entry of A:
[AT]j,ℓ= [A]ℓ,j,
j ∈{1, . . . , m}, ℓ∈{1, . . . , n}.
We shall repeatedly use the fact that if the matrix-product AB is deﬁned (i.e., if
the number of columns of A is the same as the number of rows of B), then the
transpose of the product is the product of the transposes in reverse order
(AB)T = BTAT.
(23.3)
The n × n identity matrix whose diagonal elements are all 1 and whose oﬀ-
diagonal elements are all 0 is denoted In. The all-zero matrix whose components
are all zero is denoted 0.
An n × 1 matrix is an n-vector, or a vector for short.
Thus, unless otherwise
speciﬁed, all the vectors we shall encounter are column vectors.2 The components
of an n-vector a are denoted by a(1), . . . , a(n) so
a =
⎛
⎜
⎝
a(1)
...
a(n)
⎞
⎟
⎠,
or, in a typographically more eﬃcient form,
a =

a(1), . . . , a(n)T.
1We have already made some exceptions to this rule when we dealt with deterministic con-
stants that are by convention always denoted using uppercase letters, e.g., bandwidth W, ampli-
tude A, baud period Ts, etc.
2An exception to this rule is in our treatment of linear codes where the tradition of using row
vectors is too strong to change.
23.3 Some Results on Matrices
497
The vector whose components are all zero is denoted by 0. The square root of the
sum of the squares of the components of a real n-vector a is denoted by ∥a∥:
∥a∥=
A
B
B
C
n

ℓ=1

a(ℓ)2,
a ∈Rn.
(23.4)
If a = (a(1), . . . , a(n))T and b = (b(1), . . . , b(n))T, then3
aTb =
n

ℓ=1
a(ℓ)b(ℓ)
= bTa.
In particular,
∥a∥2 =
n

ℓ=1

a(ℓ)2
= aTa.
(23.5)
Note the diﬀerence between aTa and aaT: the former is the scalar ∥a∥2 whereas
the latter is the n × n matrix whose Row-j Column-ℓelement is a(j)a(ℓ).
The determinant of a square matrix A is denoted by det A. We note that a matrix
and its transpose have equal determinants
det

AT
= det A,
(23.6)
and that the determinant of the product of two square matrices is the product of
the determinants
det (AB) = det (A) det (B).
(23.7)
We say that a square n × n matrix A is singular if its determinant is zero or,
equivalently, if its columns are linearly dependent or, equivalently, if its rows are
linearly dependent or, equivalently, if there exists some nonzero vector α ∈Rn
such that Aα = 0.
23.3
Some Results on Matrices
We next survey some of the results from Matrix Theory that we shall be using. Par-
ticularly important to us are results on positive semideﬁnite matrices, because, as
we shall see in Proposition 23.6.1, every covariance matrix is positive semideﬁnite,
and every positive semideﬁnite matrix is the covariance matrix of some random
vector.
3In (20.84) we denoted aTb by ⟨a, b⟩E.
498
The Multivariate Gaussian Distribution
23.3.1
Orthogonal Matrices
Deﬁnition 23.3.1 (Orthogonal Matrices). An n × n real matrix U is said to be
orthogonal if its transpose is its inverse
UUT = In.
(23.8)
As proved in (Axler, 2015, Section 7.C, Theorem 7.42), the condition (23.8) is
equivalent to the condition
UTU = In.
(23.9)
Thus, a real matrix is orthogonal if, and only if, its transpose is orthogonal.
If we write an n × n matrix U in terms of its columns as
U =
⎛
⎜
⎜
⎜
⎜
⎝
↑
· · ·
↑
· · ·
ψ1
· · ·
ψn
· · ·
↓
· · ·
↓
⎞
⎟
⎟
⎟
⎟
⎠
,
then (23.9) can be expressed as
In = UTU
=
⎛
⎜
⎜
⎜
⎜
⎝
←
ψT
1
→
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
←
ψT
n
→
⎞
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎝
↑
· · ·
↑
· · ·
ψ1
· · ·
ψn
· · ·
↓
· · ·
↓
⎞
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
ψT
1 ψ1
ψT
1 ψ2
· · ·
ψT
1 ψn
ψT
2 ψ1
ψT
2 ψ2
· · ·
ψT
2 ψn
...
...
...
...
ψT
nψ1
ψT
nψ2
· · ·
ψT
nψn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
thus showing that a real n × n matrix U is orthogonal if, and only if, its n columns
ψ1, . . . , ψn satisfy
ψT
ν ψν′ = I{ν = ν′},
ν, ν′ ∈{1, . . . , n}.
(23.10)
Using the same argument but starting with (23.8) we can prove a similar result
about the rows of an orthogonal matrix: if the rows of a real n × n matrix U are
denoted by φT
1 , . . . , φT
n, i.e.,
U =
⎛
⎜
⎜
⎜
⎜
⎝
←
φT
1
→
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
←
φT
n
→
⎞
⎟
⎟
⎟
⎟
⎠
,
23.3 Some Results on Matrices
499
then U is orthogonal if, and only if,
φT
ν φν′ = I{ν = ν′},
ν, ν′ ∈{1, . . . , n}.
(23.11)
Recalling that the determinant of a product of square matrices is the product of
the determinants and that the determinant of a matrix is equal to the determinant
of its transpose, we obtain that for every square matrix U
det

UUT
=

det U
2.
(23.12)
Consequently, by taking the determinant of both sides of (23.8) we obtain that the
determinant of an orthogonal matrix must be either +1 or −1. It should, however,
be noted that there are numerous examples of matrices of unit determinant that
are not orthogonal.
We leave it to the reader to verify that a 2 × 2 matrix is orthogonal if, and only if,
it is equal to one of the following matrices for some choice of −π ≤θ < π
	
cos θ
−sin θ
sin θ
cos θ

,
	
cos θ
sin θ
sin θ
−cos θ

.
(23.13)
The former matrix corresponds to a rotation by θ and has determinant +1, and
the latter to a reﬂection followed by a rotation
	
cos θ
sin θ
sin θ
−cos θ

=
	
cos θ
−sin θ
sin θ
cos θ

 	
1
0
0
−1

and has determinant −1.
23.3.2
Symmetric Matrices
A matrix A is said to be symmetric if it is equal to its transpose:
AT = A.
Only square matrices can be symmetric. A vector ψ ∈Rn is said to be an eigen-
vector of the matrix A corresponding to the real eigenvalue λ ∈R if ψ is nonzero
and if Aψ = λψ. The following is a key result about the eigenvectors of symmetric
real matrices.
Proposition 23.3.2 (Eigenvectors and Eigenvalues of Symmetric Real Matrices).
If A is a symmetric real n × n matrix, then A has n (not necessarily distinct)
real eigenvalues λ1, . . . , λn ∈R with corresponding eigenvectors ψ1, . . . , ψn ∈Rn
satisfying
ψT
ν ψν′ = I{ν = ν′},
ν, ν′ ∈{1, . . . , n}.
(23.14)
Proof. See, for example, (Axler, 2015, Section 7.B, Theorem 7.29), or (Herstein,
1975, Section 6.10, pp. 346–348), or (Horn and Johnson, 2013, Section 4.1, Theo-
rem 4.1.5).
500
The Multivariate Gaussian Distribution
The vectors ψ1, . . . , ψn are eigenvectors of the matrix A corresponding to the eigen-
values λ1, . . . , λn if
Aψν = λνψν,
ν ∈{1, . . . , n}.
(23.15)
We next express this in an alternative way. We begin by noting that
A
⎛
⎜
⎜
⎜
⎜
⎝
↑
· · ·
↑
· · ·
ψ1
· · ·
ψn
· · ·
↓
· · ·
↓
⎞
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎝
↑
· · ·
↑
· · ·
Aψ1
· · ·
Aψn
· · ·
↓
· · ·
↓
⎞
⎟
⎟
⎟
⎟
⎠
and that
⎛
⎜
⎜
⎜
⎜
⎝
↑
· · ·
↑
· · ·
ψ1
· · ·
ψn
· · ·
↓
· · ·
↓
⎞
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎝
λ1
0
· · ·
0
0
λ2
...
...
...
...
...
0
0
· · ·
0
λn
⎞
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎝
↑
· · ·
↑
· · ·
λ1ψ1
· · · λnψn
· · ·
↓
· · ·
↓
⎞
⎟
⎟
⎟
⎟
⎠
.
Consequently, Condition (23.15) can be written as
AU = UΛ,
(23.16)
where
U =
⎛
⎜
⎜
⎜
⎜
⎝
↑
· · ·
↑
· · ·
ψ1
· · ·
ψn
· · ·
↓
· · ·
↓
⎞
⎟
⎟
⎟
⎟
⎠
and
Λ =
⎛
⎜
⎜
⎜
⎜
⎝
λ1
0
· · ·
0
0
λ2
...
...
...
...
...
0
0
· · ·
0
λn
⎞
⎟
⎟
⎟
⎟
⎠
.
(23.17)
Condition (23.14) is equivalent to the condition that the above matrix U is orthog-
onal. By multiplying (23.16) from the right by the inverse of U (which, because U
is orthogonal and by (23.8), is UT) we obtain the equivalent form A = UΛUT.
Consequently, an equivalent statement of Proposition 23.3.2 is:
Proposition 23.3.3 (Spectral Theorem for Real Symmetric Matrices). A sym-
metric real n × n matrix A can be written in the form
A = UΛUT
where, as in (23.17), Λ is a diagonal real n × n matrix whose diagonal elements
are the eigenvalues of A, and where U is a real n × n orthogonal matrix whose ν-th
column is an eigenvector of A corresponding to the eigenvalue in the ν-th position
on the diagonal of Λ.
The reverse is also true: if A = UΛUT for a real diagonal matrix Λ and for a real
orthogonal matrix U, then A is symmetric, its eigenvalues are the diagonal elements
of Λ, and the ν-th column of U is an eigenvector of the matrix A corresponding to
the eigenvalue in the ν-th position on the diagonal of Λ.
23.3 Some Results on Matrices
501
23.3.3
Positive Semideﬁnite Matrices
Deﬁnition 23.3.4 (Positive Semideﬁnite and Positive Deﬁnite Matrices).
(i) We say that the n × n real matrix K is positive semideﬁnite or nonneg-
ative deﬁnite and write
K ⪰0
if K is symmetric and
αTKα ≥0,
α ∈Rn.
(ii) We say that the n × n real matrix K is positive deﬁnite and write
K ≻0
if K is symmetric and
αTKα > 0,

α ̸= 0, α ∈Rn
.
The following two propositions characterize positive semideﬁnite and positive def-
inite matrices. For proofs, see (Axler, 2015, Section 7.C, Theorem 7.35).
Proposition 23.3.5 (Characterizing Positive Semideﬁnite Matrices). Let K be a
real n × n matrix. Then the statement that K is positive semideﬁnite is equivalent
to each of the following statements:
(a) The matrix K can be written in the form
K = STS
(23.18)
for some real n × n matrix S.4
(b) The matrix K is symmetric and all its eigenvalues are nonnegative.
(c) The matrix K can be written in the form
K = UΛUT,
(23.19)
where Λ is a real n × n diagonal matrix with nonnegative entries on the
diagonal and where U is a real n × n orthogonal matrix.
Proposition 23.3.6 (Characterizing Positive Deﬁnite Matrices). Let K be a real
n × n matrix. Then the statement that K is positive deﬁnite is equivalent to each
of the following statements.
(a) The matrix K can be written in the form K = STS for some real n × n
nonsingular matrix S.
(b) The matrix K is symmetric and all its eigenvalues are positive.
4Even if S is not a square matrix, STS ⪰0.
502
The Multivariate Gaussian Distribution
(c) The matrix K can be written in the form
K = UΛUT,
where Λ is a real n × n diagonal matrix with positive entries on the diagonal
and where U is a real n × n orthogonal matrix.
Given a positive semideﬁnite matrix K, how can we ﬁnd a matrix S satisfying
K = STS? In general, there can be many such matrices. For example, if K is the
identity matrix, then S can be any orthogonal matrix. We mention here two useful
choices. Being symmetric, the matrix K can be written in the form
K = UΛUT,
(23.20)
where U and Λ are as in (23.17). Since K is positive semideﬁnite, the diagonal
elements of Λ (which are the eigenvalues of K) are nonnegative. Consequently, we
can deﬁne the matrix
Λ1/2 =
⎛
⎜
⎜
⎜
⎜
⎝
√λ1
0
· · ·
0
0 √λ2
...
...
...
...
...
0
0
· · ·
0 √λn
⎞
⎟
⎟
⎟
⎟
⎠
.
One choice of the matrix S is
S = Λ1/2UT.
(23.21)
Indeed, with this deﬁnition of S we have
STS =

Λ1/2UTTΛ1/2UT
= UΛ1/2Λ1/2UT
= UΛUT
= K,
where the ﬁrst equality follows from the deﬁnition of S; the second from the rule
(AB)T = BTAT and from the symmetry of the diagonal matrix Λ1/2; the third from
the deﬁnition of Λ1/2; and where the ﬁnal equality follows from (23.20).
A diﬀerent choice for S, which will be less useful to us in this chapter, is5
UΛ1/2UT.
The following lemmas will be used in Section 23.4.3 when we study random vectors
of singular covariance matrices.
Lemma 23.3.7. Let K be a real n × n positive semideﬁnite matrix, and let α be a
vector in Rn. Then αTKα = 0 if, and only if, Kα = 0.
5This is the only choice for S that is positive semideﬁnite (Axler, 2015, Section 7.C, Theo-
rem 7.36), (Horn and Johnson, 2013, Section 7.2, Theorem 7.2.6).
23.4 Random Vectors
503
Proof. One direction is trivial and does not require that K be positive semideﬁnite:
if Kα = 0, then αTKα must also be equal to zero. Indeed, in this case we have by
the associativity of matrix multiplication αTKα = αT(Kα) = αT0 = 0.
To prove the other direction, we ﬁrst note that, since K is positive semideﬁnite,
there exists some n × n matrix S such that K = STS. Hence,
αTKα = αTSTSα
= (Sα)T(Sα)
= ∥Sα∥2 ,
α ∈Rn,
where the second equality follows from the rule for transposing a product (23.3),
and where the third equality follows from (23.5). Consequently, if αTKα = 0, then
∥Sα∥2 = 0, so Sα = 0, and hence STSα = 0, i.e., Kα = 0.
Lemma 23.3.8. If K is a real n × n positive deﬁnite matrix, then αTKα = 0 if,
and only if, α = 0.
Proof. Follows directly from Deﬁnition 23.3.4 of positive deﬁnite matrices.
23.4
Random Vectors
23.4.1
Deﬁnitions
Recall that an n-dimensional random vector or a random n-vector X de-
ﬁned over the probability space (Ω, F, P) is a (measurable) mapping from the set
of experiment outcomes Ω to the n-dimensional Euclidean space Rn. A random
vector X is very much like a random variable, except that rather than taking values
in the real line R, it takes values in Rn. In fact, an n-dimensional random vector
can be viewed as an array of n random variables.6
The density of a random vector is the joint density of its components. The density
of a random n-vector is thus a nonnegative (Borel measurable) function from Rn
to the nonnegative reals that integrates to one.
Similarly, an n×m random matrix H is an n×m array of random variables deﬁned
over a common probability space.
23.4.2
Expectations and Covariance Matrices
The expectation E[X] of a random n-vector X = (X(1), . . . , X(n))T is a vector
whose components are the expectations of the corresponding components of X:7
E[X] ≜

E

X(1)
, . . . , E

X(n)T
.
(23.22)
6In dealing with random vectors one often abandons the “coordinate free” approach and views
vectors in a particular coordinate system. This allows one to speak of the covariance matrix in
more familiar terms.
7The expectation of a random vector is only deﬁned if the expectation of each of its compo-
nents is deﬁned.
504
The Multivariate Gaussian Distribution
The j-th element of E[X] is thus the expectation of the j-th component of X,
namely, E

X(j)
. Similarly, the expectation of a random matrix is the matrix of
expectations.
If all the components of a random n-vector X are of ﬁnite variance, then we say
that X is of ﬁnite variance. We then deﬁne its n × n covariance matrix KXX
as
KXX ≜E
%
(X −E[X]) (X −E[X])T&
.
(23.23)
That is,
KXX = E
⎡
⎢⎢⎢⎢⎣
⎛
⎜
⎜
⎜
⎜
⎝
X(1) −E

X(1)
...
...
X(n) −E

X(n)
⎞
⎟
⎟
⎟
⎟
⎠

X(1) −E

X(1)
· · ·
· · ·
X(n) −E

X(n)
⎤
⎥⎥⎥⎥⎦
=
⎛
⎜
⎜
⎜
⎝
Var

X(1)
Cov

X(1), X(2)
· · ·
Cov

X(1), X(n)
Cov

X(2), X(1)
Var

X(2)
· · ·
Cov

X(2), X(n)
...
...
...
...
Cov

X(n), X(1)
Cov

X(n), X(2)
· · ·
Var

X(n)
⎞
⎟
⎟
⎟
⎠.
(23.24)
If n = 1 and the n-dimensional random vector X hence a scalar, then the covariance
matrix KXX is a 1 × 1 matrix whose sole component is the variance of the sole
component of X.
Note that, from the n×n covariance matrix KXX of a random n-vector X, it is easy
to compute the covariance matrix of a subset of X’s components. For example, if
we are only interested in the 2 × 2 covariance matrix of (X(1), X(2))T, then we just
pick the ﬁrst two columns and the ﬁrst two rows of KXX. More generally, the r × r
covariance matrix of (X(j1), X(j2), . . . , X(jr))T for 1 ≤j1 < j2 < · · · < jr ≤n is
obtained from KXX by picking Rows and Columns j1, . . . , jr. For example, if
KXX =
⎛
⎜
⎜
⎝
30
31
9
7
31
39
11
13
9
11
9
12
7
13
12
26
⎞
⎟
⎟
⎠,
then the covariance matrix of

X(2), X(4)T is
 39 13
13 26

.
We next explore the behavior of the mean vector and the covariance matrix of
a random vector when it is multiplied by a deterministic matrix. Regarding the
mean, we shall show that since matrix multiplication is a linear transformation, it
commutes with the expectation operation. Consequently, if H is a random n × m
matrix and A is a deterministic ν × n matrix, then
E[AH] = AE[H] ,
(23.25a)
and similarly if B is a deterministic m × ν matrix, then
E[HB] = E[H] B.
(23.25b)
23.4 Random Vectors
505
To prove (23.25a) we write out the Row-j Column-ℓelement of the ν × m ma-
trix E[AH] and use the linearity of expectation to relate it to the Row-j Column-ℓ
element of the matrix AE[H]:

E[AH]

j,ℓ= E

n

κ=1
[A]j,κ[H]κ,ℓ

=
n

κ=1
E
%
[A]j,κ[H]κ,ℓ
&
=
n

κ=1
[A]j,κE

[H]κ,ℓ

=

AE[H]

j,ℓ,
j ∈{1, . . . , ν}, ℓ∈{1, . . . , m}.
The proof of (23.25b) is almost identical and is omitted.
The transpose operation also commutes with expectation: if H is a random matrix
then
E

HT
= (E[H])T .
(23.26)
As to the covariance matrix, we next show that if A is a deterministic matrix and
if X is a random vector, then the covariance matrix KYY of the random vector
Y = AX can be expressed in terms of the covariance matrix KXX of X as
KYY = A KXX AT,
Y = AX.
(23.27)
Indeed,
KYY ≜E

(Y −E[Y])(Y −E[Y])T
= E

(AX −E[AX])(AX −E[AX])T
= E

A(X −E[X])(A(X −E[X]))T
= E

A(X −E[X])(X −E[X])TAT
= AE

(X −E[X])(X −E[X])TAT
= AE

(X −E[X])(X −E[X])T
AT
= A KXX AT.
A key property of covariance matrices is that, as we shall next show, they are all
positive semideﬁnite. That is, the covariance matrix KXX of any random vector X
is a symmetric matrix satisfying
αT KXX α ≥0,
α ∈Rn.
(23.28)
(In Proposition 23.6.1 we shall see that this property fully characterizes covari-
ance matrices: every positive semideﬁnite matrix is the covariance matrix of some
random vector.)
To prove (23.28) it suﬃces to consider the case where X is of zero mean because
the covariance matrix of X is the same as the covariance matrix of X −E[X]. The
506
The Multivariate Gaussian Distribution
symmetry of KXX follows from the deﬁnition of the covariance matrix (23.23); from
the fact that expectation and transposition commute (23.26); and from the formula
for the transpose of a product of matrices (23.3):
KT
XX =

E

XXTT
= E
%
XXTT&
= E

XXT
= KXX .
(23.29)
The nonnegativity of αT KXX α for any deterministic α ∈Rn follows by noting
that by (23.27) (applied with A = αT) the term αT KXX α is the variance of the
scalar random variable αTX, i.e.,
αT KXX α = Var

αTX

(23.30)
and, as such, is nonnegative.
23.4.3
Singular Covariance Matrices
A random vector having a singular covariance matrix can be unwieldy because
it cannot have a probability density function. Indeed, as we shall see in Corol-
lary 23.4.2, any such random vector has at least one component that is determined
(with probability one) by the other components. In this section we shall propose
a way of manipulating such vectors. Roughly speaking, the idea is that if X has a
singular covariance matrix, then we choose a subset of its components so that the
covariance matrix of the chosen subset be nonsingular and so that each compo-
nent that was not chosen be equal (with probability one) to a deterministic aﬃne
function of the chosen components. We then manipulate only the chosen compo-
nents and, with some deterministic bookkeeping “on the side,” take care of the
components that were not chosen. This idea is made precise in Corollary 23.4.3.
To illustrate the idea, suppose that X is a zero-mean random vector of covariance
matrix
KXX =
⎛
⎝
3
5
7
5
9
13
7
13
19
⎞
⎠.
An application of Proposition 23.4.1 ahead will show that because the three columns
of KXX satisfy the linear relationship
−
⎛
⎝
3
5
7
⎞
⎠+ 2
⎛
⎝
5
9
13
⎞
⎠−
⎛
⎝
7
13
19
⎞
⎠= 0,
it follows that
−X(1) + 2X(2) −X(3) = 0,
with probability one.
23.4 Random Vectors
507
Consequently, in manipulating X we can pick the two components X(2), X(3),
which are of nonsingular covariance matrix
 9 13
13 19

(obtained by picking the last
two rows and the last two columns of KXX), and keep track “on the side” of the
fact that X(1) is equal, with probability one, to 2X(2) −X(3). We could, of course,
also pick the components X(1), X(2) of nonsingular covariance matrix
 3 5
5 9

and
keep track “on the side” of the relationship X(3) = 2X(2) −X(1).
To avoid cumbersome language, for the remainder of this section we shall take all
equalities between random variables to stand for equalities with probability one.
Thus, if we write X(1) = 2X(2) −X(3) we mean that the probability that X(1) is
equal to 2X(2) −X(3) is one.
The justiﬁcation of the procedure is in the following proposition and its two corol-
laries.
Proposition 23.4.1. Let X be a zero-mean random n-vector of covariance ma-
trix KXX. Then its ℓ-th component X(ℓ) is equal (with probability one) to a linear
combination of X(ℓ1), . . . , X(ℓη) if, and only if, the ℓ-th column of KXX is a linear
combination of Columns ℓ1, . . . , ℓη. Here ℓ, η, ℓ1, . . . , ℓη ∈{1, . . . , n} are arbitrary.
Proof. If ℓ∈{ℓ1, . . . , ℓη}, then the result is trivial. We shall therefore present a
proof only for the case where ℓ/∈{ℓ1, . . . , ℓη}. In this case, the ℓ-th component of
the random n-vector X is a linear combination of the η components X(ℓ1), . . . , X(ℓη)
if, and only if, there exists a vector α ∈Rn satisfying
α(ℓ) = −1,
(23.31a)
α(κ) = 0,
κ /∈{ℓ, ℓ1, . . . , ℓη} ,
(23.31b)
and
αTX = 0.
(23.31c)
Since X is of zero mean, the condition αTX = 0 is equivalent to the condition
Var

αTX

= 0. By (23.30) and Lemma 23.3.7 this latter condition is equivalent
to the condition KXX α = 0. Now KXX α is a linear combination of the columns
of KXX where the ﬁrst column is multiplied by α(1), the second by α(2), etc. Con-
sequently, the condition that KXX α = 0 for some α ∈Rn satisfying (23.31a) &
(23.31b) is equivalent to the condition that the ℓ-th column of KXX is a linear
combination of Columns ℓ1, . . . , ℓη.
Corollary 23.4.2. The covariance matrix of a zero-mean random n-vector X is
singular if, and only if, some component of X is a linear combination of the other
components.
Proof. Follows from Proposition 23.4.1 by noting that a square matrix is singular
if, and only if, its columns are linearly dependent.
Corollary 23.4.3. Let X be a zero-mean random n-vector of covariance matrix KXX.
If Columns ℓ1, . . . , ℓd of KXX form a basis for the subspace of Rn spanned by the
columns of KXX, then every component of X can be written as a linear combination
of the components X(ℓ1), . . . , X(ℓd), and the random d-vector

X(ℓ1), . . . , X(ℓd)T
has a nonsingular d × d covariance matrix.
508
The Multivariate Gaussian Distribution
Proof. Since Columns ℓ1, . . . , ℓd form a basis for the subspace spanned by the
columns of KXX, every column ℓcan be written as a linear combination of these
columns. Consequently, by Proposition 23.4.1, every component of X can be writ-
ten as a linear combination of X(ℓ1), . . . , X(ℓd). To prove that the d × d covariance
matrix K˜X˜X of the random d-vector ˜X =

X(ℓ1), . . . , X(ℓd)T is nonsingular, we
note that if this were not the case, then by Corollary 23.4.2 applied to ˜X it would
follow that one of the components of ˜X is a linear combination of the other d −1
components. But by Proposition 23.4.1 applied to X, this would imply that the
columns ℓ1, . . . , ℓd of KXX are not linearly independent, in contradiction to the
corollary’s hypothesis that they form a basis.
23.4.4
The Characteristic Function
If X is a random n-vector, then its characteristic function ΦX(·) is a mapping
from Rn to C that maps each vector ϖ = (ϖ(1), . . . ϖ(n))T in Rn to ΦX(ϖ), where
ΦX(ϖ) ≜E
%
eiϖTX&
= E

exp
	
i
n

ℓ=1
ϖ(ℓ)X(ℓ)


,
ϖ ∈Rn.
If X has the density fX(·), then
ΦX(ϖ) =
 ∞
−∞
· · ·
 ∞
−∞
fX(x) ei n
ℓ=1 ϖ(ℓ)x(ℓ) dx(1) · · · dx(n),
which is reminiscent of the multi-dimensional Fourier Transform of fX(·) (ignoring
2π’s and the sign of i).
Proposition 23.4.4 (Identical Distributions and Characteristic Functions). Two
random n-vectors X, Y are of the same distribution if, and only if, they have
identical characteristic functions:

X
L= Y

⇐⇒

ΦX(ϖ) = ΦY(ϖ),
ϖ ∈Rn
.
(23.32)
Proof. See (Dudley, 2003, Chapter 9, Section 5, Theorem 9.5.1).
This proposition is extremely useful. We shall demonstrate its power by using it
to show that two random variables X and Y are independent if, and only if,
E
%
ei(ϖ1X+ϖ2Y )&
= E

eiϖ1X
E

eiϖ2Y 
,
ϖ1, ϖ2 ∈R.
(23.33)
One direction is straightforward. If X and Y are independent, then for any Borel
measurable functions g(·) and h(·) the random variables g(X) and h(Y ) are also
independent. Thus, the independence of X and Y implies the independence of the
23.5 A Standard Gaussian Vector
509
random variables eiϖ1X and eiϖ2Y and hence implies that the expectation of their
product is the product of their expectations:
E
%
ei(ϖ1X+ϖ2Y )&
= E

eiϖ1X eiϖ2Y 
= E

eiϖ1X
E

eiϖ2Y 
,
ϖ1, ϖ2 ∈R.
As to the other direction, suppose that X′ has the same law as X, that Y ′ has the
same law as Y , and that X′ and Y ′ are independent. Since X′ has the same law
as X, it follows that
E
%
eiϖ1X′&
= E

eiϖ1X
,
ϖ1 ∈R,
(23.34)
and similarly for Y ′
E
%
eiϖ2Y ′&
= E

eiϖ2Y 
,
ϖ2 ∈R.
(23.35)
Consequently, since X′ and Y ′ are independent
E
%
ei(ϖ1X′+ϖ2Y ′)&
= E
%
eiϖ1X′ eiϖ2Y ′&
= E
%
eiϖ1X′&
E
%
eiϖ2Y ′&
= E

eiϖ1X
E

eiϖ2Y 
,
ϖ1, ϖ2 ∈R,
where the third equality follows from (23.34) and (23.35).
We thus see that if (23.33) holds, then the characteristic function of the vector
(X, Y )T is identical to the characteristic function of the vector (X′, Y ′)T.
By
Proposition 23.4.4 the joint distribution of (X, Y ) must then be the same as the
joint distribution of (X′, Y ′). Since according to the latter distribution the two
components are independent, it follows that the same must be true according to
the former, i.e., X and Y must be independent.
23.5
A Standard Gaussian Vector
Recall Deﬁnition 23.1.1 that a random n-vector W is a standard Gaussian if its n
components are independent zero-mean unit-variance Gaussian random variables.
Its density fW(·) is then given by
fW(w) =
n
@
ℓ=1
-
1
√
2π exp
	
−

w(ℓ)2
2

.
=
1
(2π)n/2 exp
	
−1
2
n

ℓ=1

w(ℓ)2

= (2π)−n/2 e−1
2 ∥w∥2,
w ∈Rn.
(23.36)
The deﬁnition of a standard Gaussian random vector is an extension of the deﬁ-
nition of a standard Gaussian random variable: the sole component of a standard
510
The Multivariate Gaussian Distribution
one-dimensional Gaussian vector is a scalar N(0, 1) random variable. Conversely,
every N(0, 1) random variable can be viewed as a one-dimensional standard Gaus-
sian.
If W is a standard Gaussian random n-vector then, as we next show, its mean
vector and covariance matrix are given by
E[W] = 0,
and
KWW = In.
(23.37)
Indeed, the mean of a random vector is the vector of the means (23.22), so the
fact that E[W] = 0 is a consequence of all the components of W having zero
mean. And using (23.24) it can be easily shown that the covariance matrix of W
is the identity matrix because the components of W are independent and hence, a
fortiori uncorrelated, and because they are each of unit variance.
23.6
Gaussian Random Vectors
Recall Deﬁnition 23.1.1 that a random n-vector X is said to be Gaussian if for some
positive integer m there exists an n × m matrix A; a standard Gaussian random
m-vector W; and a deterministic vector μ ∈Rn such that
X
L= AW + μ.
(23.38)
From (23.38), from the second order properties of standard Gaussians (23.37),
and from the behavior of the mean vector and covariance matrix under linear
transformation (23.25a) & (23.27) we obtain

X
L= AW + μ and W standard

=⇒

E[X] = μ and KXX = AAT
.
(23.39)
Recall also that X is a centered Gaussian if X
L= AW for A and W as above.
Every standard Gaussian vector is a centered Gaussian because every standard
Gaussian n-vector W is equal to AW when A is the n × n identity matrix In.
The reverse is not true: not every centered Gaussian is a standard Gaussian.
Indeed, standard Gaussians have the identity covariance matrix (23.37), whereas
the centered Gaussian vector AW has, by (23.39), the covariance matrix AAT,
which need not be the identity matrix.
Also, X is a Gaussian vector if, and only if, X −E[X] is a centered Gaussian
because, by (23.39),

X
L= AW + μ for some μ ∈Rn and W standard Gaussian

⇐⇒

X
L= AW + E[X] and W standard Gaussian

⇐⇒

X −E[X]
L= AW and W standard Gaussian

.
(23.40)
From (23.40) it also follows that the centered Gaussians are the Gaussian vectors
of zero mean.8
8Thus, the name “centered Gaussian,” which we gave in Deﬁnition 23.1.1 was not misleading.
A vector is a “centered Gaussian” if, and only if, it is Gaussian and centered.
23.6 Gaussian Random Vectors
511
Using the deﬁnition of a centered Gaussian and using (23.39) we can readily show
that every positive semideﬁnite matrix is the covariance matrix of some centered
Gaussian. In fact, more is true:
Proposition 23.6.1 (Covariance Matrices and Positive Semideﬁnite Matrices).
The covariance matrix of every ﬁnite-variance random vector is positive semideﬁ-
nite, and every positive semideﬁnite matrix is the covariance matrix of some cen-
tered Gaussian random vector.
Proof. The covariance matrix of every random vector is positive semideﬁnite be-
cause every covariance matrix is symmetric (23.29) and satisﬁes (23.28). We next
establish the reverse. Given an n × n positive semideﬁnite matrix K we shall con-
struct a centered Gaussian X whose covariance matrix KXX is equal to K. We begin
by noting that, since K is positive semideﬁnite, it follows from Proposition 23.3.5
that there exists some n × n matrix S such that STS = K. Let W be a standard
Gaussian n-vector and consider the vector X = STW. Being the result of a linear
transformation of the standard Gaussian W, this vector is a centered Gaussian. We
complete the proof by showing that its covariance matrix KXX is the prespeciﬁed
matrix K. This follows from the calculation
KXX = STS
= K,
where the ﬁrst equality follows from (23.39) (by substituting ST for A and 0 for μ)
and the second from our choice of S as satisfying STS = K.
23.6.1
Examples and Basic Properties
In this section we provide some examples of Gaussian vectors and some simple
properties that follow from their deﬁnition.
(i) Every univariate N

μ, σ2
random variable, when viewed as a one dimen-
sional random vector, is a Gaussian random vector.
Proof:
Such a univariate random variable has the same law as
σW + μ, when W is a standard univariate Gaussian.
(ii) Any deterministic vector is a Gaussian vector.
Proof: Choose the matrix A as the all-zero matrix 0.
(iii) If the components of X are independent univariate Gaussians (not necessarily
of equal variance), then X is a Gaussian vector.
Proof: Choose A to be an appropriate diagonal matrix.
For the purposes of stating the next proposition we remind the reader that the ran-
dom vectors X =

X(1), . . . , X(nx)T and Y =

Y (1), . . . , Y (ny)T are independent
512
The Multivariate Gaussian Distribution
if, for every choice of ξ1, . . . , ξnx ∈R and η1, . . . , ηny ∈R,
Pr
%
X(1) ≤ξ1, . . . , X(nx) ≤ξnx, Y (1) ≤η1, . . . , Y (ny) ≤ηny
&
= Pr
%
X(1) ≤ξ1, . . . , X(nx) ≤ξnx
&
Pr
%
Y (1) ≤η1, . . . , Y (ny) ≤ηny
&
.
The following proposition is a consequence of the fact that if X1 & X2 are inde-
pendent, X1
L= X′
1, X2
L= X′
2, and X′
1 & X′
2 are independent, then
	
X1
X2

L=
	
X′
1
X′
2

.
Proposition 23.6.2 (Stacking Independent Gaussian Vectors). Stacking two in-
dependent Gaussian vectors on top of each other results in a Gaussian vector.
Proof. Let the random n1-vector X1 = (X(1)
1 , . . . , X(n1)
1
)T be Gaussian, and let the
random n2-vector X2 = (X(1)
2 , . . . , X(n2)
2
)T be Gaussian and independent of X1.
We need to show that the (n1 + n2)-vector

X(1)
1 , . . . , X(n1)
1
, X(1)
2 , . . . , X(n2)
2
T
(23.41)
is Gaussian.
Let the n1 × m1 matrix A1 and the vector μ1 ∈Rn1 represent X1 in the sense that
X1
L= A1W1 + μ1,
(23.42)
whenever W1 is a standard Gaussian m1-vector. Likewise, let the n2 × m2 ma-
trix A2 and the vector μ2 ∈Rn2 represent X2 in the sense that
X2
L= A2W2 + μ2,
(23.43)
whenever W2 is a standard Gaussian m2-vector. Let W be a standard Gaussian
(m1 +m2)-vector and let W1 comprise its ﬁrst m1 components and W2 its last m2
components.
It follows from the deﬁnition of a standard Gaussian vector that
W1 and W2 are independent standard Gaussian vectors. Since they are standard,
both (23.42) and (23.43) hold. And since they are independent, so are their aﬃne
transformations, so
A1W1 + μ1 is independent of A2W2 + μ2.
(23.44)
We conclude the proof by showing that the vector (23.41) can be represented using
the (n1 +n2)×(m1 +m2) block-diagonal matrix A of diagonal components A1 and
A2, and using the vector μ ∈Rn1+n2 that results when the vector μ1 is stacked on
top of the vector μ2:
A =
	
A1
0
0
A2

μ =
	
μ1
μ2

.
(23.45)
23.6 Gaussian Random Vectors
513
Indeed, if W is a standard Gaussian (m1 + m2)-vector and if W1 and W2 are as
above, then
AW + μ =
	
A1
0
0
A2

 	
W1
W2

+
	
μ1
μ2

(23.46)
=
	
A1W1 + μ1
A2W2 + μ2

(23.47)
L=
	
X1
X2

,
(23.48)
where the ﬁrst equality follows from the deﬁnitions of A and μ (23.45) and of W1
and W2; the second by computing the matrix product in blocks; and where the
equality in distribution can be argued as follows: The ﬁrst n1 components of the
vector on the RHS of (23.47) have the same joint law as the ﬁrst n1 components
of the vector on the RHS of (23.48) by (23.42), and likewise the last n2 componets
by (23.43). Additionally, in both vectors the ﬁrst n1 components and the last n2
components are independent: in the former vector this follows from (23.44) and in
the latter from the hypothesis that X1 and X2 are independent.
Proposition 23.6.3 (An Aﬃne Transformation of a Gaussian Is a Gaussian).
Let X be a Gaussian n-vector. If C is a ν × n matrix and if d ∈Rν, then the
random ν-vector CX + d is Gaussian.
Proof. If X
L= AW + μ, where A is a deterministic n × m matrix, W is a standard
Gaussian m-vector, and μ ∈Rn, then
CX + d
L= C(AW + μ) + d
= (CA)W + (Cμ + d),
(23.49)
which demonstrates that CX+d is Gaussian, because (CA) is a deterministic ν ×m
matrix, W is a standard Gaussian m-vector, and Cμ + d is a deterministic vector
in Rν.
This proposition has some important consequences. The ﬁrst is that if we permute
the components of a Gaussian vector then the resulting vector is also Gaussian.
This explains why we sometimes say of random variables that they are jointly Gaus-
sian without specifying an order. Indeed, by the following corollary, the Gaussianity
of (X, Y, Z)T is equivalent to the Gaussianity of (Y, X, Z)T, etc.
Corollary 23.6.4. Permuting the components of a Gaussian vector results in a
Gaussian vector.
Proof. Follows from Proposition 23.6.3 by choosing C to be the appropriate per-
mutation matrix, i.e., the matrix that results from permuting the columns of the
identity matrix. For example,
⎛
⎝
X(3)
X(1)
X(2)
⎞
⎠=
⎛
⎝
0
0
1
1
0
0
0
1
0
⎞
⎠
⎛
⎝
X(1)
X(2)
X(3)
⎞
⎠.
514
The Multivariate Gaussian Distribution
Corollary 23.6.5 (Subsets of Jointly Gaussians Are Jointly Gaussian). Construct-
ing a random p-vector from a Gaussian n-vector by picking p of its components
(allowing for repetition) yields a Gaussian vector.
Proof. Let X be a Gaussian n-vector. For any choice of j1, . . . , jp ∈{1, . . . , n},
we can express the random p-vector (X(j1), . . . , X(jp))T as CX, where C is a deter-
ministic p × n matrix whose Row-ν Column-ℓcomponent is given by
[C]ν,ℓ= I{ℓ= jν}.
For example
	
X(3)
X(1)

=
	
0
0
1
1
0
0

 ⎛
⎝
X(1)
X(2)
X(3)
⎞
⎠.
The result thus follows from Proposition 23.6.3.
Proposition 23.6.6. Each component of a Gaussian vector is a univariate Gaus-
sian.
Proof. Let X be a Gaussian n-vector, and let j ∈{1, . . . , n} be arbitrary. We
need to show that X(j) is Gaussian. Since X is Gaussian, there exist an n × m
matrix A, a vector μ ∈Rn, and a standard Gaussian W such that the vector X
has the same law as the random vector AW + μ (Deﬁnition 23.1.1). In particular,
the j-th component of X has the same law as the j-th component of AW + μ, i.e.,
X(j) L=
m

ℓ=1
a(j,ℓ)W (ℓ) + μ(j),
j ∈{1, . . . , n}.
The sum on the RHS is a linear combination of the independent univariate Gaus-
sians W (1), . . . , W (m) and is thus, by Proposition 19.7.3, Gaussian. The result of
adding μ(j) is still Gaussian.
We caution the reader that while each component of a Gaussian vector has a
univariate Gaussian distribution, there exist random vectors that are not Gaussian
and that yet have Gaussian components.
23.6.2
The Mean and Covariance Determine the Law of a Gaussian
From (23.39) it follows that if X
L= AW + μ, where W is a standard Gaussian,
then μ must be equal to E[X]. Thus, the mean of X fully determines the vector μ.
The matrix A, however, is not determined by the covariance of X. Indeed, by
(23.39), the covariance matrix KXX of X is equal to AAT, so KXX only determines
the product AAT.
Since there are many diﬀerent ways to express KXX as the
product of a matrix by its transpose, there are many choices of A (even of diﬀerent
dimensions) that result in AW + μ having the given covariance matrix. Prima
facie, one might think that these diﬀerent choices for A yield diﬀerent Gaussian
distributions. But this is not the case. In this section we shall show that, while the
23.6 Gaussian Random Vectors
515
choice of A is not unique, all choices that result in AAT being equal to the given
covariance matrix KXX give rise to the same distribution.
We shall derive this result by computing the characteristic function ΦX(·) of a
random n-vector X whose law is equal to the law of AW + μ, where W, A, and μ
are as above and by then showing that ΦX(·) depends on A only via AAT, i.e.,
that ΦX(ϖ) can be computed for every ϖ ∈Rn from ϖ, AAT, and μ. Since, by
(23.39), AAT is equal to the covariance matrix KXX of X, it will follow that the
characteristic functions of all Gaussian vectors of a given mean vector and a given
covariance matrix are identical. Since random vectors of identical characteristic
functions must have identical distributions (Proposition 23.4.4), it will follow that
all Gaussian vectors of a given mean vector and a given covariance matrix have
identical distributions.
We thus proceed to compute the characteristic function of a random n-vector X
whose law is the law of AW + μ, where W is a standard Gaussian m-vector, A
is n × m, and μ ∈Rn. By (23.39) it follows that KXX = AAT. To that end we
need to compute E

eiϖTX
for every ϖ ∈Rn. From Proposition 23.6.3 (with the
substitution of the 1 × n matrix ϖT for C and of the scalar zero for d), it follows
that ϖTX is a Gaussian vector with only one component. By Proposition 23.6.6,
this sole component is a univariate Gaussian. Its mean is, by (23.25a), ϖTμ and
its variance is, by (23.30), ϖT KXX ϖ. Thus,
ϖTX ∼N

ϖTμ, ϖT KXX ϖ

,
ϖ ∈Rn.
(23.50)
Using the expression (19.29) for the characteristic function of the univariate Gaus-
sian distribution (with the substitution ϖTμ for μ, the substitution ϖT KXX ϖ
for σ2, and the substitution 1 for ϖ), we obtain that the characteristic func-
tion ΦX(·), which is deﬁned as E

eiϖTX
, is given by
ΦX(ϖ) = e−1
2 ϖT KXX ϖ+iϖTμ,
ϖ ∈Rn.
(23.51)
Since this characteristic function is fully determined by the mean vector and the
covariance matrix of X, it follows that the distribution is also determined by the
mean and covariance. We have thus proved:
Theorem 23.6.7 (The Mean and Covariance of a Gaussian Determine its Law).
Two Gaussian vectors of equal mean vectors and of equal covariance matrices have
identical distributions.
Note 23.6.8. Theorem 23.6.7 and Proposition 23.6.1 combine to prove that for
every μ ∈Rn and every n × n positive semideﬁnite matrix K there exists one, and
only one, Gaussian distribution of mean μ and covariance matrix K. We denote
this Gaussian distribution by N(μ, K).
By (23.51) it follows that if X ∼N(μ, K) then
ΦX(ϖ) = e−1
2 ϖTKϖ+iϖTμ,
ϖ ∈Rn.
(23.52)
516
The Multivariate Gaussian Distribution
Theorem 23.6.7 has important consequences, one of which has to do with the
properties of independence and uncorrelatedness. Recall that any two independent
random variables (of ﬁnite mean) are also uncorrelated.
The reverse is not in
general true. But for jointly Gaussians it is: if X and Y are jointly Gaussian, then
X and Y are independent if, and only if, they are uncorrelated. More generally:
Corollary 23.6.9. Let X be a centered Gaussian (n1 + n2)-vector. Let the random
n1-vector X1 = (X(1), . . . , X(n1))T correspond to its ﬁrst n1 components, and let
X2 = (X(n1+1), . . . , X(n1+n2))T correspond to the rest of its components. Then the
vectors X1 and X2 are independent if, and only if, they are uncorrelated, i.e., if,
and only if,
E

X1XT
2

= 0.
(23.53)
Proof. The easy direction, which has nothing to do with Gaussianity, is that if
X1 and X2 are centered and independent, then (23.53) holds.
Indeed, by the
independence and the fact that the vectors are of zero mean we have
E

X1XT
2

= E[X1] E

XT
2

= E[X1] (E[X2])T
= 00T
= 0.
We now prove the reverse using the Gaussianity.
We begin by expressing the
covariance matrix of X in terms of the covariance matrices of X1 and X2 as
KXX =
-
E

X1XT
1

E

X1XT
2

E

X2XT
1

E

X2XT
2

.
=
	
KX1X1
0
0
KX2X2

,
(23.54)
where the second equality follows from (23.53).
Next, let X′
1 and X′
2 be independent random vectors such that X′
1
L= X1 and
X′
2
L= X2. Let X′ be the (n1 + n2)-vector that results from stacking X′
1 on top
of X′
2. Since X is Gaussian, it follows from Corollary 23.6.5 that X1 must also
be Gaussian, and since X′
1 has the same law as X1, it too is Gaussian. Similarly,
X′
2 is also Gaussian. And since X′
1 and X′
2 are, by construction, independent, it
follows from Proposition 23.6.2 that X′ is a centered Gaussian.
Having established that X′ is Gaussian, we next compute its covariance matrix.
Since, by construction, X′
1 and X′
2 are independent and centered,
KX′X′ =
	KX′
1X′
1
0
0
KX′
2X′
2

=
	
KX1X1
0
0
KX2X2

,
(23.55)
where the second equality follows because the equality in law between X′
1 and X1
implies that KX′
1X′
1 = KX1X1 and similarly for X′
2.
23.6 Gaussian Random Vectors
517
Comparing (23.55) and (23.54) we conclude that X and X′ are centered Gaussians
of identical covariance matrices. Consequently, by Theorem 23.6.7, X′
L= X. And
since the ﬁrst n1 components of X′ are independent of its last n2 components, the
same must also be true for X.
Corollary 23.6.10. If the components of a Gaussian random vector are uncorrelated
and its covariance matrix therefore diagonal, then its components are independent.
Proof. Let X be a Gaussian random vector whose components are uncorrelated
and whose covariance matrix KXX therefore diagonal. Denote the diagonal ele-
ments of KXX by λ1, . . . , λn. Let μ be the mean vector of X. Another Gaussian
vector of this mean and of this covariance matrix is the Gaussian vector whose
components are independent N

μ(j), λj

. Since the mean and covariance deter-
mine the distribution of Gaussian vectors, it follows that the two vectors, in fact,
have identical laws so the components of X are also independent.
Another consequence of the fact that there is only one multivariate Gaussian distri-
bution of a given mean vector and of a given covariance matrix has to do with pair-
wise independence and independence. Recall that the random variables X1, . . . , Xn
are pairwise independent if for each pair of distinct indices ν′, ν′′ ∈{1, . . . , n}
the random variables Xν′ and Xν′′ are independent, i.e., if for all such ν′, ν′′ and
all ξν′, ξν′′ ∈R
Pr

Xν′ ≤ξν′, Xν′′ ≤ξν′′
= Pr

Xν′ ≤ξν′
Pr[Xν′′ ≤ξν′′].
(23.56)
The random variables X1, . . . , Xn are independent if for all ξ1, . . . , ξn in R
Pr

Xj ≤ξj, for all j ∈{1, . . . , n}

=
n
@
j=1
Pr

Xj ≤ξj

.
(23.57)
Independence implies pairwise independence, but the two are not equivalent. One
can ﬁnd triples of random variables that are pairwise independent but not inde-
pendent.9 But if X1, . . . , Xn are jointly Gaussian, then pairwise independence is
equivalent to independence:
Corollary 23.6.11. If the components of a Gaussian random vector are pairwise
independent, then they are independent.
Proof. If the components of a Gaussian vector are pairwise independent, then they
are uncorrelated and hence, by Corollary 23.6.10, independent.
Corollary 23.6.12. If W is a standard Gaussian n-vector, and if U is an n × n
orthogonal matrix, then UW is also a standard Gaussian vector.
9A classical example is the triple X, Y, Z where X and Y are IID each taking on the values
±1 equiprobably and where Z is their product.
518
The Multivariate Gaussian Distribution
Proof. By Deﬁnition 23.1.1 it follows that the random vector UW is a centered
Gaussian. By (23.39) we obtain that the orthogonality of the matrix U implies
that the covariance matrix of this centered Gaussian is the identity matrix, which
is also the covariance matrix of W; see (23.37). Consequently, UW and W are
two centered Gaussian vectors of identical covariance matrices and hence, by The-
orem 23.6.7, of equal law. Since W is standard, this implies that UW must also
be standard.
The next corollary shows that if X is a centered Gaussian n-vector, then X
L= AW
for a standard Gaussian n-vector W and some square matrix A. That is, if the
law of an n-vector X is equal to the law of ˜A ˜
W where ˜A is an n × m matrix and
where ˜
W is a standard Gaussian m-vector, then the law of X is also identical to
the law of AW, where A is some n×n matrix and where W is a standard Gaussian
n-vector. Consequently, we could have required in Deﬁnition 23.1.1 that the matrix
A be square without changing the set of distributions that we deﬁne as Gaussian.
Corollary 23.6.13. If X is a centered Gaussian n-vector, then there exists a de-
terministic square n × n matrix A such that X
L= AW, where W is a standard
Gaussian n-vector.
Proof. Let KXX denote the covariance matrix of X. Being a covariance matrix,
KXX must be positive semideﬁnite (Proposition 23.6.1). Consequently, by Propo-
sition 23.3.5, there exists some n × n matrix S such that
KXX = STS.
(23.58)
Consider now the centered Gaussian STW, where W is a standard Gaussian n-
vector.
By (23.39), the covariance matrix of STW is STS, which by (23.58) is
equal to KXX. Thus X and STW are centered Gaussians of the same covariance,
and so they must be of the same law.
We have thus established that the law
of X is the same as the law of the product of a square matrix (ST) by a standard
Gaussian (W).
23.6.3
A Canonical Representation of a Centered Gaussian
The representation of a centered Gaussian vector as the result of the multiplication
of a deterministic matrix by a standard Gaussian vector is not unique. Indeed,
whenever the n × m matrix A satisﬁes AAT = K it follows that if W is a standard
Gaussian m-vector, then AW ∼N(0, K). (This follows because AW is a random
n-vector of covariance matrix AAT (23.39); it is, by Deﬁnition 23.1.1, a centered
Gaussian; and all centered Gaussians of a given covariance matrix have the same
law.) We saw in Corollary 23.6.13 that A can always be chosen as a square matrix.
Thus, to every K ⪰0 there exists a square matrix A such that AW ∼N(0, K). In
this section we shall focus on a particular choice of the matrix A that is useful in
the analysis of Gaussian vectors. In this representation A is a square matrix that
can be written as the product of an orthogonal matrix by a diagonal matrix. The
diagonal matrix acts on W by stretching and shrinking its components, and the
orthogonal matrix then rotates (and possibly reﬂects) the result.
23.6 Gaussian Random Vectors
519
Theorem 23.6.14 (A Canonical Representation of a Gaussian Vector). Let X be
a centered Gaussian n-vector of covariance matrix KXX. Then
X
L= UΛ1/2W,
where W is a standard Gaussian n-vector; the n × n matrix U is orthogonal; the
n × n matrix Λ is diagonal; the diagonal elements of Λ are the eigenvalues of KXX;
and the j-th column of U is an eigenvector corresponding to the eigenvalue of KXX
that is equal to the j-th diagonal element of Λ.
Proof. By Proposition 23.6.1, KXX is positive semideﬁnite and a fortiori sym-
metric.
Consequently, by Proposition 23.3.5, there exists a diagonal matrix Λ
whose diagonal elements are the eigenvalues of KXX and there exists an orthogo-
nal matrix U such that KXX U = UΛ, so the j-th column of U is an eigenvector
corresponding to the eigenvalue given by the j-th diagonal element of Λ. Since
KXX ⪰0, it follows that all its eigenvalues are nonnegative, and we can deﬁne the
matrix Λ1/2 as the matrix whose components are the componentwise nonnegative
square roots of the matrix Λ. As in (23.21), choose S = Λ1/2UT. We then have
that KXX = STS. If W is a standard Gaussian, then STW is a centered Gaussian
of zero mean and covariance STS. Since STS = KXX and since there is only one
centered multivariate Gaussian distribution of a given covariance matrix, it follows
that the law of STW ( = UΛ1/2W) is the same as the law of X.
Corollary 23.6.15. A centered Gaussian vector can be expressed as the result of
an orthogonal transformation applied to a random vector whose components are
independent centered univariate Gaussians of diﬀerent variances. These variances
are the eigenvalues of the covariance matrix.
Proof. Because the matrix Λ in the theorem is diagonal, we can write Λ1/2 W as
Λ1/2 W =
⎛
⎜
⎝
√λ1 W (1)
...
√λn W (n)
⎞
⎟
⎠,
where λ1, . . . , λn are the diagonal elements of Λ, i.e., the eigenvalues of KXX. Thus,
the random vector Λ1/2 W has independent components with the ν-th component
being N(0, λν).
Figures 23.1 and 23.2 demonstrate this canonical representation. They depict the
contour lines and mesh plots of the density functions of the following four two-
dimensional Gaussian vectors:
X1 =
	
1
0
0
1

W,
KX1X1 = I2,
X2 =
	
2
0
0
1

W,
KX2X2 =
	
4
0
0
1

,
X3 =
	
1
0
0
2

W,
KX3X3 =
	
1
0
0
4

,
X4 =
1
√
2
	
1
1
−1
1

 	
1
0
0
2

W,
KX4X4 = 1
2
	
5
3
3
5

,
520
The Multivariate Gaussian Distribution
1
2
3
4
5
6
7
1
2
3
4
5
6
7
1
2
3
4
5
6
7
1
3
5
7
9
11
13
15
−2
0
2
−2
0
2
−2
0
2
−2
0
2
−2
0
2
−2
0
2
−2
0
2
−2
0
2
x(1)
3
x(2)
3
x(1)
4
x(2)
4
x(1)
1
x(2)
1
x(1)
2
x(2)
2
Figure 23.1: Contour plot of the density of four diﬀerent two dimensional Gaussian
random variables: from left to right and top to bottom X1, . . . , X4.
where W is a standard Gaussian vector with two components.
Theorem 23.6.14 can be used to ﬁnd a linear transformation that transforms a
given Gaussian vector to a standard Gaussian. The following is the multivariate
version of the univariate result showing that if X ∼N

μ, σ2
, where σ2 > 0, then
(X −μ)/σ has a N(0, 1) distribution (19.8).
Proposition 23.6.16 (From Gaussians to Standard Gaussians). Let the random
n-vector X be N(μ, K), where K ≻0 and μ ∈Rn. Let the n × n matrices Λ and U
be such that Λ is diagonal, U is orthogonal, and KU = UΛ. Then
Λ−1/2UT(X −μ) ∼N(0, In) ,
where Λ−1/2 is the diagonal matrix whose diagonal entries are the reciprocals of the
square roots of the diagonal elements of Λ.
Proof. Since an aﬃne transformation of a Gaussian vector is Gaussian (Proposi-
tion 23.6.3), it follows that Λ−1/2UT(X −μ) is a Gaussian vector. And since the
mean and covariance of a Gaussian vector fully specify its law (Theorem 23.6.7),
the result will follow once we show that the mean of Λ−1/2UT(X −μ) is the zero
vector and its covariance matrix is the identity matrix. This can be readily veriﬁed
using (23.27).
23.6 Gaussian Random Vectors
521
−5
0
5
−5
0
5
−5
0
5
−5
0
5
−5
0
5
−5
0
5
−5
0
5
−5
0
5
0.05
0.1
0.15
0.05
0.1
0.15
0.05
0.1
0.15
0.05
0.1
0.15
x(1)
3
x(2)
3
x(1)
1
x(2)
1
x(1)
4
x(2)
4
x(1)
2
x(2)
2
Figure 23.2: Mesh plots of the density functions of Gaussian random vectors: from
left to right and top to down X1, . . . , X4.
23.6.4
The Density of a Gaussian Vector
As we saw in Corollary 23.4.2, if the covariance matrix of a centered vector is
singular, then at least one of its components can be expressed as a deterministic
linear combination of its other components. Consequently, random vectors with
singular covariance matrices cannot have a density. If the covariance matrix is
nonsingular, then the vector may or may not have a density. If it is Gaussian, then
it does. In this section we shall derive the density of the multivariate Gaussian
distribution when the covariance matrix is nonsingular, i.e., when it is positive
deﬁnite
We begin with the centered case. To derive the density of a centered Gaussian
n-vector of positive deﬁnite covariance matrix K we shall use Theorem 23.6.14 to
represent the N(0, K) distribution as the distribution of UΛ1/2W where U is an
orthogonal matrix and Λ is a diagonal matrix satisfying KU = UΛ. Note that Λ
is nonsingular because its diagonal elements are the eigenvalues of K, which we
assume to be positive deﬁnite.
Let
B = UΛ1/2,
(23.59)
so the density we are after is the density of BW. Note that, by (23.59),
BBT = UΛ1/2Λ1/2UT
−5
0
5
−5
0
5
−5
0
5
−5
0
5
−5
0
5
−5
0
5
−5
0
5
−5
0
5
0.05
0.1
0.15
0.05
0.1
0.15
0.05
0.1
0.15
0.05
0.1
0.15
x(1)
3
x(2)
3
x(1)
1
x(2)
1
x(1)
4
x(2)
4
x(1)
2
x(2)
2
522
The Multivariate Gaussian Distribution
= UΛUT
= K.
(23.60)
Also, by (23.60),
|det(B)| =

det(B) det(B)
=
3
det(B) det(BT)
=
3
det(BBT)
=

det(K),
(23.61)
where the ﬁrst equality follows by expressing |x| as
√
x2; the second follows because
a square matrix and its transpose have the same determinant; the third because the
determinant of the product of square matrices is the product of the determinants;
and where the last equality follows from (23.60).
Using the formula for computing the density of BW from that of W (Theo-
rem 17.3.4), we have that if X
L= BW, then
fX(x) = fW

B−1x

|det(B)|
=
exp

−1
2

B−1x
T
B−1x

(2π)n/2|det(B)|
=
exp

−1
2xT
B−1T
B−1x

(2π)n/2|det(B)|
=
exp

−1
2xT
BBT−1x

(2π)n/2|det(B)|
= exp

−1
2xTK−1x

(2π)n/2|det(B)|
= exp

−1
2xTK−1x

(2π)n/2
det(K)
,
where the second equality follows from the density of the standard Gaussian (23.36);
the third from the rule for the transpose of the product of matrices (23.3); the fourth
from the representation of the inverse of the product of matrices as the product
of the inverses in reverse order (AB)−1 = B−1A−1 and because transposition and
inversion commute; the ﬁfth from (23.60); and the sixth from (23.61). It follows
that if X ∼N(0, K) where K is nonsingular, then
fX(x) = exp

−1
2xTK−1x


(2π)ndet(K)
,
x ∈Rn.
Accounting for the mean, we have that if X ∼N(μ, K) where K is nonsingular,
then
fX(x) = exp

−1
2(x −μ)TK−1(x −μ)


(2π)ndet(K)
,
x ∈Rn.
(23.62)
23.7 Jointly Gaussian Vectors
523
23.6.5
Linear Functionals of Gaussian Vectors
A linear functional on Rn is a linear mapping from Rn to R. For example, if α
is any ﬁxed vector in Rn, then the mapping
x 	→αTx
(23.63)
is a linear functional on Rn. In fact, as we next show, every linear functional on Rn
has this form. This can be proved by using linearity to verify that we can choose
the j-th component of α to equal the result of applying the linear functional to
the vector ej whose components are all zero except for its j-th component which
is equal to one.
If X is a Gaussian n-vector and if α ∈Rn, then, by Proposition 23.6.3 (applied
with the substitution of the 1×n matrix αT for C), it follows that αTX is a Gaus-
sian vector with only one component. By Proposition 23.6.6, this sole component
must have a univariate Gaussian distribution. We thus conclude that the result of
applying a linear functional to a Gaussian vector is a Gaussian random variable.
We next show that the reverse is also true: if X is of mean μ and of covariance
matrix K and if the result of applying every linear functional to X has a univari-
ate Gaussian distribution, then X ∼N(μ, K).10 To prove this result we compute
the characteristic function of X. For every ϖ ∈Rn the mapping x 	→ϖTx is
a linear functional on Rn. Consequently, our assumption that the result of the
application of every linear functional to X has a univariate Gaussian distribution
implies (23.50). From here we can follow the steps leading to (23.52) to conclude
that the characteristic function of X must be given by the RHS of (23.52). Since
this is also the characteristic function of a N(μ, K) random vector, it follows that
X ∼N(μ, K), because random vectors of identical characteristic functions must
have identical distributions (Proposition 23.4.4). We have thus proved:
Theorem 23.6.17 (Gaussian Vectors and Linear Functionals). A random vector X
is Gaussian if, and only if, every linear functional of X has a univariate Gaussian
distribution.
23.7
Jointly Gaussian Vectors
Three miracles occur when we compute the conditional distribution of X given
Y = y for jointly Gaussian random vectors X and Y. Before describing these
miracles we need to deﬁne jointly Gaussian vectors.
Deﬁnition 23.7.1 (Jointly Gaussian Vectors). Two random vectors are said to be
jointly Gaussian if the vector that results when one is stacked on top of the other
is Gaussian.
10It is not diﬃcult to show that the assumption that X is of ﬁnite variance is not necessary.
If every linear functional of X is of ﬁnite variance, then X must be of ﬁnite variance. Thus, we
could have stated the result as follows: if a random vector is such that the result of applying
every linear functional to it is a univariate Gaussian, then it is a multivariate Gaussian.
524
The Multivariate Gaussian Distribution
That is, the random nx-vector X = (X(1), . . . , X(nx))T and the random ny-vector
Y = (Y (1), . . . , Y (ny))T are jointly Gaussian if the random (nx + ny)-vector

X(1), . . . , X(nx), Y (1), . . . , Y (ny)T
is Gaussian.
By Corollary 23.6.5, the random vectors X and Y can only be jointly Gaussian if
each is Gaussian. But this is not enough: both X and Y can be Gaussian without
them being jointly Gaussian.
However, if X and Y are independent Gaussian
vectors, then, by Proposition 23.6.2, they are jointly Gaussian.
Proposition 23.7.2. Independent Gaussian vectors are jointly Gaussian.
By Corollary 23.6.9 we have:
Proposition 23.7.3. If two jointly Gaussian random vectors are uncorrelated, then
they are independent.
Having deﬁned jointly Gaussian random vectors we next turn to the main result of
this section. Loosely speaking, it states that if X and Y are jointly Gaussian, then
in computing the conditional distribution of X given Y = y three miracles occur:
(i) the conditional distribution is a multivariate Gaussian;
(ii) its mean vector is an aﬃne function of y;
(iii) and its covariance matrix does not depend on y.
Before stating this more formally, we justify two simplifying assumptions. The ﬁrst
assumption is that the covariance matrix of Y is nonsingular, so
KYY ≻0.
The reason is that if the covariance matrix of Y is singular, then, by Corol-
lary 23.4.2, some of its components are with probability one aﬃne functions of
the others, and we then have to consider two cases. If the realization y satisﬁes
these aﬃne relations, then we can just pick a subset of the components of Y that
determine all the other components and that have a nonsingular covariance matrix
as in Section 23.4.3 and ignore the other components of y; the ignored components
do not alter the conditional distribution of X given Y = y. The other case where
the realization y does not satisfy the relations that Y satisﬁes with probability one
can be ignored because it occurs with probability zero.
The second assumption we make is that both X and Y are centered. There is no
loss in generality in making this assumption for the following reason. Conditioning
on Y = y when Y has mean μy is equivalent to conditioning on Y −μy = y −μy.
And if X has mean μx, then we can compute the conditional distribution of X by
computing the conditional distribution of X−μx and by then shifting the resulting
distribution by μx. Thus, the conditional density fX|Y=y(·) is given by
fX|Y=y(x) = fX−μx|Y−μy=y−μy(x −μx),
(23.64)
23.7 Jointly Gaussian Vectors
525
where X −μx & Y −μy are jointly Gaussian and centered whenever X & Y are
jointly Gaussian. It is now straightforward to verify that if the miracles hold for
the centered case
x 	→fX−μx|Y−μy=y−μy(x)
then they also hold for the general case
x 	→fX−μx|Y−μy=y−μy(x −μx).
Theorem 23.7.4. Let X and Y be centered and jointly Gaussian with covariance
matrices KXX and KYY. Assume that KYY ≻0. Then the conditional distribution
of X conditional on Y = y is a multivariate Gaussian of mean
E

XYT
K−1
YY y
(23.65)
and covariance matrix
KXX −E

XYT
K−1
YY E

YXT
.
(23.66)
Moreover, the deviation of X from its conditional mean is independent of Y:
X −E

XYT
K−1
YY Y is independent of Y.
(23.67)
Proof. Let nx and ny denote the number of components of X and Y. Let D be
any deterministic real nx × ny matrix. Then clearly
X = DY +

X −DY

.
(23.68)
Since X and Y are jointly Gaussian, the vector

XT, YTT is Gaussian. Conse-
quently, since
	
X −DY
Y

=
	
Inx
−D
0
Iny

 	
X
Y

,
it follows from Proposition 23.6.3 that
(X −DY) and Y are centered and jointly Gaussian.
(23.69)
Suppose now that the matrix D is chosen so that (X−DY) and Y are uncorrelated:
E

(X −DY)YT
= 0.
(23.70)
By (23.69) and Proposition 23.7.3 it then follows that
(X −DY) is independent of Y.
(23.71)
Consequently, with this choice of D, the decomposition (23.68) expresses X as
the sum of two terms where the ﬁrst, DY, is fully determined by Y and where the
second, (X−DY), is independent of Y. It follows that the conditional distribution
of X given Y = y is the same as the distribution of (X−DY) but shifted by Dy. By
(23.69) and Corollary 23.6.5, (X −DY) is a centered Gaussian, so the conditional
distribution of X given Y = y is that of the centered Gaussian (X−DY) shifted by
the vector Dy. This already establishes the three “miracles” we discussed before:
526
The Multivariate Gaussian Distribution
the conditional distribution of X given Y = y is Gaussian; its mean Dy is a
linear function of Y; and its covariance matrix, which is the covariance matrix of
(X −DY), does not depend on the realization y of Y.
That the mean of the conditional distribution is as given in (23.65) and that the co-
variance matrix is as given in (23.66) now follow from straightforward calculations.
Indeed, by solving (23.70) for D we obtain
D = E

XYT
K−1
YY,
(23.72)
so Dy is given by (23.65). To show that the covariance of the conditional law of X
given Y = y is as given in (23.66), we note that this covariance is the covariance
of (X −DY), which is given by
E

(X −DY)(X −DY)T
= E

(X −DY)XT
−E

(X −DY)(DY)T
= E

(X −DY)XT
−E

(X −DY)YT
DT
= E

(X −DY)XT
= KXX −D E

YXT
= KXX −E

XYT
K−1
YY E

YXT
,
where the ﬁrst equality follows by opening the second set of parentheses; the second
by (23.3) and (23.25b); the third by (23.70); the fourth by opening the parentheses
and using the linearity of the expectation; and the ﬁnal equality by (23.72).
Finally, (23.67) follows from (23.71) and the explicit form of D (23.72).
Theorem 23.7.4 has important consequences in Estimation Theory. A key result
in Estimation Theory is that if after observing that Y = y for some y ∈Rny we
would like to estimate the random nx-vector X using a (Borel measurable) function
g: Rny →Rnx so as to minimize the estimation error
E

∥X −g(Y)∥2
,
(23.73)
then an optimal choice for g(·) is the conditional expectation
g(y) = E

X
 Y = y

,
y ∈Rny.
(23.74)
Theorem 23.7.4 demonstrates that if X and Y are jointly Gaussian and centered,
then E[X|Y = y] is a linear function of y and is explicitly given by (23.65). Thus,
for jointly Gaussian centered random vectors, there is no loss of optimality in
limiting ourselves to linear estimators.
The optimality of choosing g(·) as in (23.74) has a simple intuitive explanation. We
ﬁrst note that it suﬃces to establish the result when nx = 1, i.e., when estimating
a random variable rather than a random vector. Indeed, the squared-norm error in
estimating a random vector X with nx components is the sum of the squared errors
in estimating its components. To minimize the sum, one should therefore minimize
each of the terms. And the problem of estimating the j-th component of X based
on the observation Y = y is a problem of estimating a random variable. Stated
23.8 Moments and Wick’s Formula
527
diﬀerently, to estimate X so as to minimize the error (23.73) we should separately
estimate each of its components.
Having established that it suﬃces to prove the optimality of (23.74) when nx = 1,
we now assume that nx = 1 and denote the random variable to be estimated by X.
To study how to estimate X after observing that Y = y, we ﬁrst consider the
case where there is no observation. In this case, the estimate is a constant, and
by Lemma 14.4.1 the optimal choice of that constant is the mean E[X]. We now
view the general case where we observe Y = y as though there were no observables
but X had the a posteriori distribution given Y = y. Utilizing the result for the
case where there are no observables yields that estimating X by E[X |Y = y] is
optimal.
23.8
Moments and Wick’s Formula
We next describe without proof a technique for computing moments of centered
Gaussian vectors. A sketch of a proof can be found in (Zvonkin, 1997).
Theorem 23.8.1 (Wick’s Formula). Let X be a centered Gaussian n-vector and
let g1, . . . , g2k : Rn →R be an even number of (not necessarily diﬀerent) linear
functionals on Rn. Then
E

g1(X) g2(X) · · · g2k(X)

=

E

gp1(X) gq1(X)

E

gp2(X) gq2(X)

· · · E

gpk(X) gqk(X)

,
(23.75)
where the summation is over all permutations p1, q1, p2, q2, . . . , pk, qk of 1, 2, . . . , 2k
such that
p1 < p2 < · · · < pk
(23.76a)
and
p1 < q1,
p2 < q2,
. . . ,
pk < qk.
(23.76b)
The number of terms on the RHS of (23.75) is 1 × 3 × 5 × · · · × (2k −1).
Example 23.8.2. Suppose that n = 1, so X is a centered univariate Gaussian.
Let σ2 be its variance, and suppose we wish to compute E

X4
. We can express this
in the form of Theorem 23.8.1 with k = 2 and g1(x) = g2(x) = g3(x) = g4(x) = x.
By Wick’s Formula
E

X4
= E

g1(X) g2(X)

E

g3(X) g4(X)

+ E

g1(X) g3(X)

E

g2(X) g4(X)

+ E

g1(X) g4(X)

E

g2(X) g3(X)

= 3σ4,
which is in agreement with (19.31).
Example 23.8.3. Suppose that X is a bivariate centered Gaussian whose com-
ponents are of unit variance and of correlation coeﬃcient ρ ∈[−1, 1]. We com-
pute E

(X(1))2(X(2))2
using Theorem 23.8.1 by setting k = 2 and by deﬁning
528
The Multivariate Gaussian Distribution
g1(x) = g2(x) = x(1) and g3(x) = g4(x) = x(2). By Wick’s Formula
E
%
X(1)2
X(2)2&
= E

g1(X) g2(X)

E[g3(X) g4(X)] + E

g1(X) g3(X)

E

g2(X) g4(X)

+ E

g1(X) g4(X)

E

g2(X) g3(X)

= E
%
X(1)2&
E
%
X(2)2&
+ E
%
X(1)X(2)&
E
%
X(1)X(2)&
+ E
%
X(1)X(2)&
E
%
X(1)X(2)&
= 1 + 2ρ2.
(23.77)
Similarly,
E
%
X(1)3X(2)&
= 3ρ.
(23.78)
23.9
The Limit of Gaussian Vectors Is a Gaussian Vector
The results of Section 19.9 on limits of Gaussian random variables extend to Gaus-
sian vectors.
In this setting we consider random vectors X, X1, X2, . . . deﬁned
over the probability space (Ω, F, P). We say that the sequence of random vectors
X1, X2, . . . converges to the random vector X with probability one or almost
surely if
Pr
'
ω ∈Ω : lim
n→∞Xn(ω) = X(ω)
(
= 1.
(23.79)
The sequence X1, X2, . . . converges to the random vector X in probability if
lim
n→∞Pr

∥Xn −X∥≥ϵ

= 0,
ϵ > 0.
(23.80)
The sequence X1, X2, . . . converges to the random vector X in mean square if
lim
n→∞E
%
∥Xn −X∥2&
= 0.
(23.81)
Finally, the sequence of random vectors X1, X2, . . . taking values in Rd converges
to the random vector X weakly or in distribution if
lim
n→∞Pr

X(1)
n
≤ξ(1), . . . , X(d)
n
≤ξ(d)
= Pr

X(1) ≤ξ(1), . . . , X(d) ≤ξ(d)
(23.82)
for every vector ξ ∈Rd such that
lim
ϵ↓0 Pr

X(1) ≤ξ(1) −ϵ, . . . , X(d) ≤ξ(d) −ϵ

= Pr

X(1) ≤ξ(1), . . . , X(d) ≤ξ(d)
.
(23.83)
In analogy to Theorem 19.9.1 we next show that, irrespective of which of the above
forms of convergence we consider, if a sequence of Gaussian vectors converges to
some random vector X, then X must be Gaussian.
23.10 Conditionally-Independent Gaussian Vectors
529
Theorem 23.9.1. Let the random d-vectors X, X1, X2, . . . be deﬁned over a com-
mon probability space. Let X1, X2, . . . each be Gaussian (with possibly diﬀerent
mean vectors and covariance matrices). If the sequence X1, X2, . . . converges to X
in the sense of (23.79) or (23.80) or (23.81), then X must be Gaussian.
Proof. The proof is based on Theorem 23.6.17, which demonstrates that it suﬃces
to consider linear functionals of the vectors in the sequence and on the analogous
result for scalars (Theorem 19.9.1). We demonstrate the idea by considering the
case where the convergence is almost sure. If X1, X2, . . . converges almost surely
to X, then for every α ∈Rd the sequence αTX1, αTX2, . . . converges almost surely
to αTX. Since, by Theorem 23.6.17, linear functionals of Gaussian vectors are
univariate Gaussians, it follows that the sequence αTX1, αTX2, . . . is a sequence of
Gaussian random variables. And since it converges almost surely to αTX, it follows
from Theorem 19.9.1 that αTX must be Gaussian. Since this is true for every α
in Rd, it follows from Theorem 23.6.17 that X must be a Gaussian vector.
In analogy to Theorem 19.9.3 we have the following result on weakly converging
Gaussian vectors.
Theorem 23.9.2 (Weakly Converging Gaussian Vectors). Let the sequence of
random d-vectors X1, X2, . . . be such that Xn ∼N(μn, Kn) for n = 1, 2, . . . Then
the sequence converges in distribution to some limiting distribution, if, and only if,
there exist some μ ∈Rd and some d × d matrix K such that
μn →μ and Kn →K.
(23.84)
And if the sequence does converge in distribution, then it converges to the multi-
variate Gaussian distribution of mean vector μ and covariance matrix K.
Proof. See (Gikhman and Skorokhod, 1996, Chapter I, Section 3, Theorem 4).
23.10
Conditionally-Independent Gaussian Vectors
Given the covariance matrix of a Gaussian vector X, it is easy to check whether two
of its components X(j) and X(ℓ) are independent: they are independent if, and only
if, the Row-j Column-ℓcomponent of the covariance matrix is zero. More generally,
the η1-vector (X(j1), . . . , X(jη1)) is independent of the η2-vector (X(ℓ1), . . . , X(ℓη2))
if, and only if, for every 1 ≤ν′ ≤η1 and 1 ≤ν′′ ≤η2 the Row-ν′ Column-ν′′
component of the covariance matrix is zero (Proposition 23.7.3).
Is there an easy check for conditional independence? It turns out that there is,
though not in terms of the covariance matrix but in terms of its inverse—the
precision matrix. Such a check is described in the next theorem, which is the
section’s main result.
Theorem 23.10.1 (Conditional Independence and the Precision Matrix). Let X
be a Gaussian n-vector of components X(1), . . . , X(n) and nonsingular covariance
matrix K. Let J , K, and L be disjoint sets whose union is the set {1, . . . , n}. Then
the following statements are equivalent.
530
The Multivariate Gaussian Distribution
1
2
3
4
5
⎛
⎜
⎜
⎜
⎜
⎝
·
·
·
·
0
·
·
0
·
·
·
0
·
·
0
·
·
·
·
·
0
·
0
·
·
⎞
⎟
⎟
⎟
⎟
⎠
Figure 23.3: The zeros of a precision matrix and the graph associated with it.
(a) The collections of random variables {X(j), j ∈J } and {X(k), k ∈K} are
conditionally independent given {X(ℓ), ℓ∈L}.
(b) For every j ∈J and k ∈K the Row-j Column-k entry of K−1 is zero.
This theorem shows that the conditional independencies among the components
of a Gaussian vector are determined by the locations of the zeros in the precision
matrix. These locations can be described using a graph that we associate with
the precision matrix and from which the conditional independencies are easily read
oﬀ. The graph is an undirected graph whose nodes are the elements of the set
{1, . . . , n} and where an edge connects Node j with Node k whenever j ̸= k and
the Row-j Column-k entry of K−1 is nonzero. See Figure 23.3 for an example of a
precision matrix and the graph associated with it. Statement (b) in the theorem is
equivalent to the condition that removing the nodes in L and all the edges touching
them results in a graph with no path connecting a node in J with a node in K. For
example, suppose that n = 5 and the zeros in K−1 and the associated graph are as
in Figure 23.3. In this case, X(5) and (X(1), X(3)) are conditionally independent
given (X(2), X(4)). Likewise, (X(2), X(5)) and X(3) are conditionally independent
given (X(1), X(4)).
Before proving the theorem, we ﬁrst argue that it suﬃces to prove it for the special
case where the elements of J , K, and L are consecutive:
J = {1, . . . , # J }, K = {# J + 1, . . . , # J + # K}, L = {n −# L + 1, . . . , n}.
To see why, let π(·) be a permutation on {1, . . . , n} that maps {1, . . . , # J } to J ;
that maps {# J + 1, . . . , # J + # K} to K; and that maps {n −# L + 1, . . . , n}
to L. Let ˜X be the random vector that results from permuting the components
of X
˜X(i) = X(π(i)),
i = 1, . . . , n.
(23.85)
Its covariance matrix ˜K is given (using the notation of Section 23.2) by
˜K

j,k =

K

π(j),π(k),
j, k ∈{1, . . . , n}.
(23.86)
And its precision matrix ˜K−1 is given by11
˜K−1
j,k =

K−1
π(j),π(k),
j, k ∈{1, . . . , n}.
(23.87)
11This is easily seen by deﬁning the permutation matrix P whose Row-i Column-j entry is
I{j = π(i)}; by noting that ˜X = PX; by using (23.27) to conclude that ˜K = PKPT; and by using
the inversion formula for a product of matrices and the fact that P is orthogonal.
23.10 Conditionally-Independent Gaussian Vectors
531
With these deﬁnition, Statement (a) in the theorem is equivalent to the statement
that the ﬁrst # J components of ˜X and the next # K components are conditionally
independent given the last # L components, and Statement (b) in the theorem is
equivalent to the statement that the Row-j Column-k entry of ˜K−1 is zero whenever
j ∈{1, . . . , # J } and k ∈{# J + 1, . . . , n −# L}. Proving the equivalence of the
statements about ˜X thus also proves the equivalence of the statements about X.
Before proceeding to prove the above special case, we formulate it as a proposition
using slightly diﬀerent language.
Proposition 23.10.2. Let the centered random vectors X ∈Rnx, Y ∈Rny, and
Z ∈Rnz be jointly Gaussian, and let the vector (XT, YT, ZT)T that results when
they are stacked on top of each other have a nonsingular covariance matrix K.
Represent its inverse using submatrices as
K−1 =
⎛
⎝
A
B
C
BT
D
E
CT
ET
F
⎞
⎠,
(23.88)
where A ∈Rnx×nx, B ∈Rnx×ny, C ∈Rnx×nz, D ∈Rny×ny, E ∈Rny×nz, F ∈
Rnz×nz, and where A, D, and F are symmetric.12 Then X and Y are conditionally
independent given Z if, and only if, B is zero.
The signiﬁcance of B being zero can be understood in the following way. Recalling
the density of a centered Gaussian (Section 23.6.4),
fX,Y,Z(x, y, z) = α exp

−1
2

xT
yT
zT
K−1 
xT
yT
zTT 
,
(23.89)
where
α = (2π)−
nx+ny+nz
2

det(K−1).
Using this and the representation (23.88), we obtain
ln fX,Y,Z(x, y, z) = ln α −1
2

xTAx + 2xTBy + 2xTCz + yTDy + 2yTEz + zTFz

.
(23.90)
We thus see that the logarithm of the Gaussian density function is always the sum
of a constant (ln α) and a quadratic form in x, y, and z. The matrix B in the
representation (23.88) is zero if, and only if, the quadratic form associated with
K−1 does not have any cross terms involving x and y.
Before we can prove Proposition 23.10.2 and thus conclude the proof of Theo-
rem 23.10.1, we need a factorization result on the joint probability density function
of conditionally independent random variables.
Suppose the random vectors X, Y, and Z have a joint probability density function.
Then X and Y are conditionally independent given Z if, and only if, X, Y, and Z
have a joint probability density function fX,Z,Y satisfying
fX,Y,Z(x, y, z) = fZ(z) fX|Z(x|z) fY|Z(y|z),
fZ(z) > 0.
(23.91)
12These matrices are symmetric because K is symmetric and hence so is its inverse.
532
The Multivariate Gaussian Distribution
Here fZ denotes the density of Z, and fX|Z and fY|Z denote conditional densities
(cf. Deﬁnition 20.11.2).
Clearly, if fX,Y,Z has the above form, then it can be written as the product of a
function of (x, z) and a function of (y, z). That is, fX,Y,Z factorizes as
fX,Y,Z(x, y, z) = g1(x, z) · g2(y, z).
(23.92)
Indeed, g1(x, z) can be chosen as fX,Z(x, z) and g2(y, z) as fY|Z(y|z). The next
proposition shows that the reverse is also true: a factorization of the form (23.92)
implies that X and Y are conditionally independent given Z.
Proposition 23.10.3 (Factorization and Conditional Independence). If X, Y, Z
are random vectors of joint density fX,Y,Z having the form (23.92), were g1(·, ·)
and g2(·, ·) are Borel measurable, then X and Y are conditionally independent
given Z.
Proof. Assume that fX,Y,Z has the form (23.92). Taking the absolute value of
both sides of (23.92), we conclude that if a factorization of the form (23.92) exists,
then it also exists with g1(·, ·) and g2(·, ·) nonnegative. Assume then that g1(·, ·)
and g2(·, ·) are nonnegative. (This will allow us later to use Fubini’s theorem to
change the orders of integration.)
The density of Z is obtained from the joint density by integrating over x and y
fZ(z) =

fX,Y,Z,(x, y, z) dx dy
=

g1(x, z) g2(y, z) dx dy,
(23.93)
and, for each z with fZ(z) > 0, the conditional density of X and Y given Z is
fX,Y|Z(x, y|z) = fX,Z,Y(x, z, y)
fZ(z)
= g1(x, z) g2(y, z)
fZ(z)
.
(23.94)
We conclude the proof by showing that the RHS of (23.94) is fX|Z(x|z) fY|Z(y|z).
For z with fZ(z) > 0,
fX|Z(x|z) fY|Z(y|z) = fX,Z(x, z)
fZ(z)
· fY,Z(y, z)
fZ(z)
=

g1(x, z) g2(y′, z) dy′
fZ(z)
·

g1(x′, z) g2(y, z) dx′
fZ(z)
= g1(x, z)

g2(y′, z) dy′
fZ(z)
· g2(y, z)

g1(x′, z) dx′
fZ(z)
= g1(x, z) g2(y, z)
fZ(z)
·

g2(y′, z) dy′ 
g1(x′, z) dx′
fZ(z)
= g1(x, z) g2(y, z)
fZ(z)
·

g1(x′, z) g2(y′, z) dx′ dy′
fZ(z)
= g1(x, z) g2(y, z)
fZ(z)
,
(23.95)
23.11 Additional Reading
533
where the last equality follows from (23.93). From (23.94) and (23.95) we conclude
that fX,Y,Z has the form (23.91).
We are now ready to prove Proposition 23.10.2 and to thus conclude the proof of
Theorem 23.10.1.
Proof of Proposition 23.10.2. We begin by proving that if B is zero, then X and
Y are conditionally independent given Z. Substituting 0 for B in (23.90) we obtain
ln fX,Y,Z(x, y, z) = ln α −1
2

xTAx + 2xTCz + yTDy + 2yTEz + zTFz

,
and hence the factorization
fX,Y,Z(x, y, z) = α exp

−1
2(xTAx + 2xTCz + zTFz)

exp

−1
2(yTDy + 2yTEz)

,
which implies the conditional independence of X and Y given Z (by Proposi-
tion 23.10.3).
To conclude the proof, we now assume that X and Y are conditionally independent
and prove that B must be zero. From the conditional independence and (23.91),
log fX,Y,Z(x, y, z) = log fX,Z(x, z) + log fY|Z(y|z).
(23.96)
The ﬁrst term on RHS, log fX,Z(x, z), is the logarithm of the joint density function
of X, Z and, as such, is a constant plus a quadratic form in (x, z). The second
term, log fY|Z(y|z) is the logarithm of the conditional density of Y given Z, which
is a noncentered Gaussian whose parameters are given in Theorem 23.7.4. Using its
explicit density, we conclude that log fY|Z(y|z) is a constant plus a quadratic form
in (y, z). Thus, log fX,Y,Z is the sum of a constant, a quadratic form in (x, z) and
a quadratic form in (y, z): it contains no cross terms involving x and y. Hence, B
must be zero.13
23.11
Additional Reading
There are numerous books on Matrix Theory that discuss orthogonal matrices and
positive semideﬁnite matrices. We mention here (Zhang, 2011, Section 6.2), (Her-
stein, 1975, Chapter 6, Section 6.10), and (Axler, 2015, Chapter 7) on orthogonal
matrices, and (Zhang, 2011, Chapter 7), (Axler, 2015, Chapter 7), and (Horn and
Johnson, 2013, Chapter 7) on positive semideﬁnite matrices. Much more on the
multivariate Gaussian distribution can be found in (Tong, 1990), (Johnson and
Kotz, 1972, Chapter 35), and (Simon, 2002). For more on estimation and linear
estimation, see (Poor, 1994) and (Kailath, Sayed, and Hassibi, 2000). And for more
on quadratic forms see (Herstein, 1975, Section 6.11, pp. 350–354) and (Halmos,
1958).
13If you are worried because, technically speaking, the joint density function is not unique, do
not be: Two joint density functions of (X, Y, Z) can only diﬀer on a set of Lebesgue measure
zero. Consequently, if they are both continuous, then they must be identical.
534
The Multivariate Gaussian Distribution
23.12
Exercises
Exercise 23.1 (Covariance Matrices). Which of the following matrices cannot be a co-
variance matrix of some real random vector?
A =
5
0
0
−1

,
B =
5
1
2
2

,
C =
 2
10
10
1

,
D =
 1
−1
−1
1

.
Exercise 23.2 (An Orthogonal Matrix of Determinant 1). Show that in Theorem 23.6.14
the orthogonal matrix U can be chosen to have determinant +1.
Exercise 23.3 (Deriving the Characteristic Function of Sum). Let X be a random 2-
vector of characteristic function ΦX(·). Express the characteristic function of the sum of
its components in terms of ΦX(·).
Exercise 23.4 (More on Independence). Show that the components X(1) and X(2) of a
random 2-vector X are independent if, and only if, the vector’s characteristic function
ΦX(·) factorizes as
ΦX(ϖ) = g1

ϖ(1)
g2

ϖ(2)
,
ϖ =

ϖ(1), ϖ(2)T ∈R2,
for some functions g1 and g2.
Exercise 23.5 (The Probabilities of Half Spaces). Prove that a probability distribution
on R2 is uniquely determined by the probabilities of all half planes.
(See (Feller, 1971, Chapter XV, Section 7, Footnote 8).)
Hint: You are asked to show that the values of Pr[αX + βY ≤γ] for all α, β, γ ∈R fully
determine the joint distribution of X and Y .
Exercise 23.6 (The Distribution of Linear Functionals). Let X and Y be random n-vectors
of components X(1), . . . , X(n) and Y (1), . . . , Y (n). Assume that for all deterministic co-
eﬃcients α1, . . . , αn ∈R the random variables n
ν=1 ανX(ν) and n
ν=1 ανY (ν) have the
same distribution, i.e.,

n

j=1
αjX(j) L=
n

j=1
αjY (j)

,
	
α1, . . . , αn ∈R

.
(i) Show that the characteristic function of X must be equal to that of Y.
(ii) Show that X and Y must have the same distribution.
Exercise 23.7 (Independence and Independence between Linear Functionals). Let the
random n-vector X and the random m-vector Y be such that for every α ∈Rn and
β ∈Rm the random variables αTX and βTY are independent. Prove that X and Y must
be independent.
Exercise 23.8 (A Mixture of Gaussians). Let X ∼N

μx, σ2
x

and Y ∼N

μy, σ2
y

be
Gaussian random variables. Let E take on the values 0 and 1 equiprobably and indepen-
dently of (X, Y ). Deﬁne the mixture RV
Z =

X
if E = 0,
Y
if E = 1.
Must Z be Gaussian? Can Z be Gaussian? Compute Z’s characteristic function.
23.12 Exercises
535
Exercise 23.9 (Multivariate Gaussians). Show that if Z is a univariate Gaussian, then
the random vector (Z, Z)T is a Gaussian vector. What is its canonical representation?
Exercise 23.10 (Manipulating Gaussians). Let W1, W2, . . . , W5 be IID N(0, 1). Deﬁne
Y = 3W1 + 4W2 −2W3 + W4 −W5 and Z = W1 −4W2 −2W3 + 3W4 −W5. What is the
joint distribution of (Y, Z)?
Exercise 23.11 (The Sum of Independent Gaussian Vectors). Let X and Y be indepen-
dent Gaussian random n-vectors, where X ∼N(μX, KXX) and Y ∼N(μY, KYY).
(i) Compute the mean vector and covariance matrix of the sum X + Y.
(ii) Must X + Y be a Gaussian random vector?
Exercise 23.12 (Largest Eigenvalue). Let X be a zero-mean Gaussian n-vector of covari-
ance matrix K ⪰0, and let λmax denote the maximal eigenvalue of K. Show that for some
random n-vector Z independent of X
X + Z ∼N

0, λmaxIn

,
where In denotes the n × n identity matrix.
Exercise 23.13 (The Error Probability Revisited). Show that p∗(correct) of (21.69) in
Exercise 21.5 can be rewritten as
p∗(correct) = 1
M exp
	
−Es
2σ2

E
+
exp
	 1
σ max
m

Ξ(m)
,
,
where (Ξ(1), . . . , Ξ(M))T is a centered Gaussian with a covariance matrix whose Row-j
Column-ℓentry is ⟨sj, sℓ⟩E.
Exercise 23.14 (Gaussian Marginals). Let X and Z be IID N(0, 1). Let Y = |Z| sgn(X),
where sgn(X) is 1 if X ≥0 and is −1 otherwise. Show that X is Gaussian, that Y is
Gaussian, but that they are not jointly Gaussian. Sketch the contour lines of their joint
probability density function.
Exercise 23.15 (A Linear Functional Is Gaussian). Suppose αTX is Gaussian for some
deterministic nonzero α ∈R2 and some random 2-vector X. Must X be Gaussian?
Exercise 23.16 (Independence, Uncorrelatedness and Gaussianity). Let the random vari-
ables X and H be independent with X ∼N(0, 1) and with H taking on the values ±1
equiprobably. Let Y = HX denote their product.
(i) Find the density of Y .
(ii) Are X and Y correlated?
(iii) Compute Pr
$
|X| ≥1
%
and Pr
$
|Y | ≥1
%
.
(iv) Compute the probability that both |X| and |Y | exceed 1.
(v) Are X and Y independent?
(vi) Is the vector (X, Y )T a Gaussian vector?
536
The Multivariate Gaussian Distribution
Exercise 23.17 (The Squared Euclidean Norm of Gaussian Vectors). Show that the sum
of the squares of the components of a centered Gaussian vector can be written as a linear
combination of the squares of independent standard Gaussians. What are the coeﬃcients?
Exercise 23.18 (Expected Maximum of Jointly Gaussians).
(i) Let (X1, X2, . . . , Xn, Y ) have an arbitrary joint distribution with E[Y ] = 0. Here
Y need not be independent of (X1, X2, . . . , Xn). Prove that
E
+
max
1≤j≤n

Xj + Y
,
= E
+
max
1≤j≤n

Xj
,
.
(ii) Use Part (i) to prove that if (U, V ) are jointly Gaussian and of zero mean, then
E
$
max{U, V }
%
=
&
E
$
(U −V )2%
2π
.
Exercise 23.19 (The Density of a Bivariate Gaussian). Let X and Y be jointly Gaussian
with means μx and μy and with positive variances σ2
x and σ2
y. Let
ρ = Cov[X, Y ]
σx σy
be their correlation coeﬃcient. Assume |ρ| < 1.
(i) Find the joint density of X and Y .
(ii) Find the conditional density of X given Y = y.
Exercise 23.20 (A Training Symbol). Conditional on (X1, X2) = (x1, x2), the observable
(Y1, Y2) is given by
Yν = Axν + Zν,
ν = 1, 2,
where Z1, Z2, and A are independent with Z1, Z2 ∼IID N

0, σ2
and A ∼N(0, 1).
Suppose that X1 = 1 (deterministically) and that X2 takes on the values ±1 equiprobably.
(i) Derive an optimal rule for guessing X2 based on (Y1, Y2).
(ii) Consider a decoder that operates in two stages.
In the ﬁrst stage the decoder
estimates A from Y1 with an estimator that minimizes the mean squared-error.
In the second stage it uses the ML decoding rule for guessing X2 based on Y2
by pretending that A is given by its estimate from the ﬁrst stage. Compute the
probability of error of this decoder. Is it optimal?
Exercise 23.21 (The Estimation Error Need not Be Independent of the Observable).
Let Y be a uniform binary RV, and let the conditional laws of the RV X given Y = 0 and
Y = 1 be diﬀerent but both of zero mean. Find the estimator of X based on Y of least
mean squared-error, and show that its error is not independent of the observable Y .
Exercise 23.22 (On Wick’s Formula). Let X be a centered Gaussian n-vector, and let
g1, . . . , g2k+1 : Rn →R be an odd number of (not necessarily diﬀerent) linear functionals
from Rn to R. Show that
E
$
g1(X) g2(X) · · · g2k+1(X)
%
= 0.
23.12 Exercises
537
Exercise 23.23 (Jointly Gaussians with Positive Correlation). Let X and Y be jointly
Gaussian with means μx and μy; positive variances σ2
x and σ2
y; and correlation coeﬃcient ρ
as in Exercise 23.19 satisfying |ρ| < 1.
(i) Show that, conditional on Y = y, the distribution of X is Gaussian with mean
μx + ρ σx
σy (y −μy) and variance σ2
x(1 −ρ2).
(ii) Show that if ρ ≥0, then the family fX|Y (x|y) has the monotone likelihood ratio
property that the mapping
x →fX|Y (x|y)
fX|Y (x|y′)
is nondecreasing whenever y′ ≤y. Here fX|Y (·|y) is the conditional density of X
given Y = y.
(iii) Show that if ρ ≥0, then the joint density fX,Y (·) has the Total Positivity of
Order 2 (TP2) property, i.e.,
fX,Y (x′, y) fX,Y (x, y′) ≤fX,Y (x, y) fX,Y (x′, y′),
	
x′ < x, y′ < y

.
See (Tong, 1990, Chapter 4, Section 4.3.1, Fact 4.3.1 and Theorem 4.3.1).
Exercise 23.24 (Price’s Theorem). Let X be a centered Gaussian n-vector of covariance
matrix Λ. Let λ(j,ℓ) = E
$
X(j)X(ℓ)%
be the Row-j Column-ℓentry of Λ. Let fX(x; Λ)
denote the density of X (when Λ is nonsingular).
(i) Expressing the FT of the partial derivative of a function in terms of the FT of the
original function and using the characteristic function of a Gaussian (23.52), derive
Plackett’s Identities
∂fX(x; Λ)
∂λ(j,j)
= 1
2
∂2fX(x; Λ)
∂(x(j))2
,
∂fX(x; Λ)
∂λ(j,ℓ)
= ∂2fX(x; Λ)
∂x(j)∂x(ℓ) ,
j ̸= ℓ.
(ii) Using integration by parts, derive Price’s Theorem: if h: Rn →R is twice con-
tinuously diﬀerentiable with h and its ﬁrst and second derivatives growing at most
polynomially in ∥x∥as ∥x∥→∞, then
∂E
$
h(X)
%
∂λ(j,ℓ)
=

Rn
∂2h(x)
∂x(j)∂x(ℓ) fX(x; Λ) dx,
j ̸= ℓ.
(See (Adler, 1990, Chapter 2, Section 2.2) for the case where Λ is singular.)
(iii) Show that if in addition to the assumptions of Part (ii) we also assume that for
some j ̸= ℓ
∂2h(x)
∂x(j)∂x(ℓ) ≥0,
x ∈Rn,
(23.97)
then E[h(X)] is a nondecreasing function of λ(j,ℓ).
(iv) Conclude that if h(x) = 0n
ν=1 gν(x(ν)), where for each ν ∈{1, . . . , n} the function
gν : R →R is nonnegative, nondecreasing, twice continuously diﬀerentiable, and
satisfying the growth conditions of h in Part (ii), then
E
1
n
/
ν=1
gν

X(ν)
2
is monotonically nondecreasing in λ(j,ℓ) whenever j ̸= ℓ.
538
The Multivariate Gaussian Distribution
(v) By choosing gν(·) to approximate the step function α →I

α ≥ζ(ν)
for properly
chosen ζ(ν), prove Slepian’s Inequality: if X ∼N(μ, Λ), then for every choice of
ξ(1), . . . , ξ(n) ∈R
Pr
+
X(1) ≥ξ(1), . . . , X(n) ≥ξ(n),
is monotonically nondecreasing in λ(j,ℓ) whenever j ̸= ℓ. See (Tong, 1990, Chap-
ter 5, Section 5.1.4, Theorem 5.1.7).
(vi) Modify the arguments in Parts (iv) and (v) to show that if X ∼N(μ, Λ), then for
every choice of ξ(1), . . . , ξ(n) ∈R
Pr
+
X(1) ≤ξ(1), . . . , X(n) ≤ξ(n),
is monotonically nondecreasing in λ(j,ℓ) whenever j ̸= ℓ. See (Adler, 1990, Chap-
ter 2, Section 2.2, Corollary 2.4).
Exercise 23.25 (Jointly Gaussians of Equal Sign). Let X and Y be jointly Gaussian and
centered with positive variances and correlation coeﬃcient ρ. Prove that
Pr
$
XY > 0
%
= 1
2 + φ
π ,
where −π/2 ≤φ ≤π/2 is such that sin φ = ρ. We propose the following approach.
(i) Show that it suﬃces to prove the result when X and Y are of unit variance.
(ii) Show that, for such X and Y , if we deﬁne
W =
1

1 −ρ2 X −
ρ

1 −ρ2 Y,
Z = Y,
then W and Z are IID N(0, 1).
(iii) Show that X and Y can be expressed as
X = R sin(Θ + φ),
Y = R cos Θ,
where φ is as deﬁned before, Θ is uniformly distributed over the interval [−π, π),
R is independent of Θ, and fR(r) = r e−r2/2 I{r > 0}.
(iv) Justify the calculation
Pr
$
XY > 0
%
= 2 Pr
$
X > 0, Y > 0
%
= 2 Pr
$
sin(Θ + φ) > 0, cos Θ > 0
%
= 1
2 + φ
π .
Hint: Exercise 19.7 may be useful for Part (iii).
Exercise 23.26 (Bussgang’s Theorem). Let X and Y be centered and jointly Gaussian,
and let h: R →R be such that h(Y ) has ﬁnite variance. Prove that
E[X h(Y )] =
α
Var[Y ] E[XY ] ,
where α is given by
α = E[Y h(Y )]
23.12 Exercises
539
and is thus determined by Var[Y ] and h(·); it does not depend on the correlation coeﬃcient
between X and Y .
Hint: Use Exercise 23.19 (i) and note that
x2
σ2x
−2ρxy
σxσy + y2
σ2y
+ ρ2y2
σ2y
−ρ2y2
σ2y
=
 x
σx −ρ y
σy
2
+ y2(1 −ρ2)
σ2y
.
Alternatively, express E[X h(Y )] as an integral, diﬀerentiate with respect to E[XY ] under
the integral, use Plackett’s identity (Exercise 23.24) and integrate over x by parts.
Exercise 23.27 (Guessing the Sign of the Correlation). Let H be a binary RV with
Pr[H = 0] = π0
and
Pr[H = 1] = π1,
where π0, π1 > 0 sum to one. Conditional on H = 0 the observed vector Y is a centered
Gaussian 2-vector of covariance matrix
K0 =
1
ρ
ρ
1

,
where 0 < ρ < 1, whereas conditional on H = 1 it is a centered Gaussian 2-vector of
covariance matrix
K1 =
 1
−ρ
−ρ
1

.
(i) Compute the conditional densities fY|H=0(·) and fY|H=1(·).
(ii) Find a one-dimensional suﬃcient statistic for guessing H based on Y.
(iii) Describe an optimal decision rule for guessing H based on Y.
Sketch the cor-
responding decision regions for each of the three cases: π0 = π1, π0 > π1 and
π0 < π1.
(iv) Compute the Bhattacharyya Bound on the optimal probability of error p∗(error).
(v) Compute limρ↑1 p∗(error).
Hint: For Part (i) note that if
 a b
c d

is invertible, then its inverse is
1
ad−bc
 d
−b
−c
a

. For
Part (v) the answer to Part (iv) may be helpful.
Exercise 23.28 (Guessing the Permutation). Let H take on the values 0 and 1 equiprob-
ably and independently of the standard Gaussian 3-vector W. The observable Y is equal
to AW when H = 0 and to BW when H = 1, where
A =
⎛
⎝
1
0
0
0
α
0
0
0
β
⎞
⎠,
B =
⎛
⎝
0
0
β
0
α
0
1
0
0
⎞
⎠,
and α, β > 0 with β ̸= 1.
(i) If you must guess H based on two of the three components of Y, which two would
you choose in order to minimize the probability of error?
(ii) Determine the conditional densities fY|H=0 and fY|H=1.
(iii) Find a one-dimensional suﬃcient statistic for guessing H based on Y.
(iv) Describe an optimal decision rule for guessing H based on Y.
(v) Compute the Bhattacharyya Bound on the optimal probability of error.
Exercise 23.29 (Independent Binary Random Variables). Show that two zero-one valued
(i.e., binary) random variables are independent if, and only if, they are uncorrelated.
Chapter 24
Complex Gaussians and Circular Symmetry
24.1
Introduction
This chapter introduces the complex Gaussian distribution and the circular sym-
metry property. We start with the scalar case and then extend these notions to
random vectors. We rely heavily on Chapter 17 for the basic properties of complex
random variables and on Chapter 23 for the properties of the multivariate Gaussian
distribution.
24.2
Scalars
24.2.1
Standard Complex Gaussians
Deﬁnition 24.2.1 (Standard Complex Gaussian). A standard complex Gaus-
sian is a complex random variable whose real and imaginary parts are independent
N(0, 1/2) random variables.
If W is a standard complex Gaussian, then its density is given by
fW (w) = 1
π e−|w|2,
w ∈C,
(24.1)
because
fW (w) = fRe(W ),Im(W )

Re(w), Im(w)

= fRe(W )

Re(w)

fIm(W )

Im(w)

=
1
√π e−Re(w)2
1
√π e−Im(w)2
= 1
π e−|w|2,
w ∈C,
where the ﬁrst equality follows from the deﬁnition of the density fW (w) of a
CRV W at w ∈C as the joint density fRe(W ),Im(W ) of its real and imaginary
parts (Re(W), Im(W)) evaluated at

Re(w), Im(w)

(Section 17.3.1); the second
because the real and imaginary parts of a standard complex Gaussian are indepen-
dent; the third because the real and imaginary parts of a standard Gaussian are
540
24.2 Scalars
541
zero-mean variance-1/2 real Gaussians whose density can thus be computed from
(19.6) (by substituting 1/2 for σ2); and where the ﬁnal equality follows because for
any complex number w we have Re(w)2 + Im(w)2 = |w|2.
Because the real and imaginary parts of a standard complex Gaussian W are of
zero mean, it follows that
E[W] = E[Re(W)] + i E[Im(W)]
= 0.
And because they are each of variance 1/2, it follows from (17.14c) that a standard
complex Gaussian W has unit-variance
Var[W] = E

|W|2
= 1.
(24.2)
Moreover, since a standard complex Gaussian is of zero mean and since its real
and imaginary parts are of equal variance and uncorrelated, a standard Gaussian
is proper (Deﬁnition 17.3.1 and Proposition 17.3.2), i.e.,
E[W] = 0
and
E

W 2
= 0.
(24.3)
Finally note that, by (24.1), the density fW (·) of a standard complex Gaussian
is radially-symmetric, i.e., its value at w ∈C depends on w only via its mod-
ulus |w|. A CRV whose density is radially-symmetric is said to be circularly-
symmetric, but the deﬁnition of circular symmetry applies also to complex ran-
dom variables that do not have a density. This is the topic of the next section.
24.2.2
Circular Symmetry
Deﬁnition 24.2.2 (Circularly-Symmetric CRV). A CRV Z is said to be circularly-
symmetric if for any deterministic φ ∈[−π, π) the distribution of eiφZ is identical
to the distribution of Z:
eiφZ
L= Z,
φ ∈[−π, π).
(24.4)
Note 24.2.3. If the expectation of a circularly-symmetric CRV is deﬁned, then it
must be zero.
Proof. Let Z be circularly-symmetric. It then follows from (24.4) that eiφZ and Z
are of equal expectation, so
E[Z] = E

eiφZ

= eiφ E[Z] ,
φ ∈[−π, π),
which, by considering a φ for which eiφ ̸= 1, implies that E[Z] must be zero.
To shed some light on the deﬁnition of circular symmetry we shall need Proposi-
tion 24.2.5 ahead, which is highly intuitive but a bit cumbersome to state. Before
stating it we provide its discrete counterpart, which is a bit easier to state: it
makes formal the intuition that if after giving the wheel-of-fortune an arbitrary
542
Complex Gaussians and Circular Symmetry
spin, you give it another fair spin, then the combined result is a fair spin that does
not depend on the initial spin. The case η = 2 is critical in cryptography. It shows
that taking the mod-2 sum of a binary source sequence with a sequence of IID
random bits results in a sequence that is independent of the source sequence.
Proposition 24.2.4. Fix a positive integer η, and deﬁne the set A = {0, . . . , η −1}.
Let N be a RV taking values in the set A.
Then the following statements are
equivalent:
(a) The RV N is uniformly distributed over the set A.
(b) For any integer-valued RV K that is independent of N, the RV (N+K) mod η
is independent of K and uniformly distributed over A.1
Proof. We ﬁrst show (b) ⇒(a). To this end, deﬁne K to be a RV that takes on
the value zero deterministically. Being deterministic, it is independent of every
RV, and in particular of N. Statement (b) thus guarantees that (N + 0) mod η is
uniformly distributed over A. Since we have assumed from the outset that N takes
values in A, it follows that (N +0) mod η = N, so the uniformity of (N +0) mod η
over A implies the uniformity of N over A.
We next show (a) ⇒(b). To this end, we need to show that if N is uniformly
distributed over A and if K is independent of N, then2
Pr
%
(N + K) mod η

= a
 K = k
&
= 1
η ,

k ∈Z, a ∈A

.
(24.5)
By the independence of N and K it follows that
Pr
%
(N +K) mod η

= a
 K = k
&
= Pr
%
(N +k) mod η

= a
&
,

k ∈Z, a ∈A

,
so to prove (24.5) it suﬃces to prove
Pr
%
(N + k) mod η

= a
&
= 1
η ,

k ∈Z, a ∈A

.
(24.6)
This can be proved as follows. Because N is uniformly distributed over A, it follows
that N+k is uniformly distributed over the set {k, k+1, . . . , k+η−1}. And, because
the mapping m 	→(m mod η) is a one-to-one mapping from {k, k+1, . . . , k+η−1}
onto A, this implies that (N + k) mod η is also uniformly distributed over A, thus
establishing (24.6).
Proposition 24.2.5. Let Θ be a RV taking values in [−π, π). Then the following
statements are equivalent:
(a) The RV Θ is uniformly distributed over [−π, π).
1Here m mod η is the remainder of dividing m by η, i.e., the unique ν ∈A such that m −ν
is an integer multiple of η. E.g., 17 mod 8 = 1.
2Recall that the random variables X and Y are independent if, and only if, the conditional
distribution of X given Y is equal to the marginal distribution of X.
24.2 Scalars
543
−π + φ
π + φ
+π
−π
Figure 24.1: The function ξ 	→

ξ mod [−π, +π)

plotted for ξ ∈[−π + φ, π + φ).
(b) For any real RV Φ that is independent of Θ, the RV (Θ + Φ) mod [−π, π) is
independent of Φ and uniformly distributed over the interval [−π, π).3
Proof. The proof is similar to the proof of Proposition 24.2.4 but with an added
twist. The twist is needed because if X has a uniform density and if a function g
is one-to-one (injective) and onto (surjective), then g(X) need not be uniformly
distributed. (For example, if X ∼U ([0, 1]) and if g: [0, 1] →[0, 1] maps ξ to ξ2,
then g(X) is not uniform.)
To prove that (b) implies (a) we simply apply (b) to the deterministic RV Φ = 0.
We next prove that (a) implies (b). As in the discrete case, it suﬃces to show that
if Θ is uniformly distributed over [−π, π), then for any deterministic φ ∈R the
distribution of (Θ + φ) mod [−π, π) is uniform over [−π, π), irrespective of φ. To
this end we ﬁrst note that because Θ is uniform over [−π, π) it follows that Θ+φ is
uniform over [φ −π, φ + π). Consider now the mapping g: [φ −π, φ + π) →[−π, π)
deﬁned by g: ξ 	→

ξ mod [−π, π)

. This function is a one-to-one mapping onto
[−π, π) and is diﬀerentiable except at the point ξ∗∈[φ −π, φ + π) satisfying
ξ∗mod [−π, π) = π, i.e., the point ξ∗∈[φ−π, φ+π) of the form ξ∗= 2πm+π for
some integer m. At all other points its derivative is 1; see Figure 24.1. (Incidentally,
−π + φ is mapped to a negative number if φ < ξ∗and to a positive number if
φ > ξ∗. In Figure 24.1 we assume the latter.) Applying the formula for computing
the density of g(X) from the density of X (Theorem 17.3.4) we ﬁnd that if Θ + φ
is uniform over [φ −π, φ + π), then g(φ + Θ) is uniform over [−π, π).
With the aid of Proposition 24.2.5 we can now give alternative characterizations
of circular symmetry.
3Here x mod [−π, π) is the unique ξ ∈[−π, π) such that x −ξ is an integer multiple of 2π.
544
Complex Gaussians and Circular Symmetry
Proposition 24.2.6 (Characterizing Circular Symmetry). Let Z be a CRV with
a density. Then each of the following statements is equivalent to the statement
that Z is circularly-symmetric:
(a) The distribution of eiφZ is identical to the distribution of Z, for any deter-
ministic φ ∈[−π, π).
(b) The CRV Z has a radially-symmetric density function, i.e., a density fZ(·)
whose value at z depends on z only via its modulus |z|.
(c) The CRV Z can be written as Z = R eiΘ, where R ≥0 and Θ are independent
real random variables and Θ ∼U ([−π, π)).
Proof. Statement (a) is the deﬁnition of circular symmetry (Deﬁnition 24.2.2).
The proof of (a) ⇒(b) is slightly obtuse because the density of a CRV is not
unique.4
We begin by noting that if Z is of density fZ(·), then by (17.34) the
CRV eiφZ is of density w 	→fZ(e−iφw). Thus, if Z
L= eiφZ and if Z is of density
fZ(·), then Z is also of density w 	→fZ(e−iφw). Consequently, if Z is circularly-
symmetric, then for every φ ∈[−π, π) the mapping w 	→fZ(e−iφw) is a density
for Z. We can therefore conclude that the mapping
w 	→1
2π
 π
−π
fZ(e−iφw) dφ
is also a density for Z, and this function is radially-symmetric.
The fact that (b) ⇒(c) follows because if we deﬁne R to be the magnitude of Z
and Θ to be its argument, then Z = R eiΘ, and
fR,Θ(r, θ) = rfZ(r eiθ)
= rfZ(r)
=

2πrfZ(r)
 1
2π ,
where the ﬁrst equality follows from (17.29) and the second from our assumption
that fZ(z) depends on z only via its modulus |z|. The joint density of R, Θ is thus
of a product form, thereby indicating that R and Θ are independent. And it does
not depend on θ, thus indicating that its marginal Θ is uniformly distributed.
We ﬁnally show that (c) ⇒(a). To that end we assume that R ≥0 and Θ are
independent with Θ being uniformly distributed over [−π, π) and proceed to show
that R eiΘ is circularly-symmetric, i.e., that
R eiΘ L= R ei(Θ+φ),
φ ∈[−π, π).
(24.7)
To prove (24.7) we note that
ei(Θ+φ) = ei((Θ+φ) mod [−π,π))
L= eiΘ,
(24.8)
4And not all the functions that are densities for a given circularly-symmetric CRV Z are
radially-symmetric. The radial symmetry can be broken on a set of Lebesgue measure zero. We
can therefore only claim that there exists “a” radially-symmetric density function for Z.
24.2 Scalars
545
where the ﬁrst equality follows from the periodicity of the complex exponentials,
and where the equality in distribution follows from Proposition 24.2.5 because
Θ ∼U ([−π, π)). The proof is now completed by noting that (24.7) follows from
(24.8) and from the independence of R and Θ. (If X is independent of Y , if X is
independent of Z, and if Y
L= Z, then (X, Y )
L= (X, Z) and hence XY
L= XZ.)
Example 24.2.7. Let the CRV Z be given by Z = eiΦ, where Φ ∼U ([−π, π)).
Then Z is uniformly distributed over the unit circle {z : |z| = 1} and is circularly-
symmetric. It does not have a density.
24.2.3
Properness and Circular Symmetry
Proposition 24.2.8. Every ﬁnite-variance circularly-symmetric CRV is proper.
Proof. Let Z be a ﬁnite-variance circularly-symmetric CRV. By Note 24.2.3 it
follows that E[Z] = 0. To conclude the proof it remains to show that E

Z2
= 0.
To this end we note that
E

Z2
= e−i2φ E
%
eiφZ
2&
= e−i2φ E

Z2
,
φ ∈[−π, π),
(24.9)
where the ﬁrst equality follows by rewriting Z2 as e−i2φ 
eiφZ
2, and where the
second equality follows because the circular symmetry of Z guarantees that Z
and eiφZ have the same law, so the expectations of their squares must be equal.
But (24.9) cannot be satisﬁed for all φ ∈[−π, π) (or for that matter for any φ such
that ei2φ ̸= 1) unless E

Z2
= 0.
Note 24.2.9. Not every proper CRV is circularly-symmetric.
Proof. Consider the CRV Z that takes on the four values 1 + i, 1 −i, −1 + i, and
−1 −i equiprobably. Its real and imaginary parts are independent, each taking on
the values ±1 equiprobably. Computing E[Z] and E

Z2
we ﬁnd that they are both
zero, so Z is proper. To see that Z is not circularly-symmetric consider the random
variable eiπ/4Z. Its distribution is diﬀerent from the distribution of Z because Z
takes values in the set {1 + i, −1 + i, 1 −i, −1 −i}, and eiπ/4Z takes values in the
rotated set {
√
2, −
√
2,
√
2i, −
√
2i}.
The fact that not every proper CRV is circularly-symmetric is not surprising be-
cause whether a CRV is proper or not is determined solely by its mean and by the
covariance matrix of its real and imaginary parts, whereas circular symmetry has
to do with the entire distribution.
24.2.4
Complex Gaussians
The deﬁnition of a complex Gaussian builds on the deﬁnition of a real Gaussian
vector (Deﬁnition 23.1.1).
546
Complex Gaussians and Circular Symmetry
Deﬁnition 24.2.10 (Complex Gaussian). A complex Gaussian is a CRV whose
real and imaginary parts are jointly Gaussian real random variables. A centered
complex Gaussian is a complex Gaussian of zero mean.
An example of a complex Gaussian is the standard complex Gaussian, which we
encountered in Section 24.2.1.
The class of complex Gaussians is closed under multiplication by deterministic
complex numbers. Thus, if Z is a complex Gaussian and if α ∈C is deterministic,
then αZ is also a complex Gaussian. Indeed,
	
Re(αZ)
Im(αZ)

=
	
Re(α)
−Im(α)
Im(α)
Re(α)

 	
Re(Z)
Im(Z)

,
so the claim follows from the fact that multiplying a real Gaussian vector by a
deterministic real matrix results in a real Gaussian vector (Proposition 23.6.3).
We leave it to the reader to verify that, more generally, if Z is a complex Gaussian
and if α, β ∈C are deterministic, then αZ +βZ∗is also a complex Gaussian. (This
is a special case of Proposition 24.3.9 ahead.)
Not every centered complex Gaussian can be expressed as the scaling of a standard
complex Gaussian by some complex number. But the following result characterizes
those that can:
Proposition 24.2.11.
(i) For every centered complex Gaussian Z we can ﬁnd coeﬃcients α, β ∈C so
that
Z
L= αW + βW ∗,
(24.10)
where W is a standard complex Gaussian.
(ii) A centered complex Gaussian Z is proper if, and only if, there exists some
α ∈C such that Z
L= αW, where W is a standard complex Gaussian.
Proof. We begin with Part (i). First note that since Z is a complex Gaussian, its
real and imaginary parts are jointly Gaussian, and it follows from Corollary 23.6.13
that there exist deterministic real numbers a(1,1), a(1,2), a(2,1), a(2,2) such that
	Re(Z)
Im(Z)

L=
	
a(1,1)
a(1,2)
a(2,1)
a(2,2)

 	
W1
W2

,
(24.11)
where W1 and W2 are independent real standard Gaussians. Next note that by
direct computation
	
Re(αW + βW ∗)
Im(αW + βW ∗)

=
- Re(α)+Re(β)
√
2
Im(β)−Im(α)
√
2
Im(β)+Im(α)
√
2
Re(α)−Re(β)
√
2
. 	√
2 Re(W)
√
2 Im(W)

.
(24.12)
Since, by the deﬁnition of a standard complex Gaussian W,
	
W1
W2

L=
	√
2 Re(W)
√
2 Im(W)

(24.13)
24.2 Scalars
547
it follows from (24.11), (24.12), and (24.13) that if α and β are chosen so that
- Re(α)+Re(β)
√
2
Im(β)−Im(α)
√
2
Im(β)+Im(α)
√
2
Re(α)−Re(β)
√
2
.
=
	
a(1,1)
a(1,2)
a(2,1)
a(2,2)

,
i.e., if
α =
1
√
2

a(1,1) + a(2,2)
+ i

a(2,1) −a(1,2)
,
β =
1
√
2

a(1,1) −a(2,2)
+ i

a(2,1) + a(1,2)
,
then
	
Re(Z)
Im(Z)

L=
	
Re(αW + βW ∗)
Im(αW + βW ∗)

,
and (24.10) is satisﬁed.
We next turn to Part (ii). One direction is straightforward: if Z
L= αW, then Z
must be proper because from (24.3) it follows that E[αW] = αE[W] = 0 and
E

(αW)2
= α2E

W 2
= 0.
We next prove the other direction that if Z is a proper complex Gaussian, then
Z
L= αW for some α ∈C and some standard complex Gaussian W. Let Z be a
proper complex Gaussian. By Part (i) it follows that there exist α, β ∈C such that
(24.10) is satisﬁed. Consequently, for this choice of α and β we have
0 = E

Z2
= E

(αW + βW ∗)2
= α2E

W 2
+ 2αβE[WW ∗] + β2E

(W ∗)2
= 2αβ,
where the ﬁrst equality follows because Z is proper; the second because α and β
have been chosen so that (24.10) holds; the third by opening the brackets and using
the linearity of expectation; and the fourth by (24.3) and (24.2). It follows that
either α or β must be zero. Since W
L= W ∗, there is no loss in generality in assuming
that β = 0, thus establishing the existence of α ∈C such that Z
L= αW.
By Proposition 24.2.11 (ii) we conclude that if Z is a proper complex Gaussian, then
Z
L= αW for some α ∈C and some standard complex Gaussian W. Consequently,
the density of such a CRV Z (that is not deterministically zero) is given by
fZ(z) = fW (z/α)
|α|2
=
1
π|α|2 e
−|z|2
|α|2 ,
z ∈C,
where the ﬁrst equality follows from the way the density of a CRV behaves under
linear transformations (Theorem 17.3.7 or Lemma 17.4.6), and where the second
548
Complex Gaussians and Circular Symmetry
equality follows from (24.1). We thus conclude that if Z is a proper complex Gaus-
sian, then its density is radially-symmetric, and Z must be circularly-symmetric.
The reverse is also true: since every complex Gaussian is of ﬁnite variance, and since
every ﬁnite-variance circularly-symmetric CRV is also proper (Proposition 24.2.8),
we conclude that every circularly-symmetric complex Gaussian is proper. Thus:
Proposition 24.2.12. A complex Gaussian is circularly-symmetric if, and only if,
it is proper.
The picture that thus emerges is the following.
(i) Every ﬁnite-variance circularly-symmetric CRV is proper.
(ii) Some proper CRVs are not circularly symmetric.
(iii) A Gaussian CRV is circularly-symmetric, if and only if, it is proper.
We shall soon see that these observations extend to vectors too. In fact, the reader
is encouraged to consult Figure 24.2 on Page 554, which holds also for CRVs.
24.3
Vectors
24.3.1
Standard Complex Gaussian Vectors
Deﬁnition 24.3.1 (Standard Complex Gaussian Vector). A standard complex
Gaussian vector is a complex random vector whose components are IID and each
of them is a standard complex Gaussian random variable.
If W is a standard complex Gaussian n-vector, then, by the independence of its n
components and by (24.1), its density is given by
fW(w) = 1
πn e−w†w,
w ∈Cn.
(24.14)
By the independence of its components and by (24.3)
E[W] = 0
and
E

WWT
= 0.
(24.15)
Thus, every standard complex Gaussian vector is proper (Section 17.4.2). By the
independence of the components and by (24.2) it also follows that
E

WW†
= In,
(24.16)
where we remind the reader that In denotes the n × n identity matrix.
24.3.2
Circularly-Symmetric Complex Random Vectors
Deﬁnition 24.3.2 (Circularly-Symmetric Complex Random Vectors). We say that
the complex random vector Z is circularly-symmetric if for every φ ∈[−π, π)
the law of eiφZ is identical to the law of Z.
24.3 Vectors
549
An equivalent deﬁnition can be given in terms of linear functionals:
Proposition 24.3.3 (Circular Symmetry and Linear Functionals). Each of the fol-
lowing statements is equivalent to the statement that the complex random n-vector Z
is circularly-symmetric.
(a) For every φ ∈[−π, π) the law of the complex random vector eiφZ is the same
as the law of Z:
eiφZ
L= Z,
φ ∈[−π, π).
(24.17)
(b) For every deterministic vector α ∈Cn, the CRV αTZ is circularly-symmetric:
eiφαTZ
L= αTZ,

α ∈Cn, φ ∈[−π, π)

.
(24.18)
Proof. Statement (a) is just the deﬁnition of circular symmetry. We next show
that the two statements (a) and (b) are equivalent. We begin by proving that (a)
implies (b). This is the easy part because applying the same linear functional to
two random vectors that have the same law results in random variables that have
the same law. Consequently, (24.17) implies (24.18).
We now prove that (b) implies (a). We thus assume (24.18) and set out to prove
(24.17). By Theorem 17.4.4 it follows that to establish (24.17) it suﬃces to show
that the random vectors on the RHS and LHS of (24.17) have the same character-
istic function, i.e., that
E

ei Re

ϖ† eiφZ

= E

ei Re(ϖ†Z)

,
ϖ ∈Cn.
(24.19)
But this readily follows from (24.18) because upon substituting ϖ† for αT in
(24.18) we obtain that
ϖ† eiφZ
L= ϖ†Z,
ϖ ∈Cn,
and this implies (24.19), because if Z1
L= Z2, then E[g(Z1)] = E[g(Z2)] for any
measurable function g and, in particular, for the function g: ξ 	→ei Re(ξ).
The following proposition demonstrates that circular symmetry is preserved by
linear transformations.
Proposition 24.3.4 (Circular Symmetry and Linear Transformations). Let Z be a
circularly-symmetric complex random n-vector and let A be a deterministic complex
m×n matrix. Then the complex random m-vector AZ is also circularly-symmetric.
Proof. By Proposition 24.3.3 it follows that to establish that AZ is circularly-
symmetric it suﬃces to show that for every deterministic α ∈Cm the random
variable αTAZ is circularly-symmetric. To show this, ﬁx some arbitrary α ∈Cm.
Because Z is circularly-symmetric, it follows from Proposition 24.3.3 that for every
deterministic vector β ∈Cn, the random variable βTZ is circularly-symmetric.
Choosing β = ATα establishes that αTAZ is circularly-symmetric.
550
Complex Gaussians and Circular Symmetry
24.3.3
Proper vs. Circularly-Symmetric Vectors
We now extend the relationship between properness and circular symmetry to
vectors:
Proposition 24.3.5 (Circular Symmetry Implies Properness).
(i) Every ﬁnite-variance circularly-symmetric random vector is proper.
(ii) Some proper random vectors are not circularly-symmetric.
Proof. Part (ii) requires no proof because a CRV can be viewed as a complex
random vector taking values in C1, and we have already seen in Section 24.2.3 an
example of a CRV which is proper but not circularly-symmetric (Note 24.2.9).
We now prove Part (i). Let Z be a ﬁnite-variance circularly-symmetric random
n-vector. To establish that Z is proper we will show that for every α ∈Cn the
CRV αTZ is proper (Proposition 17.4.2). To this end, ﬁx an arbitrary α ∈Cn.
By Proposition 24.3.3 it follows that the CRV αTZ is circularly-symmetric. And
because Z is of ﬁnite variance, so is αTZ. Being a circularly-symmetric CRV of
ﬁnite variance, it follows from Section 24.2.3 that αTZ must be proper.
24.3.4
Complex Gaussian Vectors
Deﬁnition 24.3.6 (Complex Gaussian Vectors). A complex random n-vector Z is
said to be a complex Gaussian vector if the real random 2n-vector

Re

Z(1)
, . . . , Re

Z(n)
, Im

Z(1)
, . . . , Im

Z(n)T
(24.20)
consisting of the real and imaginary parts of its components is a real Gaussian
vector. A centered complex Gaussian vector is a zero-mean complex Gaussian
vector.
Note that, Theorem 23.6.7 notwithstanding, the distribution of a centered complex
Gaussian vector is not uniquely speciﬁed by its covariance matrix. It is uniquely
speciﬁed by the covariance matrix if the Gaussian vector is additionally known to
be proper. This is a direct consequence of the following proposition.
Proposition 24.3.7. The distribution of a centered complex Gaussian vector Z is
uniquely speciﬁed by the matrices
K = E

ZZ†
and
L = E

ZZT
.
Proof. Let R be the real 2n-vector that results from stacking the real part of Z on
top of its imaginary part as in (24.20). We will prove the proposition by showing
that the matrices K and L uniquely specify the distribution of R.
Since Z is a complex Gaussian n-vector, R is a real Gaussian 2n-vector. Since Z is
of zero mean, so is R. Consequently, the distribution of R is fully characterized by
its covariance matrix E

RRT
(Theorem 23.6.7). The proof will thus be concluded
24.3 Vectors
551
once we show that the matrices L and K determine the covariance matrix of R.
Indeed, as we next verify,
E

RRT
= 1
2
	
Re(K) + Re(L)
Im(L) −Im(K)
Im(L) + Im(K)
Re(K) −Re(L)

.
(24.21)
To verify (24.21) one needs to compute each of the block entries separately. We
shall see how this is done by computing the top-right entry. The rest of the entries
are left for the reader to verify.
E
%
Re(Z) Im(Z)T&
= E
5	Z + Z∗
2

 	Z −Z∗
2i

T6
= E
	Z + Z∗
2

 	ZT −Z†
2i


= 1
2
-
E

ZZT
−E

Z∗Z†
2i
−E

ZZ†
−E

Z∗ZT
2i
.
= 1
2

Im(L) −Im(K)

.
Corollary 24.3.8. The distribution of a proper complex Gaussian vector is uniquely
speciﬁed by its covariance matrix.
Proof. Follows from Proposition 24.3.7 by noting that by specifying that a complex
Gaussian is proper we are specifying that the matrix L is zero (Deﬁnition 17.4.1).
Proposition 24.3.9 (Linear Transformations of Complex Gaussians). If Z is a
complex Gaussian n-vector and if A and B are deterministic m × n complex ma-
trices, then the m-vector
AZ + BZ∗
is a complex Gaussian.
Proof. Deﬁne the complex random m-vector C ≜AZ + BZ∗. To prove that C is
Gaussian we recall that linearly transforming a real Gaussian vector yields a real
Gaussian vector (Proposition 23.6.3), and we note that the real random 2m-vector
whose components are the real and imaginary parts of C can be expressed as the
result of applying a linear transformation to the real Gaussian 2n-vector whose
components are the real and imaginary parts of the components of Z:
	
Re(C)
Im(C)

=
	
Re(A) + Re(B)
Im(B) −Im(A)
Im(A) + Im(B)
Re(A) −Re(B)

 	
Re(Z)
Im(Z)

.
Proposition 24.3.10 (Characterizing Complex Gaussian Vectors). Each of the
following statements is equivalent to the statement that Z is a complex Gaussian
n-vector.
(a) The real random vector whose 2n components correspond to the real and
imaginary parts of Z is a real Gaussian vector.
552
Complex Gaussians and Circular Symmetry
(b) For every deterministic vector α ∈Cn, the CRV αTZ is a complex Gaussian
random variable.
(c) There exist complex n × m matrices A and B and a vector μ ∈Cn such that
Z
L= AW + BW∗+ μ
for some standard complex Gaussian random m-vector W.
Proof. Statement (a) is just the deﬁnition of a Gaussian complex random vector.
We next prove the equivalence of (a) and (b). That (a) implies (b) follows from
Proposition 24.3.9 (by substituting αT for A and 0 for B).
To prove that (b) ⇒(a) it suﬃces (by Deﬁnition 24.3.6 and Theorem 23.6.17) to
show that (b) implies that any real linear functional of the real random 2n-vector
comprising the real and imaginary parts of Z is a real Gaussian random variable,
i.e., that for every choice of the real constants α(1), . . . , α(n) and β(1), . . . , β(n) the
random variable
n

j=1
α(j) Re

Z(j)
+
n

j=1
β(j) Im

Z(j)
(24.22)
is a Gaussian real random variable. To that end we rewrite (24.22) as
n

j=1
α(j) Re

Z(j)
+
n

j=1
β(j) Im

Z(j)
= αT Re

Z

+ βT Im

Z

(24.23)
= Re

(α −iβ)TZ

,
(24.24)
where we deﬁne the real vectors α and β as α ≜(α(1), . . . , α(n))T ∈Rn and
β ≜(β(1), . . . , β(n))T ∈Rn.
Now (b) implies that (α −iβ)TZ is a Gaussian
complex random variable, so its real part Re((α −iβ)TZ) must be a real Gaus-
sian random variable (Deﬁnition 24.2.10 and Proposition 23.6.6), thus establishing
that (b) implies that (24.22) is a real Gaussian random variable.
We next turn to proving the equivalence of (a) and (c). That (c) implies (a) follows
directly from Proposition 24.3.9 applied to the Gaussian vector W. The proof of
the implication (a) ⇒(c) is very similar to the proof of its scalar version (24.10).
We ﬁrst note that since we can choose μ = E[Z], it suﬃces to prove the result for
the centered case. Now (a) implies that there exist n × n matrices D, E, F, G such
that
	Re(Z)
Im(Z)

L=
	D
E
F
G

 	W1
W2

,
(24.25)
where W1 and W2 are independent real standard Gaussian n-vectors (Deﬁni-
tion 23.1.1). On the other hand
	Re(AW + BW∗)
Im(AW + BW∗)

=
- Re(A)+Re(B)
√
2
Im(B)−Im(A)
√
2
Im(B)+Im(A)
√
2
Re(A)−Re(B)
√
2
. 	√
2 Re(W)
√
2 Im(W)

.
(24.26)
If W is a standard complex Gaussian, then
	√
2 Re(W)
√
2 Im(W)

L=
	
W1
W2

,
24.3 Vectors
553
where W1 and W2 are as above. Consequently, the representations (24.25) and
(24.26) agree if
	
D
E
F
G

=
- Re(A)+Re(B)
√
2
Im(B)−Im(A)
√
2
Im(B)+Im(A)
√
2
Re(A)−Re(B)
√
2
.
,
i.e., if we set
A =
1
√
2

(D + G) + i(F −E)

,
B =
1
√
2

(D −G) + i(F + E)

.
24.3.5
Proper Complex Gaussian Vectors
A proper complex Gaussian vector is a complex Gaussian vector that is also proper
(Deﬁnition 17.4.1). Thus, Z is a proper complex Gaussian vector if it is a centered
complex Gaussian vector satisfying E

ZZT
= 0.
Recall that, by Proposition 24.3.5, every ﬁnite-variance circularly-symmetric com-
plex random vector is also proper, but that some random vectors are proper and
not circularly-symmetric. We next show that for Gaussian vectors, circular sym-
metry is equivalent to properness. The relationship between circular symmetry,
properness, and Gaussianity is thus as illustrated in Figure 24.2.
Proposition 24.3.11 (For Complex Gaussians, Proper = Circularly-Symmetric).
A complex Gaussian vector is proper if, and only if, it is circularly-symmetric.
Proof. Every circularly-symmetric complex Gaussian is proper, because every com-
plex Gaussian is of ﬁnite variance, and every ﬁnite-variance circularly-symmetric
complex random vector is proper (Proposition 24.3.5).
We now turn to the reverse implication, i.e., that if a complex Gaussian vector
is proper, then it is circularly-symmetric. Assume that Z is a proper Gaussian
n-vector. We will prove that Z is circularly-symmetric using Proposition 24.3.3 by
showing that for every deterministic vector α ∈Cn the random variable αTZ is
circularly-symmetric.
To that end, ﬁx some arbitrary α ∈Cn. Since Z is a Gaussian vector, it follows that
αTZ is a Gaussian CRV (Proposition 24.3.9 with the substitution of αT for A and
0 for B). Moreover, since Z is proper, so is αTZ (Proposition 17.4.2). We have thus
established that αTZ is a proper Gaussian CRV and hence, by Proposition 24.2.12,
also circularly-symmetric.
We next address the existence of a proper complex Gaussian of a given covariance
matrix. We say that a complex n×n matrix K is complex positive semideﬁnite
and write K ⪰0 if α†Kα is a nonnegative real number for every α ∈Cn. The
covariance matrix KZZ (17.36) of every ﬁnite-variance complex random vector Z
is complex positive semideﬁnite, because α†Kα is the variance of α†Z and is, as
such, nonnegative. An n × n matrix K is complex positive semideﬁnite if, and only
if, there exists a complex n × n matrix S such that K = SS†; see (Axler, 2015,
Section 7.C, Theorem 7.35).
554
Complex Gaussians and Circular Symmetry
random vectors
ﬁnite-variance
proper
circularly symmetric
Gaussian
Figure 24.2: The relationship between circular symmetry, Gaussianity, and proper-
ness. The outer region corresponds to all complex random vectors. Within that is
the set of all vectors whose components are of ﬁnite variance. Within it is the family
of all proper random vectors. The slanted lines indicate the circularly-symmetric
vectors, and the gray area corresponds to the Gaussian vectors. The same relations
hold for scalars and for stochastic processes.
Proposition 24.3.12 (Existence of Proper Gaussians). Given any n × n complex
positive semideﬁnite matrix K, there exists a proper complex Gaussian n-vector
whose covariance matrix is K.
Proof. Since K is positive semideﬁnite, it follows that there exists an n×n matrix S
such that
K = SS†.
(24.27)
Consider now the vector
Z = SW,
(24.28)
where W is a standard complex Gaussian n-vector. We will show that Z has the
desired properties. First, it must be Gaussian because it is the result of applying
a deterministic linear mapping to the Gaussian vector W (Proposition 24.3.9). It
is centered because W is centered (24.15) and because E[SW] = SE[W]. It is
proper because it is the result of linearly transforming the proper complex random
vector W (Proposition 17.4.3 and (24.15)). Finally, its covariance matrix is
E

(SW)(SW)†
= E

SWW†S†
= SE

WW†
S†
24.3 Vectors
555
= SInS†
= K.
Note 24.3.13. Since the distribution of a proper Gaussian complex vector is fully
speciﬁed by its covariance matrix (Corollary 24.3.8), we can denote the distribution
of a proper Gaussian complex vector of covariance matrix K by
NC(0, K) .
We next show that linearly transforming a proper Gaussian yields a proper Gaus-
sian.
Proposition 24.3.14 (Linearly Transforming Proper Gaussians). Multiplying a
proper complex Gaussian vector by a deterministic matrix yields another proper
complex Gaussian vector.
Proof. Let Z be a circularly-symmetric complex Gaussian n-vector, and let A be
a deterministic m × n complex matrix. Then, being the result of linearly trans-
forming a complex Gaussian vector, AZ is a complex Gaussian m-vector (Propo-
sition 24.3.9). And since Z is proper, so is AZ (Proposition 17.4.3). Thus, AZ is
a proper complex Gaussian vector and hence also circularly-symmetric (Proposi-
tion 24.3.11).
The characteristic function of a proper Gaussian n-vector has a simple form:

Z ∼NC(0, K)

⇐⇒

ΦZ(ϖ) = e−1
4 ϖ†Kϖ,
ϖ ∈Cn
.
(24.29)
To establish (24.29) we shall compute the characteristic function of a NC(0, K)
complex Gaussian n-vector and then invoke Theorem 17.4.4. But we begin with
the simpler computation of the characteristic function of a scalar standard complex
Gaussian W (Deﬁnition 24.2.1).
Starting from the deﬁnition of the characteristic function (Deﬁnition 17.3.3),
ΦW (ϖ) = E
%
ei Re(ϖ∗W )&
= E
%
ei( Re(ϖ) Re(W )+Im(ϖ) Im(W ))&
= E
%
ei Re(ϖ) Re(W ) ei Im(ϖ) Im(W )&
= E
%
ei Re(ϖ) Re(W )&
E
%
ei Im(ϖ) Im(W )&
= exp

−1
4

Re(ϖ)
2
exp

−1
4

Im(ϖ)
2
= exp

−1
4|ϖ|2
,
ϖ ∈C,
(24.30)
where the fourth equality holds because the real and imaginary parts of a stan-
dard complex Gaussian are independent (Deﬁnition 24.2.1); the ﬁfth because they
556
Complex Gaussians and Circular Symmetry
are both N(0, 1/2) and by recalling the characteristic function of real Gaussians
(19.29); and the sixth holds by the deﬁnition of the modulus of a complex number.
This result extends easily to circularly-symmetric Gaussian CRVs of general vari-
ance σ2. Indeed, if Z ∼NC

0, σ2
, then Z has the same law as σW, where W is
NC(0, 1) (Proposition 24.3.14 and Note 24.3.13). Consequently,
ΦZ(ϖ) = E
%
ei Re(ϖ∗Z)&
= E
%
ei Re(ϖ∗σW )&
= ΦW (σϖ)
= exp

−1
4σ2|ϖ|2
,
ϖ ∈C, Z ∼NC

0, σ2
,
where the second equality holds because Z and σW are of the same law, and the
fourth by (24.30). We have thus established that
	
Z ∼NC

0, σ2 
=⇒
	
ΦZ(ϖ) = e−1
4 σ2|ϖ|2,
ϖ ∈C

.
(24.31)
Suppose now that the complex n-vector Z is NC(0, K), where K is some complex
positive semideﬁnite matrix. Let ϖ be any deterministic complex n-vector. By
Proposition 24.3.14, ϖ†Z is a proper (scalar) Gaussian CRV. Its variance is
E

|ϖ†Z|2
= E

(ϖ†Z)(ϖ†Z)†
= E

ϖ†ZZ†ϖ

= ϖ†E

ZZ†
ϖ
= ϖ†Kϖ.
Thus, ϖ†Z ∼NC

0, ϖ†Kϖ

, and hence, by (24.31),
E
%
ei Re(ϖ†Z)&
= Φϖ†Z(1)
= exp

−1
4ϖ†Kϖ

,
which establishes (24.29).
Like their real counterparts (Theorem 23.6.14) proper Gaussians have a natural
canonical representation:
Theorem 24.3.15 (Canonical Representation of Proper Complex Gaussian).
Let Z be a proper complex Gaussian n-vector of covariance matrix KZZ. Then
Z
L= UΛ1/2W,
where W is a standard complex Gaussian n-vector; the n × n matrix U is unitary;
the n × n matrix Λ is diagonal; the diagonal elements of Λ are the eigenvalues of
KZZ; and the j-th column of U is an eigenvector corresponding to the eigenvalue of
KZZ that is equal to the j-th diagonal element of Λ.
24.3 Vectors
557
Proof. Let Z ∼NC(0, K) be a complex n-vector, i.e., a circularly-symmetric com-
plex Gaussian n-vector of covariance matrix K. Being a covariance matrix, K must
be complex positive semideﬁnite. As such, it has a spectral representation analo-
gous to the one described for real matrices in Proposition 23.3.3:
K = UΛU†,
where U is a unitary n × n matrix (UU† = U†U = In), and Λ is an n × n diagonal
matrix whose diagonal elements λ1, . . . , λn are nonnegative.
Let W be a standard complex Gaussian n-vector. We next argue that
Z
L= UΛ1/2W,
where Λ1/2 is a diagonal matrix whose diagonal entries are the square roots of
those of Λ, i.e., λ1/2
1
, . . . , λ1/2
n . To this end note that because W is standard, it
is proper (24.15). Consequently, UΛ1/2W must also be proper because applying
a linear transformation to a proper random vector yields a proper random vector
(Proposition 17.4.3). And UΛ1/2W is Gaussian by Proposition 24.3.9. Thus, both
Z and UΛ1/2W are circularly-symmetric complex Gaussian vectors. To establish
that they are of the same law it thus suﬃces to establish that they have iden-
tical covariance matrices (Corollary 24.3.8). This is indeed the case because the
covariance matrix of UΛ1/2W is given by
E
%
UΛ1/2W

UΛ1/2W
†&
= E

UΛ1/2WW†Λ1/2U†
= UΛ1/2 E

WW†
Λ1/2U†
= UΛ1/2InΛ1/2U†
= UΛU†
= K.
We conclude by computing the density of proper Gaussians. We only consider those
that have a nonsingular covariance matrix because otherwise the density does not
exist.
Proposition 24.3.16 (The Density of a Proper Gaussian). If Z is a proper complex
Gaussian n-vector of nonsingular covariance matrix K, then its density fZ is
fZ(z) =
1
πn det K e−z†K−1z,
z ∈Cn.
(24.32)
Proof. To compute the density we use (24.27)–(24.28) along with the change-of-
variables formula (Lemma 17.4.6) and the density of a standard Gaussian complex
random vector (24.14) to obtain
fZ(z) =
1
|det S|2 fW(S−1z)
=
1
πn det(SS†) e−(S−1z)†S−1z
=
1
πn det K e−z†K−1z,
z ∈Cn.
558
Complex Gaussians and Circular Symmetry
24.4
Exercises
Exercise 24.1 (The Complex Conjugate of a Circularly-Symmetric CRV). Must the com-
plex conjugate of a circularly-symmetric CRV be circularly-symmetric?
Exercise 24.2 (Scaled Circularly-Symmetric CRV). Show that if Z is circularly-symmetric
and if α ∈C is deterministic, then the distribution of αZ depends on α only via its
magnitude |α|.
Exercise 24.3 (The n-th Power of a Circularly-Symmetric CRV). Show that if Z is a
circularly-symmetric CRV and if n is a positive integer, then Zn is circularly-symmetric.
Exercise 24.4 (The Characteristic Function of Circularly-Symmetric CRVs). Show that a
CRV Z is circularly-symmetric if, and only if, its characteristic function ΦZ(·) is radially-
symmetric in the sense that ΦZ(ϖ) depends on ϖ only via its magnitude |ϖ|.
Exercise 24.5 (Multiplying Independent CRVs). Show that the product of two indepen-
dent complex random variables is circularly-symmetric whenever (at least) one of them
is circularly-symmetric.
Exercise 24.6 (The Complex Conjugate of a Gaussian CRV). Must the complex conjugate
of a Gaussian CRV be Gaussian?
Exercise 24.7 (Independent Components). Show that if the complex random variables
W and Z are circularly-symmetric and independent, then the random vector (W, Z)T is
circularly-symmetric.
Exercise 24.8 (Constructing a Circularly-Symmetric Complex Random Vector). Show
that if Z is a complex random vector and if Θ is uniformly distributed over the interval
[−π, π) independently of Z, then eiΘ Z is circularly-symmetric.
Exercise 24.9 (Rotating a Complex Gaussian). Let Z be a centered complex Gaussian
(not necessarily circularly-symmetric), and let Φ be independent of it and uniformly
distributed over the interval [−π, π). Must eiΦZ be a complex Gaussian?
Hint: Consider the case where the imaginary part of Z is deterministically zero, and check
whether the distribution of the squared-magnitude of eiΦZ is the same as for a circularly-
symmetric Gaussian.
Exercise 24.10 (The Squared Euclidean Norm of Complex Gaussian Vectors). Show that
the sum of the squared magnitudes of the components of a NC(0, K) complex random
vector can be written as a sum of independent exponentials of possibly diﬀerent means.
Hint: Use the canonical representation of Theorem 24.3.15.
24.4 Exercises
559
Exercise 24.11 (Jointly Circularly-Symmetric Complex Gaussians). As in Deﬁnition 23.7.1,
we can also deﬁne jointly complex Gaussians and jointly circularly-symmetric complex
Gaussians. Extend the results of Section 23.7 by showing:
(i) Two centered jointly complex Gaussian vectors Z1 and Z2 are independent if, and
only if, they satisfy
E
$
Z1Z†
2
%
= 0 and E
$
Z1ZT
2
%
= 0.
(ii) Two jointly circularly-symmetric complex Gaussian vectors Z1 and Z2 are indepen-
dent if, and only if, they satisfy
E
$
Z1Z†
2
%
= 0.
(iii) If Z1, Z2 are centered jointly complex Gaussians, then, conditional on Z2 = z2, the
complex random vector Z1 is a complex Gaussian such that
E
+
Z1 −E[Z1 |Z2 = z2]

Z1 −E[Z1 |Z2 = z2]
†  Z2 = z2
,
and
E
+
Z1 −E[Z1 |Z2 = z2]

Z1 −E[Z1 |Z2 = z2]
T  Z2 = z2
,
do not depend on z2 and such that the conditional mean E[Z1 |Z2 = z2] can be
expressed as Az2 + Bz∗
2 for some matrices A and B that do not depend on z2.
(iv) If Z1, Z2 are jointly circularly-symmetric complex Gaussians, then, conditional on
Z2 = z2, the complex random vector Z1 is a circularly-symmetric complex Gaussian
of a covariance matrix that does not depend on z2 and of a mean that can be
expressed as Az2 for some matrix A that does not depend on z2.
Exercise 24.12 (Limits of Complex Gaussians). Extend the deﬁnition of almost-sure con-
vergence (23.79) to complex random vectors, and show that if the complex Gaussian
d-vectors Z1, Z2, . . . converge to Z almost surely, then Z must be a complex Gaussian.
Exercise 24.13 (Limits of Circularly-Symmetric Complex Random Variables). Consider
a sequence Z1, Z2, . . . of circularly-symmetric complex random variables that converges
almost surely to the CRV Z. Show that Z must be circularly-symmetric. Extend this
result to complex random vectors.
Hint: Consider the characteristic functions of Z, Z1, Z2, . . ., and recall the proof of Theo-
rem 19.9.1.
Exercise 24.14 (Limits of Circularly-Symmetric Complex Gaussians). Let Z1, Z2, . . . be
a sequence of circularly-symmetric complex Gaussians that converges almost surely to
the CRV Z. Show that Z must be a circularly-symmetric Gaussian. Extend to complex
random vectors.
Hint: Either combine Exercises 24.12 & 24.13 or prove directly using the characteristic
function as in the proof of Theorem 19.9.1.
Chapter 25
Continuous-Time Stochastic Processes
25.1
Notation
Recall from Section 12.2 that a continuous-time stochastic process

X(t), t ∈R

is a family of random variables that are deﬁned on a common probability space
(Ω, F, P) and that are indexed by the real line (time). We denote by X(t) the
time-t sample of

X(t), t ∈R

, i.e., the random variable to which t is mapped
(the RV indexed by t). This RV is sometimes also called the state at time t.
Rather than writing

X(t), t ∈R

, we sometimes denote the SP by

X(t)

or
by X. Perhaps the clearest way to denote the process is as a mapping:
X: Ω × R →R,
(ω, t) 	→X(ω, t).
For a ﬁxed t ∈R, the time-t sample X(t) is the mapping X(·, t) from Ω to the real
line, i.e., the RV ω 	→X(ω, t) indexed by t. If we ﬁx ω ∈Ω and view X(ω, ·) as a
mapping t 	→X(ω, t), then we obtain a function of time. This function is called a
trajectory, sample-path, path, sample-function, or realization.
ω 	→X(ω, t)
time-t sample for a ﬁxed t ∈R
(random variable)
t 	→X(ω, t)
trajectory for a ﬁxed ω ∈Ω
(function of time)
Recall also from Section 12.2 that the process is centered if for every t ∈R the
RV X(t) is of zero mean. It is of ﬁnite variance if for every t ∈R the RV X(t)
is of ﬁnite variance.
25.2
The Finite-Dimensional Distributions
The ﬁnite-dimensional distributions (FDDs) of a continuous-time SP

X(t)

are all
the joint distributions of n-tuples of the form (X(t1), . . . , X(tn)), where n can be
any positive integer and t1, . . . , tn ∈R are arbitrary epochs. To specify the FDDs
of a SP

X(t)

one must thus specify, for every n ∈N and for every choice of
the epochs t1, . . . , tn ∈R, the distribution of the n-tuple

X(t1), . . . , X(tn)

. This
560
25.2 The Finite-Dimensional Distributions
561
is a conceptually clear if formidable task. We denote the cumulative distribution
function of the n-tuple (X(t1), . . . , X(tn)) by
Fn

ξ1, . . . , ξn; t1, . . . , tn

≜Pr

X(t1) ≤ξ1, . . . , X(tn) ≤ξn

.
We next show that the FDDs of every SP

X(t)

must satisfy two key properties:
the symmetry property and the consistency property. The symmetry property
is that Fn(·; ·) is unaltered when we simultaneously permute its right arguments
(the t’s) and its left arguments (the ξ’s) by the same permutation. That is, for
every n ∈N; every choice of the epochs t1, . . . , tn ∈R; every ξ1, . . . , ξn ∈R; and
every permutation π on {1, . . . , n}
Fn

ξπ(1), . . . , ξπ(n); tπ(1), . . . , tπ(n)

= Fn

ξ1, . . . , ξn; t1, . . . , tn

.
(25.1)
This property is a generalization to n-tuples of the obvious fact that if X and Y are
random variables, then Pr[X ≤x, Y ≤y] = Pr[Y ≤y, X ≤x] for every x, y ∈R.
The consistency property is that whenever n ∈N and t1, . . . , tn, ξ1, . . . , ξn ∈R,
lim
ξn→∞Fn

ξ1, . . . , ξn−1, ξn; t1, . . . , tn−1, tn

= Fn−1

ξ1, . . . , ξn−1; t1, . . . , tn−1

.
(25.2)
This property is a consequence of the fact that the set

ω ∈Ω : X(ω, t1) ≤ξ1, . . . , X(ω, tn−1) ≤ξn−1, X(ω, tn) ≤ξn

is increasing in ξn and converges as ξn tends to inﬁnity to the set

ω ∈Ω : X(ω, t1) ≤ξ1, . . . , X(ω, tn−1) ≤ξn−1

.
The key result on the existence of stochastic processes of given FDDs is Kol-
mogorov’s Existence Theorem, which states that the symmetry and consistency
properties suﬃce for a family of ﬁnite-dimensional distributions to correspond to
the FDDs of some SP.
Theorem 25.2.1 (Kolmogorov’s Existence Theorem). Let G1(·; ·), G2(·; ·), . . . be
a sequence of functions Gn : Rn × Rn →[0, 1] satisfying the following properties:
1) For every n ≥1 and every t1, . . . , tn ∈R, the function Gn(·; t1, . . . , tn) is a
valid joint distribution function.1
2) The symmetry property
Gn

ξπ(1), . . . , ξπ(n); tπ(1), . . . , tπ(n)

= Gn

ξ1, . . . , ξn; t1, . . . , tn

,
t1, . . . , tn, ξ1, . . . , ξn ∈R, π a permutation on {1, . . . , n}.
(25.3)
1 A function F : Rn →[0, 1] is a valid joint distribution function if there exist random variables
X1, . . . , Xn whose joint distribution function is F(·), i.e.,
Pr[X1 ≤ξ1, . . . , Xn ≤ξn] = F(ξ1, . . . , ξn),
ξ1, . . . , ξn ∈R.
Not every function F : Rn →[0, 1] is a valid joint distribution function. For example, a valid joint
distribution function must be monotonic in each variable. See, for example, (Billingsley, 1995,
Theorem 12.5) for a characterization of joint distribution functions.
562
Continuous-Time Stochastic Processes
3) The consistency property
lim
ξn→∞Gn(ξ1, . . . , ξn−1, ξn; t1, . . . , tn−1, tn)
= Gn−1(ξ1, . . . , ξn−1; t1, . . . , tn−1),
t1, . . . , tn, ξ1, . . . , ξn−1 ∈R.
(25.4)
Then there exists a SP

X(t)

whose FDDs are given by {Gn(·; ·)} in the sense
that
Pr

X(t1) ≤ξ1, . . . , X(tn) ≤ξn

= Gn

ξ1, . . . , ξn; t1, . . . , tn

for every n ∈N, all t1, . . . , tn ∈R, and all ξ1, . . . , ξn ∈R.
Proof. See, for example, (Billingsley, 1995, Chapter 7, Section 36), (Cram´er and
Leadbetter, 2004, Section 3.3), (Grimmett and Stirzaker, 2001, Section 8.6), or
(Doob, 1990, Chapter I § 5).
In the study of n-tuples of random variables we can use the joint distribution
function to answer, at least in principle, most of our probability questions. When it
comes to stochastic processes, however, there are interesting questions that cannot
be answered using the FDDs. For example, it can be shown that the probability
of the event that the SP

X(t)

produces a sample-path that is continuous at time
zero cannot be computed from the FDDs. This is not due to our limited analytic
capabilities but rather because there exist two stochastic processes of identical
FDDs where for one process this event is of zero probability whereas for the other
it is of probability one (Cram´er and Leadbetter, 2004, Section 3.6). Fortunately,
most of the questions of interest to us in Digital Communications can be answered
based on the FDDs.
Another example is a very subtle question related to measurability. From the FDDs
alone one cannot determine whether the trajectories are measurable functions of
time, i.e., whether it makes sense to talk about integrals of the form
 ∞
−∞x(ω, t) dt.
This issue will be revisited in Section 25.9.
The above discussion motivates us to deﬁne the set of events whose probability
can be determined from the FDDs using the axioms of probability, i.e., using the
rules that the probability of the set of all possible outcomes Ω is one and that
the probability of a countable union of disjoint events is the inﬁnite sum of the
probabilities of the events. In the mathematical literature what we are deﬁning is
called the σ-algebra generated by

X(t), t ∈R

or the σ-algebra generated
by the cylindrical sets of

X(t), t ∈R

.2 For the classical deﬁnition see, for
example, (Billingsley, 1995, Section 36).
Deﬁnition 25.2.2 (σ-Algebra Generated by a SP). The σ-algebra generated
by a SP

X(t), t ∈R

which is deﬁned over the probability space (Ω, F, P) is
the set of events (i.e., elements of F) whose probability can be computed from the
FDDs of

X(t)

using only the axioms of probability.
2It is the smallest σ-algebra with respect to which all the random variables
X(t), t ∈R

are
measurable.
25.3 Deﬁnition of a Gaussian SP
563
We now rephrase our previous statement about continuity as saying that the set
of ω ∈Ω for which the function t 	→X(ω, t) is continuous at t = 0 is not in the
σ-algebra generated by

X(t)

. The probability of such sets cannot be inferred
from the FDDs alone. If such sets are assigned a probability, it must be based on
some additional information that is not captured by the FDDs.
The FDDs provide a natural way to deﬁne independence between stochastic pro-
cesses.
Deﬁnition 25.2.3 (Independent Stochastic Processes). Two stochastic processes

X(t)

and

Y (t)

, deﬁned on the same probability space (Ω, F, P), are said to
be independent stochastic processes if for every n ∈N and any choice of the
epochs t1, . . . , tn ∈R, the n-tuples (X(t1), . . . , X(tn)) and (Y (t1), . . . , Y (tn)) are
independent.
25.3
Deﬁnition of a Gaussian SP
By far the most important processes for modeling noise in Digital Communications
are the Gaussian processes. Fortunately, these processes are among the mathemat-
ically most tractable. The deﬁnition of a Gaussian SP builds on that of a Gaussian
vector (Deﬁnition 23.1.1).
Deﬁnition 25.3.1 (Gaussian Stochastic Processes). A SP

X(t)

is said to be a
Gaussian stochastic process if, for every n ∈N and every choice of the epochs
t1, . . . , tn ∈R, the random vector (X(t1), . . . , X(tn))T is Gaussian.
Note 25.3.2. Gaussian stochastic processes are of ﬁnite variance.
Proof. If

X(t)

is a Gaussian process, then a fortiori at each epoch t ∈R, the
random variable X(t) is a univariate Gaussian (choose n = 1 in the above deﬁni-
tion) and hence, by the deﬁnition of the univariate distribution (Deﬁnition 19.3.1),
of ﬁnite variance.
One of the things that make Gaussian processes tractable is the ease with which
their FDDs can be speciﬁed.
Proposition 25.3.3 (The FDDs of a Gaussian SP). If

X(t)

is a centered Gaus-
sian SP, then all its FDDs are determined by the mapping that speciﬁes the covari-
ance between any two of its samples:
(t1, t2) 	→Cov

X(t1), X(t2)

,
t1, t2 ∈R.
(25.5)
Proof. Let

X(t)

be a centered Gaussian SP. We shall show that, for any choice of
the epochs t1, . . . , tn ∈R, we can compute the joint distribution of X(t1), . . . X(tn)
from the mapping (25.5). To this end we note that since

X(t)

is a Gaussian
SP, the random vector (X(t1), . . . X(tn))T is Gaussian (Deﬁnition 25.3.1). Conse-
quently, its distribution is fully speciﬁed by its mean vector and covariance matrix
(Theorem 23.6.7). Its mean vector is zero, because we assumed that

X(t)

is cen-
tered. To conclude the proof we thus only need to show that the covariance matrix
564
Continuous-Time Stochastic Processes
of (X(t1), . . . X(tn))T is determined by the mapping (25.5). But this is obvious
because the covariance matrix of (X(t1), . . . X(tn))T is the n × n matrix
⎛
⎜
⎜
⎜
⎜
⎝
Cov[X(t1), X(t1)]
Cov[X(t1), X(t2)]
· · ·
Cov[X(t1), X(tn)]
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
Cov[X(tn), X(t1)]
Cov[X(tn), X(t2)]
· · ·
Cov[X(tn), X(tn)]
⎞
⎟
⎟
⎟
⎟
⎠
,
(25.6)
and each of the entries in this matrix is speciﬁed by the mapping (25.5).
Things become even simpler if the Gaussian process is wide-sense stationary
(Deﬁnition 25.4.2 ahead). In this case the RHS of (25.5) is determined by t1 −t2,
so the mapping (25.5) (and hence all the FDDs) is determined by the mapping
τ 	→Cov[X(t), X(t + τ)]. But before discussing wide-sense stationary Gaussian
stochastic processes in Section 25.5, we ﬁrst deﬁne stationarity and wide-sense
stationarity for general processes that are not necessarily Gaussian.
25.4
Stationary Continuous-Time Processes
With an eye to deﬁning stationarity for continuous-time stochastic processes, we
deﬁne the time shift by τ of a SP X: (ω, t) 	→X(ω, t) as the SP
(ω, t) 	→X(ω, t −τ).
(25.7)
A SP is stationary if its time shifts all have the same FDDs. More formally, we
have the following continuous-time analogue of Deﬁnition 13.2.1:
Deﬁnition 25.4.1 (Stationary Continuous-Time SP). We say that a continuous-
time SP

X(t)

is stationary (or strict sense stationary, or strongly sta-
tionary) if for every n ∈N, any epochs t1, . . . , tn ∈R, and every τ ∈R,

X(t1 + τ), . . . , X(tn + τ)
 L=

X(t1), . . . , X(tn)

.
(25.8)
By considering the case where n = 1 we obtain that if

X(t)

is stationary, then
all its samples have the same distribution
X(t)
L= X(t + τ),
t, τ ∈R.
(25.9)
That is, the distribution of the random variable X(t) does not depend on t. By
considering n = 2 we obtain that if

X(t)

is stationary, then the joint distribution
of any two of its samples depends on how far apart they are and not on the absolute
time at which they are taken

X(t1), X(t2)
 L=

X(t1 + τ), X(t2 + τ)

,
t1, t2, τ ∈R.
(25.10)
That is, the joint distribution of

X(t1), X(t2)

can be computed from t2 −t1.
As we did for discrete-time processes (Deﬁnition 13.3.1), we can also deﬁne wide-
sense stationarity of continuous-time processes. Recall that a process

X(t)

is
said to be of ﬁnite variance if at every time t ∈R the random variable X(t) is of
ﬁnite variance.
25.4 Stationary Continuous-Time Processes
565
Deﬁnition 25.4.2 (Wide-Sense Stationary Continuous-Time SP). A continuous-
time SP

X(t)

is said to be wide-sense stationary (or weakly stationary or
second-order stationary) if the following three conditions are met:
1) It is of ﬁnite variance.
2) Its mean is constant
E

X(t)

= E

X(t + τ)

,
t, τ ∈R.
(25.11)
3) The covariance between its samples satisﬁes
Cov

X(t1), X(t2)

= Cov

X(t1 + τ), X(t2 + τ)

,
t1, t2, τ ∈R.
(25.12)
By considering the case where t1 = t2 in (25.12), we obtain that all the samples of
a WSS SP have the same variance:
Var

X(t)

= Var

X(0)

,
t ∈R.
(25.13)
Note 25.4.3. Every ﬁnite-variance stationary SP is WSS.
Proof. This follows because (25.9) implies (25.11), and because (25.10) implies
(25.12).
The reverse is not true: some WSS processes are not stationary.
(Wide-sense
stationarity concerns only means and covariances, whereas stationarity has to do
with distributions.)
The following deﬁnition of the autocovariance function of a continuous-time WSS
SP is the analogue of Deﬁnition 13.5.1.
Deﬁnition 25.4.4 (Autocovariance Function). The autocovariance function
KXX : R →R of a WSS continuous-time SP

X(t)

is deﬁned for every τ ∈R by
KXX(τ) ≜Cov

X(t + τ), X(t)

,
(25.14)
where the RHS does not depend on t because

X(t)

is assumed to be WSS.
By evaluating (25.14) at τ = 0 and using (25.13), we can express the variance
of X(t) in terms of the autocovariance function KXX as
Var

X(t)

= KXX(0),
t ∈R.
(25.15)
We end this section with a few simple inequalities related to WSS stochastic pro-
cesses and their autocovariance functions.
Lemma 25.4.5. Let

X(t)

be a WSS SP of autocovariance function KXX. Then
KXX(τ)
 ≤KXX(0),
τ ∈R,
(25.16)
E

|X(t)|

≤
3
KXX(0) + E[X(0)]2,
t ∈R,
(25.17)
and
E
X(t) X(t′)

≤KXX(0) + E[X(0)]2 ,
t, t′ ∈R.
(25.18)
566
Continuous-Time Stochastic Processes
Proof. Inequality (25.16) follows from the Covariance Inequality (Corollary 3.5.2):
|KXX(τ)| =
Cov[X(t + τ), X(t)]

≤

Var[X(t + τ)]

Var[X(t)]
= KXX(0),
where the last equality follows from (25.15).
Inequality (25.17) follows from the nonnegativity of the variance of |X(t)| and the
assumption that

X(t)

is WSS:
0 ≤Var[|X(t)|]
= E

X2(t)

−

E[|X(t)|]
2
= Var[X(t)] +

E[X(t)]
2 −

E[|X(t)|]
2
= KXX(0) +

E[X(0)]
2 −

E[|X(t)|]
2.
Finally, Inequality (25.18) follows from the Cauchy-Schwarz Inequality for random
variables (Theorem 3.5.1)
E[UV ]
 ≤

E[U 2] E[V 2]
by substituting |X(t)| for U and |X(t′)| for V and by noting that
E

|X(t)|2
= E

X2(t)

= Var

X(t)

+

E[X(t)]
2
= KXX(0) +

E[X(0)]
2,
t ∈R.
25.5
Stationary Gaussian Stochastic Processes
For Gaussian stochastic processes we do not distinguish between stationarity and
wide-sense stationarity. The reason is that, while for general processes the two
concepts are diﬀerent (in that every ﬁnite-variance stationary SP is WSS, but not
every WSS SP is stationary), for Gaussian stochastic processes the two concepts are
equivalent. These relationships between stationarity and wide-sense stationarity for
general stochastic processes and for Gaussian stochastic processes are illustrated
in Figure 25.1.
Proposition 25.5.1 (Stationary Gaussian Stochastic Processes).
(i) A Gaussian SP is stationary if, and only if, it is WSS.
(ii) The FDDs of a centered stationary Gaussian SP are fully speciﬁed by its
autocovariance function.
Proof. We begin by proving (i). One direction has only little to do with Gaus-
sianity. Since every Gaussian SP is of ﬁnite variance (Note 25.3.2), and since every
25.5 Stationary Gaussian Stochastic Processes
567
stochastic processes
ﬁnite-variance
WSS
(strictly) stationary
Gaussian
Figure 25.1: The relationship between wide-sense stationarity, Gaussianity, and
strict-sense stationarity. The outer region corresponds to all stochastic processes.
Within it is the set of all ﬁnite-variance processes and within that the set of all wide-
sense stationary processes. The slanted lines indicate the strict-sense stationary
processes, and the gray area corresponds to the Gaussian stochastic processes. Not
all strict-sense stationary SPs are WSS, but those of ﬁnite variance are.
ﬁnite-variance stationary SP is WSS (Note 25.4.3), it follows that every stationary
Gaussian SP is WSS.
Gaussianity plays a much more important role in the proof of the reverse direction,
namely, that every WSS Gaussian SP is stationary. We prove this by showing that
if

X(t)

is Gaussian and WSS, then for every n ∈N and any t1, . . . , tn, τ ∈R
the joint distribution of X(t1), . . . , X(tn) is identical to the joint distribution of
X(t1 + τ), . . . , X(tn + τ). To this end, let n ∈N and t1, . . . , tn, τ ∈R be ﬁxed.
Because

X(t)

is Gaussian, (X(t1), . . . , X(tn))T and (X(t1 + τ), . . . , X(tn + τ))T
are both Gaussian vectors (Deﬁnition 25.3.1). And since

X(t)

is WSS, the two
are of the same mean vector (see (25.11)). The former’s covariance matrix is
⎛
⎜
⎜
⎜
⎜
⎝
Cov[X(t1), X(t1)]
· · ·
Cov[X(t1), X(tn)]
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
Cov[X(tn), X(t1)]
· · ·
Cov[X(tn), X(tn)]
⎞
⎟
⎟
⎟
⎟
⎠
568
Continuous-Time Stochastic Processes
and the latter’s is
⎛
⎜
⎜
⎜
⎜
⎝
Cov[X(t1 + τ), X(t1 + τ)]
· · ·
Cov[X(t1 + τ), X(tn + τ)]
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
Cov[X(tn + τ), X(t1 + τ)]
· · ·
Cov[X(tn + τ), X(tn + τ)]
⎞
⎟
⎟
⎟
⎟
⎠
.
Since

X(t)

is WSS, the two covariance matrices are identical (see (25.12)). But
two Gaussian vectors of equal mean vectors and of equal covariance matrices have
identical distributions (Theorem 23.6.7), so the distribution of (X(t1), . . . , X(tn))T
is identical to that of (X(t1+τ), . . . , X(tn+τ))T. Since this has been established for
all choices of n ∈N and all choices of t1, . . . , tn, τ ∈R, the SP

X(t)

is stationary.
Part (ii) follows from Proposition 25.3.3 and the deﬁnition of wide-sense stationar-
ity. Indeed, by Proposition 25.3.3, all the FDDs of a centered Gaussian SP

X(t)

are determined by the mapping (25.5). If

X(t)

is additionally WSS, then the
RHS of (25.5) can be computed from t1 −t2 and is given by KXX(t1 −t2), so the
mapping (25.5) is fully speciﬁed by the autocovariance function KXX.
25.6
Properties of the Autocovariance Function
Many of the deﬁnitions and results on continuous-time WSS stochastic processes
have analogous discrete-time counterparts. But some technical issues are encoun-
tered only in continuous time. For example, most results on continuous-time WSS
stochastic processes require that the autocovariance function of the process be
continuous at the origin, i.e., satisfy
lim
δ→0 KXX(δ) = KXX(0),
(25.19)
and this condition has no discrete-time counterpart. As we next show, this condi-
tion is equivalent to the condition
lim
δ→0 E
%
X(t + δ) −X(t)
2&
= 0,
t ∈R.
(25.20)
This equivalence follows from the identity
E
%
X(t) −X(t + δ)
2&
= 2

KXX(0) −KXX(δ)

,
t, δ ∈R,
(25.21)
which can be proved as follows. We ﬁrst note that it suﬃces to prove it for centered
processes, and for such processes we then compute:
E
%
X(t) −X(t + δ)
2&
= E

X2(t) −2X(t) X(t + δ) + X2(t + δ)

= E

X2(t)

−2 E

X(t) X(t + δ)

+ E

X2(t + δ)

= KXX(0) −2 KXX(δ) + KXX(0)
= 2

KXX(0) −KXX(δ)

,
25.6 Properties of the Autocovariance Function
569
where the ﬁrst equality follows by opening the square; the second by the linearity of
expectation; the third by the deﬁnition of KXX; and the ﬁnal equality by collecting
terms.
We note here that if the autocovariance function of a WSS process is continuous
at the origin, then it is continuous everywhere. In fact, it is uniformly continuous:
Lemma 25.6.1. If the autocovariance function of a WSS continuous-time SP is
continuous at the origin, then it is a uniformly continuous function.
Proof. We ﬁrst note that it suﬃces to prove the lemma for centered processes.
Let

X(t)

be such a process. For every τ, δ ∈R we then have3
KXX(τ + δ) −KXX(τ)
 =
E[X(τ + δ) X(0)] −E[X(τ) X(0)]

=
E

X(τ + δ) −X(τ)

X(0)

=
Cov

X(τ + δ) −X(τ), X(0)

≤
4
E
%
X(τ + δ) −X(τ)
2& 
E[X2(0)]
=
3
2

KXX(0) −KXX(δ)
 
KXX(0)
=
3
2 KXX(0)

KXX(0) −KXX(δ)

,
(25.22)
where the equality in the ﬁrst line follows from the deﬁnition of the autocovariance
function because

X(t)

is centered; the equality in the second line by the linearity
of expectation; the equality in the third line by the deﬁnition of the covariance
between two zero-mean random variables; the inequality in the fourth line by the
Covariance Inequality (Corollary 3.5.2); the equality in the ﬁfth line by (25.21);
and the ﬁnal equality by trivial algebra.
The uniform continuity of KXX now
follows from (25.22) by noting that its RHS does not depend on τ and that, by our
assumption about the continuity of KXX at zero, it tends to zero as δ →0.
We next derive two important properties of autocovariance functions and then
demonstrate in Theorem 25.6.2 that these properties characterize those functions
that can arise as the autocovariance functions of a WSS SP. These properties are
the continuous-time analogues of (13.12) & (13.13), and the proofs are almost
identical. We ﬁrst state the properties and then proceed to prove them.
The ﬁrst property is that the autocovariance function KXX of any continuous-time
WSS process

X(t)

is a symmetric function
KXX(−τ) = KXX(τ),
τ ∈R.
(25.23)
The second is that it is a positive deﬁnite function in the sense that for every
n ∈N, and for every choice of the coeﬃcients α1, . . . , αn ∈R and of the epochs
t1, . . . , tn ∈R
n

ν=1
n

ν′=1
αναν′ KXX(tν −tν′) ≥0.
(25.24)
3This calculation is, in fact, also valid when
X(t)

is WSS but not centered.
570
Continuous-Time Stochastic Processes
To prove (25.23) we calculate
KXX(τ) = Cov

X(t + τ), X(t)

= Cov

X(t′), X(t′ −τ)

= Cov

X(t′ −τ), X(t′)

= KXX(−τ),
τ ∈R,
where the ﬁrst equality follows from the deﬁnition of KXX(τ) (25.14); the second
by deﬁning t′ ≜t + τ; the third because Cov[X, Y ] = Cov[Y, X] (for real random
variables); and the ﬁnal equality by the deﬁnition of KXX(−τ) (25.14).
To prove (25.24) we compute
n

ν=1
n

ν′=1
αναν′ KXX(tν −tν′) =
n

ν=1
n

ν′=1
αναν′Cov[X(tν), X(tν′)]
= Cov

n

ν=1
ανX(tν),
n

ν′=1
αν′X(tν′)

= Var

n

ν=1
ανX(tν)

(25.25)
≥0.
The next theorem demonstrates that Properties (25.23) and (25.24) characterize
the autocovariance functions of WSS stochastic processes (cf. Theorem 13.5.2).
Theorem 25.6.2. Every symmetric positive deﬁnite function is the autocovariance
function of some stationary Gaussian SP.
Proof. The proof is based on Kolmogorov’s Existence Theorem (Theorem 25.2.1)
and is only sketched here. Let K(·) be a symmetric and positive deﬁnite function
from R to R. The idea is to consider, for every n ∈N and for every choice of the
epochs t1, . . . , tn ∈R, the joint distribution function Gn(·; t1, . . . , tn) corresponding
to the centered multivariate Gaussian distribution of covariance matrix
⎛
⎜
⎜
⎜
⎜
⎝
K(t1 −t1)
K(t1 −t2)
· · ·
K(t1 −tn)
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
K(tn −t1)
K(tn −t2)
· · ·
K(tn −tn)
⎞
⎟
⎟
⎟
⎟
⎠
and to verify that the sequence {Gn(·; ·)} satisﬁes the symmetry and consistency
requirements of Kolmogorov’s Existence Theorem. The details, which can be found
in (Doob, 1990, Chapter II, Section § 3, Theorem 3.1), are omitted.
25.7 The Power Spectral Density of a Continuous-Time SP
571
25.7
The Power Spectral Density of a Continuous-Time SP
Under suitable conditions, engineers usually deﬁne the power spectral density of a
WSS SP as the Fourier Transform of its autocovariance function. There is nothing
wrong with this deﬁnition, and we encourage the reader to think about the PSD
in this way.4 We, however, prefer a slightly more general deﬁnition that allows us
also to consider discontinuous spectra and, more importantly, allows us to infer
that any integrable, nonnegative, symmetric function is the PSD of some Gaus-
sian SP (Proposition 25.7.3). Fortunately, the two deﬁnitions agree whenever the
autocovariance function is continuous and integrable.
Before deﬁning the PSD, we pause to discuss the Fourier Transform of the auto-
covariance function. If the autocovariance function KXX of a WSS SP

X(t)

is
integrable, i.e., if
 ∞
−∞
KXX(τ)
 dτ < ∞,
(25.26)
then we can discuss its FT ˆKXX. The following proposition summarizes the main
properties of the FT of continuous integrable autocovariance functions.
Proposition 25.7.1. If the autocovariance function KXX is continuous at the origin
and integrable, then its Fourier Transform ˆKXX is nonnegative
ˆKXX(f) ≥0,
f ∈R
(25.27)
and symmetric
ˆKXX(−f) = ˆKXX(f),
f ∈R.
(25.28)
Moreover, the Inverse Fourier Transform recovers KXX in the sense that5
KXX(τ) =
 ∞
−∞
ˆKXX(f) ei2πfτ df,
τ ∈R.
(25.29)
Proof. This result can be deduced from three results in (Feller, 1971, Chap-
ter XIX): the theorem in Section 3, Bochner’s Theorem in Section 2, and Lemma 1
in Section 2.
Deﬁnition 25.7.2 (The PSD of a Continuous-Time WSS SP). We say that the
WSS continuous-time SP

X(t)

is of power spectral density (PSD) SXX if SXX
is a nonnegative, symmetric, integrable function from R to R whose Inverse Fourier
Transform is the autocovariance function KXX of

X(t)

:
KXX(τ) =
 ∞
−∞
SXX(f) ei2πfτ df,
τ ∈R.
(25.30)
4Engineers can, however, be a bit sloppy in that they sometimes speak of a SP whose PSD
is discontinuous, e.g., the Brickwall function f →I{|f| ≤W}. This is inconsistent with their
deﬁnition because the FT of an integrable function must be continuous (Theorem 6.2.11), and
consequently if the autocovariance function is integrable then its FT cannot be discontinuous.
Our more general deﬁnition does not suﬀer from this problem and allows for discontinuous PSDs.
5Recall that without additional assumptions one is not guaranteed that the Inverse Fourier
Transform of the Fourier Transform of a function will be identical to the original function. Here we
need not make any additional assumptions because we already assumed that the autocovariance
function is continuous and because autocovariance functions are positive deﬁnite.
572
Continuous-Time Stochastic Processes
A few remarks regarding this deﬁnition:
(i) By the uniqueness of the IFT (the analogue of Theorem 6.2.12 for the IFT) it
follows that if two functions are PSDs of the same WSS SP, then they must be
equal except on a set of frequencies of Lebesgue measure zero. Consequently,
we shall often speak of “the” PSD as though it were unique.
(ii) By Proposition 25.7.1, if KXX is continuous and integrable, then

X(t)

has
a PSD in the sense of Deﬁnition 25.7.2, and this PSD is the FT of KXX.
There are, however, autocovariance functions that are not integrable and
that nonetheless have a PSD in the sense of Deﬁnition 25.7.2. For example,
τ 	→sinc(τ).
Thus, every continuous autocovariance function that has a PSD in the en-
gineers’ sense (i.e., that is integrable) also has the same PSD according to
our deﬁnition, but our deﬁnition is more general in that some autocovariance
functions that have a PSD according to our deﬁnition are not integrable and
therefore do not have a PSD in the engineers’ sense.
(iii) By substituting τ = 0 in (25.30) and using (25.15) we can express the variance
of X(t) in terms of the PSD SXX as
Var

X(t)

= KXX(0) =
 ∞
−∞
SXX(f) df,
t ∈R.
(25.31)
(iv) Only processes with continuous autocovariance functions have PSDs, because
the RHS of (25.30), being the IFT of an integrable function, must be contin-
uous (Theorem 6.2.11 (ii)).
(v) It can be shown that if the autocovariance function can be written as the
IFT of some integrable function, then this latter function must be nonneg-
ative (except on a set of frequencies of Lebesgue measure zero). This is the
continuous-time analogue of Proposition 13.6.3.
The nonnegativity, symmetry, and integrability conditions characterize PSDs in
the following sense:
Proposition 25.7.3. Every nonnegative, symmetric, integrable function is the PSD
of some stationary Gaussian SP whose autocovariance function is continuous.
Proof. Let S(·) be some integrable, nonnegative, and symmetric function from R
to the nonnegative reals. Deﬁne K(·) to be its IFT
K(τ) =
 ∞
−∞
S(f) ei2πfτ df,
τ ∈R.
(25.32)
We shall verify that K(·) satisﬁes the hypotheses of Theorem 25.6.2, namely, that
it is symmetric and positive deﬁnite. It will then follow from Theorem 25.6.2 that
25.8 The Spectral Distribution Function
573
there exists a stationary Gaussian SP

X(t)

whose autocovariance function KXX
is equal to K(·) and is thus given by
KXX(τ) =
 ∞
−∞
S(f) ei2πfτ df,
τ ∈R.
(25.33)
This will establish that

X(t)

is of PSD S(·). The continuity of KXX will follow
from the continuity of the IFT of integrable functions (Theorem 6.2.11).
To conclude the proof we need to show that the function K(·) deﬁned in (25.32)
is symmetric and positive deﬁnite. The symmetry follows from our assumption
that S(·) is symmetric:
K(−τ) =
 ∞
−∞
S(f) ei2πf(−τ) df
=
 ∞
−∞
S(−˜f) ei2π ˜
fτ d ˜f
=
 ∞
−∞
S( ˜f) ei2π ˜
fτ d ˜f
= K(τ),
τ ∈R,
where the ﬁrst equality follows from (25.32); the second from the change of variable
˜f ≜−f; the third by the symmetry of S(·); and the ﬁnal equality again by (25.32).
We next prove that K(·) is positive deﬁnite. To that end we ﬁx some n ∈N, some
constants α1, . . . , αn ∈R, and some epochs t1, . . . , tn ∈R and compute:
n

ν=1
n

ν′=1
αναν′K(tν −tν′) =
n

ν=1
n

ν′=1
αναν′
 ∞
−∞
S(f) ei2πf(tν−tν′) df
=
 ∞
−∞
S(f)
	
n

ν=1
n

ν′=1
αναν′ ei2πf(tν−tν′)

df
=
 ∞
−∞
S(f)
	
n

ν=1
n

ν′=1
αν ei2πftναν′ e−i2πftν′

df
=
 ∞
−∞
S(f)
	
n

ν=1
αν ei2πftν

	
n

ν′=1
αν′ ei2πftν′

∗
df
=
 ∞
−∞
S(f)

n

ν=1
αν ei2πftν

2
df
≥0,
where the ﬁrst equality follows from (25.32); the subsequent equalities by simple
algebra; and the last inequality from our assumption that S(·) is nonnegative.
25.8
The Spectral Distribution Function
In this section we shall state, without proof, Bochner’s Theorem on continuous
positive deﬁnite functions and discuss its application to continuous autocovariance
574
Continuous-Time Stochastic Processes
functions. We shall then deﬁne the spectral distribution function of WSS stochastic
processes. The concept of a spectral distribution function is more general than
that of a PSD, because every WSS with a continuous autocovariance function has
a spectral distribution function, but only some have a PSD. Nevertheless, for our
purposes, the notion of PSD will suﬃce, and the results of this section will not be
used in the rest of the book.
Recall that the characteristic function ΦX(·) of a RV X is the mapping from R
to C deﬁned by
ϖ 	→E

eiϖX
,
ϖ ∈R.
(25.34)
If X is symmetric (i.e., has a symmetric distribution) in the sense that
Pr[X ≥x] = Pr[X ≤−x],
x ∈R,
(25.35)
then ΦX(·) only takes on real values and is a symmetric function, as the following
argument shows. The symmetry of the distribution of X implies that X and −X
have the same distribution, which implies that their exponentiations have the same
law
eiϖX
L= e−iϖX,
ϖ ∈R,
(25.36)
and a fortiori that the expectation of the two exponentials are equal
E

eiϖX
= E

e−iϖX
,
ϖ ∈R.
(25.37)
The LHS of (25.37) is ΦX(ϖ), and the RHS is ΦX(−ϖ), thus demonstrating the
symmetry of ΦX(·). To establish that (25.35) also implies that ΦX(·) is real, we
note that, by (25.37),
ΦX(ϖ) = E

eiϖX
= 1
2

E

eiϖX
+ E

e−iϖX
= E
eiϖX + e−iϖX
2

= E

cos(ϖX)

,
ϖ ∈R,
which is real. Here the ﬁrst equality follows from (25.34); the second from (25.37);
and the third from the linearity of expectation.
Bochner’s Theorem establishes a correspondence between continuous, symmetric,
positive deﬁnite functions and characteristic functions.
Theorem 25.8.1 (Bochner’s Theorem). Let the mapping Φ(·) from R to R be
continuous. Then the following two conditions are equivalent:
a) Φ(·) is the characteristic function of some RV having a symmetric distribu-
tion.
b) Φ(·) is a symmetric positive deﬁnite function satisfying Φ(0) = 1.
Proof. See (Feller, 1971, Chapter XIX, Section 2) or (Lo`eve, 1963, Chapter IV,
Section 14) or (Katznelson, 2004, Chapter VI, Section 2.8).
25.8 The Spectral Distribution Function
575
Bochner’s Theorem is the key to understanding autocovariance functions:
Proposition 25.8.2. Let

X(t)

be a WSS SP whose autocovariance function KXX
is continuous. Then:
(i) There exists a symmetric RV S such that
KXX(τ) = KXX(0) E

ei2πτS
,
τ ∈R.
(25.38)
(ii) If KXX(0) > 0, then the distribution of S in (25.38) is uniquely determined
by KXX, and

X(t)

has a PSD if, and only if, S has a density.
Proof. If KXX(0) = 0, then

X(t)

is deterministic in the sense that for every
epoch t ∈R the variance of X(t) is zero. By the inequality |KXX(τ)| ≤KXX(0)
(Lemma 25.4.5, (25.16)) it follows that if KXX(0) = 0 then KXX(τ) = 0 for all
τ ∈R, and (25.38) holds in this case for any choice of S and there is nothing else
to prove.
Consider now the case KXX(0) > 0. To prove Part (i) we note that because KXX is
by assumption continuous, and because all autocovariance functions are symmetric
and positive deﬁnite (see (25.23) and (25.24)), it follows that the mapping
τ 	→KXX(τ)
KXX(0),
τ ∈R
is a continuous, symmetric, positive deﬁnite mapping that takes on the value one
at τ = 0. Consequently, by Bochner’s Theorem, there exists a RV R of a symmetric
distribution such that
KXX(τ)
KXX(0) = E

eiτR
,
τ ∈R.
It follows that if we deﬁne S as R/(2π) then (25.38) will hold, and Part (i) is thus
also established for the case where KXX(0) > 0.
We conclude the treatment of the case KXX(0) > 0 by proving Part (ii). That the
distribution of S is unique follows because (25.38) implies that
E

eiϖS
= KXX(ϖ/(2π))
KXX(0)
,
ϖ ∈R,
so KXX determines the characteristic function of S and hence also its distribution
(Theorem 17.4.4).
Because the distribution of S is symmetric, if S has a density then it also has a
symmetric density. Denote by fS(·) a symmetric density function for S. In terms
of fS(·) we can rewrite (25.38) as
KXX(τ) =
 ∞
−∞
KXX(0) fS(s) ei2πsτ ds,
τ ∈R,
so the nonnegative symmetric function KXX(0) fS(·) is a PSD of

X(t)

. Con-
versely, if

X(t)

has PSD SXX, then
KXX(τ) =
 ∞
−∞
SXX(f) ei2πfτ df,
τ ∈R,
(25.39)
576
Continuous-Time Stochastic Processes
and (25.38) holds with S having the density
fS(s) = SXX(s)
KXX(0),
s ∈R.
(25.40)
(The RHS of (25.40) is symmetric, nonnegative, and integrates to 1 by (25.31).)
Proposition 25.8.2 motivates us to deﬁne the spectral distribution function of a
continuous autocovariance function (or of a WSS SP having such an autocovariance
function) as follows.
Deﬁnition 25.8.3 (Spectral Distribution Function). The spectral distribution
function of a continuous autocovariance function KXX is the mapping
ξ 	→KXX(0) Pr[S ≤ξ],
(25.41)
where S is a random variable for which (25.38) holds.
25.9
The Average Power
We next address the average power in the sample-paths of a SP. We would like to
better understand formal expressions of the form
1
T
 T/2
−T/2
X2(ω, t) dt
for a SP

X(t)

deﬁned on the probability space (Ω, F, P). Recalling that if we ﬁx
ω ∈Ω then we can view the trajectory t 	→X(ω, t) as a function of time, we would
like to think about the integral above as the time-integral of the square of the
trajectory t 	→X(ω, t). Since the result of this integral is a (nonnegative) number
that depends on ω, we would like to view this result as a nonnegative RV
ω 	→1
T
 T/2
−T/2
X2(ω, t) dt,
ω ∈Ω.
Mathematicians, however, would object to our naive approach on two grounds. The
ﬁrst is that it is prima facie unclear whether for every ﬁxed ω ∈Ω the mapping
t 	→X2(ω, t) is suﬃciently well-behaved to allow us to discuss its integral. (It may
not be Lebesgue measurable.) The second is that, even if this integral could be
carried out for every ω ∈Ω, it is prima facie unclear that the result would be a
RV. While it would certainly be a mapping from Ω to the extended reals (allowing
for +∞), it is not clear that it would satisfy the technical measurability conditions
that random variables must meet.6
To address these objections we shall assume that

X(t)

is a “measurable stochastic
process.” This is a technical condition that will be foreign to most readers and
6By “X is a random variable possibly taking on the value +∞” we mean that X is a mapping
from Ω to R ∪{+∞} with the set {ω ∈Ω : X(ω) ≤ξ} being an event for every ξ ∈R and with
the set {ω ∈Ω : X(ω) = +∞} also being an event.
25.9 The Average Power
577
that will be inessential to the rest of this book. We mention it here because, in
order to be mathematically honest, we shall have to slip this attribute into some
of the theorems that we shall later state. Nothing will be lost on readers who
replace “measurable stochastic process” with “stochastic process satisfying a mild
technical condition.”
Fortunately, this technical condition is, indeed, very mild. For example, Propo-
sition 25.7.3 still holds if we slip in the attribute “measurable” before the words
“Gaussian process.” Similarly, in Theorem 25.6.2, if we add the hypothesis that
the given function is continuous at the origin, then we can slip in the attribute
“measurable” before the words “stationary Gaussian stochastic process.”7
For the beneﬁt of readers who are familiar with Measure Theory, we provide the
following deﬁnition.
Deﬁnition 25.9.1 (Measurable SP). Let

X(t), t ∈R

be a SP deﬁned over the
probability space (Ω, F, P). We say that the process is a measurable stochastic
process if the mapping (ω, t) 	→X(ω, t) is a measurable mapping from Ω × R
to R when the codomain R is endowed with the Borel σ-algebra on R and when the
domain Ω × R is endowed with the σ-algebra deﬁned by the product of F on Ω and
the Borel σ-algebra on R.
The nice thing about measurable stochastic processes is that if

X(t)

is a measur-
able SP, then for every ω ∈Ω the trajectory t 	→X(ω, t) is a Borel (and hence also
Lebesgue) measurable function of time; see (Halmos, 1950, Chapter 7, Section 34,
Theorem B) or (Billingsley, 1995, Chapter 3, Section 18, Theorem 18.1 (ii)). More-
over, for such processes we can sometimes use Fubini’s Theorem to swap the order
in which we compute time-integrals and expectations; see (Halmos, 1950, Chap-
ter 7, Section 36) or (Billingsley, 1995, Chapter 3, Section 18, Theorem 18.3 (ii)).
We can now state the main result of this section regarding the average power in a
WSS SP.
Proposition 25.9.2 (Power in a Centered WSS SP). If

X(t)

is a measurable,
centered, WSS SP deﬁned over the probability space (Ω, F, P) and having the au-
tocovariance function KXX, then, for every a, b ∈R satisfying a < b, the mapping
ω 	→
1
b −a
 b
a
X2(ω, t) dt
(25.42)
deﬁnes a RV (possibly taking on the value +∞) satisfying
1
b −a E
 b
a
X2(t) dt

= KXX(0).
(25.43)
Proof. The proof of (25.43) is straightforward and merely requires swapping the
order of integration and expectation. This swap can be justiﬁed using Fubini’s The-
orem. Heuristically, the swapping of expectation and integration can be justiﬁed by
7These are but very special cases of a much more general result that states that given FDDs
corresponding to a WSS SP of an autocovariance that is continuous at the origin, there exists
a SP of the given FDDs that is also measurable. See, for example, (Doob, 1990, Chapter II,
Section § 2, Theorem 2.6). (Replacing the values ±∞with zero may ruin the separability but
not the measurability.)
578
Continuous-Time Stochastic Processes
thinking about the integral as being a Riemann integral that can be approximated
by ﬁnite sums and by then recalling the linearity of expectation that guarantees
that the expectation of a ﬁnite sum is the sum of the expectations. We then have
E
 b
a
X2(t) dt

=
 b
a
E

X2(t)

dt
=
 b
a
KXX(0) dt
= (b −a) KXX(0),
where the ﬁrst equality follows by swapping the integration with the expectation;
the second equality holds because our assumption that

X(t)

is centered implies
that, for every t ∈R, the RV X(t) is centered and by (25.14); and the ﬁnal equality
holds because the integrand is constant.
That (25.42) is a RV (possibly taking on the value +∞) follows from Fubini’s
Theorem.
Recalling Deﬁnition 14.6.1 of the power in a SP as
lim
T→∞E
1
T
 T/2
−T/2
X2(t) dt

,
we conclude:
Corollary 25.9.3. The power in a centered, measurable, WSS SP

X(t)

of auto-
covariance function KXX is equal to KXX(0).
25.10
Stochastic Integrals and Linear Functionals
For the problem of detecting continuous-time signals corrupted by noise, we shall
be interested in stochastic integrals of the form
 ∞
−∞
X(t) s(t) dt
(25.44)
for WSS stochastic processes

X(t)

deﬁned over a probability space (Ω, F, P)
and for properly well-behaved deterministic functions s(·). We would like to think
about the result of such an integral as a RV
ω 	→
 ∞
−∞
X(ω, t) s(t) dt
(25.45)
that maps each ω ∈Ω to the real number that is the result of the integration
over time of the product of the trajectory t 	→X(ω, t) corresponding to ω by the
deterministic function t 	→s(t). That is, each ω is mapped to the inner product
between its trajectory t 	→X(ω, t) and the function s(·).
This is an excellent way of thinking about such integrals, but we do run into
some mathematical objections similar to those we encountered in Section 25.9.
25.10 Stochastic Integrals and Linear Functionals
579
For example, it is not obvious that for each ω ∈Ω the mapping t 	→X(ω, t) s(t)
is a suﬃciently well-behaved function for the time-integral to be deﬁned. As we
shall see, for this reason we must impose certain restrictions on s(·), and we will
not claim that t 	→X(ω, t) s(t) is integrable for every ω ∈Ω but only for ω’s in
some subset of Ω having probability one. Also, even if this issue is addressed, it is
unclear whether the mapping of ω to the result of the integration is a RV: while it
is clearly a mapping from Ω to the reals, it is unclear that it satisﬁes the additional
mathematical requirement of measurability, i.e., that for every ξ ∈R the set
1
ω ∈Ω :
 ∞
−∞
X(ω, t) s(t) dt ≤ξ
2
be an event, i.e., an element of F.
We ask the reader to take it on faith that these issues can be resolved and to focus
on the relatively straightforward computation of the mean and variance of (25.45).
The resolution of the measurability issues is provided in Proposition 25.10.1, whose
proof is recommended only to readers with background in Measure Theory.
We shall assume throughout that

X(t)

is WSS and that the deterministic function
s: R →R is integrable. We begin by heuristically deriving the mean:
E
 ∞
−∞
X(t) s(t) dt

=
 ∞
−∞
E

X(t) s(t)

dt
=
 ∞
−∞
E

X(t)

s(t) dt
= E

X(0)
  ∞
−∞
s(t) dt,
(25.46)
with the following informal justiﬁcation. The ﬁrst equality follows by swapping
the expectation with the time-integration; the second because s(·) is deterministic;
and the last equality from our assumption that

X(t)

is WSS, which implies that

X(t)

is of constant mean: E[X(t)] = E[X(0)] for all t ∈R.
We next heuristically derive the variance of the integral in terms of the autocovari-
ance function KXX of the process

X(t)

. We begin by considering the case where

X(t)

is of zero mean. In this case we have
Var
 ∞
−∞
X(t) s(t) dt

= E
5	 ∞
−∞
X(t) s(t) dt

26
= E
	 ∞
−∞
X(t) s(t) dt

 	 ∞
−∞
X(τ) s(τ) dτ


= E
 ∞
−∞
 ∞
−∞
X(t) s(t) X(τ) s(τ) dt dτ

=
 ∞
−∞
 ∞
−∞
s(t) s(τ) E

X(t) X(τ)

dt dτ
=
 ∞
−∞
 ∞
−∞
s(t) KXX(t −τ) s(τ) dt dτ,
(25.47)
580
Continuous-Time Stochastic Processes
where the ﬁrst equality follows because (25.46) and our assumption that

X(t)

is centered combine to guarantee that

X(t) s(t) dt is of zero mean; the second
by writing a2 as a times a; the third by writing the product of integrals over R
as a double integral (i.e., as an integral over R2); the fourth by swapping the
double-integral with the expectation; and the ﬁnal equality by the deﬁnition of the
autocovariance function (Deﬁnition 25.4.4) and because

X(t)

is centered.
There are two equivalent ways of writing the RHS of (25.47) that we wish to point
out. The ﬁrst is obtained from (25.47) by changing the integration variables from
(t, τ) to (σ, τ), where σ ≜t −τ and by performing the integration ﬁrst over τ and
then over σ:
Var
 ∞
−∞
X(t) s(t) dt

=
 ∞
−∞
 ∞
−∞
s(t) KXX(t −τ) s(τ) dt dτ
=
 ∞
−∞
 ∞
−∞
s(σ + τ) KXX(σ) s(τ) dσ dτ
=
 ∞
−∞
KXX(σ)
 ∞
−∞
s(σ + τ) s(τ) dτ dσ
=
 ∞
−∞
KXX(σ) Rss(σ) dσ,
(25.48)
where Rss is the self-similarity function of s (Deﬁnition 11.2.1 and Section 11.4).
The second equivalent way of writing (25.47) can be derived from (25.48) when

X(t)

is of PSD SXX. Since (25.48) has the form of an inner product, we can use
Proposition 6.2.4 to write this inner product in the frequency domain by noting
that the FT of Rss is f 	→|ˆs(f)|2 (see (11.35)) and that KXX is the IFT of its
PSD SXX. The result is that
Var
 ∞
−∞
X(t) s(t) dt

=
 ∞
−∞
SXX(f)
ˆs(f)
2 df.
(25.49)
We next show that (25.47) (and hence also (25.48) & (25.49), which are equivalent
ways of writing (25.47)) remains valid also when

X(t)

is of mean μ (not neces-
sarily zero). To see this we can consider the zero-mean SP
 ˜X(t)

deﬁned at every
epoch t ∈R by ˜X(t) = X(t) −μ and formally compute
Var
 ∞
−∞
X(t) s(t) dt

= Var
 ∞
−∞
 ˜X(t) + μ

s(t) dt

= Var
 ∞
−∞
˜X(t) s(t) dt + μ
 ∞
−∞
s(t) dt

= Var
 ∞
−∞
˜X(t) s(t) dt

=
 ∞
−∞
 ∞
−∞
s(t) K ˜
X ˜
X(t −τ) s(τ) dt dτ
=
 ∞
−∞
 ∞
−∞
s(t) KXX(t −τ) s(τ) dt dτ,
(25.50)
25.10 Stochastic Integrals and Linear Functionals
581
where the ﬁrst equality follows from the deﬁnition of ˜X(t) as X(t) −μ; the second
by the linearity of integration; the third because adding a deterministic quantity
to a RV does not change its covariance; the fourth by (25.47) applied to the zero-
mean process
 ˜X(t)

; and the ﬁnal equality because the autocovariance function
of
 ˜X(t)

is the same as the autocovariance function of

X(t)

(Deﬁnition 25.4.4).
As above, once a result is proved for centered stochastic processes, its extension
to WSS stochastic processes with a mean can be straightforward. Consequently,
we shall often derive our results for centered WSS stochastic processes and leave
it to the reader to extend them to mean-μ stochastic processes by expressing such
stochastic processes as the sum of a zero-mean SP and the deterministic constant μ.
As promised, we now state the results about the mean and variance of (25.45) in
a mathematically defensible proposition.
Proposition 25.10.1 (Mean and Variance of Stochastic Integrals). Let

X(t)

be a measurable WSS SP deﬁned over the probability space (Ω, F, P) and having
the autocovariance function KXX. Let s: R →R be some deterministic integrable
function. Then:
(i) For every ω ∈Ω, the mapping t 	→X(ω, t) s(t) is Lebesgue measurable.
(ii) The set
N ≜
1
ω ∈Ω :
 ∞
−∞
X(ω, t) s(t)
 dt = ∞
2
(25.51)
is an event and its probability is zero.
(iii) The mapping from Ω \ N to R deﬁned by
ω 	→
 ∞
−∞
X(ω, t) s(t) dt
(25.52)
is measurable with respect to F.
(iv) The mapping from Ω to R deﬁned by
ω 	→
⎧
⎨
⎩
 ∞
−∞
X(ω, t) s(t) dt
if ω /∈N,
0
otherwise,
(25.53)
deﬁnes a random variable.
(v) The mean of this RV is
E

X(0)
  ∞
−∞
s(t) dt.
(25.54)
(vi) Its variance is
 ∞
−∞
 ∞
−∞
s(t) KXX(t −τ) s(τ) dτ dt,
(25.55)
582
Continuous-Time Stochastic Processes
which can also be expressed as
 ∞
−∞
KXX(σ) Rss(σ) dσ,
(25.56)
where Rss is the self-similarity function of s.
(vii) If

X(t)

is of PSD SXX, then the variance of this RV can be expressed as
 ∞
−∞
SXX(f)
ˆs(f)
2 df.
(25.57)
Proof. Part (i) follows because the measurability of the process

X(t)

guarantees
that for every ω ∈Ω the mapping t 	→X(ω, t) is Borel measurable and hence a
fortiori Lebesgue measurable; see (Billingsley, 1995, Chapter 3, Section 18, Theo-
rem 18.1 (ii)).
If s happens to be Borel measurable, then Parts (ii)–(v) follow directly by Fubini’s
Theorem (Billingsley, 1995, Chapter 3, Section 18, Theorem 18.3) because in this
case the mapping (ω, t) 	→X(ω, t) s(t) is measurable (with respect to the product
of F by the Borel σ-algebra on the real line) and because
 ∞
−∞
E
X(t) s(t)

dt =
 ∞
−∞
E[|X(t)|] |s(t)| dt
≤

E[X2(0)]
 ∞
−∞
|s(t)| dt
< ∞,
where the ﬁrst inequality follows from (25.17), and where the second inequality
follows from our assumption that s is integrable.
To prove Parts (i)–(v) for the case where s is Lebesgue measurable but not Borel
measurable, recall that every Lebesgue measurable function is equal (except on
a set of Lebesgue measure zero) to a Borel measurable function (Rudin, 1987,
Chapter 8, Lemma 1 or Chapter 2, Exercise 14), and note that the RHS of (25.51)
and the mappings in (25.52) and (25.53) are unaltered when s is replaced with a
function that is identical to it outside a set of Lebesgue measure zero.
We next prove Part (vi) under the assumption that

X(t)

is centered. The more
general case then follows from the argument leading to (25.50). To prove Part (vi)
we need to justify the steps leading to (25.47). For the reader’s convenience we
repeat these steps here and then proceed to justify them.
Var
 ∞
−∞
X(t) s(t) dt

= E
5	 ∞
−∞
X(t) s(t) dt

26
= E
	 ∞
−∞
X(t) s(t) dt

 	 ∞
−∞
X(τ) s(τ) dτ


= E
 ∞
−∞
 ∞
−∞
X(t) s(t)X(τ) s(τ) dt dτ

25.10 Stochastic Integrals and Linear Functionals
583
=
 ∞
−∞
 ∞
−∞
s(t) s(τ) E[X(t)X(τ)] dt dτ
=
 ∞
−∞
 ∞
−∞
s(t) KXX(t −τ) s(τ) dt dτ.
The ﬁrst equality follows from Part (v) and the assumption that

X(t)

is centered;
the second follows by writing a2 as a times a; the third follows because for ω’s sat-
isfying

|X(ω, t) s(t)| dt < ∞we can use Fubini’s Theorem to replace the iterated
integrals with a double integral and because other ω’s occur with zero probability
and therefore do not inﬂuence the expectation; the fourth equality entails swap-
ping the expectation with the integration over R2 and can be justiﬁed by Fubini’s
Theorem because, by (25.18),
 ∞
−∞
 ∞
−∞
s(t) s(τ)
 E
X(t)X(τ)

dt dτ ≤KXX(0)
 ∞
−∞
 ∞
−∞
|s(t)| |s(τ)| dt dτ
= KXX(0) ∥s∥2
1
< ∞;
and the ﬁnal equality follows from the deﬁnition of the autocovariance function
(Deﬁnition 25.4.4).
Having derived (25.55) we can derive (25.56) by following the steps leading to
(25.48). The only issue that needs clariﬁcation is the justiﬁcation for replacing
the integral over R2 with the iterated integrals. This is justiﬁed using Fubini’s
Theorem by noting that, by (25.16), | KXX(σ)| ≤KXX(0) and that s is integrable:
 ∞
−∞
|s(τ)|
 ∞
−∞
s(σ + τ) KXX(σ)
 dσ dτ ≤KXX(0)
 ∞
−∞
|s(τ)|
 ∞
−∞
|s(σ + τ)| dσ dτ
= KXX(0) ∥s∥2
1
< ∞.
Finally, Part (vii) follows from (25.56) and from Proposition 6.2.4 by noting that,
by (11.34) & (11.35), Rss is integrable and of FT
ˆRss(f) =
ˆs(f)
2,
f ∈R,
and that, by Deﬁnition 25.7.2, if SXX is the PSD of

X(t)

, then SXX is integrable
and its IFT is KXX, i.e.,
KXX(σ) =
 ∞
−∞
SXX(f) ei2πfσ df.
Note 25.10.2.
(i) In the future we shall sometimes write
 ∞
−∞
X(t) s(t) dt
584
Continuous-Time Stochastic Processes
instead of the mathematically more explicit (25.53) & (25.51). Sometimes,
however, we shall make the argument ω ∈Ω more explicit:
	 ∞
−∞
X(t) s(t) dt

(ω) =
⎧
⎨
⎩
 ∞
−∞
X(ω, t) s(t) dt
if
 ∞
−∞
X(ω, t) s(t)
 dt < ∞,
0
otherwise.
(ii) If s1 and s2 are indistinguishable integrable real signals (Deﬁnition 2.5.2),
then the random variables
 ∞
−∞X(t)s1(t) dt and
 ∞
−∞X(t)s2(t) dt are identi-
cal.
(iii) For every α ∈R
 ∞
−∞
X(t)

α s(t)

dt = α
 ∞
−∞
X(t) s(t) dt.
(25.58)
(iv) We caution the very careful readers that if s1 and s2 are integrable func-
tions, then there may be some ω’s in Ω for which the stochastic integral
 ∞
−∞X(t) (s1(t) + s2(t)) dt

(ω) is not equal to the sum of the stochastic
integrals
 ∞
−∞X(t) s1(t) dt

(ω) and
 ∞
−∞X(t) s2(t) dt

(ω). This can hap-
pen, for example, if the trajectory t 	→X(ω, t) corresponding to ω is such
that either

|X(ω, t) s1(t)| dt or

|X(ω, t) s2(t)| dt is inﬁnite, but not both.
Fortunately, as we shall see in Lemma 25.10.3, such ω’s occur with zero prob-
ability.
(v) The value that we have chosen to assign to the integral in (25.53) when ω is
in N is immaterial. Such ω’s occur with zero probability, so this value does
not inﬂuence the distribution of the integral.8
Lemma 25.10.3 (“Almost” Linearity of Stochastic Integration). Let

X(t)

be
a measurable WSS SP, let s1, . . . , sm : R →R be integrable, and let γ1, . . . , γm be
real. Then the random variables
ω 	→
- ∞
−∞
X(t)
	 m

j=1
γj sj(t)

dt
.
(ω)
(25.59)
and
ω 	→
m

j=1
γj
-	 ∞
−∞
X(t) sj(t) dt

(ω)
.
(25.60)
diﬀer on at most a set of ω’s of probability zero. In particular, the two random
variables have the same distribution.
Note 25.10.4. In view of this lemma we shall write, somewhat imprecisely,
 ∞
−∞
X(t)

α1 s1(t) + α2 s2(t)

dt = α1
 ∞
−∞
X(t) s1(t) dt + α2
 ∞
−∞
X(t) s2(t) dt.
8The value zero is convenient because it guarantees that (25.58) holds even for ω’s for which
the mapping t →X(ω, t) s(t) is not integrable.
25.11 Linear Functionals of Gaussian Processes
585
Proof of Lemma 25.10.3. Let (Ω, F, P) be the probability space over which the
SP

X(t)

is deﬁned. Deﬁne the function
s0 : t 	→
m

j=1
γj sj(t)
(25.61)
and the sets
Nj =
1
ω ∈Ω :
 ∞
−∞
X(ω, t) sj(t)
 dt = ∞
2
,
j = 0, 1, . . . , m.
By (25.61) and the Triangle Inequality (2.12)
X(ω, t) s0(t)
 ≤
m

j=1
|γj| |X(ω, t) sj(t)|,
ω ∈Ω, t ∈R,
which implies that
N0 ⊆
m

j=1
Nj.
By the Union Bound (or more speciﬁcally by Corollary 21.5.2 (i)), the set on the
RHS is of probability zero. The proof is concluded by noting that, outside this
set, the random variables (25.59) and (25.60) are identical. This follows because,
for ω’s outside this set, all the integrals are ﬁnite so linearity holds.
In view of the linearity of stochastic integration (Lemma 25.10.3), we shall adopt
the following terminology.9
Deﬁnition 25.10.5 (Linear Functional of a SP). A linear functional of a SP

X(t)

is an expression of the form
 ∞
−∞
X(t) s(t) dt +
n

ν=1
αν X(tν),
(25.62)
where s: R →R is some deterministic integrable function; n is a nonnegative
integer; the constants α1, . . . , αn are real; and so are the epochs t1, . . . , tn. When n
is zero the linear functional reduces to a stochastic integral.
25.11
Linear Functionals of Gaussian Processes
We continue our discussion of linear functionals of stochastic processes, but this
time with the additional assumption that the underlying SP is Gaussian.
The
main result of this section is Proposition 25.11.1, which states that, subject to
some technical conditions, linear functionals of Gaussian stochastic processes are
Gaussian random variables. That is, if

X(t)

is a stationary Gaussian SP, then
 ∞
−∞
X(t) s(t) dt +
n

ν=1
αν X(tν)
(25.63)
9Our terminology is not standard: not every linear mapping of stochastic processes to the
reals has the form (25.62).
586
Continuous-Time Stochastic Processes
is a Gaussian RV. Consequently, the distribution of such a functional is fully speci-
ﬁed by its mean and variance, which, as we shall see, can be easily computed from
the autocovariance function KXX of

X(t)

.
The proof of the Gaussianity of (25.63) (Proposition 25.11.1 ahead) is technical,
so we encourage the reader to focus on the following heuristic argument. Suppose
that the integral is a Riemann integral and that we can therefore approximate it
with a ﬁnite sum
 ∞
−∞
X(t) s(t) dt ≈
K

k=−K
δ X(δk) s(δk)
for some large enough K and small enough δ > 0. (Do not bother trying to sort
out the exact sense in which this approximation holds. This is, after all, a heuristic
argument.) Consequently, we can approximate (25.63) by
 ∞
−∞
X(t) s(t) dt +
n

ν=1
αν X(tν) ≈
K

k=−K
δ s(δk) X(δk) +
n

ν=1
αν X(tν).
(25.64)
But the RHS of the above is just a linear combination of the random variables
X(−Kδ), . . . , X(Kδ), X(t1), . . . , X(tn),
which are jointly Gaussian because

X(t)

is a Gaussian SP. Since a linear func-
tional of jointly Gaussian random variables is Gaussian (Theorem 23.6.17), the
RHS of (25.64) is Gaussian, thus making it plausible that its LHS is also Gaussian.
Before stating the main result of this section in a mathematically defensible way,
we now proceed to compute the mean and variance of (25.63). We assume that s(·)
is integrable and that

X(t)

is measurable and WSS. (Gaussianity is inessential
for the computation of the mean and variance.) The computation is very similar
to the one leading to (25.46) and (25.47). For the mean we have:
E
 ∞
−∞
X(t) s(t) dt +
n

ν=1
ανX(tν)

= E
 ∞
−∞
X(t) s(t) dt

+
n

ν=1
ανE

X(tν)

= E[X(0)]
	 ∞
−∞
s(t) dt +
n

ν=1
αν

,
(25.65)
where the ﬁrst equality follows from the linearity of expectation and where the
second equality follows from (25.46) and from the wide-sense stationarity of

X(t)

,
which implies that E[X(t)] = E[X(0)], for all t ∈R.
For the purpose of computing the variance of (25.63), we assume that

X(t)

is
centered. The result continues to hold if

X(t)

has a nonzero mean, because the
mean of

X(t)

does not inﬂuence the variance of (25.63). We begin by expanding
the variance as
Var
 ∞
−∞
X(t) s(t) dt +
n

ν=1
ανX(tν)

= Var
 ∞
−∞
X(t) s(t) dt

+ Var

n

ν=1
ανX(tν)

+ 2
n

ν=1
ανCov
 ∞
−∞
X(t) s(t) dt, X(tν)

(25.66)
25.11 Linear Functionals of Gaussian Processes
587
and by noting that, by (25.48),
Var
 ∞
−∞
X(t) s(t) dt

=
 ∞
−∞
KXX(σ) Rss(σ) dσ
(25.67)
and that, by (25.25),
Var

n

ν=1
ανX(tν)

=
n

ν=1
n

ν′=1
αναν′ KXX(tν −tν′).
(25.68)
To complete the computation of the variance of (25.63) it remains to compute the
covariance in the last term in (25.66):
E

X(tν)
 ∞
−∞
X(t) s(t) dt

= E
 ∞
−∞
X(t) X(tν) s(t) dt

=
 ∞
−∞
s(t) E[X(t) X(tν)] dt
=
 ∞
−∞
s(t) KXX(t −tν) dt.
(25.69)
Combining (25.66) with (25.67)–(25.69) we obtain
Var
 ∞
−∞
X(t) s(t) dt +
n

ν=1
ανX(tν)

=
 ∞
−∞
KXX(σ) Rss(σ) dσ
+
n

ν=1
n

ν′=1
αναν′ KXX(tν −tν′) + 2
n

ν=1
αν
 ∞
−∞
s(t) KXX(t −tν) dt.
(25.70)
We now state the main result about linear functionals of Gaussian stochastic pro-
cesses. The proof is recommended for mathematically-inclined readers only.
Proposition 25.11.1 (Linear Functional of a Stationary Gaussian SP). Consider
the setup of Proposition 25.10.1 with the additional assumption that

X(t)

is a
Gaussian SP. Additionally introduce the coeﬃcients α1, . . . , αn ∈R and the epochs
t1, . . . , tn ∈R for some n ∈N. Then there exists an event N ∈F of zero probability
such that for all ω /∈N the mapping t 	→X(ω, t) s(t) is a Lebesgue integrable
function:

the mapping t 	→X(ω, t) s(t) is in L1

,
ω /∈N,
(25.71a)
and the mapping from Ω to R
ω 	→
⎧
⎪
⎨
⎪
⎩
 ∞
−∞
X(ω, t) s(t) dt +
n

ν=1
ανX(ω, tν)
if ω /∈N,
0
otherwise
(25.71b)
is a Gaussian RV whose mean and variance are given in (25.65) and (25.70).
Moreover, there is a RV that is measurable with respect to the σ-algebra generated
by

X(t)

and that is equal to the RV in (25.71b) with probability one.10
10In the future, when we write (25.63) we shall refer to such a RV.
588
Continuous-Time Stochastic Processes
Proof. We prove this result when

X(t)

is centered. The extension to the more
general case follows by noting that adding a deterministic constant to a zero-mean
Gaussian results in a Gaussian. We also assume that s(·) is Borel measurable,
because once the theorem is established for this case it immediately also extends
to the case where s(·) is only Lebesgue measurable by noting that every Lebesgue
measurable function is equal almost everywhere to a Borel measurable function.
The existence of the event N and the fact that the mapping (25.71b) is a RV follow
from Proposition 25.10.1. We next show that the RV
Y (ω) ≜
⎧
⎪
⎨
⎪
⎩
 ∞
−∞
X(ω, t) s(t) dt +
n

ν=1
ανX(ω, tν)
if ω /∈N,
0
otherwise,
(25.72)
is Gaussian.
To that end, deﬁne for every k ∈N the function
sk(t) =

s(t)
if |t| ≤k and |s(t)| ≤
√
k,
0
otherwise.
t ∈R.
(25.73)
Note that for every ω ∈Ω
lim
k→∞X(ω, t) sk(t) = X(ω, t) s(t),
t ∈R,
and
X(ω, t) sk(t)
 ≤
X(ω, t) s(t)
,
t ∈R,
so, by the Dominated Convergence Theorem and (25.71a),
lim
k→∞
 ∞
−∞
X(ω, t) sk(t) dt =
 ∞
−∞
X(ω, t) s(t) dt,
ω /∈N.
(25.74)
Deﬁne now for every k ∈N the RV
Yk(ω) =
⎧
⎪
⎨
⎪
⎩
 ∞
−∞
X(ω, t) sk(t) dt +
n

ν=1
ανX(ω, tν)
if ω /∈N,
0
otherwise.
(25.75)
It follows from (25.74) that the sequence Y1, Y2, . . . converges almost surely to Y .
To prove that Y is Gaussian, it thus suﬃces to prove that for every k ∈N the RV
Yk is Gaussian (Theorem 19.9.1).
To prove that Yk is Gaussian, we begin by showing that it is of ﬁnite variance. To
that end, it suﬃces to show that the RV
˜Yk(ω) ≜
 ∞
−∞X(ω, t) sk(t) dt
if ω /∈N,
0
otherwise
(25.76)
25.11 Linear Functionals of Gaussian Processes
589
is of ﬁnite variance. We prove this by using the deﬁnition of sk(·) (25.73) and by
using the Cauchy-Schwarz Inequality to show that for every ω /∈N
˜Y 2
k (ω) =
	 ∞
−∞
X(ω, t) sk(t) dt

2
=
	 k
−k
X(ω, t) sk(t) dt

2
≤
 k
−k
X2(ω, t) dt
 k
−k
s2
k(t) dt
≤
	 k
−k
X2(ω, t) dt

2k2,
where the equality in the ﬁrst line follows from the deﬁnition of ˜Yk (25.76); the
equality in the second line from the deﬁnition of sk(·) (25.73); the inequality in the
third line from the Cauchy-Schwarz Inequality; and the ﬁnal inequality again by
(25.73). Since N is an event of probability zero, it follows from this inequality that
E
 ˜Y 2
k

≤4k3 KXX(0) < ∞,
thus establishing that ˜Yk, and hence also Yk, is of ﬁnite variance.
To prove that Yk is Gaussian, we shall use some results about the Hilbert space
L2(Ω, F, P) of (the equivalence classes of) the random variables that are deﬁned
over (Ω, F, P) and that have a ﬁnite second moment; see, for example, (Shiryaev,
1996, Chapter II, Section 11). Let G denote the closed linear subspace of L2(Ω, F, P)
that is generated by the random variables

X(t), t ∈R

. Thus, G contains all ﬁ-
nite linear combinations of the random variables

X(t), t ∈R

as well as the
mean-square limits of such linear combinations. Since the process

X(t), t ∈R

is
Gaussian, all such linear combinations are Gaussian. And since mean-square lim-
its of Gaussian random variables are Gaussian (Theorem 19.9.1), it follows that G
contains only random variables that have a Gaussian distribution (Shiryaev, 1996,
Chapter II, Section 13, Paragraph 6). To prove that Yk is Gaussian, it thus suﬃces
to show that it is an element of G.
To prove that Yk is an element of G, decompose Yk as
Yk = Y G
k + Y ⊥
k ,
(25.77)
where Y G
k is the projection of Yk onto G and where Y ⊥
k is consequently perpendic-
ular to every element of G and a fortiori to all the random variables

X(t), t ∈R

:
E

X(t)Y ⊥
k

= 0,
t ∈R.
(25.78)
Since Yk is of ﬁnite variance, this decomposition is possible and
E
%
Y G
k
2&
, E
%
Y ⊥
k
2&
< ∞.
(25.79)
To prove that Yk is an element of G, we shall next show that
E
%
Y ⊥
k
2&
= 0,
(25.80)
590
Continuous-Time Stochastic Processes
or, equivalently (in view of (25.77)), that
E

YkY ⊥
k

= 0.
(25.81)
To establish (25.81), we evaluate its LHS as follows:
E

YkY ⊥
k

= E
	 ∞
−∞
X(t) sk(t) dt +
n

ν=1
ανX(tν)

Y ⊥
k

= E
	  ∞
−∞
X(t) sk(t) dt

Y ⊥
k

+
n

ν=1
αν E

X(tν) Y ⊥
k




0
= E
	 ∞
−∞
X(t) sk(t) dt

Y ⊥
k

=
 ∞
−∞
E

X(t) sk(t) Y ⊥
k

dt
=
 ∞
−∞
E

X(t) Y ⊥
k




0
sk(t) dt
= 0,
where the ﬁrst equality follows from the deﬁnition of Yk (25.75); the second from
the linearity of expectation; the third from the orthogonality (25.78); the fourth by
an application of Fubini’s Theorem that we shall justify shortly; the ﬁfth because
sk(·) is a deterministic function; and the ﬁnal equality again by (25.78).
This
establishes (25.81) subject to a veriﬁcation that the conditions of Fubini’s Theorem
are satisﬁed, a veriﬁcation we conduct now. That (ω, t) 	→X(ω, t) Y ⊥
k (ω) sk(t) is
measurable follows because

X(t), t ∈R

is a measurable SP; Y ⊥
k , being a RV,
is measurable with respect to F; and because the Borel measurability of s(·) also
implies the Borel measurability of sk(·). The integrability of this function follows
from the Cauchy-Schwarz Inequality for random variables
 ∞
−∞
E
%X(t) Y ⊥
k

&
|sk(t)| dt ≤
 ∞
−∞

E[X2(t)]
4
E
%
Y ⊥
k
2&
|sk(t)| dt
≤

KXX(0)
4
E
%
Y ⊥
k
2&
2k
√
k
< ∞,
where the second inequality follows from the deﬁnition of sk(·) (25.73), and where
the third inequality follows from (25.79). This justiﬁes the use of Fubini’s Theorem
in the proof of (25.81). We have thus demonstrated that Yk is in G, and hence, like
all elements of G, is Gaussian. This concludes the proof of the Gaussianity of Yk
for every k ∈N and hence the Gaussianity of Y .
We next verify that the mean and variance of Y are as stated in the theorem. The
only part of the derivation of (25.70) that we have not yet justiﬁed is the derivation
of (25.69) and, in particular, the swapping of the expectation and integration. But
this is easily justiﬁed using Fubini’s Theorem because, by (25.18),
 ∞
−∞
E
X(tν) X(t)

|s(t)| dt ≤

KXX(0) + E[X(0)]2
∥s∥1 < ∞.
(25.82)
25.12 The Joint Distribution of Linear Functionals
591
We now conclude the proof by showing that there exists a RV that is measurable
with respect to the σ-algebra generated by

X(t)

and that is equal to Y with
probability one. The elements of G are all (equivalence classes of) random variables
that are measurable with respect to the σ-algebra generated by

X(t)

, because
they are all linear combinations of the random variables

X(t), t ∈R

or limits
thereof. Consequently, by (25.77) and (25.80), the RV Yk is equal with probability
one to a RV that is measurable with respect to the σ-algebra generated by

X(t)

.
Since Yk converges almost surely to Y , the same must be true of Y .
Proposition 25.11.1 is extremely powerful because it allows us to determine the
distribution of a linear functional of a Gaussian SP from its mean and variance.
In the next section we shall extend this result and show that any ﬁnite number of
linear functionals of a Gaussian SP are jointly Gaussian. Their joint distribution
is thus fully determined by the mean vector and the covariance matrix, which, as
we shall see, can be readily computed from the autocovariance function.
25.12
The Joint Distribution of Linear Functionals
Let us now shift our focus from the distribution of a single linear functional to the
joint distribution of a collection of such functionals. Speciﬁcally, we consider m
functionals
 ∞
−∞
X(t) sj(t) dt +
nj

ν=1
αj,νX

tj,ν

,
j = 1, . . . , m
(25.83)
of the measurable, stationary Gaussian SP

X(t)

. Here the m real-valued sig-
nals s1, . . . , sm are integrable, n1, . . . , nm are in N, and αj,ν, tj,ν are deterministic
constants for all ν ∈{1, . . . , nj}.
The main result of this section is that if

X(t)

is a Gaussian SP, then the random
variables in (25.83) are jointly Gaussian.
Theorem 25.12.1 (Linear Functionals of a Gaussian SP Are Jointly Gaussian).
The m linear functionals
 ∞
−∞
X(t) sj(t) dt +
nj

ν=1
αj,νX

tj,ν

,
j = 1, . . . , m
of a measurable, stationary, Gaussian SP

X(t)

are jointly Gaussian, whenever
m ∈N; the m functions {sj}m
j=1 are integrable functions from R to R; the inte-
gers {nj} are nonnegative; and the coeﬃcients {αj,ν} and the epochs {tj,ν} are
deterministic real numbers for all j ∈{1, . . . , m} and all ν ∈{1, . . . , nj}.
Proof. It suﬃces to show that any linear combination of these linear function-
als has a univariate Gaussian distribution (Theorem 23.6.17). This follows from
Proposition 25.11.1 and Lemma 25.10.3 because, by Lemma 25.10.3, for any choice
592
Continuous-Time Stochastic Processes
of the coeﬃcients γ1, . . . , γm ∈R the linear combination
γ1
	 ∞
−∞
X(t) s1(t) dt +
n1

ν=1
α1,νX

t1,ν

+ · · ·
+ γm
	 ∞
−∞
X(t) sm(t) dt +
nm

ν=1
αm,νX

tm,ν

has the same distribution as the linear functional
 ∞
−∞
X(t)
	 m

j=1
γj sj(t)

dt +
m

j=1
nj

ν=1
γjαj,νX

tj,ν

,
which, by Proposition 25.11.1, has a univariate Gaussian distribution.
It follows from Theorem 25.12.1 that if

X(t)

is a measurable, stationary, Gaussian
SP, then the joint distribution of the random variables in (25.83) is fully speciﬁed
by their means and their covariance matrix. If

X(t)

is centered, then by (25.65)
these random variables are centered, so their joint distribution is determined by
their covariance matrix. We next show how this covariance matrix can be computed
from the autocovariance function KXX.
To this end we assume that

X(t)

is
centered, and expand the covariance between any two such functionals as follows:
Cov
5 ∞
−∞
X(t) sj(t) dt +
nj

ν=1
αj,νX(tj,ν),
 ∞
−∞
X(t) sk(t) dt +
nk

ν′=1
αk,ν′X(tk,ν′)
6
= Cov
 ∞
−∞
X(t) sj(t) dt,
 ∞
−∞
X(t) sk(t) dt

+
nj

ν=1
αj,νCov

X(tj,ν),
 ∞
−∞
X(t) sk(t) dt

+
nk

ν′=1
αk,ν′Cov

X(tk,ν′),
 ∞
−∞
X(t) sj(t) dt

+
nj

ν=1
nk

ν′=1
αj,ναk,ν′Cov

X(tj,ν), X(tk,ν′)

,
j, k ∈{1, . . . , m}. (25.84)
The second and third terms on the RHS can be computed from the autocovariance
function KXX using (25.69). The fourth term can be computed from KXX by noting
that Cov[X(tj,ν), X(tk,ν′)] = KXX(tj,ν −tk,ν′) (Deﬁnition 25.4.4). We now evaluate
the ﬁrst term:
Cov
 ∞
−∞
X(t) sj(t) dt,
 ∞
−∞
X(t) sk(t) dt

= E
 ∞
−∞
X(t) sj(t) dt
 ∞
−∞
X(τ) sk(τ) dτ

= E
 ∞
−∞
 ∞
−∞
X(t) sj(t) X(τ) sk(τ) dt dτ

25.12 The Joint Distribution of Linear Functionals
593
=
 ∞
−∞
 ∞
−∞
E[X(t) X(τ)] sj(t) sk(τ) dt dτ
=
 ∞
−∞
 ∞
−∞
KXX(t −τ) sj(t) sk(τ) dt dτ,
(25.85)
which is the generalization of (25.55). By changing variables from (t, τ) to (t, σ),
where σ ≜t−τ, we can obtain the generalization of (25.56). Starting from (25.85)
Cov
 ∞
−∞
X(t) sj(t) dt,
 ∞
−∞
X(t) sk(t) dt

=
 ∞
−∞
 ∞
−∞
KXX(t −τ) sj(t) sk(τ) dt dτ
=
 ∞
−∞
KXX(σ)
 ∞
−∞
sj(t) sk(t −σ) dt dσ
=
 ∞
−∞
KXX(σ)
 ∞
−∞
sj(t)~sk(σ −t) dt dσ
=
 ∞
−∞
KXX(σ)

sj ⋆~sk

(σ) dσ.
(25.86)
If

X(t)

is of PSD SXX, then we can rewrite (25.86) in the frequency domain
using Proposition 6.2.4 in much the same way that we rewrote (25.47) in the form
(25.49):
Cov
 ∞
−∞
X(t) sj(t) dt,
 ∞
−∞
X(t) sk(t) dt

=
 ∞
−∞
SXX(f) ˆsj(f) ˆs∗
k(f) df, (25.87)
where we have used the fact that the FT of sj ⋆~sk is the product of the FT of sj
and the FT of ~sk, and that the FT of ~sk is f 	→ˆsk(−f), which, because sk is real,
is also given by f 	→ˆs∗
k(f).
The key second-order properties of linear functionals of measurable WSS stochastic
processes are summarized in the following theorem. Using these properties and
(25.84) we can compute the covariance matrix of the linear functionals in (25.83),
a matrix which fully speciﬁes their joint distribution whenever

X(t)

is a centered
Gaussian SP.
Theorem 25.12.2 (Covariance Properties of Linear Functionals of a WSS SP).
Let

X(t)

be a measurable WSS SP.
(i) If the real signal s is integrable, then
Var
 ∞
−∞
X(t) s(t) dt

=
 ∞
−∞
KXX(σ) Rss(σ) dσ,
(25.88)
where Rss is the self-similarity function of s. Furthermore, for every ﬁxed
epoch τ ∈R
Cov
 ∞
−∞
X(t) s(t) dt, X(τ)

=
 ∞
−∞
s(t) KXX(τ −t) dt,
τ ∈R.
(25.89)
If s1, s2 are real-valued integrable signals, then
Cov
 ∞
−∞
X(t) s1(t) dt,
 ∞
−∞
X(t) s2(t) dt

=
 ∞
−∞
KXX(σ)

s1 ⋆~s2

(σ) dσ.
(25.90)
594
Continuous-Time Stochastic Processes
(ii) If

X(t)

is of PSD SXX, then for s, s1, s2, and τ as above
Var
 ∞
−∞
X(t) s(t) dt

=
 ∞
−∞
SXX(f)
ˆs(f)
2 df,
(25.91)
Cov
 ∞
−∞
X(t) s(t) dt, X(τ)

=
 ∞
−∞
SXX(f) ˆs(f) ei2πfτ df,
(25.92)
and
Cov
 ∞
−∞
X(t) s1(t) dt,
 ∞
−∞
X(t) s2(t) dt

=
 ∞
−∞
SXX(f) ˆs1(f) ˆs∗
2(f) df.
(25.93)
Proof. Most of these claims have already been proved. Indeed, (25.88) was proved
in Proposition 25.10.1 (vi), and (25.89) was proved in Proposition 25.11.1 using
Fubini’s Theorem and (25.82).
However, (25.90) was only derived heuristically
in (25.85) and (25.86). To rigorously justify this derivation one can use Fubini’s
Theorem, or use the relation
Cov[X, Y ] = 1
2

Var[X + Y ] −Var[X] −Var[Y ]

and the result for the variance, namely, (25.88).
All the results in Part (ii) of this theorem follow from the corresponding results in
Part (i) using the deﬁnition of the PSD and Proposition 6.2.4.
25.13
Filtering WSS Processes
We next discuss the result of passing a WSS SP through a stable ﬁlter, i.e., the
convolution of a SP with a deterministic integrable function. Our main result is
that, subject to some technical conditions, the following hold:
(i) Passing a WSS SP through a stable ﬁlter produces a WSS SP.
(ii) If the input to the ﬁlter is of PSD SXX, then the output of the ﬁlter is of
PSD f 	→SXX(f) |ˆh(f)|2, where ˆh(·) is the ﬁlter’s frequency response.
(iii) If the input to the ﬁlter is a Gaussian SP, then so is the output.
We state this result in Theorem 25.13.2. But ﬁrst we must deﬁne the convolution
of a SP with an integrable deterministic signal. Our approach is to build on our
deﬁnition of linear functionals of WSS stochastic processes (Section 25.10) and to
deﬁne the convolution of

X(t)

with h(·) as the SP that maps every epoch t ∈R
to the RV
 ∞
−∞
X(σ) h(t −σ) dσ,
25.13 Filtering WSS Processes
595
h(·)

X(t)


X(t)

⋆h
Figure 25.2: Passing a SP

X(t)

through a stable ﬁlter of impulse response h(·).
If

X(t)

is measurable and WSS, then so is the output

X(t)

⋆h. If, additionally,

X(t)

is of PSD SXX, then the output is of PSD f 	→SXX(f) |ˆh(f)|2. If

X(t)

is
additionally Gaussian, then so is the output.
where the above integral is the linear functional
 ∞
−∞
X(σ) s(σ) dσ
with
s: σ 	→h(t −σ).
With this approach the key results will follow by applying Theorem 25.12.2 with
the proper substitutions.
Deﬁnition 25.13.1 (Filtering a Stochastic Process). The convolution of a mea-
surable, WSS SP

X(t)

with an integrable function h: R →R is denoted by

X(t)

⋆h
and is deﬁned as the SP that maps every t ∈R to the RV
 ∞
−∞
X(σ) h(t −σ) dσ,
(25.94)
where the stochastic integral in (25.94) is the stochastic integral that was deﬁned
in Note 25.10.2

X(t)

⋆h: (ω, t) 	→
⎧
⎨
⎩
 ∞
−∞
X(ω, σ) h(t −σ) dσ
if
 ∞
−∞
X(ω, σ) h(t −σ)
 dσ < ∞,
0
otherwise.
Theorem 25.13.2. Let

Y (t)

be the result of convolving the measurable, cen-
tered, WSS SP

X(t)

of autocovariance function KXX with the integrable function
h: R →R.
(i) The SP

Y (t)

is centered, measurable, and WSS with autocovariance func-
tion
KYY = KXX ⋆Rhh,
(25.95)
where Rhh is the self-similarity function of h (Section 11.4).
(ii) If

X(t)

is of PSD SXX, then

Y (t)

is of PSD
SYY (f) =
ˆh(f)
2 SXX(f),
f ∈R.
(25.96)
596
Continuous-Time Stochastic Processes
(iii) For every t, τ ∈R,
E

X(t) Y (t + τ)

=

KXX ⋆h

(τ),
(25.97)
where the RHS does not depend on t.11
(iv) If

X(t)

is Gaussian, then so is

Y (t)

.
Moreover, for every choice of
n, m ∈N and for every choice of the epochs t1, . . . , tn, tn+1, . . . , tn+m ∈R,
the random variables
X(t1), . . . , X(tn), Y (tn+1), . . . , Y (tn+m)
(25.98)
are jointly Gaussian.12
Proof. For ﬁxed t, τ ∈R we use Deﬁnition 25.13.1 to express Y (t) and Y (t + τ) as
Y (t) =
 ∞
−∞
X(σ) s1(σ) dσ,
(25.99)
and
Y (t + τ) =
 ∞
−∞
X(σ) s2(σ) dσ,
(25.100)
where
s1 : σ 	→h(t −σ),
(25.101)
s2 : σ 	→h(t + τ −σ).
(25.102)
We are now ready to prove Part (i).
That

Y (t)

is centered follows from the
representation of Y (t) in (25.99) & (25.101) as a linear functional of

X(t)

and
from the hypothesis that

X(t)

is centered (Proposition 25.10.1).
To establish that

Y (t)

is WSS we use the representations (25.99)–(25.102) and
Theorem 25.12.2 regarding the covariance between two linear functionals as follows.
Cov

Y (t + τ), Y (t)

= Cov
 ∞
−∞
X(σ) s2(σ) dσ,
 ∞
−∞
X(σ) s1(σ) dσ

=
 ∞
−∞
KXX(σ)

s2 ⋆~s1

(σ) dσ,
(25.103)
where the convolution can be evaluated as

s2 ⋆~s1

(σ) =
 ∞
−∞
s2(μ)~s1(σ −μ) dμ
=
 ∞
−∞
h(t + τ −μ) h(t + σ −μ) dμ
=
 ∞
−∞
h(˜μ + τ −σ) h(˜μ) d˜μ
= Rhh(τ −σ),
(25.104)
11Two stochastic processes
X(t)

and
Y (t)

are said to be jointly wide-sense stationary
if each is WSS and if E[X(t)Y (t + τ)] does not depend on t.
12That is,
X(t)

and
Y (t)

are jointly Gaussian stochastic processes.
25.13 Filtering WSS Processes
597
where ˜μ ≜t + σ −μ. Combining (25.103) with (25.104) yields
Cov

Y (t + τ), Y (t)

=

KXX ⋆Rhh

(τ),
t, τ ∈R,
(25.105)
where the RHS does not depend on t. This establishes that

Y (t)

is WSS and
proves (25.95).13
To conclude the proof of Part (i) we now show that

Y (t)

is measurable. The proof
is technical and requires background in Measure Theory. Readers are encouraged
to skip it and move on to the proof of Part (ii).
We ﬁrst note that, as in the proof of Proposition 25.10.1, it suﬃces to prove the
result for impulse response functions h that are Borel measurable; the extension
to Lebesgue measurable functions will then follow by approximating h by a Borel
measurable function that diﬀers from it on a set of Lebesgue measure zero (Rudin,
1987, Chapter 8, Lemma 1 or Chapter 2, Exercise 14) and by then applying Part (ii)
of Note 25.10.2. We hence now assume that h is Borel measurable.
We shall prove that

Y (t)

is measurable by proving that the (nonstationary)
process (ω, t) 	→Y (ω, t)/(1 + t2) is measurable. This we shall prove using Fubini’s
Theorem applied to the function from (Ω × R) × R to R deﬁned by

(ω, t), σ

	→X(ω, σ) h(t −σ)
1 + t2
,

(ω, t) ∈Ω × R, σ ∈R

.
(25.106)
This function is measurable because, by assumption,

X(t)

is measurable and
because the measurability of the function h(·) implies the measurability of the
function (t, σ) 	→h(t −σ) (as proved, for example, in (Rudin, 1987, p. 171)). We
next verify that this function is integrable.
To that end, we ﬁrst integrate its
absolute value over (ω, t) and then over σ. The integral over (ω, t) is given by
 ∞
t=−∞
E[|X(σ)|] |h(t −σ)|
1 + t2
dt ≤

KXX(0)
 ∞
t=−∞
|h(t −σ)|
1 + t2
dt,
where the inequality follows from (25.17) and from our assumption that

X(t)

is
centered. We next need to integrate the RHS over σ. Invoking Fubini’s Theorem
to exchange the order of integration over t and σ we obtain that the integral of the
absolute value of the function deﬁned in (25.106) is upper-bounded by
 ∞
σ=−∞

KXX(0)
 ∞
t=−∞
|h(t −σ)|
1 + t2
dt dσ =

KXX(0)
 ∞
t=−∞
 ∞
σ=−∞
|h(t −σ)|
1 + t2
dσ dt
=

KXX(0)
 ∞
t=−∞
∥h∥1
1 + t2 dt
= π

KXX(0) ∥h∥1
< ∞.
Having established that the function in (25.106) is measurable and integrable, we
can now use Fubini’s Theorem to deduce that its integral over σ is measurable as
13That
Y (t)

is of ﬁnite variance follows from (25.105) by setting τ = 0 and noting that
the convolution on the RHS of (25.105) is between a bounded function (KXX) and an integrable
function (Rhh) and is thus deﬁned and ﬁnite at every τ ∈R and a fortiori at τ = 0.
598
Continuous-Time Stochastic Processes
a mapping of (ω, t), i.e., that the mapping
(ω, t) 	→
 ∞
σ=−∞
X(ω, σ) h(t −σ)
1 + t2
dσ
(25.107)
is measurable. This mapping is no other than the mapping (ω, t) 	→Y (ω, t)/(1+t2),
so the latter must also be measurable, and hence also (ω, t) 	→Y (ω, t).
We next prove Part (ii) using (25.95) and Proposition 6.2.5. Because h is integrable,
its self-similarity function Rhh is integrable and of FT
ˆRhh(f) =
ˆh(f)
2,
f ∈R
(25.108)
(Section 11.4). And since, by assumption,

X(t)

is of PSD SXX, it follows that SXX
is integrable and that its IFT is KXX:
KXX(τ) =
 ∞
−∞
SXX(f) ei2πfτ df,
τ ∈R.
(25.109)
Consequently, by Proposition 6.2.5,

KXX ⋆Rhh

(τ) =
 ∞
−∞
ˆh(f)
2 SXX(f) ei2πfτ df,
τ ∈R.
Combining this with (25.95) yields
KYY (τ) =
 ∞
−∞
ˆh(f)
2 SXX(f) ei2πfτ df,
τ ∈R,
and thus establishes that the PSD of

Y (t)

is as given in (25.96).
We next turn to Part (iii). To establish (25.97) we use the representation (25.100)
& (25.102) and Theorem 25.12.2:
E

X(t) Y (t + τ)

= Cov

X(t),
 ∞
−∞
X(σ) s2(σ) dσ

=
 ∞
−∞
s2(σ) KXX(t −σ) dσ
=
 ∞
−∞
h(t + τ −σ) KXX(t −σ) dσ
=
 ∞
−∞
KXX(−μ) h(τ −μ) dμ
=
 ∞
−∞
KXX(μ) h(τ −μ) dμ
=

KXX ⋆h

(τ),
τ ∈R,
where μ ≜σ −t, and where we have used the symmetry of the autocovariance
function.
Finally, we prove Part (iv). The proof is a simple application of Theorem 25.12.1.
To prove that

Y (t)

is a Gaussian process we need to show that, for every pos-
itive integer n and for every choice of the epochs t1, . . . , tn, the random variables
25.13 Filtering WSS Processes
599
Y (t1), . . . , Y (tn) are jointly Gaussian. This follows directly from Theorem 25.12.1
because Y (tν) can be expressed as
Y (tν) =
 ∞
−∞
X(σ) h(tν −σ) dσ
=
 ∞
−∞
X(σ) sν(σ) dσ,
ν = 1, . . . , n,
where
sν : σ 	→h(tν −σ),
ν = 1, . . . , n
are all integrable.
The joint Gaussianity of the random variables in (25.98) can also be deduced from
Theorem 25.12.1. Indeed, X(tν) can be trivially expressed as the functional
X(tν) =
 ∞
−∞
X(σ) sν(σ) dσ + ανX(tν),
ν = 1, . . . , n
when sν is chosen to be the zero function and when αν is chosen as 1, and Y (tν)
can be similarly expressed as
Y (tν) =
 ∞
−∞
X(σ) sν(σ) dσ + ανX(tν),
ν = n + 1, . . . , n + m
when sν : σ 	→h(tν −σ) and αν = 0.
The mathematically astute reader may have noted that, in deﬁning the result of
passing a WSS SP through a stable ﬁlter of impulse response h, we did not preclude
the possibility that for every ω there may be some epochs t for which the mapping
σ 	→X(ω, σ) h(t −σ) is not integrable.
So far, we have only established that
for every epoch t the set Nt of ω’s for which this mapping is not integrable is of
probability zero.
We next show that if h is well-behaved in the sense that it is not only integrable
but also satisﬁes
 ∞
−∞
h2(t) (1 + t2) dt < ∞,
(25.110)
then whenever ω is outside some set N ⊂Ω of probability zero, the mapping
σ 	→X(ω, σ) h(t −σ) is integrable for all t ∈R. Thus, for ω’s outside this set of
probability zero, we can think of the response of the ﬁlter as being the convolution
of the trajectory t 	→X(ω, t) and the impulse response t 	→h(t). For such ω’s this
convolution never blows up.
To prove this note that if h satisﬁes (25.110) and if the trajectory t 	→X(ω, t)
satisﬁes
 ∞
−∞
X2(ω, t)
1 + t2
dt < ∞,
(25.111)
then the function σ 	→X(ω, σ) h(t −σ) is integrable for every t ∈R (Proposi-
tion 3.4.4). To prove the claim it thus remains to prove that outside a set of ω’s of
probability zero, all the trajectories t 	→X(ω, t) satisfy (25.111):
600
Continuous-Time Stochastic Processes
Lemma 25.13.3. Let

X(t)

be a WSS measurable SP deﬁned over the probability
space (Ω, F, P). Then
E
 ∞
−∞
X2(t)
1 + t2 dt

< ∞,
(25.112)
and the set
1
ω ∈Ω :
 ∞
−∞
X2(ω, t)
1 + t2
dt < ∞
2
(25.113)
is an event of probability one.
Proof. Since

X(t)

is measurable, the mapping
(ω, t) 	→X2(ω, t)
1 + t2
(25.114)
is nonnegative and measurable. By Fubini’s Theorem it follows that if we deﬁne
W(ω) ≜
 ∞
−∞
X2(ω, t)
1 + t2
dt,
ω ∈Ω,
(25.115)
then W is a nonnegative RV taking values in the interval [0, ∞]. Consequently, the
set {ω ∈Ω : W(ω) < ∞} is measurable. Moreover, by Fubini’s Theorem,
E[W] =
 ∞
−∞
E
X2(t)
1 + t2

dt
=
 ∞
−∞
E

X2(t)

1 + t2
dt
= E

X2(0)
  ∞
−∞
1
1 + t2 dt
= π E

X2(0)

< ∞.
Thus, W is a RV taking values in the interval [0, ∞] and having ﬁnite expectation,
so the event {ω ∈Ω : W(ω) < ∞} must be of probability one.
25.14
The PSD Revisited
Theorem 25.13.2 describes the PSD of the output of a stable ﬁlter that is fed a
WSS SP

X(t)

. By integrating this PSD, we obtain the value at the origin of the
autocovariance function of the ﬁlter’s output (see (25.31)). Since the latter is the
power of the ﬁlter’s output (Corollary 25.9.3), we have:
Theorem 25.14.1 (Wiener-Khinchin). If a measurable, centered, WSS SP

X(t)

of autocovariance function KXX is passed through a stable ﬁlter of impulse response
h: R →R, then the average power of the ﬁlter’s output is given by
Power of X ⋆h = ⟨KXX, Rhh⟩.
(25.116)
25.14 The PSD Revisited
601
If, additionally,

X(t)

is of PSD SXX, then this power is given by
Power of X ⋆h =
 ∞
−∞
SXX(f)
ˆh(f)
2 df.
(25.117)
Proof. To prove (25.116), we note that by (25.95) the autocovariance function of
the ﬁltered process is KXX ⋆Rhh, which evaluates at the origin to (25.116). The
result thus follows from Proposition 25.9.2, which shows that the power in the
ﬁltered process is given by its autocovariance function evaluated at the origin.
To prove (25.117), we note that KXX is the IFT of SXX and that, by (11.35),
ˆRhh(f) = |ˆh(f)|2, so the RHS of (25.117) is equal to the RHS of (25.116) by
Proposition 6.2.4.
We next show that for WSS stochastic processes, the operational PSD (Deﬁni-
tion 15.3.1) and the PSD (Deﬁnition 25.7.2) are equivalent. That is, a WSS SP
has an operational PSD if, and only if, it has a PSD, and if the two exist, then they
are equal (outside a set of frequencies of Lebesgue measure zero). Before stating
this as a theorem, we present a lemma that will be needed in the proof. It is very
much in the spirit of Lemma 15.3.5.
Lemma 25.14.2. Let g: R →R be a symmetric continuous function satisfying the
condition that for every integrable real signal h: R →R
 ∞
−∞
g(t) Rhh(t) dt = 0.
(25.118)
Then g is the all-zero function.
Proof. For every a > 0 consider the function
h(t) =
1
√a I{|t| ≤a/2},
t ∈R
whose self-similarity function is
Rhh(t) =
	
1 −|t|
a

I{|t| ≤a},
t ∈R.
(25.119)
Since h is integrable, it follows from (25.118) that
0 =
 ∞
−∞
g(t) Rhh(t) dt
= 2
 ∞
0
g(t) Rhh(t) dt
= 2
 a
0
g(t)

1 −t
a

dt,
a > 0,
(25.120)
where the second equality follows from the hypothesis that g(·) is symmetric and
from the symmetry of Rhh, and where the third equality follows from (25.119).
Deﬁning
G(t) =
 t
0
g(ξ) dξ,
t ≥0,
(25.121)
602
Continuous-Time Stochastic Processes
and using integration by parts, we obtain from (25.120) that
0 = G(ξ)

1 −ξ
a

a
0
+ 1
a
 a
0
G(ξ) dξ,
a > 0,
from which we obtain
aG(0) =
 a
0
G(ξ) dξ,
a > 0.
Diﬀerentiating with respect to a yields
G(0) = G(a),
a ≥0,
which combines with (25.121) to yield
 a
0
g(t) dt = 0,
a ≥0.
(25.122)
Diﬀerentiating with respect to a and using the continuity of g (Rudin, 1976, Chap-
ter 6, Theorem 6.20) yields that g(a) is zero for all a ≥0 and hence, by its
symmetry, for all a ∈R.
Theorem 25.14.3 (The PSD and Operational PSD of a WSS SP). Let

X(t)

be a measurable, centered, WSS SP of a continuous autocovariance function KXX.
Let S(·) be a nonnegative, symmetric, integrable function. Then the following two
conditions are equivalent:
(a) KXX is the Inverse Fourier Transform of S(·).
(b) For every integrable h: R →R, the power in X ⋆h is given by
Power of X ⋆h =
 ∞
−∞
S(f) |ˆh(f)|2 df.
(25.123)
Proof. That (a) implies (b) follows from the Wiener-Khinchin Theorem because
(a) implies that

X(t)

is of PSD S(·). It remains to prove that (b) implies (a). We
thus assume that Condition (b) is satisﬁed and proceed to prove that KXX must
then be equal to the IFT of S(·). By Theorem 25.14.1, the power in X ⋆h is given
by (25.116). Consequently, Condition (b) implies that
 ∞
−∞
S(f) |ˆh(f)|2 df =
 ∞
−∞
KXX(τ) Rhh(τ) dτ,
(25.124)
for every integrable h: R →R.
If h is integrable, then the FT of Rhh is the mapping f 	→|ˆh(f)|2 (see (11.35)). If,
in addition, h is a real signal, then Rhh is a symmetric function, and its IFT is thus
identical to its FT (Proposition 6.2.3 (ii)). Thus, if h is real and integrable, then
the IFT of Rhh is the mapping f 	→|ˆh(f)|2. (Using the dummy variable f for the
IFT is unusual but legitimate.) Consequently, by Proposition 6.2.4 (applied with
the substitution of S(·) for x and of Rhh for g),
 ∞
−∞
S(f) |ˆh(f)|2 df =
 ∞
−∞
ˆS(τ) Rhh(τ) dτ.
(25.125)
25.15 White Gaussian Noise
603
By (25.124) & (25.125) and by the symmetry of S(·) (which implies that ˆS = ˇS)
we obtain that
 ∞
−∞
ˇS(τ) −KXX(τ)

Rhh(τ) dτ = 0,
h ∈L1.
(25.126)
It thus follows from Lemma 15.3.5 that the mapping τ 	→ˇS(τ) −KXX(τ) is the
all-zero function, and Condition (a) is established.
25.15
White Gaussian Noise
The most important continuous-time SP in Digital Communications is white
Gaussian noise (WGN), which is often used to model the additive noise in com-
munication systems. In this section we deﬁne this process and study its key proper-
ties. Our deﬁnition diﬀers from the one in most textbooks, most notably in that we
deﬁne WGN only with respect to some given bandwidth W. We give our reasons
and comment on the implications in Section 25.15.3 after providing our deﬁnition
and deriving the key results.
25.15.1
Deﬁnition and Main Properties
The parameters deﬁning WGN are the bandwidth W with respect to which the
process is white and the double-sided power spectral density N0/2.
Deﬁnition 25.15.1 (White Gaussian Noise). We say that

N(t)

is white Gaus-
sian noise of double-sided power spectral density N0/2 with respect to
the bandwidth W if

N(t)

is a measurable, stationary, centered, Gaussian SP
that has a PSD SNN satisfying
SNN(f) = N0
2 ,
f ∈[−W, W ].
(25.127)
An example of the PSD of white Gaussian noise of double-sided PSD N0/2 with
respect to the bandwidth W is depicted in Figure 25.3. Note that our deﬁnition
of WGN only speciﬁes the PSD for frequencies f satisfying |f| ≤W. We leave the
value of the PSD at other frequencies unspeciﬁed. But the PSD should, of course,
be a valid PSD, i.e., it must be nonnegative, symmetric, and integrable (Deﬁni-
tion 25.7.2). Recall also that by Proposition 25.7.3 every nonnegative, symmetric,
integrable function is the PSD of some measurable stationary Gaussian SP.14
The following proposition summarizes the key properties of WGN. The reader is
encouraged to recall the deﬁnition of an integrable function that is bandlimited to
W Hz (Deﬁnition 6.4.9); the deﬁnition of the inner product between two energy-
limited real signals (3.1); the deﬁnition of ∥s∥2 as

⟨s, s⟩; and the deﬁnition of
orthonormality of the functions φ1, . . . , φm (Deﬁnition 4.6.1).
14As we have noted in the paragraph preceding Deﬁnition 25.9.1, Proposition 25.7.3 can be
strengthened to also guarantee measurability.
Every nonnegative, symmetric, and integrable
function is the PSD of some measurable, stationary, and Gaussian SP whose autocovariance
function is continuous.
604
Continuous-Time Stochastic Processes
−W
W
N0/2
f
SNN(f)
Figure 25.3: The PSD of a SP

N(t)

which is of double-sided power spectral
density N0/2 with respect to the bandwidth W.
Proposition 25.15.2 (Key Properties of White Gaussian Noise). Let

N(t)

be
WGN of double-sided PSD N0/2 with respect to the bandwidth W.
(i) If s is any integrable function that is bandlimited to W Hz, then
 ∞
−∞
N(t) s(t) dt ∼N
	
0, N0
2 ∥s∥2
2

.
(ii) If s1, . . . , sm are integrable functions that are bandlimited to W Hz, then the
m random variables
 ∞
−∞
N(t) s1(t) dt, . . . ,
 ∞
−∞
N(t) sm(t) dt
are jointly Gaussian centered random variables of covariance matrix
N0
2
⎛
⎜
⎜
⎜
⎝
⟨s1, s1⟩
⟨s1, s2⟩
· · ·
⟨s1, sm⟩
⟨s2, s1⟩
⟨s2, s2⟩
· · ·
⟨s2, sm⟩
...
...
...
...
⟨sm, s1⟩
⟨sm, s2⟩
· · ·
⟨sm, sm⟩
⎞
⎟
⎟
⎟
⎠.
(iii) If φ1, . . . , φm are integrable functions that are bandlimited to W Hz and are
orthonormal, then the random variables
 ∞
−∞
N(t) φ1(t) dt, . . . ,
 ∞
−∞
N(t) φm(t) dt
are IID N(0, N0/2).
(iv) If s is any integrable function that is bandlimited to W Hz, and if KNN is the
autocovariance function of

N(t)

, then
KNN ⋆s = N0
2 s.
(25.128)
25.15 White Gaussian Noise
605
(v) If s is an integrable function that is bandlimited to W Hz, then for every
epoch t ∈R
Cov
 ∞
−∞
N(σ) s(σ) dσ, N(t)

= N0
2 s(t).
(25.129)
Proof. Parts (i) and (iii) are special cases of Part (ii), so it suﬃces to prove
Parts (ii), (iv), and (v). We begin with Part (ii), which, as we next show, fol-
lows from Theorem 25.12.1.
We ﬁrst note that since {sj} are assumed to be
integrable and bandlimited to W Hz, and since Note 6.4.12 guarantees that every
bandlimited integrable signal is also of ﬁnite energy, it follows that the functions
{sj} are energy-limited and the inner products ⟨sj, sk⟩are well-deﬁned. To ap-
ply Theorem 25.12.1 it remains to calculate the covariance between the diﬀerent
functionals. By (25.93),
Cov
 ∞
−∞
N(t) sj(t) dt,
 ∞
−∞
N(t) sk(t) dt

=
 ∞
−∞
SNN(f) ˆsj(f) ˆs∗
k(f) df
=
 W
−W
SNN(f) ˆsj(f) ˆs∗
k(f) df
= N0
2
 W
−W
ˆsj(f) ˆs∗
k(f) df
= N0
2 ⟨sj, sk⟩,
j, k ∈{1, . . . , m},
where the second equality follows because sj and sk are bandlimited to W Hz; the
third from (25.127); and the ﬁnal equality from Parseval’s Theorem.
To prove Part (iv), we start with the deﬁnition of the convolution and compute

KNN ⋆s

(t) =
 ∞
−∞
s(τ) KNN(t −τ) dτ
=
 ∞
−∞
s(τ)
 ∞
−∞
SNN(f) ei2πf(t−τ) df dτ
=
 ∞
−∞
SNN(f) ˆs(f) ei2πft df
=
 W
−W
SNN(f) ˆs(f) ei2πft df
= N0
2
 W
−W
ˆs(f) ei2πft df
= N0
2 s(t),
t ∈R,
where the second equality follows from the deﬁnition of the PSD of

N(t)

(Deﬁni-
tion 25.7.2); the third by Proposition 6.2.5; the fourth because s is, by assumption,
bandlimited to W Hz (Proposition 6.4.10, cf. (c)); the ﬁfth from our assumption
that

N(t)

is white with respect to the bandwidth W (25.127); and the ﬁnal
equality from Proposition 6.4.10 (cf. (b)).
606
Continuous-Time Stochastic Processes
Part (v) now follows from (25.89) and Part (iv). Alternatively, it can be proved
using (25.92) and (25.127) as follows:
Cov
 ∞
−∞
N(σ) s(σ) dσ, N(t)

=
 ∞
−∞
SNN(f) ˆs(f) ei2πft df
=
 W
−W
SNN(f) ˆs(f) ei2πft df
= N0
2
 W
−W
ˆs(f) ei2πft df
= N0
2 s(t),
t ∈R,
where the ﬁrst equality follows from (25.92); the second because s is bandlimited
to W Hz (Proposition 6.4.10, cf. (c)); the third from (25.127); and the last from
Proposition 6.4.10 (cf. (b)).
25.15.2
Projecting White Gaussian Noise
A natural way to deﬁne the projection of a stochastic process is pathwise. Ignoring
integrability issues, we would deﬁne the projection as follows: Suppose we are
given orthonormal deterministic signals φ1, . . . , φd and a SP X that we would like
to project onto span(φ1, . . . , φd). For each ω ∈Ω we could consider the sample-
path t 	→X(ω, t) and deﬁne its projection, like we did for deterministic signals (see
Deﬁnition 4.6.6 and Note 4.6.7), as the signal
t 	→
d

ℓ=1
	 ∞
−∞
X(ω, τ) φℓ(τ) dτ

φℓ(t),
(25.130a)
i.e., as
t 	→
d

ℓ=1
⟨X, φℓ⟩(ω) φℓ(t).
(25.130b)
Diﬀerent sample-paths will, of course, have diﬀerent projections, thus resulting in
the projection being a stochastic process
(ω, t) 	→
d

ℓ=1
⟨X, φℓ⟩(ω) φℓ(t).
(25.130c)
For this to make mathematical sense we need to make sure that the stochastic
integrals in (25.130a) exist (with probability one). We shall therefore limit ourselves
to scenarios where φ1, . . . , φd are integrable so that the existence of the integrals
be guaranteed by Proposition 25.10.1. And, of course, for orthonormality to make
sense we shall also require that they be energy-limited. (The latter condition is
superﬂuous when they are bandlimited; see Note 6.4.12.)
25.15 White Gaussian Noise
607
Deﬁnition 25.15.3 (Projecting a Stochastic Process). If X is a measurable WSS
SP, and if φ1, . . . , φd ∈L1 ∩L2 are orthonormal, then the projection of X onto
span(φ1, . . . , φd) is the SP
(ω, t) 	→
d

ℓ=1
⟨X, φℓ⟩(ω) φℓ(t),
which we write more succinctly as
d

ℓ=1
⟨X, φℓ⟩φℓ.
(25.131)
We refer to
X −
d

ℓ=1
⟨X, φℓ⟩φℓ
(25.132)
as the part of X that is orthogonal to span(φ1, . . . , φd).
Note 25.15.4. If ˜φ1, . . . , ˜φd ∈L1 ∩L2 are orthonormal and have the same linear
span as φ1, . . . , φd, then one can show using the “almost” linearity of stochastic
integration (Lemma 25.10.3) that outside a set of probability zero, the stochastic
processes d
ℓ=1⟨X, φℓ⟩φℓand d
ℓ=1⟨X, ˜φℓ⟩˜φℓare identical.
Hence there is no
harm in referring to (25.131) as “the” projection.
Note 25.15.5. Even if X is stationary, its projection onto span(φ1, . . . , φd) is
typically not stationary. Nor is its part that is orthogonal to span(φ1, . . . , φd).
The following result on the projection of WGN is of paramount importance in dig-
ital communications. It, and its restatement in Theorem 25.15.7, are fundamental
to the design of optimal receivers for communication in the presence of WGN.
Theorem 25.15.6 (Projecting White Gaussian Noise). Let N be WGN of double-
sided PSD N0/2 with respect to the bandwidth W, and let φ1, . . . , φd be orthonormal
integrable signals that are bandlimited to W Hz. Then
d

ℓ=1
⟨N, φℓ⟩φℓ
and
N −
d

ℓ=1
⟨N, φℓ⟩φℓ
are independent Gaussian stochastic processes.
Proof. Denote the projection of N onto span(φ1, . . . , φd) by N1,
N1 =
d

ℓ=1
⟨N, φℓ⟩φℓ,
(25.133a)
and the part of N that is orthogonal to span(φ1, . . . , φd) by N2,
N2 = N −
d

ℓ=1
⟨N, φℓ⟩φℓ.
(25.133b)
608
Continuous-Time Stochastic Processes
We need to show that for every n ∈N and epochs t1, . . . , tn ∈R the random vectors

N1(t1), . . . , N1(tn)
T
and

N2(t1), . . . , N2(tn)
T
(25.134)
are independent Gaussian vectors. To that end, we ﬁrst note that the 2n-vector

N1(t1), . . . , N1(tn), N2(t1), . . . , N2(tn)
T
is Gaussian because its components are linear functionals of the Gaussian SP N
(Theorem 25.12.1). This establishes that the random vectors in (25.134) are Gaus-
sian (Corollary 23.6.5) and are, in fact, jointly Gaussian (Deﬁnition 23.7.1). It
also establishes that to prove their independence it suﬃces to prove that they are
uncorrelated (Proposition 23.7.3). To conclude the proof it thus remains to show
that
Cov

N1(tν), N2(tν′)

= 0,
ν, ν′ ∈{1, . . . , n}.
(25.135)
Since both N1(tν) and N2(tν′) are linear functionals of the centered SP N, they
are centered (Proposition 25.10.1), and their covariance is thus E[N1(tν) N2(tν′)].
To show (25.135) we thus need to show that
E

N1(tν) N2(tν′)

= 0,
ν, ν′ ∈{1, . . . , n}
(25.136)
or, more generally, that
E

N1(t) N2(t′)

= 0,
t, t′ ∈R.
(25.137)
This we proceed to do next:
E

N1(t) N2(t′)

= E
	
d

ℓ=1
⟨N, φℓ⟩φℓ(t)

	
N(t′) −
d

ℓ′=1
⟨N, φℓ′⟩φℓ′(t′)


=
d

ℓ=1
φℓ(t) E

⟨N, φℓ⟩N(t′)




= N0
2 φℓ(t′)
−
d

ℓ=1
d

ℓ′=1
φℓ(t) φℓ′(t′) E

⟨N, φℓ⟩⟨N, φℓ′⟩




= N0
2
I{ℓ=ℓ′}
= N0
2
d

ℓ=1
φℓ(t) φℓ(t′) −N0
2
d

ℓ=1
φℓ(t) φℓ(t′)
= 0,
t, t′ ∈R,
where the ﬁrst equality follows from the deﬁnitions of N1 and N2 (25.133); the
second by swapping expectations and summations; and the third by the basic
properties of WGN (Proposition 25.15.2).
Theorem 25.15.6 has far-reaching consequences in digital communications. But
before we can reap its beneﬁts, we must restate it slightly diﬀerently. To motivate
the restatement, let us write N as the sum of its projection onto span(φ1, . . . , φd)
and the part of N that is orthogonal to span(φ1, . . . , φd):
N =
d

ℓ=1
⟨N, φℓ⟩φℓ+
	
N −
d

ℓ=1
⟨N, φℓ⟩φℓ

.
25.15 White Gaussian Noise
609
In this representation, N is the sum of two independent stochastic processes,
each of which is computed from N: the ﬁrst is computed by projecting N onto
span(φ1, . . . , φd), and the second by computing the part of N that is orthogonal
to this projection. To get a sense of the implications of this decomposition, let
us imagine a simpler scenario where stochastic processes are replaced by random
variables and projections by functions. Thus, imagine that we are told that a RV
X is such that X = g(X) + h(X), with g(X) and h(X) being independent. (In
this cartoon X plays the role of N; the mapping g: R →R plays the role of the
projection; and h(X) plays the role of the part of N that is orthogonal to the
subspace.) We could then infer that if we generate U independently of g(X) ac-
cording to the distribution of h(X), then the sum g(X) + U would have the same
law as X. How could we generate such a U? One way would be to compute the
distribution of h(X) (from that of X) and to then generate U independently of X
(and hence also independently of g(X)) according to this distribution. But there
is another way: we could generate some X′ of the same law as X but independent
of it, and then set U to equal h(X′). Since X′ is independent of X, so is h(X′),
and, because X′ is of the same law as X, it follows that h(X′) has the same law as
h(X). Adopting this approach we could conclude that g(X) + h(X′) has the same
law as X whenever X′ is generated independently of X but with same law.
Going back to stochastic processes and our WGN, we would expect the same: if
N′ is generated independently of N but with the same FDDs, then
d

ℓ=1
⟨N, φℓ⟩φℓ+
	
N′ −
d

ℓ=1
⟨N′, φℓ⟩φℓ

should have the same FDDs as N. This is the restatement of the theorem that we
are after:
Theorem 25.15.7 (Simulating WGN of a Given Projection). Let the SP N be
WGN of double-sided PSD N0/2 with respect to the bandwidth W. Let the SP N′
be of the same law as N but independent of it. Let φ1, . . . , φd be orthonormal
integrable signals that are bandlimited to W Hz. Then the SP
d

ℓ=1
⟨N, φℓ⟩φℓ+ N′ −
d

ℓ=1
⟨N′, φℓ⟩φℓ
is a measurable SP of the same FDDs as N.
The heuristic argument we gave above nearly proves that this theorem follows
directly from Theorem 25.15.6.
Some of the missing subtle details are related
to the deﬁnition of independence for stochastic processes (Deﬁnition 25.2.3). For
example, it is prima facie not clear from this deﬁnition that linear functionals of
independent stochastic processes are independent random variables. (They are; see
Proposition 25.15.8 ahead.) And it is therefore not obvious that if N′ and N are
independent, then so are
d

ℓ=1
⟨N, φℓ⟩φℓ
and
N′ −
d

ℓ=1
⟨N′, φℓ⟩φℓ.
610
Continuous-Time Stochastic Processes
(They are.) The rest of this subsection is dedicated to a rigorous proof of this
theorem.
We begin with the basic result on independent stochastic processes that we need
in order to prove Theorem 25.15.7. The proof is technical, and readers without
some background in Measure Theory should probably skip it.
Proposition 25.15.8 (Linear Functionals of Independent SPs are Independent).
Let

X(t)

and

Y (t)

be independent WSS measurable stochastic processes. Let
sx and sy be integrable functions, and let α1, . . . , αn, β1, . . . , βn and t1, . . . , tn be
real. Then
 ∞
−∞
X(t) sx(t) dt +
n

ν=1
ανX(tν)
and
 ∞
−∞
Y (t) sy(t) dt +
n

ν=1
βνY (tν)
are independent random variables.
More generally, a random vector whose components are such linear functionals of

X(t)

is independent of any random vector whose components are such linear
functionals of

Y (t)

.
Proof. This result is reminiscent of the result that if the random variables U
and V are independent then so are g(U) and h(V ) for any (Borel measurable)
functions g, h: R →R. Here things are a bit trickier because we are dealing with
stochastic processes, which are uncountable collections of random variables. The
proof therefore requires some Measure Theory.
The independence of the stochastic processes implies the independence of the σ-
algebras generated by their cylindrical sets (Billingsley, 1995, Chapter 1, Section 4,
Theorem 4.2). By Proposition 25.10.1 (iii) the above linear functionals are mea-
surable with respect to the respective σ-algebras. The result now follows by noting
that if a RV U is measurable with respect to a σ-algebra Fu, if a RV V is measurable
with respect to a σ-algebra Fv, and if the σ-algebras Fu and Fv are independent,
then U and V are independent (Billingsley, 1995, Chapter 4, Section 20).
Proof of Theorem 25.15.7. Denote the autocovariance function of N by KNN.
Since N′ has the same FDDs as N, its autocovariance function is also KNN. Denote
the projection of N onto span(φ1, . . . , φd) by N1,
N1 =
d

ℓ=1
⟨N, φℓ⟩φℓ,
(25.138)
and the part of N′ that is orthogonal to span(φ1, . . . , φd) by N′
2,
N′
2 = N′ −
d

ℓ=1
⟨N′, φℓ⟩φℓ.
(25.139)
Because the FDDs of a centered stationary Gaussian SP are fully determined by its
autocovariance function (Proposition 25.5.1 (ii)), to prove the theorem it suﬃces
to establish that, like N, the SP N1 + N′
2 is a measurable Gaussian stochastic
25.15 White Gaussian Noise
611
process of autocovariance function KNN. We ﬁrst show that N1 + N′
2 is Gaussian,
by establishing that N1 and N′
2 are independent Gaussian stochastic processes,
i.e., by arguing that for every n ∈N and epochs t1, . . . , tn ∈R the random vectors

N1(t1), . . . , N1(tn)
T
and

N ′
2(t1), . . . , N ′
2(tn)
T
(25.140)
are independent Gaussian vectors.
Indeed, independence follows from Proposi-
tion 25.15.8 (because the components of the ﬁrst vector are linear functionals of
N, whereas those of the second are linear functionals of N′), and Gaussianity fol-
lows from Theorem 25.12.1 (because the components of each of the vectors are
linear functionals of a Gaussian SP). Having established that N1 + N′
2 is a Gaus-
sian SP, it only remains to show that it is stationary with autocovariance function
KNN.
Denote N1 + N′
2 by ˜N. Since N1 and N′
2 are independent and centered, it follows
that for all t, τ ∈R
E
 ˜N(t + τ) ˜N(t)

= E

N1(t + τ) N1(t)

+ E

N ′
2(t + τ) N ′
2(t)

= E
	 d

ℓ=1
⟨N, φℓ⟩φℓ(t + τ)

	
d

ℓ′=1
⟨N, φℓ′⟩φℓ′(t)


+ E
	
N ′(t + τ) −
d

ℓ=1
⟨N′, φℓ⟩φℓ(t + τ)

	
N ′(t) −
d

ℓ′=1
⟨N′, φℓ′⟩φℓ′(t)


.
(25.141)
We next compute the above two terms separately, starting with the ﬁrst:
E
	 d

ℓ=1
⟨N, φℓ⟩φℓ(t + τ)

	
d

ℓ′=1
⟨N, φℓ′⟩φℓ′(t)


=
d

ℓ=1
d

ℓ′=1
E
%
⟨N, φℓ⟩φℓ(t + τ) ⟨N, φℓ′⟩φℓ′(t)
&
=
d

ℓ=1
d

ℓ′=1
E
%
⟨N, φℓ⟩⟨N, φℓ′⟩
&



= N0
2
I{ℓ=ℓ′}
φℓ(t + τ) φℓ′(t)
= N0
2
d

ℓ=1
φℓ(t + τ) φℓ(t),
t, τ ∈R,
(25.142)
where the last equality follows from our assumptions about N and the orthonor-
mality of (φ1, . . . , φd) (Proposition 25.15.2 (iii)).
As to the second term on the RHS of (25.141)
E
	
N ′(t + τ) −
d

ℓ=1
⟨N′, φℓ⟩φℓ(t + τ)

	
N ′(t) −
d

ℓ′=1
⟨N′, φℓ′⟩φℓ′(t)


612
Continuous-Time Stochastic Processes
= E

N ′(t + τ)N ′(t)

+ E
 	 d

ℓ=1
⟨N′, φℓ⟩φℓ(t + τ)

	
d

ℓ′=1
⟨N′, φℓ′⟩φℓ′(t)


−E

N ′(t + τ)
d

ℓ′=1
⟨N′, φℓ′⟩φℓ′(t)

−E

N ′(t)
d

ℓ=1
⟨N′, φℓ⟩φℓ(t + τ)

(25.143)
= KNN(τ) + N0
2
d

ℓ=1
φℓ(t + τ) φℓ(t)
−
d

ℓ′=1
E
%
N ′(t + τ) ⟨N′, φℓ′⟩
&



= N0
2 φℓ′(t+τ)
φℓ′(t) −
d

ℓ=1
E
%
N ′(t) ⟨N′, φℓ⟩
&



= N0
2 φℓ(t)
φℓ(t + τ) (25.144)
= KNN(τ) −N0
2
d

ℓ=1
φℓ(t + τ) φℓ(t),
t, τ ∈R.
(25.145)
Here the ﬁrst term on the RHS of (25.143) is computed using the deﬁnition of
the autocovariance function (Deﬁnition 25.4.4), and the second term is computed
as in (25.142) but with N replaced by N′. The last two terms in (25.144) are
computed using Proposition 25.15.2 (v).
It follows from (25.141), (25.142), and (25.145) that the autocovariance function
of ˜N is identical to that of N.
25.15.3
Other Deﬁnitions
As we noted earlier, our deﬁnition of WGN is diﬀerent from the one given in most
textbooks on Digital Communications. The key diﬀerence is that we deﬁne white-
ness with respect to a certain bandwidth W, whereas most textbooks do not add this
qualiﬁer. Thus, while we require that the PSD SNN(f) be equal to N0/2 only for
frequencies f satisfying |f| ≤W (leaving SNN(f) unspeciﬁed at other frequencies),
other textbooks require that SNN(f) be equal to N0/2 for all frequencies f ∈R.
With our deﬁnition of WGN we can only prove that (25.128) holds for integrable
signals that are bandlimited to W Hz, whereas with the other textbooks’ deﬁnition
one could presumably derive this relationship for all integrable functions.
We prefer our deﬁnition because there does not exist a Gaussian SP

N(t)

whose
PSD is equal to N0/2 at all frequencies. Indeed, the function of frequency that is
equal to N0/2 at all frequencies is not integrable and therefore does not qualify
as a PSD (Deﬁnition 25.7.2). Were such a PSD to exist, we would obtain from
(25.31) that such a process would have inﬁnite variance and thus be neither WSS
(Deﬁnition 25.4.2) nor Gaussian (Note 25.3.2).
Requiring that (25.128) hold for all integrable (continuous) signals would require
that KNN be given by the product of N0/2 and Dirac’s delta, which opens a whole
can of worms. Nevertheless, the reader should be aware that in some books WGN
is deﬁned as a centered, stationary Gaussian noise whose autocovariance function
is given by Dirac’s Delta scaled by N0/2 or, equivalently, whose PSD is equal to
N0/2 at all frequencies.
25.16 Exercises
613
25.15.4
White Gaussian Noise in Passband
Deﬁnition 25.15.9 (White Gaussian Noise in Passband). We say that

N(t)

is
white Gaussian noise of double-sided power spectral density N0/2 with
respect to the bandwidth W around the carrier frequency fc if

N(t)

is a
centered, measurable, stationary, Gaussian stochastic process that has a PSD SNN
satisfying
SNN(f) = N0
2 ,
|f| −fc
 ≤W
2 ,
(25.146)
and if
fc > W/2.
(25.147)
Note 25.15.10. For WGN with respect to the bandwidth W around the carrier
frequency fc, all the claims of Proposition 25.15.2 hold provided that we replace the
requirement that the functions s, {sj}, and {φj} be integrable functions that are
bandlimited to W Hz with the requirement that they be integrable functions that
are bandlimited to W Hz around the carrier frequency fc. Likewise for Theorems
25.15.6 and 25.15.7.
25.16
Exercises
Exercise 25.1 (Constructing a SP from a RV). Let W be a standard Gaussian RV. Deﬁne
the continuous-time SP

X(t)

by
X(t) = e−|t| W,
t ∈R.
(i) Is

X(t)

a stationary SP?
(ii) Is

X(t)

a Gaussian SP?
Exercise 25.2 (A SP from Jointly Gaussian Random Variables). Let X1, . . . , Xd be jointly
Gaussian, and let the real signals g1, . . . , gd be deterministic. Prove that the SP
X(t) =
d

j=1
Xj gj(t),
t ∈R
is Gaussian.
Exercise 25.3 (The Sum of Independent Gaussian Stochastic Processes). Prove that the
sum of two independent Gaussian stochastic processes is a Gaussian SP.
Exercise 25.4 (Delaying and Adding). Let

X(t)

be a stationary Gaussian SP of mean μx
and autocovariance function KXX. Deﬁne
Y (t) = X(t) + X(t −tD),
t ∈R,
where tD ∈R is deterministic.
(i) Is

Y (t)

a Gaussian SP?
(ii) Compute the mean and the autocovariance function of

Y (t)

.
614
Continuous-Time Stochastic Processes
(iii) Is

Y (t)

stationary?
Exercise 25.5 (Random Variables and Stochastic Processes). Let the random variables X
and Y be IID N

0, σ2
, and let
Z(t) = X cos(2πt) + Y sin(2πt),
t ∈R.
(i) Is Z(0.2) Gaussian?
(ii) Is

Z(t)

a Gaussian SP?
(iii) Is it stationary?
Exercise 25.6 (Sampling a SP). Let

X(t), t ∈R

be a WSS continuous-time SP of
PSD SXX. Let T be ﬁxed, and deﬁne the discrete-time SP

Yν, ν ∈Z

by
Yν = X(νT),
ν ∈Z.
Express the PSD of

Yν, ν ∈Z

in terms of SXX and T.
Exercise 25.7 (Sample-and-Hold). A centered and bounded WSS SP

X(t), t ∈R

of
autocovariance function KXX(·) is fed to a sample-and-hold circuit that produces the SP
Y (t) = X
	3 t
T
4
T

,
t ∈R,
where T > 0 is the holding duration. Express the operational PSD of

Y (t)

in terms of
KXX(·) and T as follows:
(i) Show that the discrete-time SP

Xℓ, ℓ∈Z

deﬁned for every ℓ∈Z as Xℓ= X(ℓT)
is centered and WSS, and express its autocovariance function in terms of KXX(·).
(ii) Express

Y (t)

as a PAM signal with real symbols

Xℓ, ℓ∈Z

, baud period T, and
appropriate pulse shape.
(iii) Compute the operational PSD of this PAM signal.
Exercise 25.8 (More on Covariances). Let

X(t), t ∈R

and

Y (t), t ∈R

be centered
WSS stochastic processes of autocovariance functions KXX and KYY . Prove that
E
$
X(t + τ) Y (t)
% ≤

KXX(0) KYY (0),
t, τ ∈R.
Exercise 25.9 (The Product of WSS Stochastic Processes). Show that the product of
two independent WSS stochastic processes is WSS. Express the PSD of the product in
terms of the PSDs of the individual processes and their means.
Exercise 25.10 (Stochastic Processes through Nonlinearities).
(i) Let

X(t)

be a stationary SP and let
Y (t) = g(X(t)),
t ∈R,
where g: R →R is some (Borel measurable) deterministic function. Show that the
SP

Y (t)

is stationary. Under what conditions is

Y (t)

WSS?
25.16 Exercises
615
(ii) Let

X(t)

be a centered stationary Gaussian SP of autocovariance function KXX.
Let Y (t) = sgn(X(t)), where sgn(ξ) is equal to +1 whenever ξ ≥0 and is equal
to −1 otherwise. Is

Y (t)

centered? Is it WSS? If so, what is its autocovariance
function?
Hint: For Part (ii) recall Exercise 23.25.
Exercise 25.11 (A Memoryless Nonlinearity). Let

X(t)

be a centered stationary Gaus-
sian SP of autocovariance function KXX.
Let h: R →R be such that h

X(t)

is of
ﬁnite variance. Deﬁne Z(t) for every t ∈R as Z(t) = h

X(t)

. Show that the mapping
τ →E[X(t) Z(t + τ)] does not depend on t and is proportional to KXX.
Hint: Recall Bussgang’s Theorem (Exercise 23.26).
Exercise 25.12 (The Transform Method). We wish to study the SP

h

X(t)

, t ∈R

when

X(t), t ∈R

is some given SP and h is some deterministic function, which we
assume to be of the form h = ˇg for some g ∈L1.
(i) Show that for all epochs t1, t2 ∈R,
E
+
h

X(t1)

h

X(t2)
,
=
 ∞
−∞
 ∞
−∞
g(f1) g(f2) E
+
ei(2πf1X(t1)+2πf2X(t2)),
df1 df2.
(ii) Show that if

X(t)

is a centered stationary Gaussian SP of autocovariance func-
tion KXX, then
E
+
h

X(t1)

h

X(t2)
,
=
 ∞
−∞
 ∞
−∞
g(f1) g(f2)
· exp
	
−2π2
KXX(0)f 2
1 + 2 KXX(t1 −t2)f1f2 + KXX(0)f 2
2

df1 df2.
These identities are the core of the Transform Method (Davenport and Root, 1987, Chap-
ter 13).
Exercise 25.13 (WSS Stochastic Processes). Let A and B be IID random variables taking
on the values ±1 equiprobably. Deﬁne the SP
Z(t) = A cos(2πt) + B sin(2πt),
t ∈R.
(i) Is the SP

Z(t)

WSS?
(ii) Deﬁne the SP

W(t)

by W(t) = Z2(t). Is

W(t)

WSS?
Exercise 25.14 (Valid Autocovariance Functions). Let KXX and KYY be the autocovari-
ance functions of some WSS stochastic processes

X(t)

and

Y (t)

.
(i) Show that τ →KXX(τ) + KYY (τ) is the autocovariance function of some WSS SP.
(ii) Repeat for τ →KXX(τ) KYY (τ).
Exercise 25.15 (Time Reversal). Let KXX be the autocovariance function of some WSS
SP

X(t), t ∈R

. Is the time-reversed SP (ω, t) →X(ω, −t) WSS? If so, express its
autocovariance function in terms of KXX.
616
Continuous-Time Stochastic Processes
Exercise 25.16 (Classifying Stochastic Processes). Let

X(t)

and

Y (t)

be independent
centered stationary Gaussian stochastic processes of unit variance and autocovariance
functions KXX and KYY . Deﬁne the stochastic processes

S(t)

,

T(t)

,

U(t)

,

V (t)

,
and

W(t)

at every t ∈R as
S(t) = X(t) + Y (t + τ1),
T(t) =X(t) Y (t + τ2),
U(t) = X(t) + X(t + τ3),
V (t)=X(t) X(t + τ4),
W(t) = X(t) + X(−t),
where τ1, τ2, τ3, τ4 ∈R are deterministic. Which of these stochastic processes is Gaussian?
Which is WSS? Which is stationary?
Exercise 25.17 (Another Interpretation of the PSD). Let

X(t)

be a centered WSS
measurable SP of PSD SXX. Given any T > 0 and f0 ∈R, consider the integral
 T
−T
X(t) e−i2πf0t dt =
 T
−T
X(t) cos(2πf0t) dt −i
 T
−T
X(t) sin(2πf0t) dt.
(i) Show that for every such T and f0,
E
1
1
√
2T
 T
−T
X(t) e−i2πf0t dt

22
= 2T
 ∞
−∞
SXX(f) sinc2
2T(f −f0)

df.
(ii) Argue that if SXX is continuous at f0, then the RHS of the above (and hence also
the LHS) tends to SXX(f0) as T tends to inﬁnity.
Exercise 25.18 (On the Spectral Distribution Function). Let Ts be some positive constant;
let p(·) be some real trigonometric polynomial
p(t) =
n

η=−n
aη ei2πηt/Ts,
t ∈R,
where aη = a∗
−η for all η ∈{−n, . . . , n}; let T be a RV that is uniformly distributed over
the interval [−Ts/2, Ts/2); and let X(t) = p(t + T) for all t ∈R.
(i) Show that

X(t)

is WSS.
(ii) Find the distribution of a RV S for which (25.38) holds.
Exercise 25.19 (Midpoint-Approximation). Suppose that

X(t)

is a measurable, station-
ary, centered, Gaussian SP of autocovariance function KXX(·) and that we approximate
its integral over the interval [a, b] by the product of its value at the interval’s midpoint
and the interval’s length. Express the variance of the estimation error in terms of KXX(·),
a, and b.
Exercise 25.20 (A Linear Functional of a Gaussian SP). Let

X(t)

be a measurable
stationary Gaussian SP of mean 2 and of autocovariance function KXX : τ →exp (−|τ|).
Compute
Pr
' 2
0
X(t) dt ≥2
(
.
25.16 Exercises
617
Exercise 25.21 (An Integral). Let the SP

Z(t)

be given by
Z(t) = X e−t I{t ≥0} + Y e−2t I{t ≥0},
t ∈R,
where X and Y are jointly Gaussian with means μx and μy and covariance matrix

σ2
x
ρσxσy
ρσxσy
σ2
y

.
Is

Z(t)

a Gaussian SP? Is it stationary? Compute
Pr
' ∞
−∞
Z(t) dt ≥0
(
.
Exercise 25.22 (Integrating a Gaussian SP). Let

X(t)

be a measurable stationary
Gaussian SP of nonzero mean. Is the SP

Y (t)

deﬁned at every epoch t ∈R by
Y (t) =
 |t|
0
X(τ) dτ
a Gaussian SP? Is it WSS? Is it stationary?
Exercise 25.23 (Two Filters). Let

X(t)

be a centered stationary Gaussian SP of auto-
covariance function KXX and PSD SXX. Deﬁne

Y (t)

=

X(t)

⋆hy,

Z(t)

=

X(t)

⋆hz,
where hy, hz ∈L1. Thus,

Y (t)

is the result of passing

X(t)

through a stable ﬁlter of
impulse response hy and similarly

Z(t)

.
(i) What is the joint distribution of Y (t1) and Z(t2) for given epochs t1, t2 ∈R?
(ii) Give a necessary and suﬃcient condition on ˆhy, ˆhz, and SXX for Y (17) to be
independent of Z(17).
(iii) Give a necessary and suﬃcient condition on ˆhy, ˆhz, and SXX for

Z(t)

to be
independent of

Y (t)

.
Exercise 25.24 (On the Covariance of Linear Functionals). Let

X(t)

be a measurable
WSS SP, let g1 be real-valued, integrable, and symmetric, and let g2 be real-valued,
integrable, and anti-symmetric. Show that if
s1(t) = g1(t −t0),
t ∈R,
and
s2(t) = g2(t −t0),
t ∈R,
for some real number t0, then
Cov
' ∞
−∞
X(t) s1(t) dt,
 ∞
−∞
X(t) s2(t) dt
(
= 0.
Exercise 25.25 (Functionals of Diﬀerent SPs). Let

X(t)

and

Y (t)

be centered, WSS,
measurable stochastic processes. Assume that E[X(t + τ) Y (t)] does not depend on t and
deﬁne
KXY (τ) = E
$
X(t + τ) Y (t)
%
,
τ ∈R.
Let sx, sy ∈L1 be deterministic. Prove that
Cov
' ∞
−∞
X(t) sx(t) dt,
 ∞
−∞
Y (t) sy(t) dt
(
=
 ∞
−∞
KXY (σ)

sx ⋆~sy

(σ) dσ.
Why can this be viewed as a generalization of Proposition 25.10.1 (vi) and of (25.90)?
618
Continuous-Time Stochastic Processes
Exercise 25.26 (On Jointly Gaussian Stochastic Processes). We say that

X(t)

and

Y (t)

are jointly Gaussian stochastic processes if for every n ∈N and every choice
of the epochs t1, . . . , tn ∈R the random 2n-vector (X(t1), . . . , X(tn), Y (t1), . . . , Y (tn))T
is Gaussian. Let

X(t)

and

Y (t)

be stationary and jointly Gaussian, and let sx and
sy be integrable real signals.
Either using a heuristic argument similar to the one leading to (25.64) or by extending
the proof of Proposition 25.11.1 prove that
 ∞
−∞
X(t) sx(t) dt +
 ∞
−∞
Y (t) sy(t) dt
is a Gaussian random variable. Use this to prove the joint Gaussianity of
 ∞
−∞
X(t) sx(t) dt
and
 ∞
−∞
Y (t) sy(t) dt.
Exercise 25.27 (Filters and Power). Let

X(t)

be a centered measurable WSS SP of
PSD SXX. Show that when

X(t)

is passed through a stable ﬁlter of impulse response
h ∈L1, its power is scaled by at most
max
f∈R
ˆh(f)
2.
Exercise 25.28 (Linear Functionals of White Gaussian Noise). Find the distribution of
 Ts
0
N(t) dt
and of
 ∞
0
e−tN(t) dt
when

N(t), t ∈R

is white Gaussian noise of double-sided PSD N0/2 with respect to
the bandwidth of interest. (Ignore the fact that the mappings t →I{0 ≤t ≤Ts} and
t →e−t I{t ≥0} are not bandlimited.)
Exercise 25.29 (More on Independence). Let

X(t)

and

Y (t)

be independent cen-
tered WSS stochastic processes, and let hx and hy be integrable signals. Prove that the
stochastic processes X ⋆hx and Y ⋆hy are independent.
Hint: Recall Exercise 23.7 and Proposition 25.15.8.
Exercise 25.30 (Approximately White SP). Let

X(t), t ∈R

be a measurable, centered,
stationary, Gaussian SP of autocovariance function
KXX(τ) = BN0
4
e−B|τ|,
τ ∈R,
where N0, B > 0 are given constants. Throughout this problem N0 is ﬁxed.
(i) Plot KXX for several values of B. What does KXX look like when B ≫1? Show
that KXX(τ) > 0 for all τ ∈R; that
 ∞
−∞
KXX(τ) dτ = N0
2 ;
and that for every δ > 0,
lim
B→∞
 δ
−δ
KXX(τ) dτ = N0
2 .
(In this sense, KXX approximates Dirac’s Delta scaled by N0/2 when B is large.)
25.16 Exercises
619
(ii) Compute E
$
X(t)2%
. Plot this as a function of B, with N0 held ﬁxed. What happens
when B ≫1?
(iii) Compute the PSD SXX. Plot it for several values of B. What does it look like when
B ≫1?
(iv) For the orthonormal signals deﬁned for every t ∈R by
φ1(t) =

1
if 0 ≤t ≤1,
0
otherwise,
φ2(t) =
⎧
⎪
⎨
⎪
⎩
1
if 0 ≤t ≤1
2,
−1
if 1
2 < t ≤1,
0
otherwise
compute E
$
⟨X, φ1⟩⟨X, φ2⟩
%
. What happens to this expression when B ≫1?
Exercise 25.31 (Multiplying a SP by a Periodic Signal). Let

X(t)

be a centered WSS
SP of autocovariance function KXX. Let r be a real, deterministic, ﬁnite-power, periodic
signal of period Tp > 0. Let

Y (t)

be the product of

X(t)

and r.
(i) Show that

Y (t)

is of average autocovariance function
¯KYY (τ) = KXX(τ) 1
Tp
 Tp
0
r(t) r(t + τ) dt,
τ ∈R.
(ii) Denoting by ˆr(η) the η-th Fourier Series Coeﬃcient of r with respect to the interval
[−Tp/2, Tp/2), show that if

X(t)

is of PSD SXX then

Y (t)

is of operational PSD
f →1
Tp
∞

η=−∞
ˆr(η)
2 SXX
	
f + η
Tp

.
Hint: For Part (i) recall Exercise 15.13. For Part (ii) recall Theorem A.3.3.
Chapter 26
Detection in White Gaussian Noise
26.1
Introduction
In this chapter we ﬁnally address the detection problem in continuous time. The
setup is described in Section 26.2. The key result of this chapter is that—although
the observation in this setup is a stochastic process, i.e., a continuum of ran-
dom variables—the problem can be reduced without loss of optimality to a ﬁnite-
dimensional problem where the observation is a random vector.
This result is
presented in Section 26.3. In Section 26.4 we analyze the conditional law of the
random vector under each of the hypotheses.
This analysis enables us in Sec-
tion 26.5 to derive an optimal guessing rule and in Section 26.6 to analyze its
performance. Section 26.7 addresses the front-end ﬁlter, which is a critical element
of any practical implementation of the decision rule. Extensions to passband detec-
tion are then described in Section 26.8, followed by some examples in Section 26.9.
Section 26.10 treats the problem of detection in “colored” noise, and Section 26.11
treats systems with multiple antennas. The chapter concludes with a discussion of
the detection problem for mean signals that are not bandlimited.
26.2
Setup
A discrete random variable M (“message”) takes values in the set M = {1, . . . , M},
where M ≥2, according to the a priori probabilities
πm = Pr[M = m],
m ∈M,
(26.1)
where π1, . . . , πM are positive1
πm > 0,
m ∈M
(26.2)
and sum to one

m∈M
πm = 1.
(26.3)
1There is no loss in generality in addressing the detection problem only for strictly positive
priors. Hypotheses that have a zero prior can be ignored at the receiver without loss in optimality.
620
26.3 From a Stochastic Process to a Random Vector
621
We observe a continuous-time SP

Y (t), t ∈R

, which, conditional on M = m,
can be expressed as
Y (t) = sm(t) + N(t),
t ∈R,
(26.4)
i.e.,
Y = sm + N,
(26.5)
where the “mean signals” s1, . . . , sM are real, deterministic, integrable signals that
are bandlimited to W Hz (Deﬁnition 6.4.9), and where the “noise”

N(t)

is inde-
pendent of M and is WGN of double-sided PSD N0/2 with respect to the band-
width W (Deﬁnition 25.15.1). Based on the observation

Y (t)

we wish to guess M
with the smallest possible probability of error.2
26.3
From a Stochastic Process to a Random Vector
The next theorem allows us to reduce the detection problem from one where the
observation is a SP to one where it is a random vector. The reduction is done
“without loss in optimality” in the sense that to every measurable decision rule
that is based on the SP there corresponds a (randomized) decision rule that is
based on the random vector and that is of identical performance: for every message
m ∈M, the conditional probabilities of error given M = m of the two decision
rules are identical and, consequently, so are the (unconditional) probabilities of
error, because
p(error) =

m∈M
πm p(error|M = m).
(26.6)
Before stating this key result we recall that the linear subspace span(s1, . . . , sM)
is the collection of all linear combinations of the mean signals s1, . . . , sM (Sec-
tion 4.3); that this subspace is ﬁnite dimensional; that its elements are all inte-
grable signals that are bandlimited to W Hz (because s1, . . . , sM are); and that
it has an orthonormal basis (Deﬁnition 4.6.3, Note 6.4.12, Note 6.4.2, and Propo-
sition 4.6.10). Such an orthonormal basis can be found using the Gram-Schmidt
Procedure (Section 4.6.6). If (φ1, . . . , φd) is an orthonormal basis for the subspace
span(s1, . . . , sM), then this subspace is of dimension d; by Proposition 4.6.4
sm =
d

ℓ=1
⟨sm, φℓ⟩φℓ,
m ∈M;
(26.7)
and, by Proposition 4.6.9,
∥sm∥2
2 =
d

ℓ=1
⟨sm, φℓ⟩2,
m ∈M.
(26.8)
2In mathematical terms we are looking for a mapping from Ω to M that is measurable with
respect to the σ-algebra generated by
Y (t)

and that minimizes the probability of error among
all such functions.
622
Detection in White Gaussian Noise
Theorem 26.3.1 (From a SP to a Random Vector). Consider the setup of Sec-
tion 26.2.
(i) If (φ1, . . . , φd) is an orthonormal basis for span(s1, . . . , sM), then to every
decision rule based on Y that is measurable with respect to the σ-algebra
generated by Y there corresponds a randomized decision rule based on the
random d-vector

⟨Y, φ1⟩, . . . , ⟨Y, φd⟩
T
(26.9)
of identical performance in the sense that the conditional probabilities of error
given M of the two rules are identical. Consequently, no measurable decision
rule for guessing M based on Y can have a probability of error that is smaller
than that of an optimal rule for guessing M based on the random vector
(26.9).
(ii) If ˜s1, . . . ,˜sn are integrable signals satisfying
span(s1, . . . , sM) ⊆span(˜s1, . . . ,˜sn),
(26.10)
then, in the above sense, it is optimal to guess M based on

⟨Y,˜s1⟩, . . . , ⟨Y,˜sn⟩
T.
(26.11)
(iii) In the above sense there is also no loss of optimality in guessing M based on

⟨Y, s1⟩, . . . , ⟨Y, sM⟩
T.
(26.12)
Proof. The main result of the theorem is Part (i), with which we begin. The other
parts are simple corollaries. We begin with a technicality. Since the mean signals
s1, . . . , sM are integrable signals that are bandlimited to W Hz, it follows that so
are all the signals in span(s1, . . . , sM), including φ1, . . . , φd. Consequently, the
stochastic integrals in (26.9) are well-deﬁned.
Having gotten this out of the way, we now proceed to the main part of the proof,
which is illustrated in Figure 26.1 on Page 623. At the top of the ﬁgure is some
arbitrary given (measurable) decision rule φGuess that produces the guess φGuess(Y)
based on the observed SP Y. We next construct a randomized decision rule of
identical performance but that is based on the vector of inner products (26.9). The
randomized rule ﬁrst uses its local randomness and the vector of inner products
to produce a new continuous-time SP Y′, and it then feeds Y′ to the given rule
φGuess to produce the guess φGuess(Y′). The SP Y′ is constructed so that, for
every m ∈M, its conditional law given M = m be identical to the conditional law
of Y given M = m. Consequently,
Pr

φGuess(Y) ̸= m
 M = m

= Pr

φGuess(Y′) ̸= m
 M = m

,
m ∈M, (26.13)
with the result that the two rules have identical performance. It remains to con-
struct Y′ and to demonstrate that its conditional law is identical to the conditional
law of Y.
26.3 From a Stochastic Process to a Random Vector
623
Decision
Device
Decision
Device
Guess
Guess
Projection
Reconstruction

ℓ⟨Y, φℓ⟩φℓ
⟨Y, φ1⟩
⟨Y, φ2⟩
⟨Y, φd⟩

Y (t), t ∈R

+
Projection
Subtraction
Generate

N ′(t)

of same FDDs as

N(t)

Local
Randomness
N′
N′ −
ℓ⟨N′, φℓ⟩φℓ

Y ′(t)

Figure 26.1: The top ﬁgure is of a generic decision rule that bases its decision
on the received SP

Y (t)

.
Using the above rule as one of its inner modules,
the (randomized) decision rule in the lower ﬁgure bases its decision on the d inner
products ⟨Y, φ1⟩, . . . , ⟨Y, φd⟩and achieves the same performance: for any message
m ∈M, the conditional FDDs of Y and Y′ given M = m are identical, and
consequently, so are the conditional probabilities of error.
624
Detection in White Gaussian Noise
The construction of Y′ is depicted at the bottom of Figure 26.1.
We use the
local randomness to produce a measurable SP N′ whose law is identical to that
of the noise N. Thus, N′ is a measurable, centered, stationary, Gaussian SP of
autocovariance function KNN—the autocovariance function of the WGN on the
channel. The SP Y′ is
Y′ =
d

ℓ=1
⟨Y, φℓ⟩φℓ+ N′ −
d

ℓ=1
⟨N′, φℓ⟩φℓ.
(26.14)
The ﬁrst summand, which is based on the inner products, reconstructs the projec-
tion of Y onto span(φ1, . . . , φd)—the linear subspace spanned by the mean signals.
The sum of the remaining terms is of the same law as the part of N that is or-
thogonal to span(φ1, . . . , φd) but, of course, independent of it. (It was generated
from N′, which was generated from the local randomness, which is independent
of N.)
We next verify that, conditional on M = m, the stochastic processes Y and Y′ have
identical laws (FDDs) for every m ∈M. We begin by verifying that the conditional
means are identical. It is here that we use the hypothesis that (φ1, . . . , φd) is an
orthonormal basis for span(s1, . . . , sM):
E

Y′  M = m

=
d

ℓ=1
E

⟨Y, φℓ⟩
 M = m

φℓ
=
d

ℓ=1
E

⟨sm + N, φℓ⟩

φℓ
=
d

ℓ=1
⟨sm, φℓ⟩φℓ
= sm,
m ∈M.
Here the ﬁrst equality holds because N′ is centered; the second because, conditional
on M = m, the received waveform Y is sm+N; the third by writing ⟨sm + N, φℓ⟩as
⟨sm, φℓ⟩+⟨N, φℓ⟩and by noting that ⟨N, φℓ⟩is of zero mean because N is centered;
and the last equality follows from (26.7), which holds because (φ1, . . . , φd) is an
orthonormal basis for span(s1, . . . , sM).
If we subtract the conditional mean from Y, we obtain N, whereas if we subtract
the conditional mean from Y′ we obtain
d

ℓ=1
⟨N, φℓ⟩φℓ+ N′ −
d

ℓ=1
⟨N′, φℓ⟩φℓ.
The two have the same conditional laws given M = m by Theorem 25.15.7.
We next prove Part (ii) by showing that (26.10) implies that, if φ1, . . . , φd are
as above, then from the inner-products vector (26.11) we can compute the inner-
products vector (26.9). To see this we note that since φ1, . . . , φd is an orthonormal
basis for span(s1, . . . , sM),
φℓ∈span(s1, . . . , sM),
ℓ∈{1, . . . , d},
(26.15)
26.3 From a Stochastic Process to a Random Vector
625
and hence, by the assumption (26.10),
φℓ∈span(˜s1, . . . ,˜sn),
ℓ∈{1, . . . , d}.
(26.16)
Consequently, for each such ℓthere exist n constants α(1)
ℓ, . . . , α(n)
ℓ
∈R such that
φℓ=
n

j=1
α(j)
ℓ
˜sj,
ℓ∈{1, . . . , d}.
(26.17)
This and the “almost” linearity of stochastic integration (Lemma 25.10.3) implies
that with probability one
⟨Y, φℓ⟩=
n

j=1
α(j)
ℓ
⟨Y,˜sj⟩,
ℓ∈{1, . . . , d},
(26.18)
and the inner-products vector (26.9) is indeed computable from the inner-products
vector (26.11).
Part (iii) follows from Part (ii) by substituting M for n and s1, . . . , sM for ˜s1, . . . ,˜sn.
The fact that there is no loss of optimality in basing our guess of M on the vector of
inner products (26.9) has a beautiful geometric interpretation. In order to present
it, we ﬁrst note that there is a one-to-one relationship between the inner-products
vector and the projection
d

ℓ=1
⟨Y, φℓ⟩φℓ
(26.19)
of the received SP Y onto the subspace spanned by the mean vectors (Deﬁni-
tion 25.15.3).
Indeed, we can compute this projection from the inner-products
vector using (26.19), and from this projection we can compute the inner-products
vector via the relation
⟨Y, φℓ⟩=
L
d

ℓ′=1
⟨Y, φℓ′⟩φℓ′, φℓ
M
,
ℓ∈{1, . . . , d}.
(26.20)
By Theorem 26.3.1 (i), it is optimal to guess M based on the inner-products vector,
and since this vector is computable from the projection of the received SP onto
the linear subspace spanned by the mean signals, we conclude that it is optimal
to guess M based on this projection. This is one of the most important results in
Digital Communications, so we repeat:
When guessing which of M integrable signals that are bandlimited
to W Hz is being observed in Gaussian noise that is white with
respect to the bandwidth W, there is no loss of optimality in basing
the guess on the projection of the observed waveform onto the
linear subspace spanned by the signals.
626
Detection in White Gaussian Noise
26.4
The Random Vector of Inner Products
26.4.1
The Conditional Law
Theorem 26.3.1 reduces our detection problem from one where the observable is a
SP to one where it is a random vector. But to solve the latter using the techniques
of Chapter 21 we need the conditional law of the random vector given each of the
hypotheses. Obtaining these conditional laws is, fortunately, straightforward:
Theorem 26.4.1 (The Conditional Laws of the Inner-Products Vector). Consider
the setup of Theorem 26.3.1, and let m be any message in M.
(i) Conditional on M = m, the random vector

⟨Y, φ1⟩, . . . , ⟨Y, φd⟩
T
(26.21a)
is Gaussian with mean vector

⟨sm, φ1⟩, . . . , ⟨sm, φd⟩
T
(26.21b)
and d × d covariance matrix
N0
2 Id.
(26.21c)
(ii) Conditional on M = m, the random vector

⟨Y,˜s1⟩, . . . , ⟨Y,˜sn⟩
T
(26.22a)
is Gaussian with mean vector

⟨sm,˜s1⟩, . . . , ⟨sm,˜sn⟩
T
(26.22b)
and n × n covariance matrix
N0
2
⎛
⎜
⎜
⎜
⎝
⟨˜s1,˜s1⟩
⟨˜s1,˜s2⟩
· · ·
⟨˜s1,˜sn⟩
⟨˜s2,˜s1⟩
⟨˜s2,˜s2⟩
· · ·
⟨˜s2,˜sn⟩
...
...
...
...
⟨˜sn,˜s1⟩
⟨˜sn,˜s2⟩
· · ·
⟨˜sn,˜sn⟩
⎞
⎟
⎟
⎟
⎠.
(26.22c)
(iii) Conditional on M = m, the random vector

⟨Y, s1⟩, . . . , ⟨Y, sM⟩
T
(26.23a)
is Gaussian with mean vector

⟨sm, s1⟩, . . . , ⟨sm, sM⟩
T
(26.23b)
and M × M covariance matrix
N0
2
⎛
⎜
⎜
⎜
⎝
⟨s1, s1⟩
⟨s1, s2⟩
· · ·
⟨s1, sM⟩
⟨s2, s1⟩
⟨s2, s2⟩
· · ·
⟨s2, sM⟩
...
...
...
...
⟨sM, s1⟩
⟨sM, s2⟩
· · ·
⟨sM, sM⟩
⎞
⎟
⎟
⎟
⎠.
(26.23c)
26.4 The Random Vector of Inner Products
627

Y (t)

~φ1
~φ2
~φd
...
⟨Y, φ1⟩
⟨Y, φ2⟩
⟨Y, φd⟩
Guess
Decision Rule
sample at t = 0
Figure 26.2: Computing the inner products ⟨Y, φ1⟩, . . . , ⟨Y, φd⟩from the received
waveform using d matched ﬁlters and feeding the result to a guessing device.
Proof. We shall only prove Part (ii); the other parts are special cases. Conditional
on M = m, the SP Y is sm + N, and the inner-products vector in (26.22a) is thus

⟨sm,˜s1⟩, . . . , ⟨sm,˜sn⟩
T +

⟨N,˜s1⟩, . . . , ⟨N,˜sn⟩
T.
(26.24)
Here the ﬁrst vector is deterministic, and the second is a centered Gaussian because
its components are linear functionals of WGN (Proposition 25.15.2 (ii)).
This
establishes that the vector is Gaussian (Proposition 23.6.3). Since the second vector
is centered, the mean vector equals the ﬁrst vector, i.e., is as given in (26.22b).
It remains to compute the covariance matrix, which is the covariance matrix of

⟨N,˜s1⟩, . . . , ⟨N,˜sn⟩
T.
This matrix is the one in (26.22c) by Proposition 25.15.2 (ii) on linear functionals
of WGN.
Theorems 26.3.1 and 26.4.1 suggest two diﬀerent (equivalent) receiver architec-
tures: the one depicted in Figure 26.2, where the inner-products vector (26.21a) is
computed using d matched ﬁlters, and the result is then fed to a guessing device,
and the one depicted in Figure 26.3 where we ﬁrst compute the M inner products
between Y and each of the mean signals and we then feed the result to a (diﬀerent)
decision device.
For the purpose of deriving an optimal decision rule, it is usually easier to adopt the
ﬁrst architecture and to work with the inner-products vector (26.21a), because its
conditional law is so simple. Moreover, this architecture requires only d matched
ﬁlters, and d ≤M. In contrast, the vector (26.23a) can be cumbersome because its
covariance matrix may be singular, so it need not have a density. Nevertheless, it
does have the advantage that it is “coordinate free,” in the sense that its deﬁnition
does not depend on the choice of the orthonormal basis. This makes some of the
results more transparent. With an eye to this, we note the following immediate
consequence of Theorem 26.4.1 (iii):
628
Detection in White Gaussian Noise

Y (t)

~s1
~s2
~sM
...
⟨Y, s1⟩
⟨Y, s2⟩
⟨Y, sM⟩
Guess
Decision Rule
sample at t = 0
Figure 26.3: Computing the inner product between the observed SP and each of
the mean signals and basing the guess on these inner products.
Note 26.4.2. The conditional distribution of the random vector

⟨Y, s1⟩, . . . , ⟨Y, sM⟩
T
(26.25)
given each of the hypotheses is determined by N0 and by the pairwise inner prod-
ucts between the mean signals
{⟨sm′, sm′′⟩}m′,m′′∈M.
(26.26)
The PSD of the noise at frequencies outside the band [−W, W ] is immaterial.
26.4.2
It Is all in the Geometry!
By Theorem 26.3.1 (iii), the performance of the best decision rule based on Y is the
same as the performance of the best rule based on the inner-products vector (26.25).
The performance of the latter is determined by the prior and by the conditional
laws of the vector given the diﬀerent messages. By Note 26.4.2, these conditional
laws are determined by the PSD N0/2 of the noise in the band [−W, W ] and
by the pairwise inner products between the mean signals (26.26). Other than in
determining these inner products, the nature of the mean signals is immaterial
(provided that they are integrable signals that are bandlimited to W Hz). The
PSD of the noise outside the band [−W, W ] is also immaterial. We thus conclude:
Proposition 26.4.3. For the setup of Section 26.2, the minimal probability of error
that can be achieved in guessing M based on

Y (t)

is determined by N0, the inner
products (26.26), and the prior {πm}m∈M.
26.5
Optimal Guessing Rule
26.5.1
The Decision Rule in Terms of (φ1, . . . , φd)
We are now ready to derive an optimal guessing rule. For this purpose we start
with an orthonormal basis (φ1, . . . , φd) for span(s1, . . . , sM); it does not really
26.5 Optimal Guessing Rule
629
matter which. Without loss in optimality we can then base our guess of M on the
vector of inner products (26.9) (Theorem 26.3.1 (i)). Denote this random d-vector
by T,
T =

⟨Y, φ1⟩, . . . , ⟨Y, φd⟩
T,
(26.27)
and its ℓ-th component by T (ℓ),
T (ℓ) =
 ∞
−∞
Y (t) φℓ(t) dt
= ⟨Y, φℓ⟩,
ℓ= 1, . . . , d.
(26.28)
It follows from Theorem 26.4.1 (i) that for every m ∈M the conditional distribu-
tion of T given M = m is Gaussian with mean
E

T
 M = m

=

⟨sm, φ1⟩, . . . , ⟨sm, φd⟩
T
(26.29)
and covariance matrix (N0/2) Id, where Id denotes the d × d identity matrix. The
components of T are thus conditionally independent and of equal variance N0/2
(but not of equal mean). Consequently, we can express the conditional density
of T, conditional on M = m, at every point t = (t(1), . . . , t(d))T ∈Rd using
this conditional independence and the explicit form of the univariate Gaussian
density (19.6) as
fT|M=m(t) =
d
@
ℓ=1
1

2πN0/2
exp
-
−

t(ℓ) −⟨sm, φℓ⟩
2
2N0/2
.
=
1
(πN0)d/2 exp
	
−1
N0
d

ℓ=1

t(ℓ) −⟨sm, φℓ⟩
2

,
t ∈Rd.
(26.30)
Note that with proper translation (Table 26.1, Page 630) the conditional distri-
bution of T is very similar to the one we addressed in Section 21.6; see (21.51).
In fact, it is a special case of the distribution studied in Section 21.6: Y there
corresponds to T here; J there corresponds to d here; σ2 there corresponds to N0/2
here; and the mean vector sm associated with Message m there corresponds to the
vector

⟨sm, φ1⟩, . . . , ⟨sm, φd⟩
T
(26.31)
here. Consequently, we can use the results from Section 21.6 and Proposition 21.6.1
in particular, to obtain an optimal decision rule for guessing M based on T:
Theorem 26.5.1. Consider the setup of Section 26.2, and let (φ1, . . . , φd) be an
orthonormal basis for span(s1, . . . , sM).
(i) The decision rule that guesses uniformly at random from among all the mes-
sages ˜m ∈M for which
ln π ˜m −
d
ℓ=1

⟨Y, φℓ⟩−⟨s ˜m, φℓ⟩
2
N0
= max
m′∈M

ln πm′ −
d
ℓ=1

⟨Y, φℓ⟩−⟨sm′, φℓ⟩
2
N0
!
(26.32)
630
Detection in White Gaussian Noise
In Section 21.6
Here
number of components of
observed vector
J
d
variance of noise added to
each component
σ2
N0/2
number of hypotheses
M
M
conditional mean of observa-
tion given M = m

s(1)
m , . . . , s(J)
m
T

⟨sm, φ1⟩, . . . , ⟨sm, φd⟩
T
sum of squared components
of mean vector
J

j=1

s(j)
m
2
d

ℓ=1

⟨sm, φℓ⟩
2 =
 ∞
−∞
s2
m(t) dt
Table 26.1: The setup in Section 21.6 and here.
minimizes the probability of a guessing error.
(ii) If M has a uniform distribution, then this rule does not depend on the value
of N0. It chooses uniformly at random from among all the messages ˜m ∈M
for which
d

ℓ=1

⟨Y, φℓ⟩−⟨s ˜m, φℓ⟩
2 =
min
m′∈M
 d

ℓ=1

⟨Y, φℓ⟩−⟨sm′, φℓ⟩
2
!
.
(26.33)
(iii) If M has a uniform distribution and, in addition, the mean signals are of
equal energy, i.e,
∥s1∥2
2 = ∥s2∥2
2 = · · · = ∥sM∥2
2 ,
(26.34)
then these decision rules are equivalent to the maximum-correlation rule that
guesses uniformly from among all the messages ˜m ∈M for which
d

ℓ=1
⟨Y, φℓ⟩⟨s ˜m, φℓ⟩= max
m′∈M
d

ℓ=1
⟨Y, φℓ⟩⟨sm′, φℓ⟩.
(26.35)
Proof. The theorem follows directly from Proposition 21.6.1. For Part (iii) we
need to note that, by (26.8), Condition (26.34) is equivalent to the condition
d

ℓ=1
⟨s1, φℓ⟩2 =
d

ℓ=1
⟨s2, φℓ⟩2 = · · · =
d

ℓ=1
⟨sM, φℓ⟩2,
(26.36)
and hence, upon taking square roots, to the condition that the Euclidean norm of
the vector in (26.31) not depend on the message m. This latter condition is the
additional hypothesis in Proposition 21.6.1 (iii).
Note that, because (φ1, . . . , φd) is an orthonormal basis for span(s1, . . . , sM), the
signals sm′ and sm′′ diﬀer, if, and only if, the vectors (⟨sm′, φ1⟩, . . . , ⟨sm′, φd⟩)T
and (⟨sm′′, φ1⟩, . . . , ⟨sm′′, φd⟩)T diﬀer.3 Consequently, by Proposition 21.6.2:
3Readers who expected to see the clause “outside a set of Lebesgue measure zero,” should
recall Notes 6.4.2 and 6.4.12.
26.5 Optimal Guessing Rule
631
Note 26.5.2. If the mean signals s1, . . . , sM are distinct, then the probability of a
tie, i.e., that more than one message ˜m ∈M satisﬁes (26.32), is zero.
26.5.2
The Decision Rule without Reference to a Basis
We next derive a “coordinate-free” representation of our decision rule, i.e., a rep-
resentation that does not refer to a speciﬁc orthonormal basis.
Theorem 26.5.3. Consider the setup of Section 26.2.
(i) The decision rule that guesses uniformly at random from among all the mes-
sages ˜m ∈M for which
ln π ˜m + 2
N0
	 ∞
−∞
Y (t) s ˜m(t) dt −1
2
 ∞
−∞
s2
˜m(t) dt

= max
m′∈M
1
ln πm′ + 2
N0
	 ∞
−∞
Y (t) sm′(t) dt −1
2
 ∞
−∞
s2
m′(t) dt

2
(26.37)
minimizes the probability of error.
(ii) If M has a uniform distribution, then this rule does not depend on the value
of N0. It chooses uniformly at random from among all the messages ˜m ∈M
for which
 ∞
−∞
Y (t) s ˜m(t) dt −1
2
 ∞
−∞
s2
˜m(t) dt
= max
m′∈M
1 ∞
−∞
Y (t) sm′(t) dt −1
2
 ∞
−∞
s2
m′(t) dt
2
.
(26.38)
(iii) If M has a uniform distribution and, in addition, the mean signals are of
equal energy, i.e.,
∥s1∥2
2 = ∥s2∥2
2 = · · · = ∥sM∥2
2 ,
then these decision rules are equivalent to the maximum-correlation rule that
guesses uniformly from among all the messages ˜m ∈M for which
 ∞
−∞
Y (t) s ˜m(t) dt = max
m′∈M
1 ∞
−∞
Y (t) sm′(t) dt
2
.
(26.39)
Proof. We shall prove Part (i) using Theorem 26.5.1 (i). To this end we begin by
noting that
ln πm′ −
d
ℓ=1

⟨Y, φℓ⟩−⟨sm′, φℓ⟩
2
N0
can be expressed by opening the square as
ln πm′ −1
N0
d

ℓ=1
⟨Y, φℓ⟩2 + 2
N0
d

ℓ=1
⟨Y, φℓ⟩⟨sm′, φℓ⟩−1
N0
d

ℓ=1
⟨sm′, φℓ⟩2.
632
Detection in White Gaussian Noise
Since the term
−1
N0
d

ℓ=1
⟨Y, φℓ⟩2
does not depend on the hypothesis, it is optimal to choose a message at random
from among all the messages ˜m satisfying
ln π ˜m + 2
N0
d

ℓ=1
⟨Y, φℓ⟩⟨s ˜m, φℓ⟩−1
N0
d

ℓ=1
⟨s ˜m, φℓ⟩2
= max
m′∈M

ln πm′ + 2
N0
d

ℓ=1
⟨Y, φℓ⟩⟨sm′, φℓ⟩−1
N0
d

ℓ=1
⟨sm′, φℓ⟩2
!
.
Part (i) of the theorem now follows from this rule by substituting m′ for m in (26.8)
and by noting that
d

ℓ=1
⟨Y, φℓ⟩⟨sm′, φℓ⟩=

Y,
d

ℓ=1
⟨sm′, φℓ⟩φℓ

= ⟨Y, sm′⟩,
m′ ∈M,
and likewise for ˜m. Here the ﬁrst equality follows by linearity (Lemma 25.10.3)
and the second by substituting m′ for m in (26.7).
Part (ii) follows by noting that if M is uniform, then ln πm′ does not depend on
the hypothesis m′.
Part (iii) follows from Part (ii) because if all the mean signals are of equal energy,
then the term
 ∞
−∞
s2
m′(t) dt
does not depend on the hypothesis.
By Note 26.5.2 we have:
Note 26.5.4. If the mean signals are distinct, then the probability of a tie is zero.
26.6
Performance Analysis
The decision rule we derived in Section 26.5.1 uses the observed SP

Y (t)

to
compute the vector T of inner products with an orthonormal basis (φ1, . . . , φd)
via (26.28), with the result that the vector T has the conditional law speciﬁed
in (26.30).
Our decision rule then performs MAP decoding of M based on T.
Consequently, the performance of our decoding rule is identical to the performance
of the MAP rule for guessing M based on a vector T having the conditional law
(26.30). The performance of this latter decoding rule was studied in Section 21.6.
All that remains is to translate the results from that section in order to obtain
performance bounds on our decoder.
26.6 Performance Analysis
633
To translate the results from Section 21.6 we need to substitute N0/2 for σ2 there;
d for J there; and (26.31) for the mean vectors there (see Table 26.1). But there is
one more translation we need: the bounds in Section 21.6 are expressed in terms
of the Euclidean distance between the mean vectors, and here we prefer to express
the bounds in terms of the “distance” between the mean signals.
Fortunately,
as we next show, the translation is straightforward. Because (φ1, . . . , φd) is an
orthonormal basis for span(s1, . . . , sM), it follows from Proposition 4.6.9 that
d

ℓ=1
⟨v, φℓ⟩2 = ∥v∥2
2 ,
v ∈span(s1, . . . , sM).
(26.40)
Substituting sm′ −sm′′ for v and hence ⟨sm′, φℓ⟩−⟨sm′′, φℓ⟩for ⟨v, φℓ⟩in this
identity yields
d

ℓ=1

⟨sm′, φℓ⟩−⟨sm′′, φℓ⟩
2 = ∥sm′ −sm′′∥2
2
(26.41)
=
 ∞
−∞

sm′(t) −sm′′(t)
2 dt.
(26.42)
Thus, the squared Euclidean distance between two mean vectors in Section 21.6
is equal to the energy in the diﬀerence between the corresponding mean signals in
our setup.
Denoting by pMAP(error|M = m) the conditional probability of error of our decoder
conditional on M = m, and denoting by p∗(error) its unconditional probability of
error (which is the optimal probability of error)
p∗(error) =

m∈M
πm pMAP(error|M = m),
(26.43)
we obtain from (21.58)
pMAP(error|M = m) ≤

m′̸=m
Q
	∥sm −sm′∥2
√2N0
+

N0/2
∥sm −sm′∥2
ln πm
πm′

(26.44)
and hence by, (26.43),
p∗(error) ≤

m∈M
πm

m′̸=m
Q
	∥sm −sm′∥2
√2N0
+

N0/2
∥sm −sm′∥2
ln πm
πm′

.
(26.45)
When M is uniform these bounds simplify to
pMAP(error|M = m) ≤

m′̸=m
Q
⎛
⎝

∥sm −sm′∥2
2
2N0
⎞
⎠,
M uniform
(26.46)
and
p∗(error) ≤1
M

m∈M

m′̸=m
Q
⎛
⎝

∥sm −sm′∥2
2
2N0
⎞
⎠,
M uniform.
(26.47)
634
Detection in White Gaussian Noise
Similarly, we can use the results from Section 21.6 to lower-bound the probability
of a guessing error. Indeed, using (21.64) we obtain
pMAP(error|M = m) ≥max
m′̸=m Q
	∥sm −sm′∥2
√2N0
+

N0/2
∥sm −sm′∥2
ln πm
πm′

,
(26.48)
p∗(error) ≥

m∈M
πm max
m′̸=m Q
	∥sm −sm′∥2
√2N0
+

N0/2
∥sm −sm′∥2
ln πm
πm′

.
(26.49)
For a uniform prior these bounds simplify to
pMAP(error|M = m) ≥max
m′̸=m Q
⎛
⎝

∥sm −sm′∥2
2
2N0
⎞
⎠,
M uniform,
(26.50)
p∗(error) ≥1
M

m∈M
max
m′̸=m Q
⎛
⎝

∥sm −sm′∥2
2
2N0
⎞
⎠,
M uniform.
(26.51)
26.7
The Front-End Filter
Receivers in practice rarely have the structure depicted in Figure 26.3 because—
although mathematically optimal—its hardware implementation is challenging.
The diﬃculty is related to the “dynamic range” problem in implementing the
matched ﬁlter: it is very diﬃcult to design a perfectly-linear system to exact speciﬁ-
cation. Linearity is usually only guaranteed for a certain range of input amplitudes.
Once the amplitude of the signal exceeds a certain level, the circuit often “clips”
the input waveform and no longer behaves linearly. Similarly, input signals that
are too small might be below the sensitivity of the circuit and might therefore
produce no output, thus violating linearity. This is certainly the case with circuits
that employ analog-to-digital conversion followed by digital processing, because
analog-to-digital converters can only represent the input with ﬁnite precision. The
problem with the structure depicted in Figure 26.3 is that the noise

N(t)

is typ-
ically much larger than the mean signal, so it becomes very diﬃcult to design a
circuit to exact speciﬁcations that will be linear enough to guarantee that its action
on the received waveform (consisting of the weak transmitted waveform and the
strong additive noise) be the sum of the required responses to the mean signal and
to the noise-signal. (That the noise is typically much larger than the mean signals
can be seen from the heuristic plot of its PSD in Figure 25.3. White Gaussian
noise is often of PSD N0/2 over frequency bands that are much larger than the
band [−W, W ] so, by (25.31), the variance of the noise can be extremely large.)
The engineering solution to the dynamic range problem is to pass the received
waveform through a “front-end ﬁlter” and to then feed this ﬁlter’s output to the
matched ﬁlter as in Figure 26.4. Except for a few very stringent requirements,
the speciﬁcations of the front-end ﬁlter are relatively lax. The ﬁrst requirement
is that the ﬁlter be linear over a very large range of input levels. This is usually
accomplished by using only passive elements in its implementation. The second
26.7 The Front-End Filter
635

Y (t)

~s1
~s2
~sM
...
Guess
Decision Rule
sample at t = 0
Front-End
Figure 26.4: Feeding the signal to a front-end ﬁlter and then computing the inner
products with the mean signals.
1
f
ˆhFE(f)
−W
W
Figure 26.5: An example of the frequency response of a front-end ﬁlter.
requirement is that the front-end ﬁlter’s frequency response be of unit gain over
the mean signals’ frequency band [−W, W ] so that it will not distort the mean
signals.4 Additionally, we require that the ﬁlter be stable and that its frequency
response decay to zero sharply for frequencies outside the band [−W, W ]. This
latter condition guarantees that the ﬁlter’s response to the noise be of small variance
so that the dynamic range of the signal at the ﬁlter’s output be moderate. If we
denote the front-end ﬁlter’s impulse response by hFE, then the key mathematical
requirements are linearity; stability, i.e.,
 ∞
−∞
hFE(t)
 dt < ∞;
(26.52)
and the unit-gain requirement
ˆhFE(f) = 1,
|f| ≤W.
(26.53)
An example of the frequency response of a front-end ﬁlter is depicted in Figure 26.5.
4Imprecisions here can often be corrected using signal processing.
636
Detection in White Gaussian Noise
In the rest of this section we shall prove that, as long as these assumptions are met,
there is no loss of optimality in introducing the front-end ﬁlter as in Figure 26.4.
(In the ideal mathematical world there is, of course, nothing to be gained from this
ﬁlter, because the architecture depicted in Figure 26.3 is optimal.)
The crux of the proof is in showing that—like

Y (t)

—the front-end ﬁlter’s output
is the sum of the transmitted signal and WGN of PSD N0/2 with respect to the
bandwidth W. Once this is established, the result follows by recalling that the
conditional joint distribution of the matched ﬁlters’ outputs does not depend on
the PSD of the noise outside the band [−W, W ] (Note 26.4.2).
We thus proceed to analyze the front-end ﬁlter’s output, which we denote by
 ˜Y (t)

:
 ˜Y (t)

=

Y (t)

⋆hFE.
(26.54)
We ﬁrst note that (26.53) and the assumption that sm is an integrable signal that
is bandlimited to W Hz guarantee that
sm ⋆hFE = sm,
m ∈M
(26.55)
(Proposition 6.5.2 and Proposition 6.4.10 cf. (b)). By (26.55) and by the linearity
of the ﬁlter we can thus express the ﬁlter’s output (conditional on M = m) as
 ˜Y (t)

=

Y (t)

⋆hFE
= sm ⋆hFE +

N(t)

⋆hFE
= sm +

N(t)

⋆hFE.
(26.56)
We next show that the SP

N(t)

⋆hFE on the RHS of (26.56) is WGN of PSD N0/2
with respect to the bandwidth W. This follows from Theorem 25.13.2. Indeed,
being the result of passing a measurable centered stationary Gaussian SP through
a stable ﬁlter, it is a measurable centered stationary Gaussian SP. And its PSD is
f 	→SNN(f)
ˆhFE(f)
2,
(26.57)
which is equal to N0/2 for all frequencies f ∈[−W, W ], because for these frequen-
cies SNN(f) is equal to N0/2 and ˆhFE(f) is equal to one. At frequencies outside
the band [−W, W ] the PSD of

N(t)

⋆hFE may diﬀer from that of

N(t)

.
We thus conclude that the front-end ﬁlter’s output, like its input, can be expressed
as the transmitted signal corrupted by WGN of PSD N0/2 with respect to the
bandwidth W. Note 26.4.2 now guarantees that for every m ∈M we have that,
conditional on M = m, the distribution of
	 ∞
−∞
˜Y (t) s1(t) dt, . . . ,
 ∞
−∞
˜Y (t) sM(t) dt

T
is identical to the conditional distribution of the random vector in (26.12).
The advantage of the front-end ﬁlter becomes apparent when we re-examine the
PSD of the noise at its output. If the front-end ﬁlter’s frequency response decays
very sharply to zero for frequencies outside the band [−W, W ], then, by (26.57),
26.8 Detection in Passband
637
this PSD will be nearly zero outside this band. Consequently, the variance of the
noise at the front-end ﬁlter’s output—which is the integral of this PSD—will be
greatly reduced. This will guarantee that the dynamic range at the ﬁlter’s output
be much smaller than at its input, thus simplifying the implementation of the
matched ﬁlters.
26.8
Detection in Passband
The detection problem in passband is very similar to the one in baseband. The
diﬀerence is that the mean signals {sm} are now assumed to be integrable signals
that are bandlimited to W Hz around the carrier frequency fc (Deﬁnition 7.2.1)
and that the noise is now assumed to be WGN of PSD N0/2 with respect to the
bandwidth W around fc (Deﬁnition 25.15.9).
Here too, there is no loss of optimality in basing our guess on the vector

⟨Y,˜s1⟩, . . . , ⟨Y,˜sn⟩
T
(26.58)
whenever the signals ˜s1, . . . ,˜sn are integrable signals that are bandlimited to W
Hz around fc and satisfy
sm ∈span(˜s1, . . . ,˜sn),
m ∈M.
Conditional on M = m, this vector is Gaussian with mean vector (26.22b) and
covariance matrix (26.22c). The latter covariance matrix can also be written in
terms of the baseband representation of the mean signals using the relation
⟨˜sj′,˜sj′′⟩= 2 Re

⟨˜sj′,BB,˜sj′′,BB⟩

,
(26.59)
where ˜sj′,BB and ˜sj′′,BB are the baseband representations of ˜sj′ and ˜sj′′ (Theo-
rem 7.6.10).
The computation of the inner products (26.11) can be performed in passband
by feeding the signal Y directly to ﬁlters that are matched to the passband sig-
nals {˜sj}, or in baseband by expressing the inner product ⟨Y,˜sj⟩in terms of the
baseband representation ˜sj,BB of ˜sj as follows:
⟨Y,˜sj⟩=

Y, t 	→2 Re

˜sj,BB(t) ei2πfct
= 2
 ∞
−∞

Y (t) cos(2πfct)

Re

˜sj,BB(t)

dt
−2
 ∞
−∞

Y (t) sin(2πfct)

Im

˜sj,BB(t)

dt.
This expression suggests computing the inner product ⟨Y,˜sj⟩using two baseband
matched ﬁlters: one that is matched to Re(˜sj,BB) and that is fed the product
of

Y (t)

and cos(2πfct), and one that is matched to Im(˜sj,BB) and that is fed the
product of

Y (t)

and sin(2πfct).5
5Since the baseband representation of an integrable passband signal that is bandlimited to
638
Detection in White Gaussian Noise
As discussed in Section 26.7, in practice one typically ﬁrst feeds the received sig-
nal

Y (t)

to a stable highly-linear bandpass ﬁlter of frequency response ˆhPB-FE(·)
satisfying
ˆhPB-FE(f) = 1,
|f| −fc
 ≤W/2,
(26.60)
with the frequency response decaying drastically at other frequencies to guarantee
that the ﬁlter’s output be of small dynamic range. Moreover, we typically prefer to
use matched ﬁlters that do not depend on the carrier frequency in order to allow
it to be tunable. This issue is discussed in greater detail in Section 32.5.
26.9
Some Examples
26.9.1
Binary Hypothesis Testing
Before treating the general binary hypothesis testing problem we begin with the
case of antipodal signaling with a uniform prior. In this case
s0 = −s1 = s,
(26.61)
where s is some integrable signal that is bandlimited to W Hz.
We denote its
energy by Es, i.e.,
Es = ∥s∥2
2
(26.62)
and assume that it is strictly positive. In this case the dimension of the linear
subspace spanned by the mean signals is one, and this subspace is spanned by the
unit-norm signal
φ =
s
∥s∥2
.
(26.63)
Depending on the outcome of a fair coin toss, either s or −s is sent over the channel.
We observe the SP

Y (t)

given by the sum of the transmitted signal and WGN
of PSD N0/2 with respect to the bandwidth W, and we wish to guess which signal
was sent. How should we form our guess?
By Theorem 26.3.1 there is no loss of optimality in forming our guess based on
the RV T = ⟨Y, φ⟩. Conditional on H = 0, we have T ∼N
√Es, N0/2

, whereas,
conditional on H = 1, we have T ∼N

−√Es, N0/2

. How to guess H based on T
is the problem we addressed in Section 20.10. There we showed that it is optimal
to guess “H = 0” if T ≥0 and to guess “H = 1” if T < 0. (The case T = 0 occurs
with probability zero, so we need not worry about it.) An optimal decision rule for
guessing H based on

Y (t)

is thus:
Guess “H = 0” if
 ∞
−∞
Y (t) s(t) dt ≥0.
(26.64)
W Hz around the carrier frequency fc is integrable (Proposition 7.6.2), it follows that our as-
sumption that ˜sj is an integrable function that is bandlimited to W Hz around the carrier fre-
quency fc guarantees that both t →cos(2πfct) Re

˜sj,BB(t)

and t →sin(2πfct) Im ˜sj,BB(t)

are
integrable. Hence, with probability one, both the integrals
 ∞
−∞Y (t) cos(2πfct)

Re ˜sj,BB(t)

dt
and
 ∞
−∞Y (t) sin(2πfct)

Im ˜sj,BB(t)

dt exist.
26.9 Some Examples
639
Let pMAP(error|s) denote the conditional probability of error of this decision rule
given that s was sent; let pMAP(error|−s) be similarly deﬁned; and let p∗(error)
denote the optimal probability of error of this problem. By the optimality of our
rule,
p∗(error) = 1
2

pMAP(error|s) + pMAP(error|−s)

.
Using the expression for the error probability derived in Section 20.10 we obtain
p∗(error) = Q
⎛
⎝

2 ∥s∥2
2
N0
⎞
⎠,
(26.65)
which, in view of (26.62), can also be written as
p∗(error) = Q
-4
2Es
N0
.
.
(26.66)
Note that, as expected from Section 26.4.2 and in particular from Proposition 26.4.3,
the probability of error is determined by the “geometry” of the problem, which in
this case is summarized by the energy in s.
There is also a nice geometric interpretation to (26.66). The distance between the
mean signals s and −s is ∥s −(−s)∥2 = 2√Es. Half the distance is √Es. The
inner product between the noise and the unit-length vector φ pointing from −s
to s is N(0, N0/2). Half the distance thus corresponds to √Es/

N0/2 standard
deviations of this inner product. The probability of error is thus the probability
that a standard Gaussian is greater than half the distance between the signals as
measured by standard deviations of the inner product between the noise and the
unit-length vector pointing from −s towards s.
Consider now the more general binary hypothesis testing problem where both hy-
potheses are still equally likely, but where the mean signals s0 and s1 are not
antipodal, i.e., they do not sum to zero. Our approach to this problem is to reduce
it to the antipodal case we already treated. We begin by forming the signal
 ˜Y (t)

by subtracting (s0 + s1)/2 from the received signal, so
˜Y (t) = Y (t) −1
2

s0(t) + s1(t)

,
t ∈R.
(26.67)
Since Y (t) can be recovered from ˜Y (t) by adding

s0(t) + s1(t)

/2, the smallest
probability of a guessing error that can be achieved based on
 ˜Y (t)

is no larger
than that which can be achieved based on

Y (t)

. (The two are, in fact, the same
because
 ˜Y (t)

can be computed from

Y (t)

.)
The advantage of using
 ˜Y (t)

becomes apparent once we compute its conditional
law given H. Conditional on H = 0, we have ˜Y (t) = (s0(t) −s1(t))/2 + N(t),
whereas conditional on H = 1, we have ˜Y (t) = −(s0(t)−s1(t))/2+N(t). Thus, the
guessing problem given
 ˜Y (t)

is exactly the problem we addressed in the antipodal
case with (s0 −s1)/2 playing the role of s. We thus obtain that an optimal decision
640
Detection in White Gaussian Noise
rule is to guess “H = 0” if
 ˜Y (t)

s0(t) −s1(t)

/2 dt is nonnegative. Or stated in
terms of

Y (t)

using (26.67):
Guess “H = 0” if
 ∞
−∞
	
Y (t) −s0(t) + s1(t)
2

s0(t) −s1(t)
2
dt ≥0.
(26.68)
The error probability associated with this decision rule is obtained from (26.65) by
substituting (s0 −s1)/2 for s:
p∗(error) = Q
⎛
⎝

∥s0 −s1∥2
2
2N0
⎞
⎠.
(26.69)
This expression too has a nice geometric interpretation. The inner product between
the noise and the unit-norm signal that is pointing from s1 to s0 is N(0, N0/2). The
“distance” between the signals is ∥s0 −s1∥2. Half the distance is ∥s0 −s1∥2 /2,
which corresponds to
∥s0 −s1∥2 /2

N0/2
standard deviations of a N(0, N0/2) random variable. The probability of error
(26.69) is thus the probability that the inner product between the noise and the
unit-norm signal that is pointing from s1 to s0 exceeds half the distance between
the signals.
26.9.2
8-PSK
We next present an example of detection in passband. For concreteness we consider
8-PSK, which stands for “8-ary Phase Shift Keying.” Here the number of hypothe-
ses is eight, so M = {1, 2, . . . , 8} and M = 8. We assume that M is uniformly
distributed over M. Conditional on M = m, the received signal is given by
Y (t) = sm(t) + N(t),
t ∈R,
(26.70)
where
sm(t) = 2 Re

cmsBB(t) ei2πfct
,
t ∈R;
(26.71)
cm = α eim 2π
8
(26.72)
for some positive real α; the baseband signal sBB is an integrable complex signal
that is bandlimited to W/2 Hz and of unit energy
∥sBB∥2 = 1;
(26.73)
the carrier frequency fc satisﬁes fc > W/2; and

N(t)

is WGN of PSD N0/2 with
respect to the bandwidth W around the carrier frequency fc (Deﬁnition 25.15.9).
Irrespective of M, the transmitted energy Es is given by
Es = ∥sm∥2
2
=
 ∞
−∞

2 Re

cmsBB(t) ei2πfct2
dt
= 2α2,
(26.74)
26.9 Some Examples
641
m = 1
t(1)
t(2)
guess 1
Figure 26.6: Region of points (t(1), t(2)) resulting in guessing “M = 1.”
as can be veriﬁed using the relationship between energy in passband and baseband
(Theorem 7.6.10) and using (26.73).
The transmitted waveform sm can also be written in a form that is highly suggestive
of a choice of an orthonormal basis for span(s1, . . . , sM):
sm(t) =
√
2 Re(cm)
√
2 Re

sBB(t) ei2πfct



φ1(t)
+
√
2 Im(cm)
√
2 Re

i sBB(t) ei2πfct



φ2(t)
=
√
2 Re(cm) φ1(t) +
√
2 Im(cm) φ2(t),
where
φ1(t) ≜
√
2 Re

sBB(t) ei2πfct
,
t ∈R,
φ2(t) ≜
√
2 Re

i sBB(t) ei2πfct
,
t ∈R.
From Theorem 7.6.10 on inner products in passband and baseband, it follows that
φ1 and φ2 are orthogonal. Also, from that theorem and from (26.73), it follows
that they are of unit energy. Thus, the tuple (φ1, φ2) is an orthonormal basis for
span(s1, . . . , sM). Consequently, there is no loss of optimality in basing our guess
on the random vector T = (⟨Y, φ1⟩, ⟨Y, φ2⟩)T, and conditional on M = m, the
components of T are independent with T (1) ∼N
√
2α cos(2πm/8), N0/2

and with
T (2) ∼N
√
2α sin(2πm/8), N0/2

. We have thus reduced the guessing problem to
that of guessing M based on a two-dimensional vector T.
The problem of guessing M based on T was studied in Section 21.4. To lift the
results from that section, we need to substitute
√
2α for A and to substitute N0/2
for σ2. For example, the region where we guess “M = 1” is given in Figure 26.6.
For the scenario we described, some engineers prefer to use complex random vari-
ables (Chapter 17).
Rather than viewing T as a two-dimensional real random
vector, they prefer to view it as a (scalar) complex random variable whose real
part is ⟨Y, φ1⟩and whose imaginary part is ⟨Y, φ2⟩. Conditional on M = m, this
CRV has the form
√
2cm + Z,
Z ∼NC(0, N0) ,
(26.75)
642
Detection in White Gaussian Noise
where NC(0, N0) denotes the circularly-symmetric variance-N0 complex Gaussian
distribution (Note 24.3.13).
The expression for the probability of error of our detector can also be lifted from
Section 21.4.
Substituting, as above,
√
2α for A and N0/2 for σ2, we obtain
from (21.26) that the conditional probability of error pMAP(error|M = m) of our
proposed decision rule is given for every m ∈M by
pMAP(error|M = m) = 1
π
 π−ψ
0
e
−
2α2 sin2 ψ
N0 sin2(θ+ψ) dθ,
ψ = π
8 .
The conditional probability of error can also be expressed in terms of the trans-
mitted energy Es using (26.74).
Doing that and recalling that the conditional
probability of error does not depend on the transmitted message, we obtain that
the average probability of error p∗(error) is given by
p∗(error) = 1
π
 π−ψ
0
e
−
Es sin2 ψ
N0 sin2(θ+ψ) dθ,
ψ = π
8 .
(26.76)
Note 26.9.1. The expression (26.76) continues to hold also for M-PSK where
cm = α ei2πm/M for M ≥2 not necessarily equal to 8, provided that we deﬁne
ψ = π/M in (26.76).
26.9.3
Orthogonal Keying
We next consider M-ary orthogonal keying. We assume that the RV M that we
wish to guess is uniformly distributed over the set M = {1, . . . , M}, where M ≥2.
The mean signals are assumed to be orthogonal and of equal (strictly) positive
energy Es:
⟨sm′, sm′′⟩= Es I{m′ = m′′},
m′, m′′ ∈M.
(26.77)
Since M is uniform, and since the mean signals are of equal energy, it follows
from Theorem 26.5.3 that to minimize the probability of guessing incorrectly, it is
optimal to correlate the received waveform

Y (t)

with each of the mean signals
and to pick the message whose mean signal gives the highest correlation:
Guess “m” if ⟨Y, sm⟩= max
m′∈M ⟨Y, sm′⟩
(26.78)
with ties (which occur with probability zero) being resolved by picking a random
message among those that attain the highest correlation.
We next address the probability of error of this optimal decision rule. We ﬁrst
deﬁne the vector (T (1), . . . , T (M))T by
T (ℓ) =
 ∞
−∞
Y (t) sℓ(t)
√Es
dt,
ℓ∈{1, . . . , M}
and recast the decision rule as guessing “M = m” if T (m) = maxm′∈M T (m′), with
ties being resolved at random among the components of T that are maximal.
26.9 Some Examples
643
Let pMAP(error|M = m) denote the conditional probability of error of this decoding
rule, conditional on M = m. Conditional on M = m, an error occurs in two cases:
when m does not attain the highest score or when m attains the highest score but
this score is also attained by some other message and the tie is not resolved in m’s
favor. Since the probability of a tie is zero (Note 26.5.4), we may ignore the second
case and only compute the probability that an incorrect message is assigned a score
that is (strictly) higher than the one associated with m. Thus,
pMAP(error|M = m)
= Pr

max

T (1), . . . , T (m−1), T (m+1), . . . , T (M)
> T (m)  M = m

.
(26.79)
From (26.30) and the orthogonality of the signals (26.77) we have that, condi-
tional on M = m, the random vector T is Gaussian with the mean of its m-th
component being √Es, the mean of its other components being zero, and with all
the components being of variance N0/2 and independent of each other. Thus, the
conditional probability of error given M = m is “the probability that at least one
of M −1 IID N(0, N0/2) random variables exceeds the value of a N
√Es, N0/2

random variable that is independent of them.” Having recast the probability of
error conditional on M = m in a way that does not involve m (the clause in quotes
makes no reference to m), we conclude that the conditional probability of error
given that M = m does not depend on m:
pMAP(error|M = m) = pMAP(error|M = 1),
m ∈M.
(26.80)
This conditional probability of error can be computed starting from (26.79) as:
pMAP(error|M = 1)
= Pr

max

T (2), . . . , T (M)
> T (1)  M = 1

= 1 −Pr

max

T (2), . . . , T (M)
≤T (1)  M = 1

= 1 −
 ∞
−∞
fT (1)|M=1(t) Pr

max

T (2), . . . , T (M)
≤t
 M = 1, T (1) = t

dt
= 1 −
 ∞
−∞
fT (1)|M=1(t) Pr

max

T (2), . . . , T (M)
≤t
 M = 1

dt
= 1 −
 ∞
−∞
fT (1)|M=1(t) Pr

T (2) ≤t, . . . , T (M) ≤t
 M = 1

dt
= 1 −
 ∞
−∞
fT (1)|M=1(t)

Pr

T (2) ≤t
 M = 1
M−1
dt
= 1 −
 ∞
−∞
fT (1)|M=1(t)
-
1 −Q
	
t

N0/2

.M−1
dt
= 1 −
 ∞
−∞
1
√πN0
e−
(t−√
Es)2
N0
-
1 −Q
	
t

N0/2

.M−1
dt
= 1 −
1
√
2π
 ∞
−∞
e−τ 2/2
-
1 −Q
	
τ +
4
2Es
N0

.M−1
dτ,
(26.81)
644
Detection in White Gaussian Noise
where the ﬁrst equality follows from (26.79); the second because the conditional
probability of an event and its complement add to one; the third by conditioning
on T (1) = t and integrating it out, i.e., by noting that for any random variable X
of density fX(·) and for any random variable Y ,
Pr

Y ≤X

=
 ∞
−∞
fX(x) Pr

Y ≤x
 X = x

dx,
(26.82)
with X here being equal to T (1) and with Y here being max{T (2), . . . , T (M)}; the
fourth from the conditional independence of T (1) and (T (2), . . . , T (M)) given M = 1,
which implies the conditional independence of T (1) and max{T (2), . . . , T (M)} given
M = 1; the ﬁfth because the maximum of random variables does not exceed t if,
and only if, none of them exceeds t

max{T (2), . . . , T (M)} ≤t

⇐⇒

T (2) ≤t, . . . , T (M) ≤t

;
the sixth because, conditional on M = 1, the random variables T (2), . . . , T (M) are
IID so
Pr

T (2) ≤t, . . . , T (M) ≤t
 M = 1

=

Pr

T (2) ≤t
 M = 1
M−1
;
the seventh because, conditional on M = 1, we have T (2) ∼N(0, N0/2) and using
(19.12b); the eighth because, conditional on M = 1, we have T (1) ∼N
√Es, N0/2

so its conditional density can be written explicitly using (19.6); and the ﬁnal equal-
ity using the change of variable
τ ≜t −√Es

N0/2
.
(26.83)
Using (26.80) and (26.81) we obtain that if p∗(error) denotes the unconditional
probability of error, then p∗(error) = pMAP(error|M = 1) and
p∗(error) = 1 −
1
√
2π
 ∞
−∞
e−τ 2/2
-
1 −Q
	
τ +
4
2Es
N0

.M−1
dτ.
(26.84)
An alternative expression for the probability of error can be derived using the
Binomial Expansion
(a + b)n =
n

j=0
	n
j

an−j bj,

n ∈N, a, b ∈R

.
(26.85)
Substituting
a = 1,
b = −Q
	
τ +
4
2Es
N0

,
n = M −1,
26.9 Some Examples
645
in (26.85) yields
-
1 −Q
	
τ +
4
2Es
N0

.M−1
=
M−1

j=0
(−1)j
	M −1
j

-
Q
	
τ +
4
2Es
N0

.j
= 1 +
M−1

j=1
(−1)j
	M −1
j

-
Q
	
τ +
4
2Es
N0

.j
,
from which we obtain from (26.84) (using the linearity of integration and the fact
that the Gaussian density integrates to one)
p∗(error) =
M−1

j=1
(−1)j+1
	M −1
j

  ∞
−∞
1
√
2π e−τ 2/2
-
Q
	
τ +
4
2Es
N0

.j
dτ.
(26.86)
For the case where M = 2 the expression (26.84) for the probability of error can
be simpliﬁed to
p∗(error) = Q
-4
Es
N0
.
,
M = 2,
(26.87)
as we proceed to show in two diﬀerent ways. The ﬁrst way is to note that for
M = 2 the probability of error can be expressed, using (26.79) and (26.80), as
pMAP(error|M = 1) = Pr

T (2) > T (1)  M = 1

= Pr

T (2) −T (1) > 0
 M = 1

= Q
-4
Es
N0
.
,
where the last equality follows because, conditional on M = 1, the random vari-
ables T (1) and T (2) are independent Gaussians of variance N0/2 with the ﬁrst
having mean √Es and the second having zero mean, so their diﬀerence T (2) −T (1)
is N

−√Es, N0

. (The probability that a N

−√Es, N0

RV exceeds zero can be
computed using (19.12a).) The second way of showing (26.87) it to use (26.69) and
to note that the orthogonality of s1 and s2 implies ∥s1 −s2∥2
2 = ∥s1∥2
2 + ∥s2∥2
2 =
2Es.
26.9.4
The M-ary Simplex
We next describe a detection problem that is intimately related to the problem we
addressed in Section 26.9.3. To motivate the problem we ﬁrst note:
Proposition 26.9.2. Consider the setup described in Section 26.2.
If s is any
integrable signal that is bandlimited to W Hz, then the probability of error associated
with the mean signals {s1, . . . , sM} and the prior {πm} is the same as with the mean
signals {s1 −s, . . . , sM −s} and the same prior.
646
Detection in White Gaussian Noise
Proof. We have essentially given a proof of this result in Section 14.3 and also
in Section 26.9.1 in our analysis of nonantipodal signaling. The idea is that, by
subtracting the signal s from the received waveform, the receiver can make the
problem with mean signals {s1, . . . , sM} appear as though it were the problem
with mean signals {s1 −s, . . . , sM −s}.
Conversely, by adding s, the receiver
can make the latter appear as though it were the former. Consequently, the best
performance achievable in the two settings must be identical.
The expected transmitted energy when employing the mean signals {s1, . . . , sM}
may be diﬀerent than when employing the mean signals {s1 −s, . . . , sM −s}. In
subtracting the signal s one can change the average transmitted energy for better
or worse. As we argued in Section 14.4, to minimize the expected transmitted
energy, one should choose s to correspond to the “center of gravity” of the mean
signals:
Proposition 26.9.3. Let the prior {πm} and mean signals {sm} be given. Let
s∗=

m∈M
πm sm.
(26.88)
Then, for any energy-limited signal s

m∈M
πm ∥sm −s∗∥2
2 ≤

m∈M
πm ∥sm −s∥2
2 ,
(26.89)
with equality if, and only if, s is indistinguishable from s∗.
Proof. Writing sm −s as (sm −s∗) + (s∗−s) we have

m∈M
πm ∥sm −s∥2
2
=

m∈M
πm ∥(sm −s∗) + (s∗−s)∥2
2
=

m∈M
πm ∥sm −s∗∥2
2 +

m∈M
πm ∥s∗−s∥2
2 + 2

m∈M
πm ⟨sm −s∗, s∗−s⟩
=

m∈M
πm ∥sm −s∗∥2
2 + ∥s∗−s∥2
2 + 2
 
m∈M
πm(sm −s∗), s∗−s

=

m∈M
πm ∥sm −s∗∥2
2 + ∥s∗−s∥2
2 + 2 ⟨s∗−s∗, s∗−s⟩
=

m∈M
πm ∥sm −s∗∥2
2 + ∥s∗−s∥2
2
≥

m∈M
πm ∥sm −s∗∥2
2 ,
with the inequality being an equality if, and only if, ∥s∗−s∥2
2 = 0.
We can now construct the simplex signals as follows. We start with M orthonormal
waveforms φ1, . . . , φM
⟨φm′, φm′′⟩= I{m′ = m′′},
m′, m′′ ∈M
(26.90)
26.9 Some Examples
647
φ1
φ2
¯φ
φ2 −¯φ
φ1 −¯φ
Figure 26.7: Starting with two orthonormal signals and subtracting the “center of
gravity” from each we obtain two antipodal signals. Scaling these antipodal signals
results in the simplex constellation with two signals.
that are integrable and bandlimited to W Hz. We set ¯φ to be their “center of
gravity” with respect to the uniform prior
¯φ = 1
M

m∈M
φm.
(26.91)
Using (26.90), (26.91), and the basic properties of the inner product (3.6)–(3.10)
it is easily veriﬁed that

φm′ −¯φ, φm′′ −¯φ

= I

m′ = m′′
−1
M,
m′, m′′ ∈M.
(26.92)
We now deﬁne the M-ary simplex constellation with energy Es by
sm =

Es
4
M
M −1

φm −¯φ

,
m ∈M.
(26.93)
The construction for the case where M = 2 is depicted in Figure 26.7. It yields
the binary antipodal signaling scheme. The construction for M = 3 is depicted in
Figure 26.8.
From (26.93) and (26.92) we obtain for distinct m′, m′′ ∈M
∥sm∥2
2 = Es
and
⟨sm′, sm′′⟩= −
Es
M −1.
(26.94)
Also, from (26.93) we see that {sm} can be viewed as the result of subtracting the
center of gravity from orthogonal signals of energy Es M/(M −1). Consequently,
the least error probability that can be achieved in detecting simplex signals of
energy Es is the same as the least error probability that can be achieved in detecting
orthogonal signals of energy
M
M −1Es
(26.95)
648
Detection in White Gaussian Noise
Figure 26.8: Constructing the simplex constellation with three points from three
orthonormal signals. Left ﬁgure depicts the orthonormal constellation and its cen-
ter of gravity; middle ﬁgure depicts the result of subtracting the center of gravity,
and the right ﬁgure depicts the result of scaling (from a diﬀerent perspective).
(Proposition 26.9.2). From the expression for the error probability in orthogonal
signaling (26.84) we obtain for the simplex signals with a uniform prior
p∗(error) = 1 −
1
√
2π
 ∞
−∞
e−τ 2/2
-
1 −Q
	
τ +
4
M
M −1
2Es
N0

.M−1
dτ.
(26.96)
The decision rule for the simplex constellation can also be derived by exploiting the
relationship to orthogonal keying. For example, if ψ is a unit-energy integrable sig-
nal that is bandlimited to W Hz and that is orthogonal to the signals {s1, . . . , sM},
then, by (26.94), the waveforms
1
sm +
1
√
M −1

Es ψ
2
m∈M
(26.97)
are orthogonal, each of energy Es M/(M −1). (See Figure 26.9 for a demonstra-
tion of the process of obtaining an orthogonal constellation with M = 2 signals by
adding a signal ψ to each of the signals in a binary simplex constellation.) Conse-
quently, in order to decode the simplex signals contaminated by WGN with respect
to the bandwidth W, we can add
1
√M−1
√Es ψ to the received waveform and then
feed the result to an optimal detector for orthogonal keying.
26.9.5
Bi-Orthogonal Keying
Starting with an orthogonal constellation, we can double the number of signals
without reducing the minimum distance. This construction, which results in the
“bi-orthogonal signal set” is the topic of this section. To construct the bi-orthogonal
signal set with 2κ signals, we start with κ ≥1 orthonormal signals (φ1, . . . , φκ)
and deﬁne the 2κ bi-orthogonal signal set {s1,u, s1,d, . . . , sκ,u, sκ,d} by
sν,u = +

Es φν
and
sν,d = −

Es φν,
ν ∈{1, . . . , κ}.
(26.98)
26.9 Some Examples
649
s1
s2
√Es ψ
s1 + √Es ψ
s2 + √Es ψ
Figure 26.9:
Adding a properly scaled signal ψ that is orthogonal to all the
elements of a simplex constellation results in an orthogonal constellation.
Figure 26.10: A bi-orthogonal constellation with six signals.
We can think of “u” as standing for “up” and of “d” as standing for “down,” so to
each signal φν there correspond two signals in the bi-orthogonal constellation: the
“up signal” that corresponds to multiplying √Esφν by +1 and the “down signal”
that corresponds to multiplying √Esφν by −1. Only bi-orthogonal signal sets with
an even number of signals are deﬁned. The constructed signals are all of energy Es:
∥sν,u∥2 = ∥sν,d∥2 =

Es,
ν ∈{1, . . . , κ}.
(26.99)
A bi-orthogonal constellation with six points (κ = 3) is depicted in Figure 26.10.
Consider now the case where each of the signals φ1, . . . , φκ is an integrable sig-
nal that is bandlimited to W Hz and, consequently, so are all the signals in the
constructed bi-orthogonal signal set. A signal is picked uniformly at random from
the signal set and is sent over a channel. We observe the stochastic process

Y (t)

given by the sum of the transmitted signal and WGN of PSD N0/2 with respect
to the bandwidth W. How should we guess which signal was sent?
650
Detection in White Gaussian Noise
Since the signal was chosen equiprobably, and since all the signals in the signal set
are of the same energy, it is optimal to consider the inner products
⟨Y, s1,u⟩, ⟨Y, s1,d⟩, . . . , ⟨Y, sκ,u⟩, ⟨Y, sκ,d⟩
(26.100)
and to pick the signal in the signal set corresponding to the largest of these in-
ner products. By (26.98) we have for every ν ∈{1, . . . , κ} that sν,u = −sν,d so
⟨Y, sν,u⟩= −⟨Y, sν,d⟩and hence
max

⟨Y, sν,u⟩, ⟨Y, sν,d⟩

=
⟨Y, sν,u⟩
,
ν ∈{1, . . . , κ}.
(26.101)
Equivalently, by (26.98),
max

⟨Y, sν,u⟩, ⟨Y, sν,d⟩

=

Es
⟨Y, φν⟩
,
ν ∈{1, . . . , κ}.
To ﬁnd the maximum of the 2κ terms in (26.100) we can ﬁrst compute for each
ν ∈{1, . . . , κ} the maximum between ⟨Y, sν,u⟩and ⟨Y, sν,d⟩and then compute the
maximum of the κ results:
max
'
⟨Y, s1,u⟩, ⟨Y, s1,d⟩, . . . , ⟨Y, sκ,u⟩, ⟨Y, sκ,d⟩
(
= max
'
max

⟨Y, s1,u⟩, ⟨Y, s1,d⟩

, . . . , max

⟨Y, sκ,u⟩, ⟨Y, sκ,d⟩
(
.
Using this approach, we obtain from (26.101) the following optimal two-step proce-
dure: ﬁrst ﬁnd which ν∗in {1, . . . , κ} attains the maximum of the absolute values
of the inner products
max
ν∈{1,...,κ}
⟨Y, φν⟩

and then, after you have found ν∗, guess “sν∗,u” if ⟨Y, φν∗⟩> 0 and guess “sν∗,d”
if ⟨Y, φν∗⟩≤0.
We next compute the probability of error of this optimal guessing rule. It is not
diﬃcult to see that the conditional probability of error does not depend on the
message we condition on. For concreteness, we shall analyze the probability of
error associated with the message corresponding to the signal s1,u, a probability
that we denote by pMAP(error|s1,u), with the corresponding conditional probability
of correct decoding pMAP(correct|s1,u) = 1 −pMAP(error|s1,u).
To simplify the
typesetting, we shall denote the conditional probability of the event A given that
s1,u is sent by Pr(A|s1,u).
Since the probability of ties in the likelihood function is zero (Note 26.5.4),
pMAP(correct|s1,u)
= Pr

−⟨Y, φ1⟩≤⟨Y, φ1⟩and max
2≤ν≤κ
⟨Y, φν⟩

≤⟨Y, φ1⟩
 s1,u

= Pr

⟨Y, φ1⟩≥0 and max
2≤ν≤κ
⟨Y, φν⟩

≤⟨Y, φ1⟩
 s1,u

=
 ∞
0
f⟨Y,φ1⟩|s1,u(t) Pr
%
max
2≤ν≤κ
⟨Y, φν⟩

≤t
 s1,u, ⟨Y, φ1⟩= t
&
dt
=
 ∞
0
f⟨Y,φ1⟩|s1,u(t) Pr
%
max
2≤ν≤κ
⟨Y, φν⟩

≤t
 s1,u
&
dt
26.9 Some Examples
651
=
 ∞
0
f⟨Y,φ1⟩|s1,u(t)

Pr

|⟨Y, φ2⟩| ≤t
 s1,u
κ−1
dt
=
 ∞
0
1
√πN0
e−
(t−√
Es)2
N0
-
1 −2Q
	
t

N0/2

.κ−1
dt
=
1
√
2π
 ∞
−

2Es
N0
e−τ 2/2
-
1 −2Q
	
τ +
4
2Es
N0

.κ−1
dτ,
(26.102)
with the following justiﬁcation. The ﬁrst equality follows from the deﬁnition of
our optimal decoder and from the fact that ties occur with probability zero. The
second equality follows by trivial algebra (−ξ ≤ξ if, and only if, ξ ≥0). The third
equality follows by conditioning on ⟨Y, s1,u⟩being equal to t and integrating t
out while noting that a correct decision can only be made if t ≥0, in which case
the condition ⟨Y, φ1⟩≥0 is satisﬁed automatically. The fourth equality follows
because, conditional on the signal s1,u being sent, the random variable ⟨Y, s1,u⟩is
independent of the random variables {|⟨Y, φν⟩|}2≤ν≤κ. The ﬁfth equality follows
because, conditional on s1,u being sent, the random variables {|⟨Y, φν⟩|}2≤ν≤κ are
IID. The sixth equality follows because, conditional on s1,u being sent, we have
⟨Y, φ1⟩∼N
√Es, N0/2

and ⟨Y, φ2⟩∼N(0, N0/2), so
Pr
% ⟨Y, φ2⟩
 ≤t
 s1,u
&
= Pr
 |⟨Y, φ2⟩|

N0/2
≤
t

N0/2
 s1,u

= 1 −Pr
 |⟨Y, φ2⟩|

N0/2
≥
t

N0/2
 s1,u

= 1 −Pr
 ⟨Y, φ2⟩

N0/2
≥
t

N0/2
 s1,u

−Pr
 ⟨Y, φ2⟩

N0/2
≤
−t

N0/2
 s1,u

= 1 −2Q
	
t

N0/2

.
Finally, (26.102) follows from the substitution τ ≜(t−√Es)/

N0/2 as in (26.83).
Since the conditional probability of error does not depend on the message, it follows
that all conditional probabilities of error are equal to the average probability of
error p∗(error) and
p∗(error) = 1 −
1
√
2π
 ∞
−

2Es
N0
e−τ 2/2
-
1 −2Q
	
τ +
4
2Es
N0

.κ−1
dτ,
(26.103)
or, using the Binomial Expansion (26.85) with the substitution of −Q

τ +
3
2Es
N0

for b, of 1 for a, and of κ −1 for n
p∗(error) =
κ−1

j=1
(−1)j+12j
	κ −1
j

1
√
2π
 ∞
−

2Es
N0
e−τ 2/2
-
Q
	
τ +
4
2Es
N0

.j
dτ.
(26.104)
652
Detection in White Gaussian Noise
The probability of error associated with an orthogonal constellation with κ signals
is better than that of the bi-orthogonal constellation with 2κ signals and equal
average energy. But the comparison is not quite fair because the bi-orthogonal
constellation is richer.
26.10
Detection in Colored Gaussian Noise
26.10.1
Overview
We next consider the problem of detecting known signals in noise that is Gaussian
but possibly “colored,” i.e., not white. Our setup is thus as in Section 26.2 except
that the noise N, while still stationary and Gaussian, is no longer assumed to be
white. Instead, we assume a general PSD SNN.
The solution is based on a “whitening ﬁlter” for the PSD SNN with respect to the
bandwidth W. This is a stable ﬁlter that when fed Gaussian noise of PSD SNN
produces WGN of double-sided PSD 1 with respect to the bandwidth W (Def-
inition 26.10.1 and Proposition 26.10.4 ahead).
It turns out that when such a
ﬁlter exists—and it usually does (Proposition 26.10.12)—an optimal solution to
our problem is to feed the observed SP Y to a whitening ﬁlter and to then base
our guess on the ﬁlter’s output.
The proof that there is no loss of optimality in basing our guess on the ﬁlter’s
output is nontrivial (Theorem 26.10.9). But how to guess based on the ﬁlter’s
output is straightforward. Indeed, as we next argue, this guessing problem is of
the kind we already studied, i.e., that of detecting known signals (the response of
the ﬁlter to the mean signals) in WGN. To see this, let h be the impulse response of
a whitening ﬁlter. Conditional on M = m, we can then express the ﬁlter’s output
Y ⋆h as
Y ⋆h =

sm + N) ⋆h
= sm ⋆h + N ⋆h.
(26.105)
Here, because the ﬁlter is whitening, N⋆h is WGN of double-sided PSD 1 with re-
spect to the bandwidth W (Proposition 26.10.4 ahead). Thus, the guessing problem
based on the ﬁlter’s output is that of guessing which of the signals s1⋆h, . . . , sM⋆h
is being observed in WGN of double-sided PSD 1 with respect to the band-
width W. This is the setting addressed in Theorem 26.3.1 (with the substitution
of s1 ⋆h, . . . , sM ⋆h for the mean signals and of 1 for N0/2). From this theorem
it thus follows that for guessing M based on the whitening ﬁlter’s output Y ⋆h,
there is no loss of optimality in basing the decision on the inner-products vector

Y ⋆h, φ1

, . . . ,

Y ⋆h, φd
T
,
(26.106)
whenever (φ1, . . . , φd) is any orthonormal basis for
span(s1 ⋆h, . . . , sM ⋆h).
(26.107)
The solution is thus conceptually very simple. Most of the work will go into showing
that this procedure is optimal, i.e., that there is indeed no loss of optimality in
26.10 Detection in Colored Gaussian Noise
653
guessing M based on the whitening ﬁlter’s output. This is prima facie not obvious
because the whitening ﬁlter need not be invertible. (But see Lemma 26.10.8 ahead.)
26.10.2
Whitening Filters
Deﬁnition 26.10.1 (Whitening Filter for SNN with respect to W). A ﬁlter of
impulse response h: R →R is said to be a whitening ﬁlter for SNN (or for N)
with respect to the bandwidth W if it is stable and its frequency response ˆh
satisﬁes
SNN(f)
ˆh(f)
2 = 1,
|f| ≤W.
(26.108)
Only the magnitude of the frequency response of the whitening ﬁlter is speciﬁed in
(26.108) and only for frequencies in the band [−W, W ]. The response is unspeciﬁed
outside this band. Consequently:
Note 26.10.2. There may be many diﬀerent whitening ﬁlters for SNN with respect
to the bandwidth W.
If SNN is zero at some frequencies in [−W, W ], then there is no whitening ﬁlter
for SNN with respect to W. Likewise, a whitening ﬁlter for SNN does not exist
if SNN is discontinuous in [−W, W ] (because the frequency response of a stable
ﬁlter must be continuous (Theorem 6.2.11), and if SNN is discontinuous, then so is
f 	→1/

|SNN(f)| ). Thus:
Note 26.10.3. A whitening ﬁlter for SNN with respect to W need not exist.
We shall see, however, that a whitening ﬁlter exists whenever throughout the inter-
val [−W, W ] the PSD SNN is strictly positive and is twice continuously diﬀerentiable
(Proposition 26.10.12).
The ﬁlter is called “whitening” because, by (26.108) and Theorem 25.13.2, we have:
Proposition 26.10.4. If

N(t), t ∈R

is a measurable, stationary, Gaussian SP
of PSD SNN, and if
h is the impulse response of a whitening ﬁlter for SNN with
respect to W, then

N(t)

⋆h is WGN of PSD 1 with respect to the bandwidth W.
From this proposition and (26.105) we obtain:
Corollary 26.10.5 (The Output of the Whitening Filter). If N and h are as above,
and Y ⋆h is the whitening ﬁlter’s response to the observed SP Y, then, conditional
on M = m,
Y ⋆h = sm ⋆h + ˜N,
(26.109)
where ˜N is WGN of double-sided PSD 1 with respect to the bandwidth W.
26.10.3
Optimality
Rather than proving the optimality of basing our guess on the whitening ﬁlter’s
output and then invoking Theorem 26.3.1 to prove the optimality of basing our
guess on the vector (26.106), we shall adopt a more direct approach and prove the
optimality of basing our guess on this vector directly. But ﬁrst we shall rewrite
654
Detection in White Gaussian Noise
this vector in a form that accentuates that its components are linear functionals
of Y: we will show that, with probability one, the tuples

Y ⋆h, φ1

, . . . ,

Y ⋆h, φd

and

Y, ~h ⋆φ1

, . . . ,

Y, ~h ⋆φd

are equal. It will then follow that to prove optimality it suﬃces to prove that there
is no loss of optimality in basing our guess on

Y, ~h ⋆φ1

, . . . ,

Y, ~h ⋆φd

.
(26.110)
More generally, we will show that if q1, . . . , qn are integrable signals (not necessarily
orthonormal), then, with probability one,

Y ⋆h, q1

, . . . ,

Y ⋆h, qn

=

Y, ~h ⋆q1

, . . . ,

Y, ~h ⋆qn

.
(26.111)
We begin with a deterministic setting.
Lemma 26.10.6 (The Adjoint of the Convolution: Deterministic Signals). Let h
be an integrable signal, and let ~h: t 	→h(−t) be its mirror image. Then

u ⋆h, v

=

u, ~h∗⋆v

,
u, v ∈L2,
(26.112)
where h, u, and v may be complex.
Proof. The proof is based on changing the order of integration:

u ⋆h, v

=
 ∞
t=−∞
	 ∞
τ=−∞
h(t −τ) u(τ) dτ

v∗(t) dt
=
 ∞
τ=−∞
u(τ)
	 ∞
t=−∞
h(t −τ) v∗(t) dt

dτ
=
 ∞
τ=−∞
u(τ)
	 ∞
t=−∞
h∗(t −τ) v(t) dt

∗
dτ
=
 ∞
τ=−∞
u(τ)
~h∗⋆v
∗(τ) dτ
=

u, ~h∗⋆v

.
The change can be justiﬁed using Fubini’s Theorem: we integrate |h(t−τ) u(τ) v∗(t)|
ﬁrst over τ to obtain (|h| ⋆|u|)(t) |v(t)|; we use Inequality (5.11) to conclude that
|h|⋆|u| is energy-limited; and we then use this and the hypothesis that v is energy-
limited to argue that the integral of t 	→(|h|⋆|u|)(t) |v(t)| is ﬁnite (Cauchy-Schwarz
Inequality).
The result above extends to stochastic processes. Since we only deal here with real
stochastic processes and real whitening ﬁlters, we consider the real case only. The
next lemma allows us to replace u in Lemma 26.10.6 with a SP X. We only need
the lemma for WSS stochastic processes, but we shall state it a bit more generally.
26.10 Detection in Colored Gaussian Noise
655
Lemma 26.10.7 (The Adjoint of the Convolution: Stochastic Processes). Let X
be a measurable SP for which the function t 	→E[|X(t)|] is bounded. Let h and v
be real, deterministic, integrable signals. Then
⟨X ⋆h, v⟩= ⟨X, ~h ⋆v⟩
with probability one.
(26.113)
Proof. The inner product ⟨X ⋆h, v⟩can also be expressed as

(X ⋆h) ⋆~v

(0).
Likewise, ⟨X, ~h ⋆v⟩can be expressed as the convolution, evaluated at zero, of X
with the mirror image of (~h⋆v), i.e., as

X⋆(h⋆~v)

(0). The result thus follows from
the associativity of the convolution for stochastic processes (Lemma 15.7.1).
Using Lemma 26.10.7 and the fact that the union of a ﬁnite number of events
having probability zero also has probability zero (Corollary 21.5.2 (i)), we conclude
that (26.111) holds with probability one. Hence, upon substituting φ’s for q’s,

Y ⋆h, φ1

, . . . ,

Y ⋆h, φd

=

Y, ~h ⋆φ1

, . . . ,

Y, ~h ⋆φd

(26.114)
with probability one. It thus suﬃces to prove the optimality of basing our guess
on

Y, ~h ⋆φ1

, . . . ,

Y, ~h ⋆φd

.
(26.115)
To prove this, we begin by exploring conditions that guarantee that the input to the
whitening ﬁlter be recoverable from its output. Such conditions are needed because
the ﬁlter’s frequency response could be zero outside the band [−W, W ], in which
case all input signals whose FT vanishes in this band produce the same all-zero
output, and their recovery from the ﬁlter’s output is thus impossible. However, if
the input is known to be bandlimited to W Hz, then recovery is possible:
Lemma 26.10.8 (Recovering the Input to a Whitening Filter from its Output).
Let h be the impulse response of a whitening ﬁlter for SNN with respect to the
bandwidth W. Let q be the ﬁlter’s response to the input s. If s is an integrable
signal that is bandlimited to W Hz, then it can be recovered from q via the relation
s(t) =
 W
−W
ˆq(f)
ˆh(f)
ei2πft df,
t ∈R.
(26.116)
Proof. Since q equals s ⋆h, where s is an integrable signal that is bandlimited
to W Hz and h is integrable, it follows that q is an integrable signal that is band-
limited to W Hz (Proposition 6.5.2) whose FT ˆq(f) is ˆs(f) ˆh(f) for all f ∈R
(Theorem 6.3.2). Since ˆh(f) is nonzero for all |f| ≤W (because h is a whitening
ﬁlter for SNN with respect to the bandwidth W) this implies—upon dividing by
ˆh(f) and restricting f to |f| ≤W—that
ˆs(f) = ˆq(f)
ˆh(f)
,
|f| ≤W.
(26.117)
656
Detection in White Gaussian Noise
Expressing s as the IFT of its FT (Proposition 6.4.10), we obtain
s(t) =
 W
−W
ˆs(f) ei2πft df
=
 W
−W
ˆq(f)
ˆh(f)
ei2πft df,
t ∈R.
We now come to the main result of this section.
Theorem 26.10.9 (Detecting Known Signals in Colored Gaussian Noise). Let M
take values in the ﬁnite set M = {1, . . . , M}, and let the mean signals s1, . . . , sM be
integrable signals that are bandlimited to W Hz. Let the conditional law of

Y (t)

given M = m be that of sm(t) + N(t), where

N(t)

is a stationary, measurable,
Gaussian SP of PSD SNN that can be whitened with respect to the bandwidth W.
Let h be the impulse response of a whitening ﬁlter for

N(t)

.
(i) If φ1, . . . , φd is an orthonormal basis for span(s1 ⋆h, . . . , sM ⋆h), then no
measurable decision rule for guessing M based on Y can outperform the best
rule that is based on the d-vector

Y, ~h ⋆φ1

, . . . ,

Y, ~h ⋆φd
T
.
(26.118)
Conditional on M = m, this vector is a Gaussian vector whose mean is

⟨sm ⋆h, φ1⟩, . . . , ⟨sm ⋆h, φd⟩
T
(26.119)
and whose covariance matrix is the d × d identity matrix Id.
(ii) If ˜q1, . . . , ˜qn are integrable signals for which
span(s1 ⋆h, . . . , sM ⋆h) ⊆span(˜q1, . . . , ˜qn),
(26.120)
then no measurable decision rule for guessing M based on Y can outperform
the best rule based on

Y, ~h ⋆˜q1

, . . . ,

Y, ~h ⋆˜qn
T
.
(26.121)
(iii) If each of the mean signals can be written as a linear combination of the
integrable signals ˜s1, . . . ,˜sn, then it is optimal to guess M based on the vector

Y, ~h ⋆˜s1 ⋆h

, . . . ,

Y, ~h ⋆˜sn ⋆h
T
.
(26.122)
(iv) No measurable decision rule for guessing M based on Y can outperform the
best decision rule based on the vector

Y, ~h ⋆q1

, . . . ,

Y, ~h ⋆qM
T
,
(26.123a)
26.10 Detection in Colored Gaussian Noise
657
where
qm = sm ⋆h,
m ∈M.
(26.123b)
Conditional on M = m, this vector is Gaussian with mean

⟨qm, q1⟩, . . . , ⟨qm, qM⟩
T
(26.124)
and M × M covariance matrix
⎛
⎜
⎜
⎜
⎝
⟨q1, q1⟩
⟨q1, q2⟩
· · ·
⟨q1, qM⟩
⟨q2, q1⟩
⟨q2, q2⟩
· · ·
⟨q2, qM⟩
...
...
...
...
⟨qM, q1⟩
⟨qM, q2⟩
· · ·
⟨qM, qM⟩
⎞
⎟
⎟
⎟
⎠,
(26.125)
where the inner product ⟨qm′, qm′′⟩can be expressed as
⟨qm′, qm′′⟩=
 W
−W
ˆsm′(f) ˆs∗
m′′(f)
1
SNN(f) df,
m′, m′′ ∈M.
(26.126)
Proof. We begin with Part (i). Let qm be the response of the whitening ﬁlter to
the mean signal sm:
qm = sm ⋆h,
m ∈M.
(26.127)
Since, by assumption, the mean signals are integrable signals that are bandlimited
to W Hz, it follows that so are the signals q1, . . . , qM (Proposition 6.5.2) and hence
also all the signals in span(q1, . . . , qM) including φ1, . . . , φd.
Since h is a whitening ﬁlter for SNN with respect to the bandwidth W, it is stable
h ∈L1,
(26.128a)
and
1
|ˆh(f)|2 = SNN(f),
|f| ≤W.
(26.128b)
The latter implies that
 W
−W
1
ˆh(f)
2 df < ∞
(26.128c)
because SNN, being a PSD, is integrable.
Since (φ1, . . . , φd) is an orthonormal basis for span(q1, . . . , qM),
qm =
d

ℓ=1
⟨qm, φℓ⟩φℓ,
m ∈M.
(26.129)
Deﬁne the signals g1, . . . , gd for every t ∈R as
gℓ(t) =
 W
−W
ˆφℓ(f)
ˆh(f)
ei2πft df,
ℓ∈{1, . . . , d}.
(26.130)
658
Detection in White Gaussian Noise
The integrability of φℓimplies that ˆφℓis bounded (Theorem 6.2.11), which com-
bines with (26.128c) to show that the integrand in (26.130) is square integrable.
Consequently, the signals g1, . . . , gd are energy-limited signals that are bandlimited
to W Hz (Proposition 6.4.5 (ii)). By (26.130) and Proposition 6.5.1
gℓ⋆h = φℓ,
ℓ∈{1, . . . , d}.
(26.131)
Let N′ be a measurable SP of the same law as N but independent of it. Thus,

N ′(t)

is independent of

N(t)

and is a measurable, centered, stationary, Gaus-
sian SP of PSD SNN. Deﬁne the SP
Y′ =
d

ℓ=1

Y, ~h ⋆φℓ

gℓ+ N′ −
d

ℓ=1

N′, ~h ⋆φℓ

gℓ.
(26.132)
We will show that, conditional on every message m ∈M, the FDDs of Y and Y′
are identical. This will show that there is no loss of optimality in basing our guess
of M on the vector (26.118) because the decoder can use its local randomness to
generate
N′ −
d

ℓ=1

N′, ~h ⋆φℓ

gℓ;
add this SP to
d

ℓ=1

Y, ~h ⋆φℓ

gℓ
(which it can synthesize because the vector (26.118) is at its disposal) to form
Y′; and feed Y′ to any guessing rule that was designed to base its decision on Y.
Since Y and Y′ have the same conditional FDDs, this would result in the same
performance as if Y and not Y′ were fed to the guessing device.
Conditional on M = m, the mean of Y is sm whereas the mean of Y′ is
d

ℓ=1

sm, ~h ⋆φℓ

gℓ.
We next show that the two are identical. Indeed, by (26.127) and Lemma 26.10.8
sm(t) =
 W
−W
ˆqm(f)
ˆh(f)
ei2πft df
=
 W
−W

ℓ⟨qm, φℓ⟩ˆφℓ(f)
ˆh(f)
ei2πft df
=
d

ℓ=1
⟨qm, φℓ⟩
 W
−W
ˆφℓ(f)
ˆh(f)
ei2πft df
=
d

ℓ=1
⟨qm, φℓ⟩gℓ(t)
=
d

ℓ=1

sm, ~h ⋆φℓ

gℓ(t),
t ∈R,
26.10 Detection in Colored Gaussian Noise
659
where the second equality follows by taking the FT of both sides of (26.129);
the third by swapping summation and integration; the fourth by the deﬁnition of
gℓ(26.130); and the ﬁfth by (26.127) and Lemma 26.10.6.
Having established that Y and Y′ have identical conditional means, we next show
that, conditional on M = m, they also have identical autocovariance functions.
Since both Y and Y′ are Gaussian (conditional on M = m), this will establish
that Y and Y′ have identical FDDs given M = m. Subtracting the means, we
need to show that the SP
d

ℓ=1

N, ~h ⋆φℓ

gℓ+ N′ −
d

ℓ=1

N′, ~h ⋆φℓ

gℓ
has the same autocovariance function as N, namely, KNN. Starting from the deﬁ-
nition of Y′ (26.132),
Cov

Y ′(t + τ), Y ′(t)
 M = m

= E
	
d

ℓ=1

N, ~h ⋆φℓ

gℓ(t + τ) + N ′(t + τ) −
d

ℓ=1

N′, ~h ⋆φℓ

gℓ(t + τ)

	
d

ℓ′=1

N, ~h ⋆φℓ′
gℓ′(t) + N ′(t) −
d

ℓ′=1

N′, ~h ⋆φℓ′
gℓ′(t)


=
d

ℓ=1
d

ℓ′=1
E

N, ~h ⋆φℓ

N, ~h ⋆φℓ′
gℓ(t + τ) gℓ′(t) + E

N ′(t + τ)N ′(t)

−
d

ℓ′=1
E

N ′(t + τ)

N′, ~h ⋆φℓ′
gℓ′(t) −
d

ℓ=1
E

N ′(t)

N′, ~h ⋆φℓ

gℓ(t + τ)
+
d

ℓ=1
d

ℓ′=1
E

N′, ~h ⋆φℓ

N′, ~h ⋆φℓ′
gℓ(t + τ) gℓ′(t)
= 2
d

ℓ=1
d

ℓ′=1
E

N, ~h ⋆φℓ

N, ~h ⋆φℓ′
gℓ(t + τ) gℓ′(t) + KNN(τ)
−
d

ℓ=1
E

N(t + τ)

N, ~h ⋆φℓ

gℓ(t) −
d

ℓ=1
E

N(t)

N, ~h ⋆φℓ

gℓ(t + τ),
(26.133)
where the second equality follows because N and N′ are independent and con-
sequently so are linear functionals of them (Proposition 25.15.8), and the third
because N and N′ have identical laws.
We next study the diﬀerent terms on the RHS of (26.133). We begin with the ﬁrst.
From Theorem 25.12.2 (ii) and the fact that h is real,
E
%
N, ~h ⋆φℓ

N, ~h ⋆φℓ′&
=
 ∞
−∞
SNN(f) ˆh∗(f) ˆφℓ(f) ˆh(f) ˆφ∗
ℓ′(f) df
=
 W
−W
SNN(f)
ˆh(f)
2 ˆφℓ(f) ˆφ∗
ℓ′(f) df
660
Detection in White Gaussian Noise
=
 W
−W
ˆφℓ(f) ˆφ∗
ℓ′(f) df
(26.134)
=

φℓ, φℓ′
= I{ℓ= ℓ′},
ℓ, ℓ′ ∈{1, . . . , d},
(26.135)
where the second equality holds because φ1, . . . , φd are integrable signals that are
bandlimited to W Hz; the third by (26.128b); the fourth by Parseval’s Theorem;
and the ﬁfth by the orthonormality of (φ1, . . . , φd). Consequently, for the ﬁrst
term on the RHS of (26.133)
2
d

ℓ=1
d

ℓ′=1
E

N, ~h ⋆φℓ

N, ~h ⋆φℓ′
gℓ(t + τ) gℓ′(t) = 2
d

ℓ=1
gℓ(t) gℓ(t + τ).
(26.136a)
As to the third term, we invoke Theorem 25.12.2 (ii) again to obtain
E
%
N(t + τ)

N, ~h ⋆φℓ
&
=
 ∞
−∞
SNN(f) ˆh∗(f) ˆφℓ(f) ei2πf(t+τ) df
=
 W
−W
SNN(f) ˆh∗(f) ˆφℓ(f) ei2πf(t+τ) df
=
 W
−W
ˆφℓ(f)
ˆh(f)
ei2πf(t+τ) df
= gℓ(t + τ),
where the second equality follows because φ1, . . . , φd are integrable signals that
are bandlimited to W Hz; the third follows from (26.128b); and the last equality
from (26.130). Consequently, for the third term on the RHS of (26.133)
d

ℓ=1
E

N(t + τ)

N, ~h ⋆φℓ

gℓ(t) =
d

ℓ=1
gℓ(t + τ) gℓ(t).
(26.136b)
For the fourth term we invoke similar arguments to obtain
E

N(t)

N, ~h ⋆φℓ

= gℓ(t),
and to thus conclude that
d

ℓ=1
E

N(t)

N, ~h ⋆φℓ

gℓ(t + τ) =
d

ℓ=1
gℓ(t) gℓ(t + τ).
(26.136c)
It now follows from (26.136) that the RHS of (26.133) equals KNN(τ), thus demon-
strating that conditional on M the autocovariance function of Y′ is KNN, and
hence concluding the proof that the conditional laws of Y and Y′ given M = m
are identical.
To conclude the proof of Part (i) it remains to compute the conditional law of
the vector (26.118) given M = m. Since (26.114) holds with probability one, this
conditional law is equal to the conditional law of

Y ⋆h, φ1

, . . . ,

Y ⋆h, φd
T
.
26.10 Detection in Colored Gaussian Noise
661
The conditional law of the latter can be easily computed, because the conditional
law of Y ⋆h given M = m is that of sm ⋆h + ˜N where ˜N is WGN of double-sided
PSD 1 with respect to W (Corollary 26.10.5), so the result follows directly from
Proposition 25.15.2 on linear functionals of WGN (with the substitution of 1 for
the double-sided PSD N0/2).
Part (ii) is a consequence of the linearity of the stochastic integral (Lemma 25.10.3).
It can be proved as follows. Let (φ1, . . . , φd) be, as before, an orthonormal ba-
sis for span(s1 ⋆h, . . . , sM ⋆h). Since, by assumption, each signal in the latter
subspace can be written as a linear combination of the signals ˜q1, . . . , ˜qn so can
the signals φ1, . . . , φd. Consequently, by the linearity of stochastic integration, the
vector (26.118) can be computed from (26.121). And since it is optimal to base our
guess of M on the former, it must also be optimal to base our guess on the latter.
Part (iii) follows from Part (ii) by deﬁning ˜qj as ˜sj ⋆h for every j ∈{1, . . . , n}.
As to Part (iv), the optimality of basing our guess on the vector (26.123) follows
from Part (iii), and its conditional law can be computed as follows: by Theo-
rem 25.12.1 it is conditionally Gaussian, and its conditional mean (26.124) and
conditional covariance (26.125) are readily derived using Theorem 25.12.2. To de-
rive (26.126), we note that by (26.123b) and Proposition 6.5.2 we have for every
m ∈M
qm(t) =
 W
−W
ˆsm(f) ˆh(f) ei2πft df,
t ∈R.
(26.137)
This, the Mini Parseval Theorem (Proposition 6.2.6 (i)), and (26.108) combine to
establish (26.126).
A diﬀerent way to compute the conditional distribution is to use (26.111): we
ﬁrst substitute M for n and q1, . . . , qM for q1, . . . , qn in (26.111) to infer that the
conditional law of the vector (26.123) is the same as the conditional law of

Y ⋆h, q1

, . . . ,

Y ⋆h, qM
T
.
(26.138)
We then compute the conditional law of the latter by noting that, conditional on
M = m, the law of Y ⋆h is that of sm ⋆h + ˜N, where ˜N is WGN of double-
sided PSD 1 with respect to W (Corollary 26.10.5), and by then invoking Propo-
sition 25.15.2 on linear functionals of WGN (with the substitution of 1 for the
double-sided PSD N0/2).
Since the conditional law of the random vector (26.123) is fully determined by the
inner products ⟨qm′, qm′′⟩for m′, m′′ ∈M (see (26.124) and (26.125)), and since,
by (26.126), the inner product ⟨qm′, qm′′⟩does not depend on the choice of the
whitening ﬁlter we obtain:
Note 26.10.10 (The Choice of the Whitening Filter Is Immaterial). Neither the
conditional distribution of the random vector in (26.123) nor the optimal proba-
bility of error depends on the choice of the whitening ﬁlter.
Using Theorem 26.10.9 we can now derive an optimal rule for guessing M. Indeed,
in analogy to Theorem 26.5.1 we have:
662
Detection in White Gaussian Noise
Theorem 26.10.11. Consider the setting of Theorem 26.10.9 with M of prior {πm}.
The decision rule that guesses uniformly at random from among all the messages
˜m ∈M for which
ln π ˜m −1
2
d

ℓ=1

Y, ~h ⋆φℓ

−

s ˜m, ~h ⋆φℓ
2
= max
m′∈M
1
ln πm′ −1
2
d

ℓ=1

Y, ~h ⋆φℓ

−

sm′, ~h ⋆φℓ
22
(26.139)
minimizes the probability of error whenever h is a whitening ﬁlter and the tuple
(φ1, . . . , φd) forms an orthonormal basis for span(s1 ⋆h, . . . , sM ⋆h).
26.10.4
Existence of A Whitening Filter
Before concluding our discussion of detection in the presence of colored noise we
derive here a suﬃcient condition for the existence of a whitening ﬁlter.
Proposition 26.10.12 (Existence of a Whitening Filter). Let W > 0 be ﬁxed. If
throughout the interval [−W, W ] the PSD SNN is strictly positive and twice con-
tinuously diﬀerentiable, then there exists a whitening ﬁlter for SNN with respect to
the bandwidth W.
Proof. The proof hinges on the following basic result from harmonic analysis
(Katznelson, 2004, Chapter VI, Section 1, Exercise 7): if a function f 	→g(f)
is twice continuously diﬀerentiable and is zero outside some interval [−Δ, Δ], then
it is the FT of some integrable function.
To prove the proposition using this result we begin by picking some Δ > W. We
now deﬁne a function g: R →R as follows.
For f ≥Δ, we deﬁne g(f) = 0.
For f in the interval [0, W], we deﬁne g(f) = 1/

SNN(f). And for f ∈(W, Δ),
we deﬁne g(f) so that g be twice continuously diﬀerentiable in [0, ∞). We can
thus think of g in [W, Δ] as an interpolation function whose values and ﬁrst two
derivatives are speciﬁed at the endpoints of the interval. Finally, for f < 0, we
deﬁne g(f) as g(−f). Figure 26.11 depicts SNN, g, W, and Δ.
A whitening ﬁlter for SNN with respect to the bandwidth W is the integrable
function whose FT is g and whose existence is guaranteed by the quoted result.
26.11
Multiple Antennas
In some applications the receiver can guess the message based on more than one
received waveform. For example, the receiver may employ a number of antennas,
each of which receives the desired signal contaminated by diﬀerent noise. More
generally, we consider nR received waveforms Y1, . . . , YnR and assume that con-
ditional on M = m, they are given by
Yη = sm,η + Nη,
η = 1, . . . , nR,
(26.140)
26.11 Multiple Antennas
663
W
Δ
−W
−Δ
f
g(f)
1

SNN(f)
SNN(f)
Figure 26.11: The frequency response of a whitening ﬁlter for the PSD SNN with
respect to the bandwidth W.
where the nR stochastic processes N1, . . . , NnR are independent stochastic pro-
cesses, each of which is WGN of spectral density N0/2 with respect to the band-
width W, and where sm,η—which is assumed to be an integrable signal that is
bandlimited to W Hz—is the mean-signal in the η-th antenna corresponding to the
m-th message.
Theorem 26.3.1 extends to this setup as follows:
Theorem 26.11.1 (Detection with Multiple Antennas). Consider the setup above
and assume that for each antenna η ∈{1, . . . , nR}, the dη signals φ1,η, . . . , φdη,η
form an orthonormal basis for
span(s1,η, . . . , sM,η).
Then, in the same sense as in Theorem 26.3.1, there is no loss of optimality in
guessing M based on the d1 + · · · + dnR inner products
'
Yη, φℓ,η
(
,
ℓ∈{1, . . . , dη}, η ∈{1, . . . , nR}.
Conditional on M = m, these inner products are independent Gaussians with
⟨Yη, φℓ,η⟩being
N
	
sm,η, φℓ,η

, N0
2

.
(26.141)
Proof. We only sketch the proof because it is just a small variation on that of
Theorem 26.3.1. Given any (measurable) decoder that is based on Y1, . . . , Yη we
propose a randomized decision rule of identical performance that is based on the
inner products only. The proposed decoder uses its local randomness to produce
nR independent WGN processes N′
1, . . . , N′
nR, where for each η ∈{1, . . . , nR} the
664
Detection in White Gaussian Noise
law of N′
η is identical to the law of Nη. Based on the inner products and on these
locally-generated WGN processes, it then produces the nR waveforms
Y′
η =
dη

ℓ=1
⟨Yη, φℓ,η⟩φℓ,η + N′
η −
dη

ℓ=1

N′
η, φℓ,η

φℓ,η,
η ∈{1, . . . , nR}, (26.142)
which it then feeds to the given rule.
26.12
Detecting Signals of Inﬁnite Bandwidth
So far we have only dealt with the detection problem when the mean signals are
bandlimited. What if they are not? The diﬃculty in this case is that we can no
longer assume that the noise PSD is constant over the bandwidth occupied by the
mean signals, or that the noise can be whitened with respect to this bandwidth.
We can address this issue in three diﬀerent ways. In the ﬁrst we can try to ﬁnd the
optimal detector by studying this more complicated hypothesis testing problem. It
will no longer be optimal to base our guess on the inner-products vector (26.12).
The optimal detector will greatly depend on the relationship between the rate of
decay of the PSD of the noise as the frequency tends to ±∞and the rate of decay of
the FT of the mean signals. This approach will often lead to bad designs, because
the structure of the receiver will depend greatly on how we model the noise, and
inaccuracies in our modeling of the noise PSD at ultra-high frequencies might lead
us completely astray in our design.
A more level-headed approach that is valid if the noise PSD is “essentially ﬂat
over the bandwidth of interest” is to ignore the fact that the mean signals are not
bandlimited and to base our decision on the inner-products vector, even if this is
not fully justiﬁed mathematically. This approach leads to robust designs that are
insensitive to inaccuracies in our modeling of the noise process. If the PSD is not
essentially ﬂat, we can whiten it with respect to a suﬃciently large band [−W, W ]
that contains most of the energy of the mean signals.
The third approach is to use very complicated mathematical machinery involving
the Itˆo Calculus (Karatzas and Shreve, 1991) to model the noise in a way that will
result in the optimality of basing our decision on the inner-products vector. We
have chosen not to pursue this approach because it requires modeling the noise as
a process of inﬁnite power, which is physically unappealing. This approach just
swaps one diﬃculty for another: the Itˆo Calculus can now prove for us that it is
optimal to base our decision on the inner-products vector, but we need a leap of
faith in modeling the noise as a process of inﬁnite power.
In the future, in dealing with mean signals that are not bandlimited, we shall refer
to the “white noise paradigm” as the paradigm under which the receiver forms
its decision based on the inner-products vector (26.12) and under which these inner
products have the conditional law derived in Section 26.4.1.
26.13 Exercises
665
26.13
Exercises
Exercise 26.1 (Monotonicity in the Noise). In the setup of Section 26.2 show that the
minimal probability of error that can be achieved in guessing M based on

Y (t)

is
nondecreasing in N0.
Exercise 26.2 (Reducing the Number of Matched Filters). Consider the setup of Sec-
tion 26.2, and let s0 be any integrable signal that is bandlimited to W Hz. Deﬁne d′ as the
dimension of span(s1 −s0, . . . , sM −s0). Exhibit a random d′-vector that is computable
from Y and on which we can base our guess without any loss of optimality. Show that d′
is sometimes smaller than d (the dimension of the subspace spanned by the mean signals).
Exercise 26.3 (Nearest-Neighbor Decoding Revisited). The form of the decoder in The-
orem 26.5.3 (ii) is diﬀerent from the nearest-neighbor rule of Proposition 21.6.1 (ii).
Why does minimizing ∥Y −sm∥2 not make mathematical sense in the setting of The-
orem 26.5.3?
Exercise 26.4 (Minimum Shift Keying). Let the signals s0, s1 be given at every t ∈R by
s0(t) =

2Es
Ts cos(2πf0t) I{0 ≤t ≤Ts},
s1(t) =

2Es
Ts cos(2πf1t) I{0 ≤t ≤Ts}.
(i) Compute the energies ∥s0∥2
2, ∥s1∥2
2. You may assume that f0Ts ≫1 and f1Ts ≫1.
(ii) Under what conditions on f0, f1, and Ts are s0 and s1 orthogonal?
(iii) Assume that the parameters are chosen as in Part (ii). Let H take on the values 0
and 1 equiprobably, and assume that, conditional on H = ν, the time-t received
waveform is sν(t) + N(t) where

N(t)

is WGN of double-sided PSD N0/2 with
respect to the bandwidth of interest, and ν ∈{0, 1}.
Find an optimal rule for
guessing H based on the received waveform.
(iv) Compute the optimal probability of error.
Exercise 26.5 (Signaling in White Gaussian Noise). Let the RV M take value in the set
M = {1, 2, 3, 4} uniformly. Conditional on M = m, the observed waveform

Y (t)

is
given at every time t ∈R by sm(t) + N(t), where the signals s1, s2, s3, s4 are given by
s1(t) = A I{0 ≤t ≤T},
s2(t) = A I{0 ≤t ≤T/2} −A I{T/2 < t ≤T},
s3(t) = 2A I{0 ≤t ≤T/2},
s4(t) = −A I{0 ≤t ≤T/2} + A I{T/2 < t ≤T},
and where

N(t)

is WGN of double-sided PSD N0/2 over the bandwidth of interest.
(Ignore the fact that the signals are not bandlimited.)
(i) Derive the MAP rule for guessing M based on

Y (t)

.
(ii) Use the Union-of-Events Bound to upper-bound pMAP(error|M = 3). Are all the
terms in the bound needed?
(iii) Compute pMAP(error|M = 3) exactly.
(iv) Show that by subtracting a waveform s∗from each of the signals s1, s2, s3, s4, we
can reduce the average transmitted energy without degrading performance. What
waveform s∗should be subtracted to minimize the transmitted energy?
666
Detection in White Gaussian Noise
Exercise 26.6 (QPSK). Let the IID random bits D1 and D2 be mapped to the symbols
X1, X2 according to the rule
(0, 0) →(1, 0),
(0, 1) →(−1, 0),
(1, 0) →(0, 1),
(1, 1) →(0, −1).
The received waveform

Y (t)

is given by
Y (t) = AX1 φ1(t) + AX2 φ2(t) + N(t),
t ∈R,
where A > 0, the signals φ1, φ2 are orthonormal integrable signals that are bandlimited
to W Hz, and the SP

N(t)

is independent of (D1, D2) and is WGN of double-sided PSD
N0/2 with respect to the bandwidth W.
(i) Find an optimal rule for guessing (D1, D2) based on

Y (t)

.
(ii) Find an optimal rule for guessing D1 based on

Y (t)

.
(iii) Compare the rule that you have found in Part (ii) with the rule that guesses that D1
is the ﬁrst component of the tuple produced by the decoder that you have found
in Part (i). Evaluate the probability of error for both rules.
(iv) Repeat when (D1, D2) are mapped to (X1, X2) according to the rule
(0, 0) →(1, 0),
(0, 1) →(0, 1),
(1, 0) →(−1, 0),
(1, 1) →(0, −1).
Exercise 26.7 (Mismatched Decoding of Antipodal Signaling). Let the received wave-
form

Y (t)

be given at every t ∈R by (1−2H) s(t)+N(t), where s is an integrable signal
that is bandlimited to W Hz,

N(t)

is WGN of double-sided PSD N0/2 with respect to
the bandwidth W, and H takes on the values 0 and 1 equiprobably and independently
of

N(t)

. Let s′ be an integrable signal that is bandlimited to W Hz. A suboptimal
detector feeds the received waveform to a matched ﬁlter for s′ and guesses according to
the ﬁlter’s time-0 output: if it is positive, it guesses “H = 0,” and if it is negative, it
guesses “H = 1.” Express this detector’s probability of error in terms of s, s′, and N0.
Exercise 26.8 (Imperfect Automatic Gain Control). Let the received signal

Y (t)

be
given by
Y (t) = AX s(t) + N(t),
t ∈R,
where A > 0 is some deterministic positive constant, X is a RV that takes value in the
set {−3, −1, +1, +3} uniformly, s is an integrable signal that is bandlimited to W Hz, and

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W.
(i) Find an optimal rule for guessing X based on

Y (t)

.
(ii) Using the Q-function compute the optimal probability of error.
(iii) Suppose you use the rule you have found in Part (i), but the received signal is
Y (t) = 3
4AX s(t) + N(t),
t ∈R.
(You were misinformed about the amplitude of the signal.) What is the probability
of error now?
26.13 Exercises
667
Exercise 26.9 (Early-Late Gate). Let H take on the values 0 and 1 equiprobably, and let
Y (t) = (1 −2H) s(t) + N(t),
t ∈R,
where s: t →A I{0 ≤t ≤T}, where A and T are positive numbers, and where

N(t)

is
WGN of double-sided PSD N0/2 with respect to the bandwidth of interest (white noise
paradigm). The observation

Y (t)

is fed to a matched ﬁlter for s, and the output of the
ﬁlter is sampled at times −T/2 and T/2. If the sum of the two samples is positive, then
the receiver guesses “H = 0”; otherwise it guesses “H = 1”.
(i) Express the probability of error for the above guessing rule using the Q-function in
terms of A, T, and N0. (Ignore the fact that the signal s is not bandlimited.)
(ii) Suppose that the synchronization between the transmitter and the receiver is not
perfect, and instead of sampling at times −T/2 and T/2, the output of the matched
ﬁlter is sampled at times −T/2 + τ and T/2 + τ, where −T/2 ≤τ ≤T/2. How does
this inﬂuence the probability of error?
(iii) Compute the optimal probability of error for guessing H based on

Y (t)

and
compare it to the result of Part (i).
Exercise 26.10 (More General Front-End Filters). Let the impulse response h ∈L1 be
invertible with respect to the band [−W, W ] in the sense that there exists some g ∈L1
such that ˆg(f) = 1/ˆh(f) for all |f| ≤W. Show that in the setup of Section 26.2 there is
no loss in optimality in basing our guess of M on the result of convolving

Y (t)

with h.
Exercise 26.11 (Positive Semideﬁnite Matrices).
(i) Let s1, . . . , sM be of ﬁnite energy.
Show that the M × M matrix whose Row-j
Column-ℓentry is ⟨sj, sℓ⟩is positive semideﬁnite.
(ii) Show that any M × M positive semideﬁnite matrix can be expressed in this form
with a proper choice of the signals s1, . . . , sM.
Exercise 26.12 (A Lower Bound on the Minimum Distance). Let s1, . . . , sM be equi-
energy signals of energy Es. Let
¯d2 ≜
1
M(M −1)

m′

m′′̸=m′
∥sm′ −sm′′∥2
2
denote the average squared-distance between the signals.
(i) Justify the following bound on ¯d:
¯d2 =
1
M(M −1)
M

m′=1
M

m′′=1
∥sm′ −sm′′∥2
2
=
2M
M −1Es −
2M
M −1
1
M2
M

m′=1
M

m′′=1
⟨sm′, sm′′⟩
=
2M
M −1Es −
2M
M −1

1
M
M

m=1
sm

2
2
≤
2M
M −1Es.
668
Detection in White Gaussian Noise
(ii) Show that if, in addition, ⟨sm′, sm′′⟩= ρEs for all m′ ̸= m′′ in {1, . . . , M}, then
−
1
M −1 ≤ρ ≤1.
(iii) Are equalities possible in the above bounds?
Exercise 26.13 (Generalizations of the Simplex). Let p∗(error; Es; ρ; M; N0) denote the
optimal probability of error for the setup of Section 26.2 for the case where the prior
on M is uniform and where
⟨sm′, sm′′⟩=

Es
if m′ = m′′,
ρEs
otherwise,
m′, m′′ ∈{1, . . . , M}.
Show that
p∗
error; Es; ρ; M; N0

= p∗
error; Es(1 −ρ); 0; M; N0

,
−
1
M −1 ≤ρ ≤1.
Hint: You may need a diﬀerent proof depending on the sign of ρ.
Exercise 26.14 (Decoding the Simplex without Gain Control). Let the simplex con-
stellation s1, . . . , sM be constructed from the orthonormal signals φ1, . . . , φM as in Sec-
tion 26.9.4. In that section we proposed to decode by adding
1
√
M −1
√
Esψ
to the received signal Y and then feeding the result to a decoder that was designed for
the orthogonal signals
s1 +
1
√
M −1
√
Esψ, . . . , sM +
1
√
M −1
√
Esψ.
Here ψ is any signal that is orthogonal to the signals {s1, . . . , sM}. Show that feeding the
signal Y+αψ to the above orthogonal-keying decoder also results in an optimal decoding
rule, irrespective of the value of α ∈R.
Exercise 26.15 (Pretending the Noise Is White). Let H take on the values 0 and 1
equiprobably, and let the received waveform

Y (t)

be given at time t by
Y (t) = (1 −2H) s(t) + N(t),
where s: t →I{0 ≤t ≤1}, and where the SP

N(t)

is independent of H and is a
measurable, centered, stationary, Gaussian SP of autocovariance function
KNN(τ) = 1
4αe−|τ|/α,
τ ∈R,
where 0 < α < ∞is some deterministic real parameter. Compute the probability of error
of a detector that guesses “H = 0” whenever
 1
0
Y (t) dt ≥0.
To what does this probability of error converge when α tends to zero?
26.13 Exercises
669
Exercise 26.16 (Antipodal Signaling in Colored Noise). Let s be an integrable signal that
is bandlimited to W Hz, and let H take on the values 0 and 1 equiprobably. Let the time-t
value of the received signal

Y (t)

be given by (1 −2H) s(t) + N(t), where

N(t)

is a
measurable, centered, stationary, Gaussian SP of PSD SNN. Assume that H and

N(t)

are independent, and that SNN can be whitened with respect to the bandwidth W. Find
the optimal probability of error in guessing H based on

Y (t)

.
Exercise 26.17 (Modeling Artifacts). Let H take on the values 0 and 1 equiprobably, and
let the received signal

Y (t)

be given by
Y (t) = (1 −2H) s(t) + N(t),
t ∈R,
where s: t →I{0 ≤t ≤1} and the SP

N(t)

is independent of H and is a measurable,
centered, stationary, Gaussian SP of autocovariance function
KNN(τ) = α e−τ2/β,
τ ∈R,
for some α, β > 0.
Argue heuristically that—irrespective of the values of α and β—for any ϵ > 0 we can ﬁnd
a rule for guessing H based on

Y (t)

whose probability of error is smaller than ϵ.
Hint: Study ˆs(f) and SNN(f) at high frequencies f.
Exercise 26.18 (Comparing Noises). Consider two channels of the kind considered in
Section 26.10. In the ﬁrst the PSD of the noise is S1 and in the second S2. Show that if
S1(f) ≥S2(f) for every f ∈R, then any performance that can be achieved in the ﬁrst
can also be achieved in the second.
Exercise 26.19 (Measurability in Theorem 26.3.1).
(i) Let

N(t)

be WGN of double-sided PSD N0/2 with respect to the bandwidth W.
Let R be a unit-mean exponential RV that is independent of

N(t)

. Deﬁne the SP
˜
N(t) = N(t) I{t ̸= R},
t ∈R.
Show that
 ˜
N(t)

is WGN of double-sided PSD N0/2 with respect to the band-
width W.
(ii) Let s be a nonzero integrable signal that is bandlimited to W Hz. To be concrete,
s(t) = sinc2(Wt),
t ∈R.
Suppose that the SP

N(t)

is as above and that for every ω ∈Ω the sample-path
t →N(ω, t) is continuous. Construct
 ˜
N(t)

as above. Suppose you wish to test
whether you are observing s or −s in the additive noise
 ˜
N(t)

. Show that you can
guess with zero probability of error by ﬁnding an epoch where the observed SP is
discontinuous and by comparing the value of the received signal at that epoch to
the value of s. (This does not violate Theorem 26.3.1 because this decision rule is
not measurable with respect to the Borel σ-algebra generated by the observed SP.)
Chapter 27
Noncoherent Detection and Nuisance
Parameters
27.1
Introduction and Motivation
This chapter discusses a detection problem that arises in noncoherent communi-
cations.
For motivation, consider a transmitter that sends one of two diﬀerent
passband waveforms
t 	→2 Re

s0,BB(t) ei2πfct
or
t 	→2 Re

s1,BB(t) ei2πfct
,
where s0,BB and s1,BB are integrable baseband signals that are bandlimited to W/2
Hz, and where the carrier frequency fc satisﬁes fc > W/2. To motivate our problem
it is instructive to consider the case where
fc ≫W.
(27.1)
(In wireless communications it is common for fc to be orders-of-magnitude larger
than W.)
Let X(t) denote the transmitted waveform at time t.
Suppose that
the received waveform

Y (t)

is a delayed version of the transmitted waveform
corrupted by WGN of double-sided PSD N0/2 with respect to the bandwidth W
around the carrier frequency fc (Deﬁnition 25.15.9):
Y (t) = X(t −tD) + N(t),
t ∈R,
where tD denotes the delay (typically proportional to the distance between the
transmitter and the receiver) and

N(t)

is the additive noise. Suppose further
that the receiver estimates the delay to be t′
D and moves its clock back by deﬁning
t′ ≜t −t′
D.
(27.2)
If ˜Y (t′) is what the receiver receives when its clock shows t′, then by (27.2)
˜Y (t′) = Y (t′ + t′
D)
= X(t′ + t′
D −tD) + N(t′ + t′
D)
= X(t′ + t′
D −tD) + ˜N(t′),
t′ ∈R,
670
27.1 Introduction and Motivation
671
where ˜N(t′) ≜N(t′ + t′
D) and is thus, by the stationarity of

N(t)

, also WGN of
double-sided PSD N0/2 with respect to the bandwidth W around fc. The term
X(t′ + t′
D −tD) can be more explicitly written for every t′ ∈R as
X(t′ + t′
D −tD) = 2 Re

sν,BB(t′ + t′
D −tD) ei2πfc(t′+t′
D−tD)
,
(27.3)
where ν is either zero or one, depending on which waveform is sent.
We next argue that if
t′
D −tD
 ≪1
W,
(27.4)
then
sν,BB(t′ + t′
D −tD) ≈sν,BB(t′),
t′ ∈R.
(27.5)
This can be seen by considering a Taylor Series expansion for sν,BB(·) around t′
sν,BB(t′ + t′
D −tD) ≈sν,BB(t′) + dsν,BB(τ)
dτ

τ=t′

t′
D −tD

and by then using Bernstein’s Inequality (Theorem 6.7.1) to heuristically argue
that the magnitude of the derivative of the baseband signal is of the same order of
magnitude as W, so its product by the timing error is, by (27.4), negligible.
From (27.3) and (27.5) we obtain that, as long as (27.4) holds,
X(t′ + t′
D −tD) ≈2 Re

sν,BB(t′) ei2πfc(t′+t′
D−tD)
= 2 Re

sν,BB(t′) ei(2πfct′+θ)
,
t′ ∈R,
(27.6a)
where
θ = 2πfc(t′
D −tD)
mod [−π, π).
(27.6b)
(Recall that ξ mod [−π, π) is the element in the interval [−π, π) that diﬀers from ξ
by an integer multiple of 2π.) Note that even if (27.4) holds, the term 2πfc(t′
D−tD)
may be much larger than 1 when fc ≫W.
We conclude that if the error in estimating the delay is negligible compared to the
reciprocal of the signal bandwidth but signiﬁcantly larger than the reciprocal of
the carrier frequency, then the received waveform can be modeled as
˜Y (t′) = 2 Re

sν,BB(t′) ei(2πfct′+θ)
+ ˜N(t′),
t′ ∈R,
(27.7)
where the receiver needs to determine whether ν is equal to zero or one;
 ˜N(t′)

is additive WGN of double-sided PSD N0/2 with respect to the bandwidth W
around fc; and where the phase θ is unknown to the receiver. Since the phase is
unknown to the receiver, the detection is said to be noncoherent. In the statistics
literature an unknown parameter such as θ is called a nuisance parameter.
It would make engineering sense to ask for a decision rule for guessing ν based
on
 ˜Y (t′)

that would work well irrespective of the value of θ, but this is not the
question we shall ask. This question is related to “composite hypothesis testing,”
672
Noncoherent Detection and Nuisance Parameters
which is not treated in this book.1 Instead we shall adopt a probabilistic approach.
We shall assume that θ is a random variable—and therefore henceforth denote it
by Θ and its realization by θ—that is uniformly distributed over the interval [−π, π)
independently of the noise and the message, and we shall seek a decision rule that
has the smallest average probability of error. Thus, if we denote the probability
of error conditional on Θ = θ by p(error|θ), then we seek a decision rule based
on
 ˜Y (t)

that minimizes
1
2π
 π
−π
p(error|θ) dθ.
(27.8)
The conservative reader may prefer to minimize the probability of error on the
“worst case θ”
sup
θ∈[−π,π)
p(error|θ)
(27.9)
but, miraculously, it will turn out that the decoder we shall derive to minimize (27.8)
has a conditional probability of error p(error|θ) that does not depend on the real-
ization θ so, as we shall see in Section 27.7, our decoder also minimizes (27.9).
27.2
The Setup
We next deﬁne our hypothesis testing problem.
We denote time by t and the
received waveform by

Y (t)

(even though in the scenario we described in Sec-
tion 27.1 these correspond to t′ and
 ˜Y (t′)

, i.e., to the time coordinate and to the
corresponding signal at the receiver). We denote the RV we wish to guess by H
and assume a uniform prior:
Pr[H = 0] = Pr[H = 1] = 1
2.
(27.10)
For each ν ∈{0, 1} the observation

Y (t)

is, conditionally on H = ν, of the form
Y (t) = Sν(t) + N(t),
t ∈R,
(27.11)
where

N(t)

is WGN of positive double-sided PSD N0/2 with respect to the
bandwidth W around the carrier frequency fc (Deﬁnition 25.15.9), and where Sν(t)
can be described as
Sν(t) = 2 Re

sν,BB(t) ei(2πfct+Θ)
= 2 Re

sν,BB(t) ei2πfct
cos Θ −2 Im

sν,BB(t) ei2πfct
sin Θ
= 2 Re

sν,BB(t) ei2πfct
cos Θ + 2 Re

i sν,BB(t) ei2πfct
sin Θ
= sν,c(t) cos Θ + sν,s(t) sin Θ,
t ∈R,
(27.12)
where Θ is a RV that is uniformly distributed over the interval [−π, π) indepen-
dently of

H,

N(t)

, and where we deﬁne for ν ∈{0, 1}
sν,c(t) ≜2 Re

sν,BB(t) ei2πfct
,
t ∈R,
(27.13a)
sν,s(t) ≜2 Re

i sν,BB(t) ei2πfct
,
t ∈R.
(27.13b)
1See, for example, (Lehmann and Romano, 2005, Chapter 3).
27.3 From a SP to a Random Vector
673
Notice that by (27.13) and by the relationship between inner products in baseband
and passband (Theorem 7.6.10),
⟨sν,c, sν,s⟩= 0,
ν = 0, 1.
(27.14)
We assume that the baseband signals s0,BB, s1,BB are integrable complex signals
that are bandlimited to W/2 Hz and that they are orthogonal:
⟨s0,BB, s1,BB⟩= 0.
(27.15)
Consequently, by (27.13) and Theorem 7.6.10,
⟨s0,c, s1,c⟩= ⟨s0,s, s1,c⟩= ⟨s0,c, s1,s⟩= ⟨s0,s, s1,s⟩= 0.
(27.16)
We ﬁnally assume that the baseband signals s0,BB and s1,BB are of equal positive
energy:
∥s0,BB∥2
2 = ∥s1,BB∥2
2 > 0.
(27.17)
Deﬁning2
Es = 2 ∥s0,BB∥2
2
(27.18)
we have by the relationship between energy in baseband and passband (Theo-
rem 7.6.10)
Es = ∥S0∥2
2 = ∥S1∥2
2 = ∥s0,s∥2
2 = ∥s0,c∥2
2 = ∥s1,s∥2
2 = ∥s1,c∥2
2 .
(27.19)
By (27.14), (27.16), and (27.18)
1
√Es

s0,c, s0,s, s1,c, s1,s

is an orthonormal 4-tuple.
(27.20)
Our problem is to guess H based on the observation

Y (t)

.
27.3
From a SP to a Random Vector
To derive an optimal guessing rule, we begin by exhibiting a random 4-vector, which
is computable from the observed SP

Y (t)

and on which we can base our decision
without loss of optimality. This will transform the problem from one where the
observation is a SP to one where it is a random 4-vector. (For the latter problem
we shall later exhibit a suﬃcient statistic vector with only two components.) We
denote random vector by T and its four components by T0,c, T0,s, T1,c, and T1,s:
T =

T0,c, T0,s, T1,c, T1,s
T.
We denote its realization by t with corresponding components
t =

t0,c, t0,s, t1,c, t1,s
T.
2The “s” in Es stands for “signal,” whereas the “s” in s0,s and s1,s stands for “sine.”
674
Noncoherent Detection and Nuisance Parameters
The vector T is deﬁned by
T ≜
	 #
Y, s0,c
√Es
$
,
#
Y, s0,s
√Es
$
,
#
Y, s1,c
√Es
$
,
#
Y, s1,s
√Es
$ 
T
(27.21)
=
1
√Es
	  ∞
−∞
Y (t) s0,c(t) dt, . . . ,
 ∞
−∞
Y (t) s1,s(t) dt

T
.
(27.22)
We now prove that there is no loss of optimality in guessing H based on T. It is
interesting to note that this holds for Θ of arbitrary distribution (not necessarily
uniform) provided that the pair (H, Θ) is independent of the additive noise. More-
over, it holds even if the baseband signals s0,BB and s1,BB are not orthogonal. The
key is that, irrespective of the realization of Θ and of the value of ν, the signal Sν
lies in the four-dimensional subspace spanned by the signals s0,c, s0,s, s1,c, and s1,s.
The result thus follows from a generalization of Theorem 26.3.1 that we state next.
Theorem 27.3.1 (White Gaussian Noise with Nuisance Parameters). Let V be
a d-dimensional subspace of the space of all integrable signals that are bandlimited
to W Hz, and let (φ1, . . . , φd) be an orthonormal basis for V. Let the RV M take
values in a ﬁnite set M. Suppose that, conditional on M = m, the SP

Y (t)

is
given by
Y (t) =
d

ℓ=1
A(ℓ)φℓ(t) + N(t),
t ∈R,
(27.23)
where A = (A(1), . . . , A(d))T is a random d-vector whose law typically depends
on m, and where the SP

N(t)

is WGN with respect to the bandwidth W and
independent of the pair (M, A). Then no measurable rule for guessing M based on

Y (t)

can outperform an optimal rule for guessing M based on the d-vector
T =

⟨Y, φ1⟩, . . . , ⟨Y, φd⟩
T.
(27.24)
The theorem also holds in passband, i.e., if V is a d-dimensional subspace of the
space of all integrable signals that are bandlimited to W Hz around the carrier
frequency fc and if

N(t)

is WGN with respect to the bandwidth W around fc.
Note 27.3.2. Theorem 27.3.1 continues to hold even if (φ1, . . . , φd) is not or-
thonormal; it suﬃces that it be a basis for V. Indeed, if (u1, . . . , ud) is a basis
(not necessarily orthonormal) for V and if (v1, . . . , vd) is another basis (possibly
orthonormal) for V, then the inner products {⟨Y, vℓ⟩}d
ℓ=1 are computable from the
inner products {⟨Y, uℓ⟩}d
ℓ=1 (Lemma 25.10.3).
Before presenting the proof of Theorem 27.3.1 we show how it can be applied to
the noncoherent detection problem at hand. Here d = 4 and
V = span(s0,c, s0,s, s1,c, s1,s),
(27.25)
with φ1 ≜s0,c/√Es, φ2 ≜s0,s/√Es, φ3 ≜s1,c/√Es, and φ4 ≜s1,s/√Es. We note
that, conditional on H = 0, the received waveform

Y (t)

can be written in the
27.4 The Conditional Law of the Random Vector
675
form (27.23) where A(3) & A(4) are deterministically zero and the pair

A(1), A(2)
is uniformly distributed over the unit circle:

A(1)2 +

A(2)2 = 1.
Similarly, conditional on H = 1, the random variables A(1) and A(2) are deter-
ministically zero and the pair

A(3), A(4)
is uniformly distributed over the unit
circle. Thus, once we prove Theorem 27.3.1, it will follow that there is no loss of
optimality in forming our guess based on the 4-vector in (27.22).
Proof of Theorem 27.3.1. The proof is similar to the proof of Theorem 26.3.1.
We prove the result by showing that, given any measurable guessing rule φGuess(·)
that is based on Y, we can ﬁnd a randomized decision rule of identical performance
that is based on T. As in the proof of Theorem 26.3.1, our decision rule uses local
randomness to generate a SP N′ of the same FDDs as N and independent of
(M, A, N). From T and N′ it then generates
Y′ =
d

ℓ=1
⟨Y, φℓ⟩φℓ+ N′ −
d

ℓ=1
⟨N′, φℓ⟩φℓ
(27.26)
and produces φGuess(Y′). To see that our rule has the same performance as the rule
that guesses φGuess(Y), we note that, as in the proof of Theorem 26.3.1, Y and Y′
have identical conditional FDDs given (M = m, A = a).
Consequently, upon
taking the expectation over A, we obtain that they also have the same conditional
FDDs given M = m. Hence the guesses φGuess(Y) and φGuess(Y′) have identical
performance.
27.4
The Conditional Law of the Random Vector
Having established in the previous section that there is no loss of optimality in
basing our guess on the vector T in (27.21), we next proceed to calculate its
conditional distribution given H. This will allow us to compute the likelihood-
ratio fT|H=0(t)/fT|H=1(t) and to thus obtain an optimal guessing rule.
Rather than computing the conditional distribution directly, we begin with the
simpler conditional distribution of T given (H, Θ).
Conditional on (H, Θ), the
vector T is Gaussian (Theorem 25.12.1). Consequently, to compute its conditional
distribution we only need to compute its conditional mean vector and covariance
matrix, which we proceed to do.
Conditional on (H, Θ) = (ν, θ), the observed
process

Y (t)

can be expressed as
Y (t) = sν,c(t) cos θ + sν,s(t) sin θ + N(t),
t ∈R.
(27.27)
Hence, since

N(t)

is of zero mean, we have from (27.22) and (27.20)
E

T
 (H, Θ) = (0, θ)

=

Es

cos θ, sin θ, 0, 0
T
,
(27.28a)
E

T
 (H, Θ) = (1, θ)

=

Es

0, 0, cos θ, sin θ
T
,
(27.28b)
676
Noncoherent Detection and Nuisance Parameters
as we next calculate. The calculation is a bit tedious because we need to compute
the conditional mean of each of four random variables conditional on each of two
hypotheses, thus requiring eight calculations, which are all very similar but not
identical. We shall carry out only one calculation:
E

T0,c
 (H, Θ) = (0, θ)

=
1
√Es

⟨s0,c cos θ + s0,s sin θ, s0,c⟩+ E[⟨N, s0,c⟩]

=
1
√Es
⟨s0,c cos θ + s0,s sin θ, s0,c⟩
=
1
√Es

∥s0,c∥2
2 cos θ + ⟨s0,s, s0,c⟩sin θ

=

Es cos θ,
where the ﬁrst equality follows from (27.27); the second because

N(t)

is of zero
mean (Proposition 25.10.1); the third from the linearity of the inner product and
by writing ⟨s0,c, s0,c⟩as ∥s0,c∥2
2; and the ﬁnal equality from (27.20).
We next compute the conditional covariance matrix of T given (H, Θ) = (ν, θ). By
the orthonormality (27.20) and the whiteness of the noise (Proposition 25.15.2) we
have that, irrespective of ν and θ, this conditional covariance matrix is given by
the 4 × 4 matrix (N0/2)I4, where I4 is the 4 × 4 identity matrix.
Using the explicit form of the Gaussian distribution (19.6) and deﬁning
σ2 ≜N0
2 ,
(27.29)
we can thus write the conditional density as
fT|H=0,Θ=θ(t)
=
1
(2πσ2)2 exp
	
−1
2σ2

t0,c −

Es cos θ
2 +

t0,s −

Es sin θ
2 + t2
1,c + t2
1,s

=
1
(2πσ2)2 exp

−Es
2σ2 −t0 + t1
2

× exp
 1
σ2

Es t0,c cos θ + 1
σ2

Es t0,s sin θ

,
t ∈R4,
(27.30)
where the second equality follows by opening the squares, by using the identity
cos2 θ + sin2 θ = 1, and by deﬁning
T0 ≜T 2
0,c + T 2
0,s
σ2
,
t0 ≜t2
0,c + t2
0,s
σ2
,
(27.31a)
T1 ≜T 2
1,c + T 2
1,s
σ2
,
t1 ≜t2
1,c + t2
1,s
σ2
.
(27.31b)
(We deﬁne T0 and T1 not only to simplify the typesetting: it will turn out that the
vector (T0, T1)T forms a suﬃcient statistic for guessing H based on T.)
27.4 The Conditional Law of the Random Vector
677
To derive fT|H=0(t) (unconditioned on Θ) we can integrate out Θ. Thus, for every
t =

t0,c, t0,s, t1,c, t1,s
T in R4
fT|H=0(t) =
 π
−π
fΘ|H=0(θ) fT|H=0,Θ=θ(t) dθ
=
 π
−π
fΘ(θ) fT|H=0,Θ=θ(t) dθ
= 1
2π
 π
−π
fT|H=0,Θ=θ(t) dθ
=
1
(2πσ2)2 e−Es/(2σ2) e−(t1+t2)/2
× 1
2π
 π
−π
exp
	 1
σ2

Es t0,c cos θ + 1
σ2

Es t0,s sin θ

dθ
=
1
(2πσ2)2 e−Es/(2σ2) e−(t0+t1)/2
× 1
2π
 π
−π
exp
-4
Es
σ2
√t0 cos

θ −arctan(t0,s/t0,c)
.
dθ
=
1
(2πσ2)2 e−Es/(2σ2) e−(t0+t1)/2
× 1
2π
 π−arctan(t0,s/t0,c)
−π−arctan(t0,s/t0,c)
exp
-4
Es
σ2
√t0 cos ψ
.
dψ
=
1
(2πσ2)2 e−Es/(2σ2) e−(t0+t1)/2 1
2π
 π
−π
exp
-4
Es
σ2
√t0 cos ψ
.
dψ
=
1
(2πσ2)2 e−Es/(2σ2) e−(t1+t0)/2 I0
-4
Es
σ2
√t0
.
,
(27.32)
where the ﬁrst equality follows by averaging out Θ; the second because Θ and H
are independent; the third because Θ is uniform; the fourth by the explicit form of
fT|H=0,Θ=θ(t) (27.30); the ﬁfth by the trigonometric identity
α cos θ + β sin θ =

α2 + β2 cos

θ −arctan(β/α)

;
(27.33)
the sixth by the change of variable ψ ≜θ −arctan(t0,s/t0,c); the seventh from
the periodicity of the cosine function; and the ﬁnal equality by recalling that the
zeroth-order modiﬁed Bessel function I0(·) is deﬁned by
I0(ξ) ≜1
2π
 π
−π
eξ cos φ dφ
(27.34)
= 1
π
 π
0
eξ cos φ dφ
= 1
π
 π/2
0

eξ cos φ + e−ξ cos φ
dφ,
ξ ∈R.
(27.35)
678
Noncoherent Detection and Nuisance Parameters
By symmetry,
fT|H=1(t) =
1
(2πσ2)2 e−Es/(2σ2) e−(t0+t1)/2 I0
-4
Es
σ2
√t1
.
,
t ∈R4.
(27.36)
27.5
An Optimal Detector
By (27.32) and (27.36), the likelihood-ratio is given by
fT|H=0(t)
fT|H=1(t) =
I0
	3
Es
σ2
√t0

I0
	3
Es
σ2
√t1

,
t ∈R4,
(27.37)
which is computable from t0 and t1. This proves that the pair (T0, T1) deﬁned in
(27.31) forms a suﬃcient statistic for guessing H based on T (Deﬁnition 20.12.2).
Having identiﬁed (T0, T1) as a suﬃcient statistic, we now proceed to derive an
optimal decision rule using two diﬀerent methods.
The ﬁrst method, which is
summarized in (20.79), ignores the fact that (T0, T1) is suﬃcient and proceeds to
base the decision on the likelihood-ratio of T (27.37). The second method, which
is summarized in (20.80), bases the decision on the likelihood-ratio of the pair
(T0, T1).
Method 1:
Since we assumed a uniform prior (27.10), an optimal decision rule
is to guess “H = 0” whenever fT|H=0(t)/fT|H=1(t) ≥1, which, by (27.37) is
equivalent to
Guess “H = 0” if
I0
-4
Es
σ2
√t0
.
≥I0
-4
Es
σ2
√t1
.
.
(27.38)
This rule can be further simpliﬁed by noting that I0(ξ) is (strictly) increasing in ξ
for ξ ≥0. (This can be veriﬁed by computing the derivative from (27.35)
d I0(ξ)
dξ
= 1
π
 π/2
0
cos φ

eξ cos φ −e−ξ cos φ
dφ
and by noting that for ξ > 0 the integrand is positive for all φ ∈(0, π/2).) Conse-
quently, the function ξ 	→I0
√ξ

is also (strictly) increasing and the guessing rule
(27.38) is thus equivalent to the rule
Guess “H = 0” if t0 ≥t1.
(27.39)
In terms of the observable

Y (t)

this can be paraphrased using (27.31) and (27.22)
as guessing “H = 0” whenever
	 ∞
−∞
Y (t) Re

s0,BB(t) ei2πfct
dt

2
+
	 ∞
−∞
Y (t) Re

i s0,BB(t) ei2πfct
dt

2
≥
	 ∞
−∞
Y (t) Re

s1,BB(t) ei2πfct
dt

2
+
	 ∞
−∞
Y (t) Re

i s1,BB(t) ei2πfct
dt

2
.
27.5 An Optimal Detector
679
Method 2:
We can obtain the same result by considering the likelihood-ratio
function of the suﬃcient statistic (T0, T1)
fT0,T1|H=0(t0, t1)
fT0,T1|H=1(t0, t1).
We begin by arguing that, conditional on H = 0, the random variables T0, T1,
and Θ are independent with
fT0,T1,Θ|H=0(t0, t1, θ) = 1
2π fχ2
2,λ1(t0) fχ2
2,λ0(t1),
(27.40)
where fχ2
n,λ(x) denotes the density at x of the noncentral χ2 distribution with n
degrees of freedom and noncentrality parameter λ (Section 19.8.2), and where
λ0 = 0
and
λ1 = Es
σ2 .
(27.41)
To prove (27.40) we compute for every t0, t1 ∈R and θ ∈[−π, π)
fT0,T1,Θ|H=0(t0, t1, θ) = fΘ|H=0(θ) fT0,T1|H=0,Θ=θ(t0, t1)
= 1
2π fT0,T1|H=0,Θ=θ(t0, t1)
= 1
2π fT0|H=0,Θ=θ(t0) fT1|H=0,Θ=θ(t1)
= 1
2π fχ2
2,λ1(t0) fχ2
2,λ0(t1),
where the ﬁrst equality follows from the deﬁnition of the conditional density; the
second because Θ is independent of H and is uniformly distributed over the interval
[−π, π); the third because, conditional on (H, Θ) = (0, θ), the random variables
T0,c, T0,s, T1,c, T1,s are independent (Section 27.4), and because T0 is a function
of (T0,c, T0,s) whereas T1 is a function of (T1,c, T1,s) (see (27.31)); and the ﬁnal
equality follows because, conditional on (H, Θ) = (0, θ), the random variables
T0,c, T0,s, T1,c, T1,s are variance-σ2 Gaussians with means speciﬁed in (27.28a) (Sec-
tion 19.8.2).
Integrating out θ in (27.40) we obtain that, conditional on H, the random variables
T0 and T1 are independent with
fT0,T1|H=0(t0, t1) = fχ2
2,λ1(t0) fχ2
2,λ0(t1)
(27.42a)
fT0,T1|H=1(t0, t1) = fχ2
2,λ0(t0) fχ2
2,λ1(t1),
(27.42b)
where the expression for fT0,T1|H=1(t0, t1) is obtained using analogous steps.
Since H has a uniform prior, an optimal decision rule is thus to guess “H = 0”
whenever
fχ2
2,λ1(t0) fχ2
2,λ0(t1) ≥fχ2
2,λ0(t0) fχ2
2,λ1(t1).
Since λ1 > λ0, this will hold, by Proposition 19.8.3, whenever t0 ≥t1. And by the
same proposition the inequality
fχ2
2,λ1(t0) fχ2
2,λ0(t1) ≤fχ2
2,λ0(t0) fχ2
2,λ1(t1)
680
Noncoherent Detection and Nuisance Parameters
will hold whenever t0 ≤t1. It is thus optimal to guess “H = 0” whenever t0 ≥t1
and to guess “H = 1” whenever t0 < t1. (It does not matter how we guess when
t0 = t1.) The decision rule (27.39) has thus been recovered.
27.6
The Probability of Error
In this section we compute the probability of error for the optimal guessing rule
(27.39). Since the probability of a tie (i.e., of T0 = T1) is zero both conditional on
H = 0 and conditional on H = 1, we shall analyze a slightly simpler guessing rule
that guesses “H = 0” if T0 > T1, and guesses “H = 1” if T1 > T0.
We begin with the conditional probability of error given that H = 0, i.e., with
Pr[T1 ≥T0 |H = 0]. Conditional on H = 0, the question of whether our decoder
errs depends prima facie not only on the realization of the additive noise

N(t)

but also on the realization of Θ. But this is not the case because, conditionally on
H = 0, the pair (T0, T1) is independent of Θ (see (27.40)), so the realization of Θ
does not play a role in the sense that for every θ ∈[−π, π)
Pr

T1 ≥T0
 H = 0, Θ = θ

= Pr

T1 ≥T0
 H = 0, Θ = 0

.
(27.43)
Conditional on (H, Θ) = (0, θ) we have by (27.40) that T0 and T1 are independent
with T0 ∼χ2
2,λ1 and with T1 ∼χ2
2,λ0, i.e., with T1 having a mean-2 exponential
distribution (Note 19.8.1)
fT1|H=0,Θ=θ(t1) = 1
2 e−t1/2,
t1 ≥0.
Consequently, for every θ ∈[−π, π) and ξ ≥0,
Pr

T1 ≥ξ
 H = 0, Θ = θ

=
 ∞
ξ
1
2 e−t/2 dt = e−ξ/2.
(27.44)
Starting with (27.43) we now have for every θ ∈[−π, π)
Pr

T1 ≥T0
 H = 0, Θ = θ

= Pr

T1 ≥T0
 H = 0, Θ = 0

=
 ∞
0
fT0|H=0,Θ=0(t0) Pr

T1 ≥t0
 H = 0, Θ = 0, T0 = t0

dt0
=
 ∞
0
fT0|H=0,Θ=0(t0) Pr

T1 ≥t0
 H = 0, Θ = 0

dt0
=
 ∞
0
fT0|H=0,Θ=0(t0) e−t0/2 dt0
= E
%
esT0
 H = 0, Θ = 0
&
s=−1/2
= Mχ2
2,Es/σ2(s)

s=−1/2
= 1
2 e−Es
4σ2 ,
(27.45)
27.7 Discussion
681
where the ﬁrst equality follows from (27.43); the second from (26.82); the third
because conditional on H = 0 (and Θ = 0) the random variables T0 and T1
are independent; the fourth from (27.44); the ﬁfth by expressing

fZ(z) g(z) dz as
E[g(Z)] (with g(·) the exponential function); the sixth by the deﬁnition of the MGF
(19.23) and because, conditional on H = 0 and Θ = 0, we have that T0 ∼χ2
2,Es/σ2;
and the ﬁnal equality from the explicit expression for the MGF of a χ2
2,Es/σ2 RV,
i.e., from (19.46) with the substitution n = 2 for the number of degrees of freedom,
λ = Es/σ2 for the noncentrality parameter, and s = −1/2.
By symmetry we also have for every θ ∈[−π, π)
Pr

T0 ≥T1
 H = 1, Θ = θ

= 1
2 e−Es
4σ2 .
(27.46)
Thus, if we denote by pMAP(error|Θ = θ) the conditional probability of error of
our decoder conditional on Θ = θ, then by the uniformity of the prior (27.10) and
by (27.45) & (27.46)
pMAP(error|Θ = θ)
= Pr[H = 0] pMAP(error|H = 0, Θ = θ) + Pr[H = 1] pMAP(error|H = 1, Θ = θ)
= 1
2 Pr

T1 ≥T0
 H = 0, Θ = θ

+ 1
2 Pr

T0 ≥T1
 H = 1, Θ = θ

= 1
2 e−Es
4σ2 ,
θ ∈[−π, π).
(27.47)
Integrating (27.47) over θ yields the optimal unconditional probability of error
p∗(error) = 1
2 e−Es
4σ2 .
(27.48)
Using (27.29), this can also be expressed as
p∗(error) = 1
2 e−Es
2N0 .
(27.49)
27.7
Discussion
The detector we derived has the property that its error probability does not depend
on the realization of the nuisance parameter Θ; see (27.47). This property makes
the detector robust with respect to the distribution of Θ: since the conditional
probability of error does not depend on the realization of Θ, neither does the
average performance depend on the distribution of Θ.
(Of course, if Θ is not
uniform, then our decoder need not be optimal.)
We next show that our guessing rule is also conservative in the sense that it mini-
mizes the worst-case performance:
sup
θ∈[−π,π)
p(error|Θ = θ).
682
Noncoherent Detection and Nuisance Parameters
That is, for any guessing rule of conditional error probability p′(error|Θ = θ)
sup
θ∈[−π,π)
p′(error|Θ = θ) ≥
sup
θ∈[−π,π)
pMAP(error|Θ = θ) = 1
2 e−Es
4σ2 .
(27.50)
Thus, while other decoders may outperform our decoder for some realizations of Θ,
for other realizations their probability of error will be at least as high. Indeed, if
p′(error|Θ = θ) is the conditional probability of error associated with any guessing
rule, then
sup
θ∈[−π,π)
p′(error|Θ = θ) ≥1
2π
 π
−π
p′(error|Θ = θ) dθ
≥1
2π
 π
−π
pMAP(error|Θ = θ) dθ
=
sup
θ∈[−π,π)
pMAP(error|Θ = θ) dθ
= 1
2 e−Es
4σ2 ,
where the ﬁrst inequality follows because the average (over θ) cannot exceed the
supremum; the second inequality because the decoder we designed minimizes the
unconditional probability of error; and the last two equalities follow from (27.47),
i.e., from the fact that the conditional probability of error pMAP(error|Θ = θ) of
our decoder does not depend on θ and is equal to the RHS of (27.47).
It is interesting to assess the degradation in performance due to our ignorance
of Θ. To that end we now compare the performance of our decoder with that of
the “coherent decoder.” The coherent decoder is an optimal decoder for the setting
where the realization of Θ is known to the receiver, i.e., when the receiver can form
its guess based on both

Y (t)

and Θ. If the receiver knows Θ, it can compute
S0 and S1, and the problem reduces to the problem of guessing which of the two
known signals is being observed in WGN. Irrespective of Θ, the signals S0 and S1
are orthogonal and of energy Es (by (27.12), (27.16), and (27.19)). Consequently,
the coherent guessing problem is the binary version of the problem we discussed in
Section 26.9.3. An optimal coherent guessing rule is thus
guess “H = 0” if
 ∞
−∞
Y (t) S0(t) dt >
 ∞
−∞
Y (t) S1(t) dt
with optimal probability of error (see (26.87))
p∗
coherent(error|Θ = θ) = Q
-4
Es
2σ2
.
≈
1

πEs/σ2 exp

−Es
4σ2

,
Es
σ2 ≫1,
(27.51)
where the approximation follows from (19.18). Integrating over θ we obtain
p∗
coherent(error) ≈
1

πEs/σ2 exp

−Es
4σ2

,
Es
σ2 ≫1.
(27.52)
27.8 Extension to M ≥2 Signals
683
Comparing (27.52) with (27.48) we see that if Es/σ2 is large, then we pay only
a small penalty for not knowing the phase.3 Of course, if the phase were known
precisely we might have used antipodal signaling with the resulting probability of
error being lower; see (26.66).4
27.8
Extension to M ≥2 Signals
We next brieﬂy address the M-ary version of the problem of noncoherent detec-
tion of orthogonal signals. We now denote the RV to be guessed by M and re-
place (27.10) with the assumption that M is uniformly distributed over the set
M = {1, . . . , M}, where M ≥2.
We wish to guess the value of M based on
the observation

Y (t)

(27.11), where ν now takes values in M and where the
orthogonality conditions (27.15) & (27.18) are now written as
⟨sν′,BB, sν′′,BB⟩= 1
2 Es I{ν′ = ν′′},
ν′, ν′′ ∈M.
(27.53)
We ﬁrst argue that no measurable decision rule that is based on Y can outperform
an optimal rule for guessing M based on the vector

T1, . . . , TM
T,
(27.54)
where, in analogy to (27.31), we deﬁne
Tν = T 2
ν,c + T 2
ν,s
σ2
,
ν ∈M,
and where
Tν,c =
#
Y, sν,c
√Es
$
and
Tν,s =
#
Y, sν,s
√Es
$
,
ν ∈M.
To this end, we ﬁrst note that, by Theorem 27.3.1, we can base our decision on the
collection
'
(Tν,c, Tν,s)
(
ν∈M.
(27.55)
To show that (T1, . . . , TM)T forms a suﬃcient statistic for guessing M based on this
collection, it is enough to show pairwise suﬃciency (Proposition 22.3.2). Pairwise
suﬃciency can be proved using Proposition 22.4.2 because for every m′ ̸= m′′
in M our analysis of the binary problem shows that the tuple (Tm′, Tm′′) forms
a suﬃcient statistic for testing between m′ and m′′, and this tuple is computable
from the vector in (27.54).
Our analysis of the binary case shows that, after observing

Y (t)

, the a posteriori
probability of the event M = m is larger than the a posteriori distribution of the
3Although p∗(error)/p∗
coherent(error) tends to inﬁnity as Es/σ2 →∞, it does so only subex-
ponentially.
4Comparing (26.87) and (26.66) we see that, to achieve the same probability of error, binary
orthogonal keying requires twice as much energy as antipodal signaling.
684
Noncoherent Detection and Nuisance Parameters
event M = m′ whenever Tm > Tm′. Consequently, Message m has the highest a
posteriori probability if Tm = maxm′∈M Tm′. Thus, the decision rule
Guess “M = m” if Tm = max
m′∈M Tm′
(27.56)
is optimal. The probability of a tie is zero, so it does not matter how ties are
resolved.
We next turn to the analysis of the probability of error. We shall assume that
a tie results in an error, so, conditional on M = m, an error occurs whenever
max{T1, . . . , Tm−1, Tm+1, . . . , TM} ≥Tm.
We ﬁrst show that, as in the binary
case, the probability of error associated with this guessing rule depends neither on
the realization of Θ nor on the message, i.e., that for every m ∈M and θ ∈[−π, π)
pMAP(error|M = m, Θ = θ) = pMAP(error|M = 1, Θ = 0).
(27.57)
To see this note that, conditional on (M, Θ) = (m, θ), the components of the vec-
tor (27.54) are independent, with the m-th component being χ2
2,Es/σ2 and with the
other components being χ2
2,0. Consequently, irrespective of θ and m, the condi-
tional probability of error is the probability that a χ2
2,Es/σ2 RV is exceeded by, or
is equal to, at least one of M −1 IID χ2
2,0 random variables that are independent
of it. In the analysis of the probability of error we shall thus assume that M = 1
and that θ = 0.
The probability that the maximum among the random variables T2, . . . , TM exceeds
or is equal to ξ is given for every ξ ≥0 by
Pr

max{T2, . . . , TM} ≥ξ
 M = 1, Θ = 0

= 1 −Pr

max{T2, . . . , TM} < ξ
 M = 1, Θ = 0

= 1 −Pr

T2 < ξ, . . . , TM < ξ
 M = 1, Θ = 0

= 1 −

Pr

T2 < ξ
 M = 1, Θ = 0
M−1
= 1 −

1 −e−ξ/2M−1
= 1 −
M−1

j=0
(−1)j
	M −1
j

e−jξ/2,
(27.58)
where the ﬁrst equality follows because the probabilities of an event and of its
complement sum to one; the second because the maximum is smaller than ξ if,
and only if, all the random variables are smaller than ξ; the third because, con-
ditionally on M = 1 and Θ = 0, the random variables T2, . . . , TM are IID; the
fourth because conditional on M = 1 and Θ = 0, the RV T2 is a mean-2 exponen-
tial (Note 19.8.1); and the ﬁnal equality follows from the binomial formula (26.85)
with the substitution a = 1, b = −e−ξ/2, and n = M −1. The probability of error
is thus:
27.9 Exercises
685
Pr

max{T2, . . . , TM} ≥T1
 M = 1, Θ = θ

= Pr

max{T2, . . . , TM} ≥T1
 M = 1, Θ = 0

=
 ∞
0
fT1|M=1,Θ=0(t1) Pr

max{T2, . . . , TM} ≥t1
 M = 1, Θ = 0, T1 = t1

dt1
=
 ∞
0
fT1|M=1,Θ=0(t1) Pr

max{T2, . . . , TM} ≥t1
 M = 1, Θ = 0

dt1
=
 ∞
0
fT1|M=1,Θ=0(t1)
	
1 −
M−1

j=0
(−1)j
	M −1
j

e−jt1/2

dt1
= 1 −
M−1

j=0
(−1)j
	M −1
j

  ∞
0
fT1|M=1,Θ=0(t1) e−jt1/2 dt1
= 1 −
M−1

j=0
(−1)j
	M −1
j

E
%
esT1
 M = 1, Θ = 0
&
s=−j/2
= 1 −
M−1

j=0
(−1)j
	M −1
j

Mχ2
2,Es/σ2 (s)

s=−j/2
= 1 −
M−1

j=0
(−1)j
	M −1
j

1
j + 1 e−
j
j+1
Es
2σ2 ,
where the justiﬁcations are very similar to the justiﬁcations of (27.45) except that
we use (27.58) instead of (27.44). Denoting the probability of error by p∗(error)
and noting that for j = 0 the summand is 1, we have
p∗(error) =
M−1

j=1
(−1)j+1
	M −1
j

1
j + 1 e−
j
j+1
Es
2σ2 ,
(27.59)
or, upon recalling that σ2 was deﬁned in (27.29) as N0/2,
p∗(error) =
M−1

j=1
(−1)j+1
	M −1
j

1
j + 1 e−
j
j+1
Es
N0 .
(27.60)
27.9
Exercises
Exercise 27.1 (The Conditional Law of the Random Vector). Conditional on M = m,
are the components of the random vector T in Theorem 27.3.1 independent? What about
conditional on (M, A) = (m, a) for m ∈M and a ∈Rd?
Exercise 27.2 (A Silly Design Criterion). Let ˜p(error|Θ = θ) denote the conditional
probability of error given Θ = θ of some decision rule for the setup of Section 27.2. Show
that
inf
−π≤θ<π ˜p(error|Θ = θ) ≥Q
)
Es
N0
*
.
Can you think of a detector that achieves this bound with equality? Would you recom-
mend using it?
686
Noncoherent Detection and Nuisance Parameters
Exercise 27.3 (A Coherent Detector for an Incoherent Channel). Alice designs a coherent
detector for the setup of Section 27.2 by pretending that Θ is deterministically equal to
zero and by then using the results on the detection of known signals in white Gaussian
noise. Show that if her detector is used over our channel where Θ ∼U

[−π, π)

, then the
resulting average probability of error (averaged over Θ) is 1/2.
Exercise 27.4 (Noncoherent Antipodal Signaling). Show that if in the setup of Sec-
tion 27.2 the baseband signals s0,BB and s1,BB—rather than orthogonal—are antipodal
in the sense that s0,BB = −s1,BB, then the optimal probability of error is 1/2.
Exercise 27.5 (A Fading Scenario). Consider the setup of Section 27.2 but with (27.11)
replaced by Y (t) = ASν(t) + N(t), where A is a Rayleigh RV that is independent of

H, Θ,

N(t)

. Find an optimal detector and the associated probability of error when A
is observed by the receiver. Repeat when A is unobserved.
Exercise 27.6 (Uniform Phase Noise Is the Worst Phase Noise). Consider the setup of
Section 27.2 but with Θ not necessarily uniformly distributed over [−π, π). Show that
the optimal probability of error is upper-bounded by the optimal probability of error
corresponding to the case where Θ ∼U

[−π, π)

.
Exercise 27.7 (Unknown Frequency-Selective Channel). Let H take on the values 0 and 1
equiprobably, and let s be an integrable signal that is bandlimited to W Hz. When H = 0
the transmitted signal is s, and when H = 1 it is −s. Let U take on the values {up, down}
equiprobably and independently of H. When U = up the transmitted signal is passed
through a stable ﬁlter of impulse response hu; when U = down it is passed through
a stable ﬁlter of impulse response hd. At the receiver, white Gaussian noise

N(t)

of
PSD N0/2 over the bandwidth W is added to the received signal. The noise is independent
of (H, U). Based on the received waveform

Y (t)

, the receiver wishes to guess H. The
receiver has no knowledge of the realization of the switch U.
(i) Find a two-dimensional suﬃcient statistic vector (T1, T2)T for this problem.
(ii) Find a decision rule that minimizes the probability of error.
Express your rule
using the function φ(x, y; σ2
x, σ2
y, ρ), which is the value at the point (x, y) of the
joint density of the zero-mean jointly Gaussian random variables X, Y of variances
σ2
x and σ2
y and covariance E[XY ] = σxσyρ.
Exercise 27.8 (Noncoherent Detection with Two Antennas). Consider the setup of Sec-
tion 27.2 but with the signal now received at two antennas. Denote the received signals
by

Y1(t)

and

Y2(t)

Y1(t) = 2 Re
	
sν,BB(t) ei(2πfct+Θ1)
+ N1(t),
t ∈R,
Y2(t) = 2 Re
	
sν,BB(t) ei(2πfct+Θ2)
+ N2(t),
t ∈R,
where the additive white noises

N1(t)

and

N2(t)

at the two antennas are independent.
(i) Suppose that the random phase at the two antennas Θ1 and Θ2 are unknown but
identical. Find an optimal detector and the optimal probability of error.
(ii) Assume now that Θ1 and Θ2 are independent. Find an optimal guessing rule for H.
27.9 Exercises
687
Exercise 27.9 (Antenna Selection). Let X take on the values ±1 equiprobably, and let the
transmitted signal be X s, where s is an energy-Es integrable signal that is bandlimited
to W Hz. The transmitted signal is received with two antennas: the ﬁrst receives Y1 =
A1X s + N1 and the second Y2 = A2X s + N2.
Here N1 and N2 are independent
white Gaussian noise processes of double-sided PSD N0/2 with respect to W; the positive
random variables A1 and A2 are IID, each taking values in a ﬁnite subset A of the positive
reals according to the probability mass function PA(·) of cumulative distribution function
FA(·); and the pair (A1, A2) is independent of (N1, N2). Deﬁne T1 =

Y1, s/ ∥s∥2

and
T2 =

Y2, s/ ∥s∥2

. The receiver wishes to guess X based on (A1, A2, T1, T2).
(i) Find an optimal rule for guessing X based on (A1, A2, T1, T2). What is its prob-
ability of error? For which β does e−βEs/N0 best approximate this probability of
error when Es/N0 ≫1?
(ii) The antenna-selection rule is to decide according to the sign of T1 when A1 ≥A2
and according to the sign of T2 otherwise. What is the probability of error of this
rule, and for which β does e−βEs/N0 best approximate it when Es/N0 ≫1?
(iii) Repeat for the decision rule that ignores A1, A2 and decides according to the sign
of T1 + T2.
Exercise 27.10 (Unknown Polarity). Consider the setup of Section 27.2 but with Θ now
taking on the values −π and 0 equiprobably.
(i) Find an optimal decision rule for guessing H.
(ii) Bob suggests accounting for the random phase as follows. Pretend that the trans-
mitted signal is drawn uniformly from the set {±s0,c, ±s1,c} and that it is observed
in white Gaussian noise. Feed the received signal to an optimal receiver for guessing
which of these four signals is being observed in white Gaussian noise, and if the
receiver produces the guess “s0,c” or “−s0,c”, declare “H = 0”; otherwise declare
“H = 1”. Is Bob’s receiver optimal?
Exercise 27.11 (Additional Channel Randomness). Consider the setup of Section 27.2
but when the observed SP

Y (t), t ∈R

, rather than being given by (27.11), is now given
by
Y (t) = Sν(t) + AN(t),
t ∈R,
where A is a positive RV that is independent of

H, Θ,

N(t)

. Find an optimal decision
rule when A is observed. Repeat when A is not observed.
Exercise 27.12 (Mismatched Noncoherent Detection). Suppose that the signal fed to
the detector of Section 27.5 is
2 Re
	
uBB(t) ei(2πfct+Θ)
+ N(t),
t ∈R,
where uBB is an integrable signal that is bandlimited to W/2 Hz and that is orthogonal
to s0,BB, and where the other quantities are as deﬁned in Section 27.2. Compute the
probability that the detector produces the guess “H = 0.” Express your answer in terms
of the inner product ⟨uBB, s1,BB⟩, the energy in uBB, and N0.
Chapter 28
Detecting PAM and QAM Signals in White
Gaussian Noise
28.1
Introduction and Setup
In Chapter 26 we addressed the problem of detecting one of M bandwidth-W sig-
nals corrupted by additive Gaussian noise that is white with respect to the band-
width W. Except for assuming that the mean signals are integrable signals that
are bandlimited to W Hz, we made no assumptions about their structure. In this
chapter we study the implication of the results of Chapter 26 for Pulse Amplitude
Modulation, where the mean signals correspond to diﬀerent possible outputs of a
PAM modulator. The conclusions we shall draw are extremely important for the
design of receivers for systems employing PAM.
The key result of this chapter is that when PAM signals are observed in WGN, there
is no loss of optimality in the receiver basing its guess on the inner products between
the received waveform and the time shifts of the pulse shape by integer multiples of
the baud period Ts. These inner products can be computed by feeding the received
waveform to a matched ﬁlter that is matched to the pulse shape deﬁning the PAM
signals and by then sampling the ﬁlter’s output at integer multiples of the baud
period Ts (Corollary 5.8.3).
Using this result we can reduce the guessing problem from one where the observa-
tion is a continuous-time stochastic process to one where it is a discrete-time SP.
In fact, since we shall only consider the problem of detecting a ﬁnite number of
data bits, the reduction will be to a ﬁnite number of random variables. This will
justify the canonical structure of a PAM receiver where the received continuous-
time waveform is fed to a matched ﬁlter whose sampled output is then used by the
decision circuitry to produce its guess. We shall derive the results ﬁrst for PAM
and then brieﬂy describe their extension to QAM in Section 28.5.
In our setup k data bits D1, . . . , Dk are mapped by an encoder ϕ: {0, 1}k →Rn to
the real symbols X1, . . . , Xn, which are then mapped to the transmitted waveform
X(t) = A
n

ℓ=1
Xℓg(t −ℓTs),
t ∈R,
(28.1)
where A > 0 is a scaling constant; Ts > 0 is the baud period; and g(·) is the pulse
688
28.2 A Random Vector and Its Conditional Law
689
shape, which is assumed to be a real integrable signal that is bandlimited to W
Hz. The received waveform

Y (t)

is
Y (t) = X(t) + N(t)
= A
n

ℓ=1
Xℓg(t −ℓTs) + N(t),
t ∈R,
(28.2)
where

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W
and is independent of the data bits D1, . . . , Dk and hence also of

X(t)

. Based
on the received waveform

Y (t)

we wish to guess the data bits D1, . . . , Dk.
To simplify the typesetting we stack the k data bits D1, . . . , Dk in a binary vector
D = (D1, . . . , Dk)T,
(28.3)
the n symbols X1, . . . , Xn in a vector
X = (X1, . . . , Xn)T,
(28.4)
and we write
X = ϕ(D).
(28.5)
We express the n-tuple ϕ(d) that the encoder produces when D = d as
ϕ(d) =

x1(d), . . . , xn(d)
T,
(28.6)
where xν(d) is the ν-th real symbol that the encoder produces when it is fed d.
We denote the waveform that is transmitted to convey d by x(·; d), so
x(t; d) = A
n

ℓ=1
xℓ(d) g(t −ℓTs),
t ∈R.
(28.7)
Conditional on D = d, the received waveform is thus
Y (t) = x(t; d) + N(t),
t ∈R.
(28.8)
28.2
A Random Vector and Its Conditional Law
We can view the vector D = (D1, . . . , Dk)T as a message and view the 2k diﬀerent
values it can take as the set of messages. To promote this view we deﬁne
D ≜{0, 1}k
(28.9)
to be the set of all 2k binary k-tuples, and we view D as the set of possible mes-
sages. In Chapter 21 on multi-hypothesis testing we denoted the set of hypotheses
(messages) by M and we labeled them 1, . . . , M, but we never attached a meaning
to the labels. So there is no harm in now labeling the hypotheses (messages) by
the binary k-tuples.
690
Detecting PAM and QAM Signals in White Gaussian Noise
Associated with every message d ∈D is its prior πd
πd = Pr[D = d]
= Pr[D1 = d1, . . . , Dk = dk],
d ∈D.
(28.10)
If we assume that the data bits are IID random bits (Deﬁnition 14.5.1), then
πd = 2−k for every k-tuple d ∈D, but this assumption is inessential to our
derivation.
Conditional on D = d, the transmitted waveform is x(·; d) of (28.7). Thus, the
problem of guessing D is equivalent to guessing which of the 2k signals
'
t 	→x(t; d)
(
d∈D
(28.11)
is being observed in WGN of double-sided PSD N0/2 with respect to the band-
width W. From (28.7) it follows that for every message d ∈D the transmitted
waveform t 	→x(t; d) is a (deterministic) linear combination of the n functions
{t 	→g(t −ℓTs)}n
ℓ=1. Moreover, if the pulse shape g(·) is an integrable function
that is bandlimited to W Hz, then so is each waveform t 	→x(t; d) . Consequently,
from Theorem 26.3.1 (ii) and Theorem 26.4.1 (ii) we obtain:
Proposition 28.2.1 (Reducing the Output to a Random Vector: PAM in WGN).
Let the conditional law of

Y (t)

given D = d be given by (28.5), (28.7), and (28.8),
where the pulse shape g is a real integrable signal that is bandlimited to W Hz, and
where

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W.
Then no measurable rule for guessing D based on

Y (t)

can outperform an optimal
rule for guessing D based on the n inner products
T (ℓ) =
 ∞
−∞
Y (t) g(t −ℓTs) dt,
ℓ∈{1, . . . , n}.
(28.12)
Moreover, conditional on D = d, the vector T = (T (1), . . . , T (n))T is a Gaussian
n-vector whose ℓ-th component T (ℓ) is of conditional mean
E

T (ℓ)  D = d

= A
n

ℓ′=1
xℓ′(d) Rgg

(ℓ−ℓ′)Ts

,
ℓ∈{1, . . . , n}
(28.13)
and whose conditional covariance matrix is
N0
2
⎛
⎜
⎜
⎜
⎜
⎝
Rgg(0)
Rgg(Ts)
· · ·
Rgg

(n −1)Ts

Rgg(Ts)
Rgg(0)
· · ·
Rgg

(n −2)Ts

· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
Rgg

(n −1)Ts

Rgg

(n −2)Ts

· · ·
Rgg(0)
⎞
⎟
⎟
⎟
⎟
⎠
,
(28.14)
i.e.,
Cov

T (ℓ′), T (ℓ′′)  D = d

= N0
2 Rgg

(ℓ′ −ℓ′′)Ts

,
ℓ′, ℓ′′ ∈{1, . . . , n}.
(28.15)
Here Rgg is the self-similarity function of the real pulse shape g (Deﬁnition 11.2.1),
and (x1(d), . . . , xn(d))T = ϕ(d) is the real n-tuple to which d is encoded.
28.3 Other Optimality Criteria
691
Proof. This follows directly from Theorem 26.3.1 (ii) and Theorem 26.4.1 (ii) upon
substituting the mapping t 	→g(t −ℓTs) for ˜sj and upon computing the inner
product

t 	→g(t −ℓTs), t 	→g(t −ℓ′Ts)

= Rgg

(ℓ−ℓ′)Ts

,
ℓ, ℓ′ ∈Z.
28.3
Other Optimality Criteria
Proposition 28.2.1 establishes that if our objective is to minimize the probability of
a message error, then there is no loss of optimality in basing our guess on the vector
of inner products T = (T (1), . . . , T (n))T. But what if our objective is diﬀerent? We
shall next consider other design criteria and show that, for those too, there is no
loss of optimality in basing our guess on T. Readers who can recall the proof of
Theorem 26.3.1 on which Proposition 28.2.1 is based will surely not be surprised.
We ﬁrst elaborate on what a message error is. If we denote our guess by
˜d =
 ˜d1, . . . , ˜dk
T,
then a message error occurs if our guess diﬀers from the message d in at least one
component, i.e., if ˜dℓ̸= dℓfor some ℓ∈{1, . . . , k}. The probability of a message
error is thus
Pr
 ˜D ̸= D

.
(28.16)
Designing the receiver to minimize the probability of a message error is reason-
able, for example, when the k data bits constitute a computer ﬁle, and we wish to
minimize the probability that the ﬁle is corrupted. In such applications the user’s
primary concern is whether the ﬁle was successfully received (no error occurred)
or whether it was corrupted (at least one error occurred). Minimizing the proba-
bility of a message error corresponds to minimizing the probability that the ﬁle is
corrupted.
In other applications, engineers are more interested in the average probability
of a bit error or bit error rate (BER). That is, they may wish to minimize
1
k
k

j=1
Pr
 ˜Dj ̸= Dj

.
(28.17)
To better appreciate the diﬀerence between the BER (28.17) and the probability
of a message error (28.16), deﬁne the RV
Ej = I
 ˜Dj ̸= Dj

,
j ∈{1, . . . , k},
which indicates whether the j-th bit was incorrectly decoded.
Minimizing the
probability of a message error minimizes
Pr

k

j=1
Ej > 0

,
692
Detecting PAM and QAM Signals in White Gaussian Noise
whereas minimizing the BER minimizes
1
k E

k

j=1
Ej

.
(28.18)
Thus, minimizing the probability of a message error is equivalent to minimizing
the probability that one or more of the data bits are in error, whereas minimizing
the BER is equivalent to minimizing the expected number of data bits in error.
We next argue that there is no loss of optimality in basing our guess on T also
when we wish to minimize the BER (28.17). We ﬁrst note that to minimize (28.17)
we should choose for each j ∈{1, . . . , k} our guess ˜Dj to minimize
Pr
 ˜Dj ̸= Dj

.
That is, we should consider the binary hypothesis testing problem of guessing
whether Dj is equal to zero or one, and we should guess ˜Dj to minimize the
probability of error associated with this problem. To conclude our argument we
next show that for the purpose of minimizing Pr
 ˜Dj ̸= Dj

, there is no loss of
optimality in basing our decision on T.
To prove this we will show that for every measurable guessing rule φGuess(·) for
guessing Dj based on Y, we can ﬁnd a randomized decision rule based on T of
identical probability of error. As in the proof of Theorem 26.3.1, the randomized
decision rule uses its local randomness and T to generate a SP Y′ whose conditional
FDDs given D are identical to those of Y. It then produces the guess φGuess(Y′).
Since the conditional FDDs of Y and Y′ given D = d are identical for every d ∈D,
Pr
%
φGuess(Y) ̸= Dj
 D = d
&
= Pr
%
φGuess(Y′) ̸= Dj
 D = d
&
,
d ∈D.
(28.19)
Since this is true for every d ∈D, we can take the expectation over D to obtain
Pr
%
φGuess(Y) ̸= Dj
&
= Pr
%
φGuess(Y′) ̸= Dj
&
,
(28.20)
i.e., that the randomized decoder that is based on T has the same performance as
φGuess(·).
This argument can be applied also to more general performance measures:
Proposition 28.3.1. Consider the setup of Proposition 28.2.1. Let ψ: d 	→ψ(d)
be any function of the data bits, and let D have an arbitrary prior.
Then no
measurable guessing rule for guessing ψ(D) based on

Y (t)

can outperform an
optimal rule for guessing ψ(D) based on

T (1), . . . , T (n)
.
Proof. The proof is almost identical to the one we used above to treat the bit-
error-rate criterion. Once again we consider any measurable decision rule φGuess(·)
for guessing ψ(D), and we mimic its performance by guessing φGuess(Y′), where Y′
is generated using local randomness from T. Analogously to (28.19),
Pr
%
φGuess(Y) ̸= ψ(D)
 D = d
&
= Pr
%
φGuess(Y′) ̸= ψ(D)
 D = d
&
,
d ∈D.
(28.21)
from which the result follows by taking expectation over D.
28.4 Consequences of Orthonormality
693
D1, D2,
. . .
, DK,
enc(·)
X1, X2,
. . .
, XN,
enc(D1, . . . , DK)
DK+1,
. . .
, D2K,
enc(·)
XN+1,
. . .
, X2N,
enc(DK+1, . . . , D2K)
, Dk−K+1, . . . , Dk
enc(·)
, Xn−N+1,
. . .
, Xn
enc(Dk−K+1, . . . , Dk)
Figure 28.1: Block-mode encoding.
The examples we have seen so far correspond to the case where ψ: d 	→d (with the
probability of guessing ψ(D) incorrectly corresponding to a message error) and the
case ψ: d 	→dj (with the probability of guessing ψ(D) incorrectly corresponding
to the probability that the j-th bit Dj is incorrectly decoded). Another useful
example is when ψ: d 	→

dν, . . . , dν′
for some given ν, ν′ ∈N satisfying ν′ ≥ν.
This situation corresponds to the case where (Dν, . . . , Dν′) constitutes a packet
and we are interested in the probability that the packet is erroneously decoded.
Yet another example arises in block-mode transmission—which is described in Sec-
tion 10.4 and which is depicted in Figure 28.1—where the data bits D1, . . . , Dk are
mapped to the symbols X1, . . . , Xn using a (K, N) binary-to-reals block encoder
enc: {0, 1}K →RN.
Here we assume that k is divisible by K and that n = N k/K.
If we wish to guess the K-tuple

D(ν−1)K+1, . . . , D(ν−1)K+K

with the smallest
probability of error, then there is no loss of optimality in basing our guess on
T (1), . . . , T (n).
This follows by applying Proposition 28.3.1 with the function
ψ(d) =

d(ν−1)K+1, . . . , d(ν−1)K+K

.
28.4
Consequences of Orthonormality
The conditional distribution of the inner products in (28.12) is simpler when the
time shifts of the pulse shape by integer multiples of Ts are orthonormal. In this
case we denote the pulse shape by φ(·) and state the orthonormality condition as
 ∞
−∞
φ(t −ℓTs) φ(t −ℓ′Ts) dt = I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z,
(28.22)
or, equivalently, as
Rφφ(ℓTs) =

1
if ℓ= 0,
0
if ℓ̸= 0,
ℓ∈Z.
(28.23)
28.4.1
The Conditional Law of the Inner-Products Vector
From Proposition 28.2.1 we obtain a key result on PAM communication in WGN:
694
Detecting PAM and QAM Signals in White Gaussian Noise
Corollary 28.4.1. Consider PAM where data bits D1, . . . , Dk are mapped by an
encoder to the real symbols X1, . . . , Xn, which are then mapped to the waveform
X(t) = A
n

ℓ=1
Xℓφ(t −ℓTs),
t ∈R,
(28.24)
where the pulse shape φ(·) is an integrable signal that is bandlimited to W Hz and
whose time shifts by integer multiples of the baud period Ts are orthonormal. Let
the observed waveform

Y (t)

be given by
Y (t) = X(t) + N(t),
t ∈R,
where

N(t)

is independent of the data bits and is WGN of double-sided PSD N0/2
with respect to the bandwidth W.
(i) No measurable rule for guessing D1, . . . , Dk based on

Y (t)

can outperform
an optimal rule for guessing those bits based on the n inner products
T (ℓ) =
 ∞
−∞
Y (t) φ(t −ℓTs) dt,
ℓ∈{1, . . . , n}.
(28.25)
(ii) Conditional on D = d with corresponding encoder outputs (X1, . . . , Xn) =

x1(d), . . . , xn(d)

, the inner products (28.25) are independent with
T (ℓ) ∼N
	
A xℓ(d), N0
2

,
ℓ∈{1, . . . , n}.
(28.26)
(iii) The conditional distribution of these inner products can also be expressed as
T (ℓ) = A xℓ(d) + Zℓ,
ℓ∈{1, . . . , n},
(28.27a)
where
Z1, . . . , Zn ∼IID N
	
0, N0
2

.
(28.27b)
From Proposition 28.3.1 we obtain that basing our guess on

T (1), . . . , T (n)
is
optimal also for guessing the value of any function of the data bits (D1, . . . , Dk).
28.4.2
A Further Reduction of Dimensionality
In block-mode transmission (with the pulse shape φ(·) still satisfying (28.23)) we
can typically base our decision on N inner products instead of n. For this to hold
we need to assume that the data bits are independent or that the k/K tuples

D1, . . . , DK

,

DK+1, . . . , D2K

, . . . ,

Dk−K+1, . . . , Dk

(28.28)
are independent.
28.4 Consequences of Orthonormality
695
Proposition 28.4.2. In addition to the assumptions of Corollary 28.4.1, assume
that X1, . . . , Xn are generated from D1, . . . , Dk in block-mode using a (K, N) binary-
to-reals block encoder. Further assume that the K-tuples in (28.28) are independent.
Then for every ν ∈{1, . . . , k/K}, the N-tuple

T((ν−1)N+1), . . . , T(νN)
(28.29)
forms a suﬃcient statistic for guessing the K-tuple

D(ν−1)K+1, . . . , DνK

(28.30)
based on (T (1), . . . , T (n)).
Proof. Fix some ν ∈{1, . . . , k/K}.
The statement that the N-tuple (28.29) is
suﬃcient for guessing the K-tuple (28.30) based on the n-tuple (T (1), . . . , T (n)) is
equivalent to the irrelevancy of
R ≜
	
T(1), . . . , T(N)
, . . . ,

T((ν−2)N+1), . . . , T((ν−1)N)
,

T(νN+1), . . . , T((ν+1)N)
, . . . ,

T(n−N+1), . . . , T(n)
T
for guessing the K-tuple (28.30) based on the N-tuple (28.29). To prove this irrele-
vancy, it suﬃces to prove two claims: that R is independent of the K-tuple (28.30)
and that, conditionally on this K-tuple, R is independent of the N-tuple (28.29)
(Proposition 22.6.5). These claims follow from three observations: that, by the
orthonormality assumption (28.23), R is determined by the data bits
D1, . . . , D(ν−1)K, DνK+1, . . . , Dk
(28.31)
and by the random variables
Z1, . . . , Z(ν−1)N, ZνN+1, . . . , Zn;
(28.32)
that the N-tuple (28.29) is determined by the K-tuple (28.30) and by the random
variables
Z(ν−1)N+1, . . . , ZνN;
(28.33)
and that the tuples in (28.30), (28.31), (28.32), and (28.33) are independent.
Having established that the N-tuple (28.29) forms a suﬃcient statistic for guessing
the K-tuple (28.30), it now follows, using arguments very similar to those employed
in proving Proposition 28.3.1, that the N-tuple (28.29) is also suﬃcient for guessing
the value of any function ψ(·) of the K-tuple (28.30).
In combination with Corollary 28.4.1 and Exercise 28.10, Proposition 28.4.2 im-
plies:
Corollary 28.4.3. Under the assumptions of Proposition 28.4.2, no measurable rule
for guessing the value of a function of the K-tuple in (28.30) can outperform an
optimal rule for guessing that value based on the N-tuple in (28.29).
696
Detecting PAM and QAM Signals in White Gaussian Noise
28.4.3
The Discrete-Time Single-Block Model
Corollary 28.4.3 is the starting point of much of the literature on block codes, upon
which we shall touch in Chapter 29. In Coding Theory N is usually called the
blocklength, and K/N is called the rate in bits per dimension. Coding theorists
envision that the function enc(·) is used to map k bits to n real numbers using
the block-encoding rule of Figure 10.1 (with k being divisible by K) and that the
resulting real symbols are then transmitted over a WGN channel using PAM with
a pulse shape satisfying the orthogonality condition (28.23). Assuming that the
data tuples are independent, and by then resorting to Corollary 28.4.3, they then
focus on the problem of decoding the K-tuple (28.30) from the N matched ﬁlter
outputs (28.29).
In this problem the index ν of the block is immaterial, and coding theorists re-
label the data bits of the K tuple (28.30) as D1, . . . , DK; they re-label the symbols
to which they are mapped as X1, . . . , XN; and they re-label the corresponding
observations as Y1, . . . , YN.
The resulting model is the discrete-time single-
block model where

X1, . . . , XN

= enc

D1, . . . , DK

,
(28.34a)
Yη = AXη + Zη,
η ∈{1, . . . , N},
(28.34b)
Zη ∼N
	
0, N0
2

,
η ∈{1, . . . , N},
(28.34c)
and where Z1, . . . , ZN are IID and independent of D1, . . . , DK.
We recall that
this model is appropriate when the pulse shape φ satisﬁes the orthonormality
condition (28.22); the data bits are “block IID” in the sense that the k/K tuples in
(28.28) are independent; and the additive noise is WGN of double-sided PSD N0/2
with respect to the bandwidth occupied by the pulse shape φ. It is customary to
additionally assume that D1, . . . , DK are IID random bits (Deﬁnition 14.5.1). This
is a good assumption if, prior to transmission, the data bits are compressed using
an eﬃcient data compression algorithm.
28.5
Extension to QAM Communications
28.5.1
Introduction and Setup
We next extend our discussion to the detection of QAM signals. We assume that
an encoding function
ϕ: {0, 1}k →Cn
is used to map the k data bits D = (D1, . . . , Dk)T to the n complex symbols
C = (C1, . . . , Cn)T, and we denote by
ϕ(d) =

c1(d), . . . , cn(d)
T
(28.35)
the result of encoding the data bits d. The complex symbols are mapped to the
passband signal
XPB(t) = 2 Re

XBB(t) ei2πfct
,
t ∈R,
28.5 Extension to QAM Communications
697
where
XBB(t) = A
n

ℓ=1
Cℓg(t −ℓTs),
t ∈R;
the pulse shape g(·) is a complex integrable signal that is bandlimited to W/2 Hz;
A > 0 is a real constant; and fc > W/2. Conditional on D = d, we denote the
transmitted signal by x(·; d), so
x(t; d) = 2A Re
	
n

ℓ=1
cℓ(d) g(t −ℓTs) ei2πfct

(28.36)
=
√
2A
n

ℓ=1
Re

cℓ(d)

gI,ℓ(t)



2 Re
	 1
√
2 g(t −ℓTs)



gI,ℓ,BB(t)
ei2πfct

+
√
2A
n

ℓ=1
Im

cℓ(d)

gQ,ℓ(t)



2 Re
	
i 1
√
2 g(t −ℓTs)



gQ,ℓ,BB(t)
ei2πfct

,
t ∈R,
(28.37)
where (28.37) follows from (16.7); and where {gI,ℓ}, {gQ,ℓ}, {gI,ℓ,BB}, {gQ,ℓ,BB}
are as indicated in (28.37) and as deﬁned in (16.8) and (16.9).
Conditional on D = d, the received waveform is
Y (t) = x(t; d) + N(t),
t ∈R,
(28.38)
where

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W
around the carrier frequency fc (Deﬁnition 25.15.9).
28.5.2
From a SP to a Random Vector
The representation (28.37) makes it clear that for every d ∈{0, 1}k the signal
t 	→x(t; d) can be expressed as a linear combination of the 2n real-valued signals
{gI,ℓ}n
ℓ=1,
{gQ,ℓ}n
ℓ=1.
(28.39)
Since these signals are integrable signals that are bandlimited to W Hz around
the carrier frequency fc, it follows (see Section 26.8) that no measurable rule for
guessing D based on

Y (t)

can outperform an optimal rule for guessing D based
on the 2n inner products
T (ℓ)
I
=
 ∞
−∞
Y (t) gI,ℓ(t) dt,
ℓ∈{1, . . . , n},
(28.40a)
T (ℓ)
Q
=
 ∞
−∞
Y (t) gQ,ℓ(t) dt,
ℓ∈{1, . . . , n}.
(28.40b)
To describe the conditional joint distribution of these inner products conditional on
each of the hypotheses, we next express the inner products between the functions
698
Detecting PAM and QAM Signals in White Gaussian Noise
in (28.39) in terms of the self-similarity function Rgg of the complex pulse shape g
Rgg(τ) =
 ∞
−∞
g(t + τ) g∗(t) dt,
τ ∈R
(28.41)
(Deﬁnition 11.2.1). Key to these calculations is the relationship between the inner
product between real passband signals and the inner product between their complex
baseband representations (Theorem 7.6.10). Thus,
⟨gI,ℓ′, gI,ℓ⟩= 2 Re

⟨gI,ℓ′,BB, gI,ℓ,BB⟩

= Re

t 	→g(t −ℓ′Ts), t 	→g(t −ℓTs)

= Re
	 ∞
−∞
g(t −ℓ′Ts) g∗(t −ℓTs) dt

= Re

Rgg

(ℓ−ℓ′)Ts

,
ℓ, ℓ′ ∈Z,
(28.42a)
where the ﬁrst equality follows by relating the inner product in passband to the
inner product in baseband; the second from the expressions for the corresponding
baseband representations (16.9a); the third from the deﬁnition of the inner product
for complex-valued signals (3.4); and the ﬁnal equality from the deﬁnition of the
self-similarity function (28.41). Similarly,
⟨gQ,ℓ′, gQ,ℓ⟩= 2 Re

⟨gQ,ℓ′,BB, gQ,ℓ,BB⟩

= Re

t 	→ig(t −ℓ′Ts), t 	→ig(t −ℓTs)

= Re
	 ∞
−∞
ig(t −ℓ′Ts) (−i) g∗(t −ℓTs) dt

= Re
	 ∞
−∞
g(t −ℓ′Ts) g∗(t −ℓTs) dt

= Re

Rgg

(ℓ−ℓ′)Ts

,
ℓ, ℓ′ ∈Z,
(28.42b)
and
⟨gQ,ℓ′, gI,ℓ⟩= 2 Re

⟨gQ,ℓ′,BB, gI,ℓ,BB⟩

= Re

t 	→ig(t −ℓ′Ts), t 	→g(t −ℓTs)

= Re
	
i
 ∞
−∞
g(t −ℓ′Ts) g∗(t −ℓTs) dt

= −Im
	 ∞
−∞
g(t −ℓ′Ts) g∗(t −ℓTs) dt

= −Im

Rgg

(ℓ−ℓ′)Ts

,
ℓ, ℓ′ ∈Z,
(28.42c)
where the ﬁrst equality leading to (28.42c) follows from the relationship between
the inner product between real passband signals and the inner product between
their baseband representations (Theorem 7.6.10); the second from the expressions
for the corresponding baseband representations (16.9); the third from the deﬁnition
of the inner product between complex signals (3.4); the fourth from the identity
28.5 Extension to QAM Communications
699
Re(iz) = −Im(z); and where the last equality follows from the deﬁnition of the
self-similarity function of complex signals (28.41).
We are now ready to compute the conditional laws of the inner products given
each of the hypotheses. Conditional on D = d with corresponding encoder output
ϕ(d) of (28.35), the 2n random variables

T (ℓ)
I
, T (ℓ)
Q
n
ℓ=1 are jointly Gaussian (Sec-
tion 26.8). Their conditional law is thus fully speciﬁed by the conditional mean
vector and by the conditional covariance matrix. We begin with the former:
E
%
T (ℓ)
I
 D = d
&
=

t 	→x(d; t), gI,ℓ

=
√
2A
n

ℓ′=1
Re

cℓ′(d)

gI,ℓ′ +
√
2A
n

ℓ′=1
Im

cℓ′(d)

gQ,ℓ′, gI,ℓ

=
√
2A
n

ℓ′=1

Re

cℓ′(d)

⟨gI,ℓ′, gI,ℓ⟩+ Im

cℓ′(d)

⟨gQ,ℓ′, gI,ℓ⟩

=
√
2A
n

ℓ′=1
	
Re

cℓ′(d)

Re

Rgg

(ℓ−ℓ′)Ts

−Im

cℓ′(d)

Im

Rgg

(ℓ−ℓ′)Ts

=
√
2A
n

ℓ′=1
Re

cℓ′(d) Rgg

(ℓ−ℓ′)Ts

=
√
2A Re
	
n

ℓ′=1
cℓ′(d) Rgg

(ℓ−ℓ′)Ts

,
ℓ∈{1, . . . , n},
(28.43a)
where the ﬁrst equality follows from the deﬁnition of T (ℓ)
I
(28.40a), from (28.38),
and from our assumption that the noise

N(t)

is of zero mean; the second from
(28.37); the third from the linearity of the inner product; the fourth by express-
ing the inner products using the self-similarity function, i.e., using (28.42a) and
(28.42c); the ﬁfth by the complex-numbers identity Re(wz) = Re(w) Re(z) −
Im(w) Im(z); and the ﬁnal equality because the sum of the real parts is the real
part of the sum. Similarly,
E
%
T (ℓ)
Q
 D = d
&
=

t 	→x(d; t), gQ,ℓ

=
√
2A
n

ℓ′=1
Re

cℓ′(d)

gI,ℓ′ +
√
2A
n

ℓ′=1
Im

cℓ′(d)

gQ,ℓ′, gQ,ℓ

=
√
2A
n

ℓ′=1

Re

cℓ′(d)

⟨gI,ℓ′, gQ,ℓ⟩+ Im

cℓ′(d)

⟨gQ,ℓ′, gQ,ℓ⟩

=
√
2A
n

ℓ′=1
-
Re

cℓ′(d)
	
−Im

Rgg

(ℓ′ −ℓ)Ts

+ Im

cℓ′(d)

Re

Rgg

(ℓ−ℓ′)Ts
.
700
Detecting PAM and QAM Signals in White Gaussian Noise
=
√
2A
n

ℓ′=1
	
Re

cℓ′(d)

Im

Rgg

(ℓ−ℓ′)Ts

+ Im

cℓ′(d)

Re

Rgg

(ℓ−ℓ′)Ts

=
√
2A
n

ℓ′=1
Im

cℓ′(d) Rgg

(ℓ−ℓ′)Ts

=
√
2A Im
	
n

ℓ′=1
cℓ′(d) Rgg

(ℓ−ℓ′)Ts

,
ℓ∈{1, . . . , n},
(28.43b)
where the ﬁrst equality follows from the deﬁnition of T (ℓ)
Q
(28.40b), from (28.38),
and from our assumption that the noise

N(t)

is of zero mean; the second from
(28.37); the third from the linearity of the inner product; the fourth by express-
ing the inner products using the self-similarity function, i.e., using (28.42c) and
(28.42b); the ﬁfth by the conjugate symmetry of the self-similarity function (Propo-
sition 11.2.2); the sixth by the complex-numbers identity Im(wz) = Re(w) Im(z) +
Im(w) Re(z); and the ﬁnal equality by noting that the sum of the imaginary parts
is equal to the imaginary part of the sum.
The conditional covariances are easily computed using Note 25.15.10. Using the
inner products expressions (28.42), we obtain:
Cov
%
T (ℓ′)
I
, T (ℓ′′)
I
 D = d
&
= N0
2

gI,ℓ′, gI,ℓ′′
= N0
2 Re

Rgg

(ℓ′ −ℓ′′)Ts

,
(28.44a)
Cov
%
T (ℓ′)
Q
, T (ℓ′′)
Q
 D = d
&
= N0
2

gQ,ℓ′, gQ,ℓ′′
= N0
2 Re

Rgg

(ℓ′ −ℓ′′)Ts

,
(28.44b)
and
Cov
%
T (ℓ′)
I
, T (ℓ′′)
Q
 D = d
&
= N0
2

gI,ℓ′, gQ,ℓ′′
= −N0
2 Im

Rgg

(ℓ′ −ℓ′′)Ts

.
(28.44c)
We summarize our results on QAM detection in WGN as follows.
Proposition 28.5.1 (Reducing the Output to a Random Vector: QAM in WGN).
Let a QAM signal (28.37) of an integrable pulse shape g(·) that is bandlimited
to W/2 Hz be observed in WGN of double-sided PSD N0/2 with respect to the
bandwidth W around the carrier frequency fc. Then:
(i) No measurable rule for guessing D based on

Y (t)

can outperform an optimal
rule for guessing D based on the 2n inner products
T (ℓ)
I
=
 ∞
−∞
Y (t) gI,ℓ(t) dt,
ℓ∈{1, . . . , n},
(28.45a)
T (ℓ)
Q
=
 ∞
−∞
Y (t) gQ,ℓ(t) dt,
ℓ∈{1, . . . , n},
(28.45b)
28.5 Extension to QAM Communications
701
where
gI,ℓ(t) = 2 Re
 1
√
2 g(t −ℓTs) ei2πfct
,
t ∈R,
gQ,ℓ(t) = 2 Re
 1
√
2i g(t −ℓTs) ei2πfct
,
t ∈R.
(ii) Conditional on D = d with corresponding transmitted symbols as is (28.35),
these 2n real random variables are jointly Gaussian with conditional means as
speciﬁed by (28.43) and with conditional covariances as speciﬁed by (28.44).
28.5.3
From a SP to a Complex Random Vector
The notation is simpler if we introduce the n complex random variables
T (ℓ) ≜T (ℓ)
I
+ i T (ℓ)
Q
=
 ∞
−∞
Y (t) gI,ℓ(t) dt + i
 ∞
−∞
Y (t) gQ,ℓ(t) dt,
ℓ∈{1, . . . , n}.
(28.46)
There is, of course, a one-to-one relationship between these n complex random
variables and the 2n real inner products, so it is also optimal to base our guess on
the former. Using (28.43) we obtain
E
%
T (ℓ)  D = d
&
= E
%
T (ℓ)
I
 D = d
&
+ i E
%
T (ℓ)
Q
 D = d
&
=
√
2A Re
	
n

ℓ′=1
cℓ′(d) Rgg

(ℓ−ℓ′)Ts

+ i
√
2A Im
	
n

ℓ′=1
cℓ′(d) Rgg

(ℓ−ℓ′)Ts

=
√
2A
n

ℓ′=1
cℓ′(d) Rgg

(ℓ−ℓ′)Ts

,
ℓ∈{1, . . . , n}.
(28.47)
The advantage of the complex notation is that—as we shall see in Proposition 28.5.2
ahead—conditional on D = d, the random vector T−E[T|D = d] is proper (Deﬁ-
nition 17.4.1). And since conditionally on D = d it is also Gaussian, it follows from
Proposition 24.3.11 that, conditional on D = d, the random vector T−E[T|D = d]
is a circularly-symmetric complex Gaussian (Deﬁnition 24.3.2). Its conditional law
is thus determined by its conditional covariance matrix (Corollary 24.3.8). This
covariance matrix is an n × n (complex) matrix, whereas the covariance matrix for
the 2n real variables in Proposition 28.5.1 is a (2n) × (2n) (real) matrix.
We summarize our results for QAM detection with complex notation as follows.
Proposition 28.5.2 (A Complex Random Vector for QAM in WGN). Consider
the setup of Proposition 28.5.1.
(i) No measurable rule for guessing D based on

Y (t)

can outperform an optimal
rule for guessing D based on the complex random vector T = (T (1), . . . , T (n))T
deﬁned by
T (ℓ) =
 ∞
−∞
Y (t) gI,ℓ(t) dt + i
 ∞
−∞
Y (t) gQ,ℓ(t) dt,
ℓ∈{1, . . . , n}.
702
Detecting PAM and QAM Signals in White Gaussian Noise
(ii) The ℓ-th component of T can be expressed as
T (ℓ) =
√
2A
n

ℓ′=1
Cℓ′ Rgg

(ℓ−ℓ′)Ts

+ Z(ℓ),
ℓ∈{1, . . . , n},
where Rgg is the self-similarity function of the pulse shape g(·) (28.41), and
where the random vector Z = (Z(1), . . . , Z(n))T is independent of D and is a
circularly-symmetric complex Gaussian of covariance
Cov
%
Z(ℓ′), Z(ℓ′′)&
= E
%
Z(ℓ′)
Z(ℓ′′)∗&
= N0 Rgg

(ℓ′ −ℓ′′)Ts

,
ℓ′, ℓ′′ ∈{1, . . . , n}.
(28.48)
(iii) If the time shifts of the pulse shape by integer multiples of Ts are orthonormal,
then
T (ℓ) =
√
2ACℓ+ Z(ℓ),
ℓ∈{1, . . . , n, },
(28.49)
where the complex random variables {Z(ℓ)} are independent of D and are IID
circularly-symmetric complex Gaussians of variance N0.
Proof. Part (i) follows directly from Proposition 28.5.1 because there is a one-to-
one relationship between the complex n-vector and the 2n (real) inner products.
To prove Part (ii) deﬁne
Z(ℓ) ≜T (ℓ) −
√
2A
n

ℓ′=1
Cℓ′ Rgg

(ℓ−ℓ′)Ts

,
ℓ∈{1, . . . , n},
(28.50)
and note that by (28.47) the conditional distribution of Z given D = d is of zero
mean. Moreover, from Proposition 28.5.1 and from the deﬁnition of a complex
Gaussian random vector as one whose real and imaginary parts are jointly Gaussian
(Deﬁnition 24.3.6), it follows that, conditional on D = d, the vector Z is Gaussian.
To prove that it is proper we compute
E
%
Z(ℓ′)Z(ℓ′′)  D = d
&
= E
%
Re

Z(ℓ′)
Re

Z(ℓ′′)
−Im

Z(ℓ′)
Im

Z(ℓ′′)  D = d
&
+ i E
%
Re

Z(ℓ′)
Im

Z(ℓ′′)
+ Im

Z(ℓ′)
Re

Z(ℓ′′)  D = d
&
= Cov
%
T (ℓ′)
I
, T (ℓ′′)
I
 D = d
&
−Cov
%
T (ℓ′)
Q
, T (ℓ′′)
Q
 D = d
&
+ i

Cov
%
T (ℓ′)
I
, T (ℓ′′)
Q
 D = d
&
+ Cov
%
T (ℓ′)
Q
, T (ℓ′′)
I
 D = d
&
= 0,
ℓ′, ℓ′′ ∈{1, . . . , n},
where the second equality follows from (28.50) and the last equality from (28.44).
28.6 Additional Reading
703
The calculation of the conditional covariance matrix is very similar except that Z(ℓ′′)
is now conjugated:
Cov
%
Z(ℓ′), Z(ℓ′′) D = d
&
= E
%
Re

Z(ℓ′)
Re

Z(ℓ′′)
+ Im

Z(ℓ′)
Im

Z(ℓ′′)  D = d
&
+ i E
%
−Re

Z(ℓ′)
Im

Z(ℓ′′)
+ Im

Z(ℓ′)
Re

Z(ℓ′′)  D = d
&
= Cov
%
T (ℓ′)
I
, T (ℓ′′)
I
 D = d
&
+ Cov
%
T (ℓ′)
Q
, T (ℓ′′)
Q
 D = d
&
+ i

−Cov
%
T (ℓ′)
I
, T (ℓ′′)
Q
 D = d
&
+ Cov
%
T (ℓ′)
Q
, T (ℓ′′)
I
 D = d
&
= N0
2 Re

Rgg

(ℓ′ −ℓ′′)Ts

+ N0
2 Re

Rgg

(ℓ′ −ℓ′′)Ts

+ i
-
N0
2 Im

Rgg

(ℓ′ −ℓ′′)Ts

−N0
2 Im

Rgg

(ℓ′′ −ℓ′)Ts
.
= N0 Rgg

(ℓ′ −ℓ′′)Ts

,
ℓ′, ℓ′′ ∈{1, . . . , n},
(28.51)
where the ﬁrst equality follows from the deﬁnition of the covariance between cen-
tered complex random variables (17.17); the second by (28.50); the third by (28.44);
and the last equality by the conjugate-symmetry of the self-similarity function
(Proposition 11.2.2 (iii)).
Conditional on D = d, the complex n-vector Z is thus a proper Gaussian, and its
conditional law is thus fully speciﬁed by its conditional covariance matrix (Corol-
lary 24.3.8). By (28.51), this conditional covariance matrix does not depend on d,
and we thus conclude that the conditional law of Z conditional on D = d does not
depend on d, i.e., that Z is independent of D.
Part (iii) follows from Part (ii).
28.6
Additional Reading
Proposition 28.2.1 and Proposition 28.5.2 are the starting points of much of the
literature on equalization and on the use of the Viterbi Algorithm for channels
with intersymbol interference (ISI). More on this in Chapter 32. Additional re-
sources include, for example, (Proakis and Salehi, 2007, Chapter 9), (Viterbi and
Omura, 1979, Chapter 4, Section 4.9), and (Barry, Lee, and Messerschmitt, 2004,
Chapter 8).
28.7
Exercises
Exercise 28.1 (A Dispersive Channel). Let the transmitted signal

X(t)

be as in (28.1),
and let the received signal

Y (t)

be given by
Y (t) =

X ⋆h

(t) + N(t),
t ∈R,
where

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W, and
where h is the impulse response of some stable real ﬁlter.
704
Detecting PAM and QAM Signals in White Gaussian Noise
(i) Show that no measurable rule for guessing D based on

Y (t)

can outperform an
optimal rule for guessing D based on the n inner products
 ∞
−∞
Y (t)

g ⋆h

(t −ℓTs) dt,
ℓ∈{1, . . . , n}.
(ii) Compute their conditional law.
Exercise 28.2 (PAM in Colored Noise). Let the transmitted signal

X(t)

be as in (28.1),
and let the received signal

Y (t)

be given by
Y (t) = X(t) + N(t),
t ∈R,
where

N(t)

is a centered, stationary, measurable, Gaussian SP of PSD SNN that can be
whitened with respect to the bandwidth W. Let h be the impulse response of a whitening
ﬁlter for

N(t)

with respect to W.
(i) Show that no measurable rule for guessing D based on

Y (t)

can outperform an
optimal rule for guessing D based on the n inner products
 ∞
−∞
Y (t)

g ⋆h ⋆~h

(t −ℓTs) dt,
ℓ∈{1, . . . , n}.
(ii) Compute their conditional law.
Exercise 28.3 (A Channel with an Echo). Bits D1, . . . , Dk are mapped to real symbols
X1, . . . , Xk using the antipodal mapping, so Xℓ= 1 −2Dℓ, for every ℓ∈{1, . . . , k}. The
transmitted signal

X(t)

is given by X(t) = A 
ℓXℓφ(t−ℓTs), where φ is an integrable
signal that is bandlimited to W Hz and that satisﬁes the orthonormality condition (28.22).
The received signal

Y (t)

is
Y (t) = X(t) + αX(t −Ts) + N(t),
t ∈R,
where

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W, and α
is a real constant. Let Yℓbe the time-(ℓTs) output of a ﬁlter that is matched to φ and
that is fed

Y (t)

.
(i) Is there a loss of optimality in guessing D1, . . . , Dk based on Y1, . . . , Yk+1 instead
of

Y (t)

?
(ii) Consider a suboptimal rule that guesses “Dj = 0” if Yj ≥0, and otherwise guesses
“Dj = 1.” Express the probability that this rule guesses Dj incorrectly in terms
of j, α, A, and N0. To what does this probability converge as N0 tends to zero?
Exercise 28.4 (Another Channel with an Echo). Consider the setup of Exercise 28.3 but
where the echo is delayed by a noninteger multiple of the baud period. Thus,
Y (t) = X(t) + αX(t −τ) + N(t),
t ∈R,
where 0 < τ < Ts. Show that no measurable rule for guessing D1, . . . , Dk based on

Y (t)

can outperform an optimal rule for guessing these bits based on the 2k inner products
 ∞
−∞
Y (t) φ(t −ℓTs) dt,
 ∞
−∞
Y (t) φ(t −ℓTs −τ) dt,
ℓ∈{1, . . . , k}.
28.7 Exercises
705
Exercise 28.5 (A Multiple-Access Scenario). Two transmitters communicate with a single
receiver. The receiver observes the signal
Y (t) = A1X1 φ1(t) + A2X2 φ2(t) + N(t),
t ∈R,
where A1, A2 > 0; φ1 and φ2 are orthonormal integrable signals that are bandlimited
to W Hz; the pair (X1, X2) takes value in the set {(+1, +1), (+1, −1), (−1, +1), (−1, −1)}
equiprobably; and where

N(t)

is WGN of double-sided PSD N0/2 with respect to the
bandwidth W.
(i) Can you recover (X1, X2) from A1X1φ1 + A2X2φ2?
(ii) Find an optimal receiver for guessing (X1, X2) based on

Y (t)

.
(iii) Compute the optimal probability of error for guessing (X1, X2) based on

Y (t)

.
(iv) Suppose that a genie informs the receiver of the value of X2.
How should the
receiver then guess X1 based on

Y (t)

and the information provided by the genie?
(v) A receiver guesses “X1 = +1” if ⟨Y, φ1⟩> 0 and guesses “X1 = −1” otherwise. Is
this receiver optimal for guessing X1?
Exercise 28.6 (Two Receiver Antennas). Consider the setup of (28.1). We observe two
signals

Y1(t)

,

Y2(t)

that are given at every epoch t ∈R by
Y1(t) =

X ⋆h1

(t) + N1(t),
Y2(t) =

X ⋆h2

(t) + N2(t),
where h1 and h2 are the impulse responses of two real stable ﬁlters, and where the
stochastic processes

N1(t)

and

N2(t)

are independent white Gaussian noise processes
of double-sided PSD N0/2 with respect to the bandwidth W. Show that no measurable
rule for guessing D based on (Y1, Y2) can outperform an optimal rule for guessing D
based on the 2n inner products
 ∞
−∞
Y1(t)

g ⋆h1

(t −ℓTs) dt,
 ∞
−∞
Y2(t)

g ⋆h2

(t −ℓTs) dt,
ℓ∈{1, . . . , n}.
Exercise 28.7 (Bits of Unequal Importance). Consider the setup of Section 28.3 but
where some data bits are more important than others. We therefore wish to minimize the
weighted average
k

j=1
αj Pr
$ ˆDj ̸= Dj
%
,
for some positive α1, . . . , αk that sum to one.
(i) Is it still optimal to base our guess of D1, . . . , Dk on the inner products in (28.12)?
(ii) Does this criterion lead to a diﬀerent receiver design than the bit error rate?
Exercise 28.8 (Sandwiching the Probability of a Message Error). In the notation of
Section 28.3, show that
1
k
k

j=1
Pr[ ˜Dj ̸= Dj] ≤max
1≤j≤k

Pr[ ˜Dj ̸= Dj]

≤Pr
$ ˜D ̸= D
%
≤
k

j=1
Pr[ ˜Dj ̸= Dj].
706
Detecting PAM and QAM Signals in White Gaussian Noise
Exercise 28.9 (Sandwiching the Bit Error Rate). In the notation of Section 28.3, show
that
1
k Pr
$ ˜D ̸= D
%
≤1
k
k

j=1
Pr[ ˜Dj ̸= Dj] ≤Pr
$ ˜D ̸= D
%
.
Exercise 28.10 (Guessing ψ(M)). Show that if T : Rd →Rd′ forms a suﬃcient statistic
for guessing the message M ∈{1, . . . , M} based on the random d-vector Y, and if ψ(·)
is any function of M, then no rule for guessing ψ(M) based on Y can outperform an
optimal rule for guessing ψ(M) based on T(Y).
Hint: Recall Theorem 22.3.5.
Exercise 28.11 (Transmission via an Unknown Dispersive Channel). A random switch
outside our control whose state we cannot observe determines whether our observation Y
is
X ⋆h1 + N
or
X ⋆h2 + N,
where

X(t)

is the transmitted signal of (28.1);

N(t)

is WGN of double-sided PSD N0/2
with respect to the bandwidth W; and h1 & h2 are the impulse responses of two stable
real ﬁlters. Show that no measurable rule for guessing D1, . . . , Dk based on

Y (t)

can
outperform an optimal rule for guessing these bits based on the 2n inner products
 ∞
−∞
Y (t)

g ⋆h1

(t −ℓTs) dt,
 ∞
−∞
Y (t)

g ⋆h2

(t −ℓTs) dt,
ℓ∈{1, . . . , n}.
Exercise 28.12 (The Matched-Filter Bound). Consider the setting of Proposition 28.2.1
with the additional assumptions that the data bits are IID random bits and that the
encoder in (28.5) is the one of (10.2). Show that for every j ∈{1, . . . , k} the probability
of guessing Dj incorrectly based on

Y (t)

is lower-bounded by
Q
⎛
⎝
&
2A2 ∥g∥2
2
N0
⎞
⎠.
Can this bound hold with equality? Would the bound continue to hold if we relaxed the
assumption that the data are IID random bits and only required that Dj ∼U ({0, 1})?
Hint: Knowing the other data bits cannot hurt.
Exercise 28.13 (More Inner Products than Necessary). Consider the setting of Proposi-
tion 28.5.2 but where we deﬁne T (ℓ) not only for ℓ∈{1, . . . , n} but for all ℓ∈Z. Deﬁne
Z(ℓ) as in (28.50) but, again, not just for ℓ∈{1, . . . , n} but for all ℓ∈Z. Prove that
the bi-inﬁnite sequence . . . , Z(−1), Z(0), Z(1), . . . forms a circularly-symmetric, stationary,
Gaussian, discrete-time CSP. What is its autocovariance function? What is its PSD?
Chapter 29
Linear Binary Block Codes with Antipodal
Signaling
29.1
Introduction and Setup
We have thus far said very little about the design of good encoders. We mentioned
block encoders but, apart from deﬁning and studying some of their basic prop-
erties (such as rate and energy per symbol), we have said very little about how
to design such encoders. The design of block encoders falls under the heading of
Coding Theory and is the subject of numerous books such as (MacWilliams and
Sloane, 1977), (van Lint, 1998), (Blahut, 2003), (Roth, 2006) and (Richardson and
Urbanke, 2008). Here we provide only a glimpse of this theory for one class of such
encoders: the class of binary linear block encoders with antipodal pulse amplitude
modulation.
Such encoders map the data bits D1, . . . , DK to the real symbols
X1, . . . , XN by ﬁrst applying a one-to-one linear mapping of binary K-tuples to
binary N-tuples and by then applying the antipodal mapping
0 	→+1
1 	→−1
to each component of the binary N-tuple to produce the {±1}-valued symbols
X1, . . . , XN.
Our emphasis in this chapter is not on the design of such encoders, but on how
their properties inﬂuence the performance of communication systems that employ
them in combination with Pulse Amplitude Modulation. We thus assume that the
transmitted waveform is given by
A

ℓ
Xℓφ(t −ℓTs),
t ∈R,
(29.1)
where A > 0 is a scaling factor, Ts > 0 is the baud period, φ(·) is a real integrable
signal that is bandlimited to W Hz, and where the time shifts of φ(·) by integer
multiples of Ts are orthonormal
 ∞
−∞
φ(t −ℓTs) φ(t −ℓ′Ts) dt = I{ℓ= ℓ′},
ℓ, ℓ′ ∈Z.
(29.2)
707
708
Linear Binary Block Codes with Antipodal Signaling
The summation in (29.1) can be ﬁnite, as in the block-mode that we discussed
in Section 10.4, or inﬁnite, as in the bi-inﬁnite block-mode that we discussed in
Section 14.5.2. We shall further assume that the PAM signal is transmitted over
an additive noise channel where the transmitted signal is corrupted by Gaussian
noise that is white with respect to the bandwidth W. We also assume that the
data are IID random bits (Deﬁnition 14.5.1).
In Section 29.2 we brieﬂy discuss the binary ﬁeld F2 and discuss some of the basic
properties of the set of all binary κ-tuples when it is viewed as a vector space
over this ﬁeld. This allows us in Section 29.3 to deﬁne linear binary encoders and
codes. Section 29.4 introduces binary encoders with antipodal signaling, and Sec-
tion 29.5 discusses the power and power spectral density when they are employed
in conjunction with PAM. Section 29.6 begins the study of decoding with a dis-
cussion of two performance criteria: the probability of a block error (also called
message error) and the probability of a bit error. It also recalls the discrete-
time single-block channel model (Section 28.4.3). Section 29.7 contains the design
and performance analysis of the guessing rule that minimizes the probability of a
block error, and Section 29.8 contains a similar analysis for the guessing rule that
minimizes the probability of a bit error. Section 29.9 explains why performance
analysis and simulation are often done under the assumption that the transmitted
data are the all-zero data. Section 29.10 discusses how the encoder and the PAM
parameters inﬂuence the overall system performance. The chapter concludes with
a discussion of the (suboptimal) Hard Decision decoding rule in Section 29.11 and
of bounds on the minimum distance of a code in Section 29.12.
29.2
The Binary Field F2 and the Vector Space Fκ
2
29.2.1
The Binary Field F2
The binary ﬁeld F2 consists of two elements that we denote by 0 and 1.
An
operation that we denote by ⊕is deﬁned between any two elements of F2 through
the relation
0 ⊕0 = 0,
0 ⊕1 = 1,
1 ⊕0 = 1,
1 ⊕1 = 0.
(29.3)
This operation is sometimes called “mod 2 addition” or “exclusive-or” or “GF(2)
addition.” (Here GF(2) stands for the Galois Field of two elements after the French
mathematician ´Evariste Galois (1811–1832) who did ground-breaking work on ﬁnite
ﬁelds and groups.) Another operation—“GF(2) multiplication”—is denoted by a
dot and is deﬁned via the relation
0 · 0 = 0,
0 · 1 = 0,
1 · 0 = 0,
1 · 1 = 1.
(29.4)
Combined with these operations, the set F2 forms a ﬁeld, which is sometimes
called the Galois Field of size two. We leave it to the reader to verify that the ⊕
operation satisﬁes
a ⊕b = b ⊕a,
a, b ∈F2,
(a ⊕b) ⊕c = a ⊕(b ⊕c),
a, b, c ∈F2,
a ⊕0 = 0 ⊕a = a,
a ∈F2,
29.2 The Binary Field F2 and the Vector Space Fκ
2
709
a ⊕a = 0,
a ∈F2;
and that the operations ⊕and · satisfy the distributive law
(a ⊕b) · c = (a · c) ⊕(b · c),
a, b, c ∈F2.
29.2.2
The Vector Field Fκ
2
We denote the set of all binary κ-tuples by Fκ
2 and deﬁne the componentwise-⊕
operation between κ-tuples u =

u1, . . . , uκ

∈Fκ
2 and v =

v1, . . . , vκ

∈Fκ
2 as
u ⊕v ≜

u1 ⊕v1, . . . , uκ ⊕vκ

,
u, v ∈Fκ
2.
(29.5)
We deﬁne the product between a scalar α ∈F2 and a κ-tuple u =

u1, . . . , uκ

∈Fκ
2
by
α · u ≜

α · u1, . . . , α · uκ

.
(29.6)
With these operations the set Fκ
2 forms a vector space over the ﬁeld F2. The all-zero
κ-tuple is denoted by 0.
29.2.3
Linear Mappings
A mapping T: Fκ
2 →Fη
2 is said to be linear if
T(α · u ⊕β · v) = α · T(u) ⊕β · T(v),

α, β ∈F2, u, v ∈Fκ
2

.
(29.7)
The kernel of a linear mapping T: Fκ
2 →Fη
2 is denoted by ker(T) and is the set of
all κ-tuples in Fκ
2 that are mapped by T(·) to the all-zero η-tuple 0:
ker(T) =

u ∈Fκ
2 : T(u) = 0

.
(29.8)
The kernel of every linear mapping contains the all-zero tuple 0.
The range of T: Fκ
2 →Fη
2 is denoted by range(T) and consists of those elements
of Fη
2 to which some element of Fκ
2 is mapped by T(·):
range(T) =

T(u) : u ∈Fκ
2

.
(29.9)
The key results from Linear Algebra that we need are summarized in the following
proposition.
Proposition 29.2.1. Let T: Fκ
2 →Fη
2 be linear.
(i) The kernel of T(·) is a linear subspace of Fκ
2.
(ii) The mapping T(·) is one-to-one if, and only if, ker(T) = {0}.
(iii) The range of T(·) is a linear subspace of Fη
2.
(iv) The sum of the dimension of the kernel and the dimension of the range space
is equal to the dimension of the domain:
dim

ker(T)

+ dim

range(T)

= κ.
(29.10)
(v) If U is a linear subspace of Fη
2 of dimension κ, then there exists a one-to-one
linear mapping from Fκ
2 to Fη
2 whose range is U.
710
Linear Binary Block Codes with Antipodal Signaling
29.2.4
Hamming Distance and Hamming Weight
The Hamming distance dH(u, v) between two binary κ-tuples u and v is deﬁned
as the number of components in which they diﬀer. For example, the Hamming
distance between the tuples (1, 0, 1, 0) and (0, 0, 1, 1) is two. It is easy to prove
that for u, v, w ∈Fκ
2:
dH(u, v) ≥0 with equality if, and only if, u = v;
(29.11a)
dH(u, v) = dH(v, u);
(29.11b)
dH(u, w) ≤dH(u, v) + dH(v, w).
(29.11c)
The Hamming weight wH(u) of a binary κ-tuple u is deﬁned as the number of
its nonzero components. Thus,
wH(u) = dH(u, 0),
u ∈Fκ
2,
(29.12)
and
dH(u, v) = wH(u ⊕v),
u, v ∈Fκ
2.
(29.13)
29.2.5
The Componentwise Antipodal Mapping
The antipodal mapping Υ: F2 →{−1, +1} maps the zero element of F2 to the
real number +1 and the unit element of F2 to −1:
Υ(0) = +1,
Υ(1) = −1.
(29.14)
This rule is not as arbitrary as it may seem. Although one might be somewhat
surprised that we do not map 1 ∈F2 to +1, we have our reasons: we prefer the
mapping (29.14) because it maps mod-2 sums to real products. Thus,
Υ(a ⊕b) = Υ(a)Υ(b),
a, b ∈F2,
(29.15)
where the operation on the RHS between Υ(a) and Υ(b) is the regular real-numbers
multiplication. This extends by induction to any ﬁnite number of elements of F2:
Υ

c1 ⊕c2 ⊕· · · ⊕cν

=
ν
@
ℓ=1
Υ(cℓ),
c1, . . . , cν ∈F2.
(29.16)
The componentwise antipodal mapping Υη : Fη
2 →{−1, +1}η maps elements
of Fη
2 to elements of {−1, +1}η by applying the mapping (29.14) to each component:
Υη :

c1, . . . , cη

	→

Υ(c1), . . . , Υ(cη)

.
(29.17)
For example, Υ3 maps the triple (0, 0, 1) to (+1, +1, −1).
29.3 Binary Linear Encoders and Codes
711
29.2.6
Hamming Distance and Euclidean Distance
We next relate the Hamming distance dH(u, v) between any two binary η-tuples
u = (u1, . . . , uη) and v = (v1, . . . , vη) to the squared Euclidean distance between
the results of applying the componentwise antipodal mapping Υη to them. We
argue that
d2
E

Υη(u), Υη(v)

= 4 dH(u, v),
u, v ∈Fη
2,
(29.18)
where dE(·, ·) denotes the Euclidean distance, so
d2
E

Υη(u), Υη(v)

=
η

ν=1

Υ(uν) −Υ(vν)
2.
(29.19)
To prove (29.18) it suﬃces to consider the case where η = 1, because the Hamming
distance is the sum of the Hamming distances between the respective components,
and likewise for the squared Euclidean distance. To prove this result for η = 1 we
note that if the Hamming distance is zero, then u and v are identical and hence so
are Υ(u) and Υ(v), so the Euclidean distance between them must be zero. And if
the Hamming distance is one, then u ̸= v, and hence Υ(u) and Υ(v) are of opposite
sign but of equal unit magnitude, so the squared Euclidean distance between them
is four.
29.3
Binary Linear Encoders and Codes
Deﬁnition 29.3.1 (Linear (K, N) F2 Encoder and Code). Let N and K be positive
integers.
(i) A linear (K, N) F2 encoder is a one-to-one linear mapping from FK
2 to FN
2 .
(ii) A linear (K, N) F2 code is a linear subspace of FN
2 of dimension K.1
In both deﬁnitions N is called the blocklength and K is called the dimension.
For example, the (K, K + 1) systematic single parity check encoder is the
mapping

d1, . . . , dK

	→

d1, . . . , dK, d1 ⊕d2 ⊕· · · ⊕dK

.
(29.20)
It appends to the data tuple a single bit that is chosen so that the resulting (K+1)-
tuple be of even Hamming weight. The (K, K+1) single parity check code is the
subset of FK+1
2
consisting of those binary (K + 1)-tuples whose Hamming weight is
even.
Recall that the range of a mapping g: A →B is the subset of B comprising those
elements y ∈B to which there corresponds some x ∈A such that g(x) = y.
1The terminology here is not standard. In the Coding Theory literature a linear (K, N) F2
code is often called a “binary linear [N, K] code.”
712
Linear Binary Block Codes with Antipodal Signaling
Proposition 29.3.2 (F2 Encoders and Codes).
(i) If T: FK
2 →FN
2 is a linear (K, N) F2 encoder, then its range is a linear (K, N)
F2 code.
(ii) Every linear (K, N) F2 code is the range of some (nonunique) linear (K, N)
F2 encoder.
Proof. We begin with Part (i). Let T: FK
2 →FN
2 be a linear (K, N) F2 encoder.
That its range is a linear subspace of FN
2
follows from Proposition 29.2.1 (iii).
That its dimension must be K follows from Proposition 29.2.1 (iv) (see (29.10))
because the fact that T(·) is one-to-one implies, by Proposition 29.2.1 (ii), that
ker(T) = {0} so dim

ker(T)

= 0.
To prove Part (ii) we note that FK
2 is of dimension K and that, by deﬁnition, every
linear (K, N) F2 code is also of dimension K. The result now follows by noting
that there exists a one-to-one linear mapping between any two subspaces of equal
dimensions over the same ﬁeld (Proposition 29.2.1 (v)).
Any linear transformation from a ﬁnite-dimensional space to a ﬁnite-dimensional
space can be represented as matrix multiplication. A linear (K, N) F2 encoder is
no exception. What is perhaps unusual is that coding theorists use row vectors
to denote the data K-tuples and the N-tuples to which they are mapped. They
consequently use matrix multiplication from the left. This tradition is so ingrained
that we shall begrudgingly adopt it.
Deﬁnition 29.3.3 (Matrix Representation of an Encoder). We say that the linear
(K, N) F2 encoder T: FK
2 →FN
2 is represented by the generator matrix G if G
is a K × N matrix whose elements are in F2 and
T(d) = dG,
d ∈FK
2 .
(29.21)
Note that in the matrix multiplication in (29.21) we use F2 arithmetic, so the η-th
component of dG is d(1)·g(1,η)⊕· · ·⊕d(K)·g(K,η), where g(κ,η) is the Row-κ Column-η
component of the generator matrix G, and where d(κ) is the κ-th component of d.
For example, the (K, K + 1) F2 systematic single parity check encoder (29.20) is
represented by the K × (K + 1) generator matrix
G =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
· · ·
0
1
0
1
0
· · ·
0
1
0
0
1
· · ·
0
1
...
...
...
...
0
1
0
0
0
· · ·
1
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
(29.22)
The generator matrix G in (29.21) is uniquely speciﬁed by the linear transformation
T(·): its η-th row is the result of applying T(·) to the K-tuple (0, . . . , 0, 1, 0, . . . , 0)
(the K-tuple whose components are all zero except for the η-th, which is one).
29.3 Binary Linear Encoders and Codes
713
Moreover, every K × N binary matrix G deﬁnes a linear transformation T(·) via
(29.21), but this linear transformation need not be one-to-one. It is one-to-one if,
and only if, the subspace of FN
2 spanned by the rows of G is of dimension K.
Deﬁnition 29.3.4 (Generator Matrix). A matrix G is a generator matrix for a
given linear (K, N) F2 code if G is a binary K × N matrix such that the range of
the mapping d 	→dG is the given code.
Note that there may be numerous generator matrices for a given code. For example,
the matrix (29.22) is a generator matrix for the single parity check code.
But
there are others. Indeed, replacing any row of the above matrix by the sum of
that row and another diﬀerent row results in another generator matrix for this
code. Likewise, swapping two rows of a generator matrix for a code yields another
generator matrix for the code.
Coding theorists like to distinguish between a code property and an encoder
property.
Code properties are properties that are common to all encoders of
the same range. Encoder properties are speciﬁc to an encoder. Examples of code
properties are the blocklength and dimension. We shall soon encounter more. An
example of an encoder property is the property of being systematic:
Deﬁnition 29.3.5 (Systematic Encoder). A linear (K, N) F2 encoder T: FK
2 →FN
2
is said to be systematic (or strictly systematic) if, for every K-tuple

d1, . . . , dK

in FK
2 , the ﬁrst K components of T

(d1, . . . , dK)

are equal to d1, . . . , dK.
For example, the encoder (29.20) is systematic. An encoder whose range is the
single-parity check code and which is not systematic is the encoder

d1, . . . , dK

	→

d1, d1 ⊕d2, d2 ⊕d3, . . . , dK−1 ⊕dK, dK

.
(29.23)
The reader is encouraged to verify that if a linear (K, N) F2 encoder T: FK
2 →FN
2
is represented by the matrix G, then T(·) is systematic if, and only if, the K × K
matrix that results from deleting the last N −K columns of G is the K × K identity
matrix.
Deﬁnition 29.3.6 (Parity-Check Matrix). A parity-check matrix for a given
linear (K, N) F2 code is a binary matrix H with N columns such that a (row) N-
tuple c is in the code if, and only if, cHT is the all-zero (row) vector.
For example, a parity-check matrix for the (K, K + 1) single-parity check code is
the 1 × (K + 1) matrix
H = (1, 1, . . . , 1).
(Codes typically have numerous diﬀerent parity-check matrices, but the single-
parity check code is an exception.)
714
Linear Binary Block Codes with Antipodal Signaling
29.4
Binary Encoders with Antipodal Signaling
Deﬁnition 29.4.1.
(i) We say that a (K, N) binary-to-reals block encoder enc: {0, 1}K →RN is a
linear binary (K, N) block encoder with antipodal signaling if
enc(d) = ΥN

T(d)

,
d ∈FK
2 ,
(29.24)
where T: FK
2 →FN
2 is a linear (K, N) F2 encoder, and where ΥN(·) is the
componentwise antipodal mapping (29.17). Thus, if (X1, . . . , XN) denotes
the N-tuple produced by enc(·) when fed the data K-tuple (D1, . . . , DK), then
Xη =

+1
if the η-th components of T

(D1, . . . , DK)

is zero,
−1
otherwise.
(29.25)
(ii) A linear binary (K, N) block code with antipodal signaling is the range
of some linear binary (K, N) block encoder with antipodal signaling.
In analogy to Proposition 29.3.2, the range of every linear binary (K, N) block
encoder with antipodal signaling is a linear binary (K, N) block code with antipodal
signaling.
If enc(·) can be represented by the application of T(·) to the data K-tuple followed
by the application of the componentwise antipodal mapping ΥN, then we shall
write
enc = ΥN ◦T.
(29.26)
Since ΥN is invertible, there is a one-to-one correspondence between T and enc.
An important code property is the distribution of the result of applying an encoder
to IID random bits.
Proposition 29.4.2. Let T: FK
2 →FN
2 be a linear (K, N) F2 encoder.
(i) Applying T to a K-tuple of IID random bits results in a random N-tuple that
is uniformly distributed over range(T).
(ii) Applying ΥN ◦T to IID random bits produces an N-tuple that is uniformly
distributed over the range of range(T) under the componentwise antipodal
mapping ΥN.
Proof. Part (i) follows from the fact that the mapping T(·) is one-to-one. Part (ii)
follows from Part (i) and from the fact that ΥN(·) is one-to-one.
For example, it follows from Proposition 29.4.2 (ii) and from (29.16) that if we
feed IID random bits to any encoder (be it systematic or not) whose range is the
(K, K + 1) single parity check code and then employ the componentwise antipodal
mapping ΥN(·), then the resulting random (K + 1)-tuple (X1, . . . , XK+1) will be
uniformly distributed over the set
1
ξ1, . . . , ξK+1

∈{−1, +1}K+1 :
K+1
@
η=1
ξη = +1
2
.
29.5 Power and Operational Power Spectral Density
715
Corollary 29.4.3. Any property that is determined by the joint distribution of the
result of applying the encoder to IID random bits is a code property.
Examples of such properties are the power and operational power spectral density,
which are discussed next.
29.5
Power and Operational Power Spectral Density
To discuss the transmitted power and the operational power spectral density we
shall consider bi-inﬁnite block encoding (Section 14.5.2). We shall then use the
results of Section 14.5.2 and Section 15.4.3 to compute the power and operational
PSD of the transmitted signal in this mode.
The impatient reader who is only interested in the transmitted power for pulse
shapes satisfying the orthogonality condition (29.2) can apply the results of Sec-
tion 14.5.3 directly to obtain that, subject to the decay condition (14.46), the
transmitted power P is given by
P = A2
Ts
.
(29.27)
We next extend the discussion to general pulse shapes and to the operational PSD.
To remind the reader that we no longer assume the orthogonality condition (29.2),
we shall now denote the pulse shape by g(·) and assume that it is bandlimited
to W Hz and that it satisﬁes the decay condition (14.17). Before proceeding with
the analysis of the power and PSD, we wish to characterize linear binary (K, N)
block encoders with antipodal signaling that map IID random bits to zero-mean
N-tuples. Note that by Corollary 29.4.3 this is, in fact, a code property. Thus, if
enc = ΥN ◦T, then the question of whether enc(·) maps IID random bits to zero-
mean N-tuples depends only on the range of T. Aiding us in this characterization
is the following lemma on linear functionals. A linear functional on Fκ
2 is a linear
mapping from Fκ
2 to F2. The zero functional maps every κ-tuple in Fκ
2 to zero.
Lemma 29.5.1. Let L: FK
2 →F2 be a linear functional that is not the zero func-
tional. Then the RV X deﬁned by
X =

+1
if L

(D1, . . . , DK)

= 0,
−1
if L

(D1, . . . , DK)

= 1
(29.28)
is of zero mean whenever D1, . . . , DK are IID random bits.
Proof. We begin by expressing the expectation of X as
E[X] =

d∈FK
2
Pr[D = d] Υ

L(d)

= 2−K 
d∈FK
2
Υ

L(d)

716
Linear Binary Block Codes with Antipodal Signaling
= 2−K

d∈FK
2 : L(d)=0
(+1) + 2−K

d∈FK
2 : L(d)=1
(−1)
= 2−K
# L−1(0) −# L−1(1)

,
where
L−1(0) =

d ∈FK
2 : L(d) = 0

is the set of all K-tuples in FK
2 that are mapped by L(·) to 0, where L−1(1) is anal-
ogously deﬁned, and where # A denotes the number of elements in the set A. It
follows that to prove that E[X] = 0 it suﬃces to show that if L(·) is not determin-
istically zero, then the sets L−1(0) and L−1(1) have the same number of elements.
We prove this by exhibiting a one-to-one mapping from L−1(0) onto L−1(1). (If
there is a one-to-one mapping from a ﬁnite set A onto a ﬁnite set B, then A and B
must have the same number of elements.) To exhibit this mapping, note that the
assumption that L(·) is not the zero transformation implies that the set L−1(1) is
not empty. Let d∗be an element of this set, so
L(d∗) = 1.
(29.29)
The required mapping maps each d0 ∈L−1(0) to d0 ⊕d∗:
L−1(0) ∋d0 	→d0 ⊕d∗.
(29.30)
We next verify that it is a one-to-one mapping from L−1(0) onto L−1(1). That it is
one-to-one follows because if d0 ⊕d∗= d′
0 ⊕d∗then by adding d∗to both sides we
obtain d0 ⊕d∗⊕d∗= d′
0 ⊕d∗⊕d∗, i.e., that d0 = d′
0 (because d∗⊕d∗= 0). That
this mapping maps each element of L−1(0) to an element of L−1(1) follows because,
as we next show, if d0 ∈L−1(0), then L(d0 ⊕d∗) = 1. Indeed, if d0 ∈L−1(0), then
L(d0) = 0,
(29.31)
and consequently,
L(d0 ⊕d∗) = L(d0) ⊕L(d∗)
= 0 ⊕1
= 1,
where the ﬁrst equality follows from the linearity of L(·), and where the second
equality follows from (29.29) and (29.31). That the mapping is onto follows by
noting that if d1 is any element of L−1(1), then d1 ⊕d∗is in L−1(0) and it is
mapped by this mapping to d1.
Using this lemma we can show:
Proposition 29.5.2. Let (X1, . . . , XN) be the result of applying a linear binary
(K, N) block encoder with antipodal signaling to a binary K-tuple comprising IID
random bits.
(i) For every η ∈{1, . . . , N}, the RV Xη is either deterministically equal to +1,
or else of zero mean.
29.5 Power and Operational Power Spectral Density
717
(ii) For every η, η′ ∈{1, . . . , N}, the random variables Xη and Xη′ are either
deterministically equal to each other or else E[XηXη′] = 0.
Proof. Let the linear binary (K, N) block encoder with antipodal signaling enc(·)
be given by enc = ΥN ◦T, where T: FK
2 →FN
2 is one-to-one and linear. Let
(X1, . . . , XN) be the result of applying enc to the K-tuple D = (D1, . . . , DK),
where D1, . . . , DK are IID random bits.
To prove Part (i), ﬁx some η ∈{1, . . . , N}, and let L(·) be the linear functional that
maps d to the η-th component of T(d), so Xη = Υ

L(D)

, where D denotes the
row vector comprising the K IID random bits. If L(·) maps all data K-tuples to zero,
then Xη is deterministically equal to +1. Otherwise, E[Xη] = 0 by Lemma 29.5.1.
To prove Part (ii), let the matrix G represent the mapping T(·), so Xη = Υ

DG(·,η)
,
where G(·,η) denotes the η-th column of G. Expressing Xη′ in a similar way, we
obtain from (29.15)
XηXη′ = Υ

DG(·,η)
Υ

DG(·,η′)
= Υ

DG(·,η) ⊕DG(·,η′)
= Υ

D

G(·,η) ⊕G(·,η′)
.
(29.32)
Consequently, if we deﬁne the linear functional L: d 	→d

G(·,η) ⊕G(·,η′)
, then
XηXη′ = Υ

L(D)

. This linear functional is the zero functional if the η-th column
of G is identical to its η′-th column, i.e., if Xη is deterministically equal to Xη′.
Otherwise, it is not the zero functional, and E[XηXη′]

= E

Υ

L(D)

must be
zero (Lemma 29.5.1).
Proposition 29.5.3 (Producing Zero-Mean Uncorrelated Symbols). A linear bi-
nary (K, N) block encoder with antipodal signaling enc = ΥN ◦T produces zero-
mean uncorrelated symbols when fed IID random bits if, and only if, the columns
of the matrix G representing T(·) are distinct and neither of these columns is the
all-zero column.
Proof. The η-th symbol Xη produced by enc = ΥN ◦T when fed the K-tuple of
IID random bits D = (D1, . . . , DK) is given by
Xη = Υ

DG(·,η)
= Υ

D1 · G(1,η) ⊕· · · ⊕DK · G(K,η)
where G(·,η) is the η-th column of the K × N generator matrix of T(·). Since the
linear functional
d 	→d1 · G(1,η) ⊕· · · ⊕dK · G(K,η)
is the zero functional if, and only if,
G(1,η) = · · · = G(K,η) = 0,
(29.33)
718
Linear Binary Block Codes with Antipodal Signaling
it follows that Xη is deterministically zero if, and only if, the η-th column of G is
zero. From this and Lemma 29.5.1 it follows that all the symbols produced by enc
are of zero mean if, and only if, none of the columns of G is zero.
A similar argument shows that the product XηXη′, which by (29.32) is given by
Υ

D

G(·,η) ⊕G(·,η′)
,
is deterministically zero if, and only if, the functional
d 	→d1 · (G(1,η) ⊕G(1,η′)) ⊕· · · ⊕dK · (G(K,η) ⊕G(K,η′))
is zero, i.e., if, and only if, the η-th and η′-th columns of G are equal. Otherwise,
by Lemma 29.5.1, we have E[XηXη′] = 0.
Note 29.5.4. By Corollary 29.4.3 the property of producing zero-mean uncorre-
lated symbols is a code property.
Proposition 29.5.5 (Power and PSD). Let the linear binary (K, N) block encoder
with antipodal signaling enc = ΥN ◦T produce zero-mean uncorrelated symbols
when fed IID random bits, and let the pulse shape g satisfy the decay condition
(14.17). Then the transmitted power P in bi-inﬁnite block-encoding mode is given
by
P = A2
Ts
∥g∥2
2
(29.34)
and the operational PSD is
SXX(f) = A2
Ts
ˆg(f)
2,
f ∈R.
(29.35)
Proof. The expression (29.34) for the power follows either from (14.33) or (14.38).
The expression for the operational PSD follows either from (15.23) or from (15.26).
Engineers rarely check whether an encoder produces uncorrelated symbols when
fed IID random bits. The reason may be that they usually deal with pulse shapes
φ satisfying the orthogonality condition (29.2) and the decay condition (14.46).
For such pulse shapes, the power is given by (29.27) without any additional as-
sumptions. Also, by Theorem 15.4.1, the bandwidth of the PAM signal is typically
equal to the bandwidth of the pulse shape. In fact, by that theorem, for linear
binary (K, N) block encoders with antipodal signaling
bandwidth of PAM signal = bandwidth of pulse shape,
(29.36)
whenever A ̸= 0; the pulse shape g is a Borel measurable function satisfying the
decay condition (14.17) for some α, β > 0; and the encoder produces zero-mean
29.6 Performance Criteria
719
symbols when fed IID random bits. Thus, if one is not interested in the exact form
of the operational PSD but only in its support, then one need not check whether
the encoder produces uncorrelated symbols when fed IID random bits.
29.6
Performance Criteria
Designing an optimal decoder for linear binary block encoders with antipodal sig-
naling is conceptually very simple but algorithmically very diﬃcult. The structure
of the decoder depends on what we mean by “optimal.” In this chapter we focus
on two notions of optimality: minimizing the probability of a block error—also
called message error—and minimizing the probability of a bit error. Referring
to Figure 28.1, we say that a block error occurred in decoding the ν-th block if
at least one of the data bits

D(ν−1)K+1, . . . , D(ν−1)K+K

was incorrectly decoded.
We say that a bit error occurred in decoding the j-th bit if Dj was incorrectly
decoded.
We consider the case where IID random bits are transmitted in block-mode, and
the transmitted waveform is corrupted by additive Gaussian noise that is white
with respect to the bandwidth W of the pulse shape. The pulse shape is assumed
to satisfy the orthonormality condition (29.2) and the decay condition (14.17).
From Proposition 28.3.1 it follows that for both optimality criteria, there is no
loss of optimality in feeding the received waveform to a matched ﬁlter for φ and
in basing the decision on the ﬁlter’s output sampled at integer multiples of Ts.
Moreover, for the purposes of decoding a given message, it suﬃces to consider only
the samples corresponding to the symbols that were produced when the encoder
encoded the given message (Proposition 28.4.2). Similarly, for decoding a given
data bit it suﬃces to consider only the samples corresponding to the symbols that
were produced when the encoder encoded the message of which the given bit is part.
These observations lead us (as in Section 28.4.3) to the discrete-time single-block
model (28.34). For convenience, we repeat this model here (with the additional
assumption that the data are IID random bits):

X1, . . . , XN

= enc

D1, . . . , DK

;
(29.37a)
Yη = AXη + Zη,
η ∈{1, . . . , N};
(29.37b)
Z1, . . . , ZN ∼IID N
	
0, N0
2

;
(29.37c)
D1, . . . , DK ∼IID U ({0, 1}) ,
(29.37d)
where (Z1, . . . , ZN) are independent of (D1, . . . , DK).
We also introduce some
additional notation. We use xη(d) for the η-th component of the N-tuple to which
the binary K-tuple d is mapped by enc(·):
xη(d) ≜η-th component of enc(d),

η ∈{1, . . . , N}, d ∈FK
2

.
(29.38)
Denoting the conditional density of (Y1, . . . , YN) given (X1, . . . , XN) by fY|X(·),
we have for every y ∈RN of components y1, . . . , yN and for every x ∈{−1, +1}N
720
Linear Binary Block Codes with Antipodal Signaling
Parameter
In Section 21.6
In Section 29.7
number of observations
J
N
number of hypotheses
M
2K
set of hypotheses
{1, . . . , M}
FK
2
dummy hypothesis variable
m
d
prior
{πm}
uniform
conditional mean tuple

s(1)
m , . . . , s(J)
m


Ax1(d), . . . , AxN(d)

conditional variance
σ2
N0/2
Table 29.1: A conversion table for the setups of Section 21.6 and of Section 29.7.
of components x1, . . . , xN
fY|X=x(y) = (πN0)−N/2
N
@
η=1
exp
	
−(yη −Axη)2
N0

.
(29.39)
Likewise, for every y ∈RN and every data tuple d ∈FK
2 ,
fY|D=d(y) = (πN0)−N/2
N
@
η=1
exp
-
−

yη −Axη(d)
2
N0
.
.
(29.40)
29.7
Minimizing the Block Error Rate
29.7.1
Optimal Decoding
To minimize the probability of a block error, we need to use the random N-vector
Y = (Y1, . . . , YN) to guess the K-tuple D =

D1, . . . , DK

. This is the type of
problem we addressed in Section 21.6. The translation between the setup of that
section and our current setup is summarized in Table 29.1: the number of observa-
tions, which was there J, is here N; the number of hypotheses, which was there M,
is here 2K; the set of possible messages, which was there M = {1, . . . , M}, is here
the set of binary K-tuples FK
2 ; the dummy variable for a generic message, which
was there m, is here the binary K-tuple d; the prior, which was there {πm}, is
here uniform; the mean tuple corresponding to the m-th message, which was there

s(1)
m , . . . , s(J)
m

is here

Ax1(d), . . . , AxN(d)

(see (29.38)); and the conditional
variance of each observation, which was there σ2, is here N0/2.
Because all the symbols produced by the encoder take value in {−1, +1}, it follows
that
N

η=1

Axη(d)
2 = A2N,
d ∈FK
2 ,
so all the mean tuples are of equal Euclidean norm. From Proposition 21.6.1 (iii)
we thus obtain that, to minimize the probability of a block error, our guess should
be the K-tuple d∗that satisﬁes
N

η=1
xη(d∗) Yη = max
d∈FK
2
N

η=1
xη(d) Yη,
(29.41)
29.7 Minimizing the Block Error Rate
721
with ties being resolved uniformly at random among the data tuples that achieve
the maximum. Our guess should thus be the data sequence that when fed to the
encoder produces the {±1}-valued N-tuple of highest correlation with the observed
tuple Y.
Note that, by deﬁnition, all block encoders are one-to-one mappings
and thus the mean tuples are distinct. Consequently, by Proposition 21.6.2, the
probability that more than one tuple d∗satisﬁes (29.41) is zero.
Since guessing the data tuple is equivalent to guessing the N-tuple to which it is
mapped, we can also describe the optimal decision rule in terms of the encoder’s
output.
Proposition 29.7.1 (The Max-Correlation Decision Rule). Consider the problem
of guessing D based on Y for the setup of Section 29.6.
(i) Picking at random a message from the set

˜d ∈FK
2 :
N

η=1
xη(˜d) Yη = max
d∈FK
2
N

η=1
xη(d) Yη
!
(29.42)
minimizes the probability of incorrectly guessing D.
(ii) The probability that the above set contains more than one element is zero.
(iii) For the problem of guessing the encoder’s output, picking at random an N-
tuple from the set

˜x ∈range(enc) :
N

η=1
˜xη Yη =
max
x∈range(enc)
N

η=1
xη Yη
!
(29.43)
minimizes the probability of error. This set contains more than one element
with probability zero.
Conceptually, the problem of ﬁnding an N-tuple that has the highest correlation
with (Y1, . . . , YN) among all the N-tuples in the range of enc(·) is very simple: one
goes over the list of all the 2K N-tuples that are in the range of enc(·) and picks
the one that has the highest correlation with (Y1, . . . , YN). But algorithmically
this is very diﬃcult because 2K is in most applications a huge number. It is one of
the challenges of Coding Theory to come up with encoders for which the decoding
does not require an exhaustive search over all 2K tuples.
As we shall see, the
single parity check code is an example of such a code. But the performance of this
encoder is, alas, not stellar.
29.7.2
Wagner’s Rule
For the (K, K + 1) systematic single parity check encoder (29.20), the decoding can
be performed very eﬃciently using a decision algorithm that is called Wagner’s
Rule in honor of C.A. Wagner. Unlike the brute-force approach that considers all
possible data tuples and which thus has a complexity which is exponential in K,
the complexity of Wagner’s Rule is linear in K.
722
Linear Binary Block Codes with Antipodal Signaling
Wagner’s Rule can be summarized as follows. Consider the (K + 1) tuple
ξη ≜

+1
if Yη ≥0,
−1
otherwise,
η = 1, . . . , K + 1.
(29.44)
If this tuple has an even number of negative components, then guess that the en-
coder’s output is (ξ1, . . . , ξK+1) and that the data sequence is thus the inverse of
(ξ1, . . . , ξK) under the componentwise antipodal mapping ΥK, i.e., that the data
tuple is (1 −ξ1)/2, . . . , (1 −ξK)/2. Otherwise, ﬂip the sign of ξη∗corresponding to
the Yη∗of smallest magnitude. I.e., guess that the encoder’s output is
ξ1, . . . , ξη∗−1, −ξη∗, ξη∗+1 . . . , ξK+1,
(29.45)
and that the data bits are
1 −ξ1
2
, . . . , 1 −ξη∗−1
2
, 1 + ξη∗
2
, 1 −ξη∗+1
2
. . . , 1 −ξK
2
,
(29.46)
where η∗is the element of {1, . . . , K + 1} satisfying
|Yη∗| =
min
1≤η≤K+1 |Yη|.
(29.47)
Proof that Wagner’s Rule is Optimal. Recall that the (K, K + 1) single parity
check code with antipodal signaling consists of all ±1-valued (K + 1)-tuples having
an even number of −1’s. We seek to ﬁnd the tuple that among all such tuples max-
imizes the correlation with the received tuple (Y1, . . . , YK+1). The tuple deﬁned in
(29.44) is the tuple that among all tuples in {−1, +1}K+1 has the highest correla-
tion with (Y1, . . . , YK+1). Since ﬂipping the sign of ξη reduces the correlation by
2|Yη|, the tuple (29.45) has the second-highest correlation among all the tuples in
{−1, +1}K+1. Since the tuples (29.44) and (29.45) diﬀer in one component, exactly
one of them has an even number of negative components. That tuple thus maxi-
mizes the correlation among all tuples in {−1, +1}K+1 that have an even number
of negative components and is thus the tuple we are after.
Since the encoder is systematic, the data tuple that generates a given encoder
output is easily found by considering the ﬁrst K components of the encoder output
and by then applying the mapping +1 	→0 and −1 	→1, i.e., ξ 	→(1 −ξ)/2.
29.7.3
The Probability of a Block Error
We next address the performance of the detector that we designed in Section 29.7.1
when we sought to minimize the probability of a block error. We continue to assume
that the encoder is a linear binary (K, N) block encoder with antipodal signaling,
so the encoder function enc(·) can be written as enc = ΥN ◦T where T: FK
2 →FN
2
is a linear one-to-one mapping and ΥN(·) is the componentwise antipodal mapping.
29.7 Minimizing the Block Error Rate
723
An Upper Bound
It is usually very diﬃcult to precisely evaluate the probability of a block error. A
very useful bound is the Union Bound, which we encountered in Section 21.6.3.
Denoting by pMAP(error|D = d) the probability of error of our guessing rule con-
ditional on the binary K-tuple D = d being fed to the encoder, we can use (21.60),
Table 29.1, and (29.18) to obtain
pMAP(error|D = d) ≤

d′∈FK
2 \{d}
Q
⎛
⎝

2A2dH

T(d′), T(d)

N0
⎞
⎠.
(29.48)
It is customary to group all the equal terms on the RHS of (29.48) and to write
the bound in the equivalent form
pMAP(error|D = d) ≤
N

ν=1
#
'
d′ ∈FK
2 : dH

T(d′), T(d)

= ν
(
Q
⎛
⎝

2A2ν
N0
⎞
⎠,
(29.49)
where
#
'
d′ ∈FK
2 : dH

T(d′), T(d)

= ν
(
(29.50)
is the number of data tuples that are mapped by T(·) to a binary N-tuple that
is at Hamming distance ν from T(d), and where the sum excludes ν = 0 because
the fact that T(·) is one-to-one implies that if d′ ̸= d then the Hamming distance
between T(d′) and T(d) must be at least one.
We next show that the linearity of T(·) implies that the RHS of (29.49) does not
depend on d. (In Section 29.9 we show that this is also true of the LHS.) To this
end we show that for every ν ∈{1, . . . , N} and for every d ∈FK
2 ,
#
'
d′ ∈FK
2 : dH

T(d′), T(d)

= ν
(
= #
'
˜d ∈FK
2 : wH

T(˜d)

= ν
(
(29.51)
= #

c ∈range(T) : wH(c) = ν

,
(29.52)
where the RHS of (29.51) is the evaluation of the LHS at d = 0. To prove (29.51)
we note that the mapping d′ 	→d′ ⊕d is a one-to-one mapping from the set whose
cardinality is written on the LHS to the set whose cardinality is written on the
RHS, because

dH

T(d′), T(d)

= ν

⇐⇒

wH

T(d) ⊕T(d′)

= ν

⇐⇒

wH

T(d ⊕d′)

= ν

,
where the ﬁrst equivalence follows from (29.13), and where the second equivalence
follows from the linearity of T(·). To prove (29.52) we merely substitute c for T(˜d)
in (29.51) and use the fact that T(·) is one-to-one.
Combining (29.49) with (29.52) we obtain the union bound
pMAP(error|D = d) ≤
N

ν=1
#

c ∈range(T) : wH(c) = ν

Q
⎛
⎝

2A2ν
N0
⎞
⎠. (29.53)
724
Linear Binary Block Codes with Antipodal Signaling
The list of N + 1 nonnegative integers

#

c ∈range(T) : wH(c) = 0

, . . . , #

c ∈range(T) : wH(c) = N

(whose ﬁrst term is equal to one and whose terms sum to 2K) is called the weight
enumerator of the code.
For example, for the (K, K + 1) single parity check code
#
'
˜d ∈FK
2 : wH

T(˜d)

= ν
(
=

0
if ν is odd,
K+1
ν

if ν is even,
ν = 0, . . . , K + 1
because this code consists of all (K + 1)-tuples of even Hamming weight. Conse-
quently, this code’s weight enumerator is
-
1, 0,
	K + 1
2

, 0,
	K + 1
4

, 0, . . . , 0,
	K + 1
K + 1

.
,
if K is odd;
-
1, 0,
	K + 1
2

, 0,
	K + 1
4

, 0, . . . ,
	K + 1
K

, 0
.
,
if K is even.
The minimum Hamming distance dmin,H of a linear (K, N) F2 code is the
smallest Hamming distance between distinct elements of the code. (If K = 0, i.e.,
if the only codeword is the all-zero codeword, then, by convention, the minimum
distance is said to be inﬁnite.) By (29.52) it follows that (for K > 0) the minimum
Hamming distance of a code is also the smallest weight that a nonzero codeword
can have
dmin,H =
min
c∈range(T)\{0} wH(c).
(29.54)
With this deﬁnition we can rewrite (29.53) as
pMAP(error|D = d) ≤
N

ν=dmin,H
#

c ∈range(T) : wH(c) = ν

Q
⎛
⎝

2A2ν
N0
⎞
⎠.
(29.55)
Engineers sometimes approximate the RHS of (29.55) by its ﬁrst term:
#

c ∈range(T) : wH(c) = dmin,H

Q
⎛
⎝

2A2dmin,H
N0
⎞
⎠.
(29.56)
This is reasonable when A2/N0 ≫1 because the Q(·) function decays very rapidly;
see (19.18).
The term
#

c ∈range(T) : wH(c) = dmin,H

(29.57)
is sometimes called the number of nearest neighbors.
29.8 Minimizing the Bit Error Rate
725
A Lower Bound
Using the results of Section 21.6.4, we can obtain a lower bound on the probability
of a block error. Indeed, by (21.66), Table 29.1, (29.18), the monotonicity of Q(·),
and the deﬁnition of dmin,H
pMAP(error|D = d) ≥Q
⎛
⎝

2A2dmin,H
N0
⎞
⎠.
(29.58)
29.8
Minimizing the Bit Error Rate
In some applications we want to minimize the number of data bits that are incor-
rectly decoded. This performance criterion leads to a diﬀerent guessing rule, which
we derive and analyze in this section.
29.8.1
Optimal Decoding
We next derive the guessing rule that minimizes the average probability of a bit
error, or the Bit Error Rate. Conceptually, this is simple. For each κ ∈{1, . . . , K}
our guess of the κ-th data bit Dκ should minimize the probability of error. This
problem falls under the category of binary hypothesis testing, and, since Dκ is a
priori equally likely to be 0 or 1, the Maximum-Likelihood rule of Section 20.8 is
optimal. To compute the likelihood-ratio function, we treat the other data bits
D1, . . . , Dκ−1, Dκ+1, . . . , DK as unobserved random parameters (Section 20.15.1).
Thus, using (20.101) with the random parameter Θ now corresponding to the tuple
(D1, . . . , Dκ−1, Dκ+1, . . . , DK) we obtain2
fY|Dκ=0(y1, . . . , yN)
= 2−(K−1)

d∈Aκ,0
fY1,...,YN|D=d(y1, . . . , yN)
(29.59)
= 2−(K−1)(πN0)−N/2

d∈Aκ,0
N
@
η=1
exp
-
−

yη −Axη(d)
2
N0
.
,
(29.60)
where the set Aκ,0 consists of those tuples in FK
2 whose κ-th component is zero
Aκ,0 =
'
(d1, . . . , dK) ∈FK
2 : dκ = 0
(
.
(29.61)
Likewise,
fY|Dκ=1(y1, . . . , yN)
= 2−(K−1)

d∈Aκ,1
fY1,...,YN|D=d(y1, . . . , yN)
(29.62)
= 2−(K−1)(πN0)−N/2

d∈Aκ,1
N
@
η=1
exp
-
−

yη −Axη(d)
2
N0
.
,
(29.63)
2Our assumption that the data are IID random bits guarantees that the random parameter
Θ ≜(D1, . . . , Dκ−1, Dκ+1, . . . , DK) is independent of the RV Dκ that we wish to guess.
726
Linear Binary Block Codes with Antipodal Signaling
where we similarly deﬁne
Aκ,1 =
'
(d1, . . . , dK) ∈FK
2 : dκ = 1
(
.
(29.64)
Using Theorem 20.7.1 and (29.60) & (29.63) we obtain the following.
Proposition 29.8.1 (Minimizing the BER). Consider the problem of guessing Dκ
based on Y for the setup of Section 29.6, where κ ∈{1, . . . , K}. The decision rule
that guesses “Dκ = 0” if

d∈Aκ,0
N
@
η=1
exp
-
−

yη −Axη(d)
2
N0
.
>

d∈Aκ,1
N
@
η=1
exp
-
−

yη −Axη(d)
2
N0
.
;
that guesses “Dκ = 1” if

d∈Aκ,0
N
@
η=1
exp
-
−

yη −Axη(d)
2
N0
.
<

d∈Aκ,1
N
@
η=1
exp
-
−

yη −Axη(d)
2
N0
.
;
and that guesses at random in case of equality minimizes the probability of guessing
the data bit Dκ incorrectly.
The diﬃculty in implementing this decision rule is that, unless we exploit some
algebraic structure, the computation of the sums above has exponential complexity
in K, because the number of terms in each sum is 2K−1.
It is interesting to note that, unlike the decision rule that minimizes the probability
of a block error, the above decision rule depends on the value of N0/2.
29.8.2
The Probability of a Bit Error
We next obtain bounds on the probability that the detector of Proposition 29.8.1
errs in guessing the κ-th data bit Dκ. We denote this probability by p∗
κ.
An Upper Bound
Since the detector of Proposition 29.8.1 is optimal, the probability that it errs
in decoding the κ-th data bit Dκ cannot exceed the probability of error of the
suboptimal rule whose guess for Dκ is the κ-th bit of the message produced by the
detector of Section 29.7. Thus, if φMAP(·) denotes the decision rule of Section 29.7,
then
p∗
κ ≤Pr

D ⊕φMAP(Y) ∈Aκ,1

,
κ ∈{1, . . . , K},
(29.65)
where the set Aκ,1 was deﬁned in (29.64) as the set of messages whose κ-th com-
ponent is equal to one, and where Y is the observed N-tuple whose components
are given in (29.37b).
Since the data are IID random bits, we can rewrite (29.65) as
p∗
κ ≤1
2K

d∈FK
2

˜d∈Aκ,1
Pr

φMAP(Y) = d ⊕˜d
 D = d

,
κ ∈{1, . . . , K}.
(29.66)
29.8 Minimizing the Bit Error Rate
727
Since φMAP(Y) can only equal d ⊕˜d if Y is at least as close in Euclidean distance
to enc(d ⊕˜d) as it is to enc(d), it follows from Lemma 20.14.1, Table 29.1, and
(29.18) that
Pr

φMAP(Y) = d ⊕˜d
 D = d

≤Q
⎛
⎝
AdE

ΥN

T(d ⊕˜d)

, ΥN

T(d)

2
3
N0
2
⎞
⎠
= Q
⎛
⎜
⎜
⎝
A
B
B
CA2d2
E

ΥN

T(d ⊕˜d)

, ΥN

T(d)

2N0
⎞
⎟
⎟
⎠
= Q
⎛
⎝

2A2dH

T(d ⊕˜d), T(d)

N0
⎞
⎠
= Q
⎛
⎝

2A2wH

T(d ⊕˜d) ⊕T(d)

N0
⎞
⎠
= Q
⎛
⎝

2A2wH

T(˜d)

N0
⎞
⎠.
(29.67)
It follows from (29.66) and (29.67) upon noting that RHS of (29.67) does not
depend on the transmitted message d that
p∗
κ ≤

˜d∈Aκ,1
Q
⎛
⎝

2A2wH

T(˜d)

N0
⎞
⎠,
κ ∈{1, . . . , K}.
(29.68)
This bound is sometimes written as
p∗
κ ≤
N

ν=dmin,H
γ(ν, κ) Q
⎛
⎝

2A2ν
N0
⎞
⎠,
κ ∈{1, . . . , K},
(29.69a)
where γ(ν, κ) denotes the number of elements ˜d of FK
2 whose κ-th component is
equal to one and for which T(˜d) is of Hamming weight ν, i.e.,
γ(ν, κ) = #
'
˜d ∈Aκ,1 : wH

T(˜d)

= ν
(
,
(29.69b)
and where the minimum Hamming distance dmin,H is deﬁned in (29.54).
Sometimes one is more interested in the arithmetic average of p∗
κ
1
K
K

κ=1
p∗
κ,
(29.70)
728
Linear Binary Block Codes with Antipodal Signaling
which is the optimal bit error rate. We next show that (29.68) leads to the
upper bound
1
K
K

κ=1
p∗
κ ≤1
K

d∈FK
2
wH(d) Q
⎛
⎝

2A2wH

T(d)

N0
⎞
⎠.
(29.71)
This follows from the calculation
K

κ=1
p∗
κ ≤
K

κ=1

d∈Aκ,1
Q
⎛
⎝

2A2wH

T(d)

N0
⎞
⎠
=
K

κ=1

d∈FK
2
Q
⎛
⎝

2A2wH

T(d)

N0
⎞
⎠I{d ∈Aκ,1}
=

d∈FK
2
Q
⎛
⎝

2A2wH

T(d)

N0
⎞
⎠
K

κ=1
I{d ∈Aκ,1}
=

d∈FK
2
Q
⎛
⎝

2A2wH

T(d)

N0
⎞
⎠wH(d),
where the inequality in the ﬁrst line follows from (29.68); the equality in the second
by introducing the indicator function for the set Aκ,1 and extending the summa-
tion; the equality in the third line by changing the order of summation; and the
equality in the last line by noting that every d ∈FK
2 is in exactly wH(d) of the sets
A1,1, . . . , AK,1.
A Lower Bound
We next show that, for every κ ∈{1, . . . , K}, the probability p∗
κ that the optimal
detector for guessing the κ-th data bit errs is lower-bounded by
p∗
κ ≥max
d∈Aκ,1 Q
⎛
⎝

2A2wH

T(d)

N0
⎞
⎠,
(29.72)
where Aκ,1 denotes the set of binary K-tuples whose κ-th component is equal to
one (29.64). To derive (29.72), ﬁx some d ∈Aκ,1 and note that for every d′ ∈FK
2

d′ ∈Aκ,0

⇐⇒

d′ ⊕d ∈Aκ,1

.
(29.73)
This allows us to express fY|Dκ=1(y) for every y ∈RN as
fY|Dκ=1(y) = 2−(K−1)

˜d∈Aκ,1
fY|D=˜d(y)
= 2−(K−1)

d′∈Aκ,0
fY|D=d⊕d′(y),
(29.74)
29.9 Assuming the All-Zero Codeword
729
where the ﬁrst equality follows from (29.62) and the second from (29.73).
Using the exact expression for the probability of error in binary hypothesis testing
(20.20) we have:
p∗
κ = 1
2

y∈RN min
'
fY|Dκ=0(y), fY|Dκ=1(y)
(
dy
= 1
2

min
1
2−(K−1)

d′∈Aκ,0
fY|D=d′(y), 2−(K−1)

d′∈Aκ,0
fY|D=d⊕d′(y)
2
dy
= 1
2 2−(K−1)

min
1

d′∈Aκ,0
fY|D=d′(y),

d′∈Aκ,0
fY|D=d⊕d′(y)
2
dy
≥1
2 2−(K−1)


d′∈Aκ,0
min
'
fY|D=d′(y), fY|D=d⊕d′(y)
(
dy
= 2−(K−1)

d′∈Aκ,0
 1
2 min
'
fY|D=d′(y), fY|D=d⊕d′(y)
(
dy
= 2−(K−1)

d′∈Aκ,0
Q
⎛
⎝

2A2dH

T(d′), T(d′ ⊕d)

N0
⎞
⎠
= 2−(K−1)

d′∈Aκ,0
Q
⎛
⎝

2A2wH

T(d)

N0
⎞
⎠
= Q
⎛
⎝

2A2wH

T(d)

N0
⎞
⎠,
d ∈Aκ,1,
where the ﬁrst line follows from (20.20); the second by the explicit forms (29.59) &
(29.74) of the conditional densities fY|Dκ=0(·) and fY|Dκ=1(·); the third by pulling
the common term 2−(K−1) outside the minimum; the fourth because the minimum
between two sums with an equal number of terms is lower-bounded by the sum of
the minima between the corresponding terms; the ﬁfth by swapping the summation
and integration; the sixth by Expression (20.20) for the optimal probability of error
for the binary hypothesis testing between D = d′ and D = d ⊕d′; the seventh by
the linearity of T(·); and the ﬁnal line because the cardinality of Aκ,0 is 2(K−1).
Since the above derivation holds for every d ∈Aκ,1, we may choose d to yield the
tightest bound, thus establishing (29.72).
29.9
Assuming the All-Zero Codeword
When simulating linear binary block encoders with antipodal signaling over the
Gaussian channel we rarely simulate the data as IID random bits.
Instead we
assume that the message that is fed to the encoder is the all-zero message and that
the encoder’s output is hence the N-tuple whose components are all +1. In this
section we shall explain why it is correct to do so. More speciﬁcally, we shall show
that pMAP(error|D = d) does not depend on the message d and is thus equal to
pMAP(error|D = 0). We shall also prove an analogous result for the decoder that
730
Linear Binary Block Codes with Antipodal Signaling
minimizes the probability of a bit error. The proofs are based on two features of
our setup: the encoder is linear and the Gaussian channel with antipodal inputs is
symmetric in the sense that
fY |X=−1(y) = fY |X=+1(−y),
y ∈R.
(29.75)
Indeed, by (29.37b),
fY |X=−1(y) =
1
√πN0
e−(y+A)2
N0
=
1
√πN0
e−(−y−A)2
N0
= fY |X=+1(−y),
y ∈R.
Deﬁnition 29.9.1 (Memoryless Binary-Input/Output-Symmetric Channel). We
say that the conditional distribution of Y = (Y1, . . . , YN) given X = (X1, . . . , XN)
corresponds to a memoryless binary-input/output-symmetric channel if
fY|X=x(y) =
N
@
η=1
fY |X=xη(yη),
x ∈{−1, +1}N,
(29.76a)
where
fY |X=−1(y) = fY |X=+1(−y),
y ∈R.
(29.76b)
For every d ∈FK
2 deﬁne the mapping ψd : RN →RN as
ψd :

y1, . . . , yN

	→

y1x1(d), . . . , yNxN(d)

.
(29.77)
The function ψd(·) thus changes the sign of those components of its argument
that correspond to the negative components of enc(d). The key properties of this
mapping are summarized in the following lemma.
Lemma 29.9.2. As in (29.38), let xη(d) denote the result of applying the antipodal
mapping Υ to the η-th component of T(d), where T: FK
2 →FN
2 is some one-to-one
linear mapping. Let the conditional law of (Y1, . . . , YN) given D = d be given by
?N
η=1 fY |X=xη(d)(yη), where fY |X(·) satisﬁes the symmetry property (29.75). Let
ψd(·) be deﬁned as in (29.77). Then
(i) ψ0(·) maps each y ∈RN to itself.
(ii) For any d, d′ ∈FK
2 the composition of ψd′ with ψd is given by ψd⊕d′:
ψd ◦ψd′ = ψd⊕d′.
(29.78)
(iii) ψd is equal to its inverse
ψd

ψd(y)

= y,
y ∈RN.
(29.79)
(iv) For every d ∈FK
2 , the Jacobian of the mapping ψd(·) is one.
29.9 Assuming the All-Zero Codeword
731
(v) For every d ∈FK
2 and every y ∈RN,
fY|D=d(y) = fY|D=0

ψd(y)

.
(29.80)
(vi) For any d, d′ ∈FK
2 and every y ∈RN,
fY|D=d′
ψd(y)

= fY|D=d′⊕d(y).
(29.81)
Proof. Part (i) follows from the deﬁnition (29.77) because the linearity of T(·) and
the deﬁnition of ΥN guarantee that xη(0) = +1, for all η ∈{1, . . . , N}. Part (ii)
follows by linearity and from (29.15):
(ψd ◦ψd′)(y1, . . . , yN) = ψd

y1x1(d′), . . . , yNxN(d′)

=

y1x1(d′)x1(d), . . . , yNxN(d′)xN(d)

=

y1x1(d′ ⊕d), . . . , yNxN(d′ ⊕d)

= ψd⊕d′(y1, . . . , yN),
where in the third equality we used (29.15) and the linearity of the encoder.
Part (iii) follows from Parts (i) and (ii). Part (iv) follows from Part (iii) or di-
rectly by computing the partial derivative matrix and noting that it is diagonal
with the diagonal elements being ±1 only. Part (v) follows from (29.75). To prove
Part (vi) we substitute d′ for d and ψd(y) for y in Part (v) to obtain
fY|D=d′
ψd(y)

= fY|D=0

ψd′
ψd(y)

= fY|D=0

ψd⊕d′(y)

= fY|D=d⊕d′(y),
where the second equality follows from Part (ii), and where the third equality
follows from Part (v).
With the aid of this lemma we can now justify the all-zero assumption in the
analysis of the probability of a block error. We shall state the result not only for
the Gaussian setup but also for the more general case where the conditional den-
sity fY|X(·) corresponds to a memoryless binary-input/output-symmetric channel.
Theorem 29.9.3. Consider the setup of Section 29.6 with the conditional den-
sity fY|X(·) corresponding to a memoryless binary-input/output-symmetric chan-
nel. Let pMAP(error|D = d) denote the conditional probability of a block error for
the detector of Proposition 29.7.1, conditional on the data tuple being d. Then,
pMAP(error|D = d) = pMAP(error|D = 0),
d ∈FK
2 .
(29.82)
Proof. The proof of this result is not very diﬃcult, but there is a slight technicality
that arises from the way ties are resolved. Since on the Gaussian channel ties occur
with probability zero (Proposition 21.6.2), this issue could be ignored. But we
prefer not to ignore it because we would like the proof to apply also to channels
satisfying (29.76) that are not necessarily Gaussian.
To address ties, we shall
732
Linear Binary Block Codes with Antipodal Signaling
assume that they are resolved at random as in Proposition 29.7.1 (i.e., as in the
Deﬁnition 21.3.2 of the MAP rule).
For every d ∈FK
2 and every ν ∈{1, . . . , 2K}, deﬁne the set Dd,ν ⊂RN to contain
those y ∈RN for which the following two conditions hold:
fY|D=d(y) = max
d′∈FK
2
fY|D=d′(y),
(29.83a)
#
'
˜d ∈FK
2 : fY|D=˜d(y) = fY|D=d(y)
(
= ν.
(29.83b)
Whenever y ∈Dd,ν, the MAP rule guesses “D = d” with probability 1/ν. Thus,
pMAP(error|D = d) = 1 −
2K

ν=1
1
ν

y∈RN I{y ∈Dd,ν} fY|D=d(y) dy.
(29.84)
The key is to note that, by Lemma 29.9.2 (v), for every d ∈FK
2 and ν ∈{1, . . . , 2K}

y ∈Dd,ν

⇐⇒

ψd(y) ∈D0,ν

.
(29.85)
(Please pause to verify this.) Consequently, by (29.84),
pMAP(error|D = d) = 1 −
2K

ν=1
1
ν

y∈RN I{y ∈Dd,ν} fY|D=d(y) dy
= 1 −
2K

ν=1
1
ν

y∈RN I{y ∈Dd,ν} fY|D=0

ψd(y)

dy
= 1 −
2K

ν=1
1
ν

˜y∈RN I{ψd(˜y) ∈Dd,ν} fY|D=0(˜y) d˜y
= 1 −
2K

ν=1
1
ν

˜y∈RN I{˜y ∈D0,ν} fY|D=0(˜y) d˜y
= pMAP(error|D = 0),
where the ﬁrst equality follows from (29.84); the second by Lemma 29.9.2 (v); the
third by deﬁning ˜y ≜ψd(y) and using Parts (iv) and (iii) of Lemma 29.9.2; the
fourth by (29.85); and the ﬁnal equality by (29.84).
We now formulate a similar result for the detector of Proposition 29.8.1.
Let
p∗
κ(error|D = d) denote the conditional probability that the decoder of Proposi-
tion 29.8.1 incorrectly decodes the κ-th data bit, conditional on the tuple d being
fed to the encoder. Since the data are IID random bits,
p∗
κ = 2−K 
d∈FK
2
p∗
κ(error|D = d).
(29.86)
29.9 Assuming the All-Zero Codeword
733
Since ties are resolved at random
p∗
κ(error|D = d)
= Pr


d′∈Aκ,0
fY|D=d′(Y) >

d′∈Aκ,1
fY|D=d′(Y)
 D = d

+ 1
2 Pr


d′∈Aκ,0
fY|D=d′(Y) =

d′∈Aκ,1
fY|D=d′(Y)
 D = d

, d ∈Aκ,1, (29.87)
and
p∗
κ(error|D = d)
= Pr


d′∈Aκ,0
fY|D=d′(Y) <

d′∈Aκ,1
fY|D=d′(Y)
 D = d

+ 1
2 Pr


d′∈Aκ,0
fY|D=d′(Y) =

d′∈Aκ,1
fY|D=d′(Y)
 D = d

, d ∈Aκ,0.(29.88)
Theorem 29.9.4. Under the assumptions of Theorem 29.9.3, we have for every
κ ∈{1, . . . , K}
p∗
κ(error|D = d) = p∗
κ(error|D = 0),
d ∈FK
2 ,
(29.89)
and consequently
p∗
κ = p∗
κ(error|D = 0).
(29.90)
Proof. It suﬃces to prove (29.89) because (29.90) will then follow by (29.86). To
prove (29.89) we begin by deﬁning e(d) for d ∈FK
2 as follows. If d is in Aκ,1, then
we deﬁne e(d) as
e(d) ≜Pr


d′∈Aκ,0
fY|D=d′(Y) >

d′∈Aκ,1
fY|D=d′(Y)
 D = d

,
d ∈Aκ,1.
Otherwise, if d is in Aκ,0, then we deﬁne e(d) as
e(d) ≜Pr


d′∈Aκ,0
fY|D=d′(Y) <

d′∈Aκ,1
fY|D=d′(Y)
 D = d

,
d ∈Aκ,0.
We shall prove (29.89) for the case where
d ∈Aκ,1.
(29.91)
The proof for the case where d ∈Aκ,0 is almost identical and is omitted. For d
satisfying (29.91) we shall prove that e(d) does not depend on d. The second term
in (29.87) which accounts for the random resolution of ties can be treated very
734
Linear Binary Block Codes with Antipodal Signaling
similarly. To show that e(d) does not depend on d we compute:
e(d)
=

y∈RN I
1

d′∈Aκ,0
fY|D=d′(y) >

d′∈Aκ,1
fY|D=d′(y)
2
fY|D=d(y) dy
=

y∈RN I
1

d′∈Aκ,0
fY|D=d′(y) >

d′∈Aκ,1
fY|D=d′(y)
2
fY|D=0

ψd(y)

dy
=

˜y∈RN I
1

d′∈Aκ,0
fY|D=d′
ψd(˜y)

>

d′∈Aκ,1
fY|D=d′
ψd(˜y)
2
fY|D=0(˜y) d˜y
=

˜y∈RN I
1

d′∈Aκ,0
fY|D=d′⊕d(˜y) >

d′∈Aκ,1
fY|D=d′⊕d(˜y)
2
fY|D=0(˜y) d˜y
=

˜y∈RN I
1

˜d∈Aκ,1
fY|D=˜d(˜y) >

˜d∈Aκ,0
fY|D=˜d(˜y)
2
fY|D=0(˜y) d˜y
= e(0),
where the second equality follows from Lemma 29.9.2 (v); the third by deﬁning
the vector ˜y as ˜y ≜ψd(y) and by Parts (iv) and (iii) of Lemma 29.9.2; the fourth
by Lemma 29.9.2 (vi); and the ﬁfth equality by deﬁning ˜d ≜d ⊕d′ and using
(29.73).
29.10
System Parameters
We next summarize how the system parameters such as power, bandwidth, and
block error rate are related to the parameters of the encoder. We only address the
case where the pulse shape φ satisﬁes the orthonormality condition (29.2). As we
next show, in this case the bandwidth W in Hz of the pulse shape can be expressed
as
W = 1
2 Rb
N
K (1 + excess bandwidth),
(29.92)
where Rb is the bit rate at which the data are fed to the modem in bits per
second, and where the excess bandwidth, which is deﬁned in Deﬁnition 11.3.6, is
nonnegative. To verify (29.92) note that if the data arrive at the encoder at the
rate of Rb bits per second and if the encoder produces N real symbols for every K
bits that are fed to it, then the encoder produces real symbols at a rate
Rs = N
K Rb
real symbol
second

,
(29.93)
so the baud period must be
Ts = K
N
1
Rb
.
(29.94)
It then follows from Deﬁnition 11.3.6 that the bandwidth of φ is given by (29.92)
with the excess bandwidth being nonnegative by Corollary 11.3.5.
29.11 Hard vs. Soft Decisions
735
As to the transmitted power P, by (29.27) and (29.94) it is given by
P = Eb Rb,
(29.95)
where Eb denotes the energy per data bit and is given by
Eb = N
K A2.
(29.96)
It is customary to describe the error probability by which one measures performance
as a function of the energy-per-bit Eb.3 Thus, for example, one typically writes the
upper bound (29.55) on the probability of a block error using (29.96) as
pMAP(error|D = d)
≤
N

ν=dmin,H
#

c ∈range(T) : wH(c) = ν

Q
⎛
⎝

2Eb(K/N)ν
N0
⎞
⎠.
(29.97)
29.11
Hard vs. Soft Decisions
In Section 29.7 we derived the decision rule that minimizes the probability of a block
error. We saw that, in general, its complexity is exponential in the dimension K of
the code because a brute-force implementation of this rule requires correlating the
N-tuple Y with each of the 2K tuples in range(enc). For the single parity check
rule we found a much simpler implementation of this rule, but for general codes
the decoding problem can be very diﬃcult.
A suboptimal decoding rule that is sometimes implemented is the Hard Decision
decoding rule, which has two steps. In the ﬁrst step one uses the observed real-
valued N-tuple (Y1, . . . , YN) to form the binary tuple ( ˆC1, . . . , ˆCN) according to
the rule
ˆCη =

0
if Yη ≥0,
1
if Yη < 0,
η = 1, . . . , N,
and in the second step one searches for the message d for which T(d) is closest in
Hamming distance to ( ˆC1, . . . , ˆCN). The advantage of this decoding rule is that
the ﬁrst step is very simple and that the second step can be often performed very
eﬃciently if the code has a strong algebraic structure.
29.12
The Varshamov and Singleton Bounds
Motivated by the approximation (29.56) and by (29.58), a fair bit of eﬀort in
Coding Theory has been invested in ﬁnding (K, N) codes that have a large minimum
Hamming weight and reasonable decoding complexity. One of the key existence
3The terms “energy-per-bit,” “energy-per-data-bit,” and “energy-per-information-bit” are
used interchangeably.
736
Linear Binary Block Codes with Antipodal Signaling
results in this area is the Varshamov Bound. We state here a special case of this
bound pertaining to our binary setting.
Theorem 29.12.1 (The Varshamov Bound). Let K and N be positive integers,
and let d be an integer in the range 2 ≤d ≤N −K + 1. If
d−2

ν=0
	N −1
ν

< 2N−K,
(29.98)
then there exists a linear (K, N) F2 code whose minimum distance dmin,H satisﬁes
dmin,H ≥d.
Proof. See, for example, (MacWilliams and Sloane, 1977, Chapter 1, Section 10,
Theorem 12) or (Blahut, 2003, Chapter 12, Section 3, Theorem 12.3.3).
A key upper bound on dmin,H is given by the Singleton Bound.
Theorem 29.12.2 (The Singleton Bound). If N and K are positive integers, then
the minimum Hamming distance dmin,H of any linear (K, N) F2 code must satisfy
dmin,H ≤N −K + 1.
(29.99)
Proof. See, for example, (Blahut, 2003, Chapter 3, Section 3, Theorem 3.2.6) or
(van Lint, 1998, Chapter 5, Section 2, Corollary 5.2.2) or Exercise 29.11.
29.13
Additional Reading
We have only had a glimpse of Coding Theory.
A good starting point for the
literature on Algebraic Coding Theory is (Roth, 2006). For more on the modern
coding techniques such as low-density parity-check codes (LDPC) and turbo-codes,
see (Richardson and Urbanke, 2008) and (Ryan and Lin, 2009).
For more bounds on the error probabilities, see (Sason and Shamai, 2006).
The degradation resulting from hard decisions is addressed, e.g., in (Viterbi and
Omura, 1979, Chapter 3, Section 3.4).
The results of Section 29.9 can be extended also to nonbinary codes with other
mappings. See, for example, (Loeliger, 1991) and (Forney, 1991).
For some of the literature on the minimum distance and its asymptotic behavior
in the block length, see, for example, (Roth, 2006, Chapter 4)
For more on the decoding complexity see the notes on Section 2.4 in Chapter 2 of
(Roth, 2006).
29.14 Exercises
737
29.14
Exercises
Exercise 29.1 (Orthogonality of Signals). Recall that, given a binary K-tuple d ∈FK
2
and a linear (K, N) F2 encoder T(·), we use xη(d) to denote the result of applying the
antipodal mapping Υ(·) to the η-th component of T(d). Let the pulse shape φ be such
that its time shifts by integer multiples of the baud period Ts are orthonormal. Show that
5
t →
N

η=1
xη(d) φ(t −ηTs), t →
N

η=1
xη(d′) φ(t −ηTs)
6
= 0,
if, and only if, dH

T(d), T(d′)

= N/2.
Exercise 29.2 (Permuting the Components of a Linear Code). Suppose we swap the ﬁrst
and second components of each of the N-tuples of a given linear (K, N) F2 code. Is the
resulting collection of N-tuples a linear (K, N) F2 code?
Exercise 29.3 (How Many Encoders Does a Code Have?). Let the linear (K, N) F2
encoder T: FK
2 →FN
2 be represented by the K × N matrix G. Show that any linear (K, N)
F2 encoder whose image is equal to the image of T can be written in the form
d →dAG,
where A is a K × K invertible matrix whose entries are in F2. How many such matrices A
are there?
Exercise 29.4 (The (4,7) Hamming Code). A systematic encoder for the linear (4, 7) F2
Hamming code maps the four data bits d1, d2, d3, d4 to the 7-tuple

d1, d2, d3, d4, d1 ⊕d3 ⊕d4, d1 ⊕d2 ⊕d4, d2 ⊕d3 ⊕d4

.
Suppose that this encoder is used in conjunction with the componentwise antipodal map-
ping Υ7(·) over the white Gaussian noise channel with PAM of pulse shape whose time
shifts by integer multiples of the baud period are orthonormal.
(i) Write out the 16 binary codewords and compute the code’s weight enumerator.
(ii) Assuming that the codewords are equally likely and that the decoding minimizes the
probability of a message error, use the Union Bound to upper-bound the probability
of codeword error. Express your bound using the transmitted energy per bit Eb.
(iii) Find a lower bound on the probability that the ﬁrst bit D1 is incorrectly decoded.
Express your bound in terms of the energy per bit. Compare with the exact ex-
pression in uncoded communication.
Exercise 29.5 (The Repetition Code). Consider the linear (1, N) F2 repetition code
consisting of the all-zero and all-one N-tuples (0, . . . , 0) and (1, . . . , 1).
(i) Find its weight enumerator.
(ii) Find an optimal decoder for a system employing this code with the componentwise
antipodal mapping ΥN(·) over the white Gaussian noise channel in conjunction
with PAM with a pulse shape whose times shifts by integer multiples of the baud
period are orthonormal.
738
Linear Binary Block Codes with Antipodal Signaling
(iii) Find the optimal probability of error. Express your answer using the energy per
bit Eb. Compare with uncoded antipodal signaling.
(iv) Describe the hard decision rule for this setup. Find its performance in terms of Eb.
Exercise 29.6 (The Dual Code). We say that two binary κ-tuples u = (u1, . . . , uκ) and
v = (v1, . . . , vκ) are orthogonal if
u1 · v1 ⊕u2 · v2 ⊕· · · ⊕uκ · vκ = 0.
(Beware! A nonzero κ-tuple may be orthogonal to itself.) Consider the set of all N-tuples
that are orthogonal to every codeword of some given linear (K, N) F2 code. Show that
this set is a linear (N −K, N) F2 code. This code is called the dual code. What is the
dual code of the (K, K + 1) single parity check code?
Exercise 29.7 (Hadamard Code). For a positive integer N which is a power of two, deﬁne
the N × N binary matrix HN recursively as
H2 =
0
0
0
1

,
HN =
HN/2
HN/2
HN/2
¯HN/2

,
N = 4, 8, 16, . . . ,
(29.100)
where ¯H denotes the componentwise negation of the matrix H, that is, the matrix whose
Row-j Column-ℓelement is given by 1⊕[H]j,ℓ, where [H]j,ℓis the Row-j Column-ℓelement
of H. Consider the set of all rows of HN.
(i) Show that this collection of N binary N-tuples forms a linear (log2 N, N) F2 code.
This code is called the Hadamard code. Find this code’s weight enumerator.
(ii) Suppose that, as in Section 29.6, this code is used in conjunction with PAM over
the white Gaussian noise channel and that Y1, . . . , YN are as deﬁned there. Show
that the following rule minimizes the probability of a message error: compute the
vector
˜HN
⎛
⎜
⎜
⎜
⎝
Y1
Y2
...
YN
⎞
⎟
⎟
⎟
⎠
(29.101)
and guess that the m-th message was sent if the m-th component of this vector is
largest. Here ˜HN is the N × N matrix whose Row-j Column-ℓentry is the result of
applying Υ(·) to the Row-j Column-ℓentry of HN.
(iii) A brute-force computation of the vector in (29.101) requires N2 additions, which
translates to N2/ log2 N additions per information bit. Use the structure of HN that
is given in (29.100) to show that this can be done with N log2 N additions (or N
additions per information bit).
Hint: For Part (iii) provide an algorithm for which c(N) = 2c(N/2) + N, where c(n)
denotes the number of additions needed to compute this vector when the matrix is n × n.
Show that the solution to this recursion for c(2) = 2 is c(n) = n log2 n.
Exercise 29.8 (Bi-Orthogonal Code). Referring to the notation introduced in Exer-
cise 29.7, consider the 2N × N matrix
HN
¯HN

,
where N is some positive power of two.
29.14 Exercises
739
(i) Show that the rows of this matrix form a linear

log2(2N), N

F2 code.
(ii) Compute the code’s weight enumerator.
(iii) Explain why we chose the title “Bi-Orthogonal Code” for this exercise.
(iv) Find an eﬃcient decoding algorithm for the setup of Section 29.6.
Exercise 29.9 (Non-IID Data). How would you modify the decision rule of Section 29.8 if
the data bits (D1, . . . , DK) are not necessarily IID but have the general joint probability
mass function PD(·)?
Exercise 29.10 (Asymmetric Channels). Show that Theorem 29.9.3 will no longer hold if
we drop the hypothesis that the channel is symmetric.
Exercise 29.11 (A Proof of the Singleton Bound). Use the following steps to prove the
Singleton Bound.
(i) Consider a linear (K, N) F2 code. Let π : FN
2 →FK−1
2
map each N-tuple to the
(K −1)-tuple consisting of its ﬁrst K −1 components. By comparing the number of
codewords with the cardinality of the range of π, argue that there must exist two
codewords whose ﬁrst K −1 components are identical.
(ii) Show that these two codewords are at Hamming distance of at most N −K + 1.
(iii) Show that the minimum Hamming distance of the code is at most N −K + 1.
(iv) Does linearity play a role in the proof?
Exercise 29.12 (Binary MDS Codes). Codes that satisfy the Singleton Bound with equal-
ity are called Maximum Distance Separable (MDS). Show that the linear (K, K + 1) F2
single parity check code is MDS. Can you think of other binary MDS codes?
Exercise 29.13 (Existence via the Varshamov Bound). Can the existence of a linear (4, 7)
F2 code of minimum Hamming distance 3 be deduced from the Varshamov Bound?
Chapter 30
The Radar Problem
The radar problem is to guess whether an observed waveform is noise or a signal
corrupted by noise.
In its simplest form, the signal—which corresponds to the
reﬂection from a target—is deterministic. In the more general setting of a moving
target at an unknown distance or velocity, some of the signal’s parameters (e.g.,
delay or phase) are either unknown or random.
Unlike the hypothesis-testing problem that we encountered in Chapter 20, here
there is no prior. Consequently, it is meaningless to discuss the probability of er-
ror, and a new optimality criterion must be introduced. Typically one wishes to
minimize the probability of missed-detection (guessing “no target” in its presence)
subject to a constraint on the maximal probability of false alarm (guessing “tar-
get” when none is present). More generally, one studies the trade-oﬀbetween the
probability of false alarm and the probability of missed-detection.
There are many other scenarios where one needs to guess in the absence of a prior,
e.g., in guessing whether a drug is helpful against some ailment or in guessing
whether there is a housing bubble. Consequently, although we shall refer to our
problem as “the radar problem,” we shall pose it in greater generality.
The radar problem is closely related to the Knapsack Problem in Computer
Science. This relation is explored in Section 30.2.
Readers who prefer to work on their jigsaw puzzle after peeking at the picture on
the box should—as recommended—glance at Section 30.11 (without the proofs and
referring to Deﬁnition 30.5.1 if they must) after reading about the setup and the
connection with the Knapsack Problem (Sections 30.1–30.2) and before proceeding
to Section 30.3. Others can read in the order in which the results are derived.
30.1
The Setup
Two probability density functions f0(·) and f1(·) on the d-dimensional Euclidean
space Rd are given. A random vector Y is drawn according to one of them, and
our task is to guess according to which. We refer to Y as the observation and to
the space Rd, which we denote by Y, as the observation space. We refer to the
hypothesis that Y was drawn according to f0(·) as the null hypothesis. In the
radar problem this corresponds to the hypothesis that no target is present and that
740
30.1 The Setup
741
we are therefore observing only noise; the other hypothesis—the alternative—
corresponds to the hypothesis that a target is present.
A (deterministic) guessing rule is speciﬁed by a (Borel measurable) mapping
φGuess : Y →{0, 1}
(30.1)
as follows. Having observed that Y = yobs, our guess is determined by φGuess(yobs):
if φGuess(yobs) is 0, then we guess that Y was drawn according to f0(·), and oth-
erwise we guess that it was drawn according to f1(·). We refer to guessing that Y
was drawn according to f0(·) as guessing “H = 0,” and to guessing that it was
drawn according to f1(·) as guessing “H = 1.” The mapping φGuess(·)—and hence
also the guess—can also be speciﬁed in terms of the subset D ⊆Y that it maps to
zero
D =

y ∈Y : φGuess(y) = 0

.
(30.2)
The event that we guess that Y was drawn according to f1(·) when it was in fact
drawn according to f0(·) is called a false alarm, because it corresponds to the
event that we erroneously guess that a target is present. The probability pFA of
this event is
pFA =

{y∈Y : φGuess(y)=1}
f0(y) dy
(30.3a)
=

Dc f0(y) dy
(30.3b)
=

Y
f0(y) I{y ∈Dc} dy,
(30.3c)
where in the second equality we have used (30.2).
The other error event of guessing that Y was drawn according to f0(·) when it was
in fact drawn according to f1(·) is called a missed-detection, because it corre-
sponds to the event that we fail to detect that a target is present. Its probability
pMD is
pMD =

{y∈Y : φGuess(y)=0}
f1(y) dy
(30.4a)
=

D
f1(y) dy
(30.4b)
=

Y
f1(y) I{y ∈D} dy.
(30.4c)
Rather than specifying the missed-detection probability, it is more common in the
radar community to specify the probability of detection pD, which is deﬁned as
pD = 1 −pMD.
(30.5)
This probability can be expressed as
pD =

Y
f1(y) I{y ∈Dc} dy.
(30.6)
742
The Radar Problem
We would like the probability of detection to be high and the probability of false
alarm to be small, but there is a tension between these objectives. This tension is
best seen by comparing (30.6) with (30.3c): for pD to be large, the set Dc should
be “large” so that the integral in (30.6) be large; but for pFA to be small, the same
set Dc should be “small” so that the integral in (30.3c) be small. Stated diﬀerently,
by including an inﬁnitesimal region around y in Dc we increase pD by f1(y) dy,
but at the cost of increasing pFA by f0(y) dy. The radar problem is to study the
trade-oﬀbetween pD and pFA and to identify the guessing rules that achieve the
best trade-oﬀ.
We say that a pair (pFA, pMD) is achievable if there exists a decision rule of these
probabilities of false alarm and missed-detection. The set of all achievable pairs is
denoted R. We say that a pair (p∗
FA, p∗
MD) is Pareto-optimal if it is achievable
and there does not exist a “better” achievable pair. More precisely, (p∗
FA, p∗
MD)
is Pareto-optimal if it is achievable and there does not exist an achievable pair
(p′
FA, p′
MD) satisfying both p′
FA < p∗
FA and p′
MD ≤p∗
MD nor an achievable pair
(p′
FA, p′
MD) satisfying both p′
FA ≤p∗
FA and p′
MD < p∗
MD. Thus, (p∗
FA, p∗
MD) is Pareto-
optimal if it is achievable and if for every other achievable pair (p′
FA, p′
MD)

p′
FA ≤p∗
FA and p′
MD ≤p∗
MD

=⇒

p′
FA = p∗
FA and p′
MD = p∗
MD

.
(30.7)
The radar problem is to identify the Pareto-optimal pairs and the guessing rules
that achieve them.
Examples of various achievable regions (shadowed) and their corresponding Pareto-
optimal pairs (solid lines) can be found in Figure 30.1. In (b) the support of f0 is
strictly contained in that of f1, so—by guessing “H = 0” whenever the observation
falls in the support set of f0— we can achieve zero pFA with pMD strictly smaller
than 1. In (c) the support of f1 is strictly contained in that of f0, so—by guessing
“H = 1” whenever the observation falls in the support set of f1— we can achieve
zero pMD with pFA strictly smaller than 1. In (d) the support sets are disjoint. In
this case all probability pairs (pFA, pMD) are achievable, and the sole Pareto-optimal
pair is (0, 0).
In the radar literature the problem is often formulated slightly diﬀerently. For a
given maximal tolerated false-alarm probability πFA, we seek the highest probabil-
ity of detection p∗
D(πFA). Thus,1
p∗
D(πFA) = max

Y
f1(y) I{y ∈Dc} dy,
(30.8a)
where the maximization is over all (Borel measurable) subsets D for which

Y
f0(y) I{y ∈Dc} dy ≤πFA.
(30.8b)
1Strictly speaking, it is prima facie not clear that the supremum is achieved, and it would
have been more appropriate to replace the maximum with a supremum. However, the maximum
is achieved: see Lemma 30.4.1 and Proposition 30.5.5 ahead.
30.1 The Setup
743
1
1
1
1
1
1
1
1
p∗
MD(0)
p∗
FA(0)
pMD
pMD
pMD
pMD
pFA
pFA
pFA
pFA
(a)
(b)
(c)
(d)
Figure 30.1:
Some achievable regions and their corresponding Pareto-optimal
pairs. In (b) the support of f0(·) is contained in that of f1(·). In (c) the sup-
port of f1(·) is contained in that of f0(·). And in (d) the supports are disjoint.
Alternatively, for a given maximal tolerated false-alarm probability πFA, we seek
the smallest probability of missed-detection p∗
MD(πFA):
p∗
MD(πFA) = min

Y
f1(y) I{y ∈D} dy = 1 −p∗
D(πFA),
(30.9)
where the minimization is over sets D as above. The receiver operating char-
acteristic (ROC) is a plot of the function πFA 	→p∗
D(πFA).
Notice the inequality sign in (30.8b). Usually, p∗
D(πFA) and p∗
MD(πFA) are achieved
by some D for which (30.8b) holds with equality, but this is not a requirement. It is
because of this inequality sign that the pair (πFA, p∗
MD(πFA)) is not always Pareto-
optimal: the deﬁnition of p∗
MD(πFA) precludes any achievable pair (p′
FA, p′
MD) with
p′
FA ≤πFA and p′
MD smaller than p∗
MD(πFA), but it does not preclude a pair
(p′
FA, p′
MD) with p′
MD equal to p∗
MD(πFA) and p′
FA strictly smaller than πFA. In
744
The Radar Problem
1
1
1
1
p∗
MD(0)
p∗
MD(0)
p∗
FA(0)
p∗
FA(0)
pMD
pMD
pFA
pFA
(a)
(b)
Figure 30.2: A generic achievable region. In (a) the solid line depicts the Pareto-
optimal pairs. In (b) the dashed line is the graph of p∗
MD(·).
other words, (πFA, p∗
MD(πFA)) need not be Pareto-optimal because it is conceiv-
able that the same missed-detection probability could be achievable with a smaller
false-alarm probability.
The reverse, however, does hold: as we next argue,

(p∗
FA, p∗
MD) is Pareto-optimal

=⇒

p∗
MD(p∗
FA) = p∗
MD

.
(30.10)
To see this, assume that (p∗
FA, p∗
MD) is Pareto-optimal.
In this case (p∗
FA, p∗
MD)
is a fortiori achievable, and p∗
MD(p∗
FA) cannot be larger than p∗
MD because the
rule that achieves (p∗
FA, p∗
MD) is in the feasible set of the minimization (30.9) that
deﬁnes p∗
MD(p∗
FA). It cannot be smaller than p∗
MD because if it were, then the rule
that achieves p∗
MD(p∗
FA) would have false-alarm and missed-detection probabilities
that are “better” than (p∗
FA, p∗
MD) in contradiction to the Pareto-optimality of
(p∗
FA, p∗
MD).
The relationship between Pareto-optimality and the function pFA 	→p∗
MD(pFA) is
illustrated in Figure 30.2. The shaded region corresponds to pairs (pFA, pMD) that
are achievable. The solid line corresponds to those achievable pairs that are also
Pareto-optimal. And the dashed line is the graph of the function pFA 	→p∗
MD(pFA).
In analogy to (30.9), we deﬁne p∗
FA(πMD) as the least false-alarm probability that
is achievable by guessing rules whose missed-detection probability does not exceed
πMD:
p∗
FA(πMD) = min

Y
f0(y) I{y ∈Dc} dy,
(30.11a)
where the minimization is over all (Borel measurable) sets D satisfying

Y
f1(y) I{y ∈D} dy ≤πMD.
(30.11b)
30.1 The Setup
745
As in Section 20.6, we can extend the class of decision rules to allow randomization.
A randomized decision rule is speciﬁed by a mapping b: Y →[0, 1] with the
understanding that if we observe yobs then we guess “H = 0” with probability
b(yobs) and we guess “H = 1” with probability 1 −b(yobs); cf. Figure 20.1 on
Page 404. For such a rule the probability of false alarm pFA(b) is
pFA(b) =

Y

1 −b(y)

f0(y) dy;
(30.12)
the probability of missed-detection pMD(b) is
pMD(b) =

Y
b(y) f1(y) dy,
(30.13)
and the probability of detection is
pD(b) = 1 −pMD(b) =

Y

1 −b(y)

f1(y) dy.
(30.14)
The set of all pairs that are achievable by some randomized guessing rule is de-
noted Rrnd. Since every deterministic rule that guesses “H = 0” when y ∈D can
be viewed as a randomized rule with b(y) = I{y ∈D},
R ⊆Rrnd.
(30.15)
A pair (p∗
FA, p∗
MD) is said to be Pareto-optimal with respect to the class
of randomized guessing rules if it is achievable by some randomized guessing
rule and if (30.7) holds for all pairs (p′
FA, p′
MD) that are achievable with a possibly-
randomized guessing rule. A guessing rule is said to be Pareto-optimal with respect
to the class of randomized guessing rules if the error probabilities (pFA, pMD) asso-
ciated with it are Pareto-optimal with respect to the class of randomized guessing
rules.
In analogy to p∗
MD(πFA), we deﬁne p∗
MD,rnd(πFA) as the minimal2 probability of
missed-detection that can be achieved by a randomized rule whose probability of
false alarm does not exceed πFA:
p∗
MD,rnd(πFA) =
min
b: pFA(b)≤πFA pMD(b).
(30.16)
Using arguments similar to those justifying (30.10) we conclude:
Proposition 30.1.1. If (p∗
FA, p∗
MD) is Pareto-optimal with respect to the class of
randomized guessing rules, then p∗
MD,rnd(p∗
FA) = p∗
MD.
Since every pair (pFA, pMD) that is achievable with a deterministic guessing rule is
also achievable with a randomized decision rule (30.15),
p∗
MD,rnd(πFA) ≤p∗
MD(πFA).
(30.17)
Later we shall see (Corollary 30.5.6) that, because the observation has a density
under both hypotheses, this inequality is in fact an equality.
2Again, prima facie the minimum need not exist, and we should replace the minimum
in (30.16) with an inﬁmum.
However, it turns out that the inﬁmum can be achieved:
see
Lemma 30.4.1 and Lemma 30.5.4.
746
The Radar Problem
30.2
The Radar and the Knapsack Problems
Among all guessing rules whose false-alarm probability does not exceed πFA, which
one has the highest probability of detection? For some intuition, let us think about
each y in Y as an object of weight f0(y) and of monetary value f1(y). (The set of
objects is uncountable, but let us ignore this.) Rather than focusing on the set D,
which deﬁnes the guessing rule via (30.2), let us focus on its complement Dc, which
also deﬁnes the guessing rule. And let us think of Dc as a bag of objects. The
problem of choosing Dc—and hence a guessing rule—is then the problem of picking
which objects to place in the bag. The total weight of the bag is the sum of the
weights of the objects in the bag and, as such, corresponds to the RHS of (30.3c).
The weight of the bag is thus the probability of false alarm of the rule it deﬁnes.
Similarly, the total monetary value of the bag corresponds to the RHS of (30.6)
and is thus the probability of detection of the rule it deﬁnes. Viewed this way,
our problem is similar to the one encountered by a robber who breaks into a home
containing many objects of various weights and values. The robber must decide
which objects to put in the bag with the goal of maximizing the bag’s value subject
to the maximal weight the robber can carry.
A natural approach is the greedy one, according to which the robber ﬁrst places
in the bag the object whose value-to-weight ratio is highest (diamond?), followed
by the one whose value-to-weight ratio is the second highest, etc. The robber thus
continues to place objects in the bag in descending order of value-to-weight ratio
until the bag reaches the maximal weight the robber can carry. Stated diﬀerently,
in the greedy approach the robber judiciously chooses a threshold to which the
robber compares the value-to-weight ratios of the diﬀerent objects: any object
having a higher ratio is placed in the bag, and any object of a lower ratio is not.
While perfectly reasonable, in some scenarios this approach is suboptimal. For
example, suppose there are three objects in the home, the ﬁrst of which weighs 6
kilograms and is worth $1, 200, whereas each of the other two weighs 5 kilograms
and is worth $900. If the maximal weight the robber can carry is 10 kilograms, then
the greedy approach is suboptimal: after placing in the bag the object of highest
value-to-weight ratio (the ﬁrst) the bag is worth $1, 200 and weighs 6 kilograms;
nothing more will ﬁt in the bag, and the robber’s loot is $1, 200. The robber could
do better by forgoing the ﬁrst object and placing the other two objects in the bag
with a combined value of $1, 800.
Of course, an even better approach is to place the ﬁrst object in the bag and to
then place 80% of the second (whatever this may mean). This would result in a bag
of weight 6+0.8×5 (= 10) kilograms and of value $1, 200+0.8×$900 (= $1, 920).
In the Computer Science literature the robber’s problem is called the Knapsack
Problem. If the robber can take a fraction of each object, the problem is called
the Fractional Knapsack Problem; otherwise it is called the 0-1 Knapsack
Problem.3
The Fractional Knapsack Problem can indeed be solved using the
greedy approach; the 0-1 Knapsack Problem is harder and is often solved using
Dynamic Programming techniques.
3Computer scientists also assume that the number of objects is ﬁnite and that the weights
and values are positive integers.
30.3 Pareto-Optimality and Linear Functionals
747
The radar problem with deterministic decision rules is reminiscent of the 0-1 Knap-
sack Problem, because for each “object” (y) we must decide whether to place it
in the bag (Dc) or not (i.e., place it in D). When randomized rules are allowed
the problem is reminiscent of the Fractional Knapsack Problem, because specifying
some b(y) is tantamount to contributing

1 −b(y)

f0(y) to the weight of the bag
(cf. (30.12)) and (1 −b(y)) f1(y) to its value (cf. (30.14)).
Nevertheless, as we shall see, in the radar problem the greedy approach is optimal
even when we do not allow randomization. This is because we have assumed a
continuum of objects and densities for the observation (under both hypotheses).
The greedy approach is optimal even if we do not assume densities, provided we
allow for randomization. Either way, randomization makes the solution a bit more
elegant in allowing us to describe the guessing rule in terms of the likelihood-ratio
function, LR: Y →[0, ∞], which we encountered in (20.38)–(20.39):
LR(y) = f0(y)
f1(y),
(30.18a)
with the convention
α
0 = ∞,
α > 0

and
0
0 = 1.
(30.18b)
30.3
Pareto-Optimality and Linear Functionals
Some of the properties of Pareto-optimal pairs have less to do with the radar prob-
lem and more with the fact that if (p∗
FA, p∗
MD) is Pareto-optimal then it is achievable
and no other achievable pair can be “better” than it. For example, there cannot
be two diﬀerent Pareto-optimal pairs of equal false-alarm probabilities, because
otherwise the pair of smaller missed-detection probability would be “better” than
the other and thus contradict the latter’s Pareto-optimality. Hence:
Note 30.3.1. Two Pareto-optimal pairs of equal false-alarm probabilities must also
have equal missed-detection probabilities.
Another such property is related to the minimization of linear functionals with
positive coeﬃcients. If (p′
FA, p′
MD) is “better” than (pFA, pMD), and if α and β are
both positive, then α p′
FA + β p′
MD must be smaller than α pFA + β pMD. Turn-
ing this around, if α and β are positive and for no achievable pair (p′
FA, p′
MD) is
α p′
FA +β p′
MD smaller than α pFA +β pMD, then no achievable pair is “better” than
(pFA, pMD). Thus, if (p∗
FA, p∗
MD) minimizes α pFA + β pMD among all achievable
pairs (pFA, pMD), then (p∗
FA, p∗
MD) must be Pareto-optimal. We have thus proved:
Proposition 30.3.2 (Minimizers of Positive Functionals Are Pareto-Optimal).
(i) If, among all guessing rules, a given guessing rule minimizes α pFA + β pMD
for some positive α and β, then this rule’s probabilities of false alarm and
missed-detection are Pareto-optimal.
(ii) If, among all randomized guessing rules, a given guessing rule minimizes
α pFA +β pMD for some positive α and β, then this rule’s probabilities of false
748
The Radar Problem
alarm and missed-detection are Pareto-optimal with respect to the class of
randomized guessing rules.
Proof. It is probably easier to prove this on one’s own than to follow someone
else’s proof. But we provide a proof nonetheless. We begin with the deterministic
case, i.e., with Part (i). Let (pFA, pMD) be the probabilities of error associated with
the given guessing rule. This part’s hypothesis guarantees that if (p′
FA, p′
MD) are
the probabilities of error associated with any other guessing rule, then
α pFA + β pMD ≤α p′
FA + β p′
MD.
(30.19)
Equivalently,
α(p′
FA −pFA) + β(p′
MD −pMD) ≥0.
(30.20)
Consequently, if p′
FA ≤pFA then p′
MD cannot be smaller than pMD because in
this case the ﬁrst term on the LHS of (30.20) is nonpositive, so for (30.20) to
hold β(p′
MD −pMD) cannot be negative and hence, since β is strictly positive,
(p′
MD −pMD) cannot be negative. Similarly, if p′
MD ≤pMD then p′
FA cannot be
smaller than pFA because in this case the second term on the LHS of (30.20) is
nonpositive, so for (30.20) to hold α(p′
FA −pFA) cannot be negative and hence,
since α is strictly positive, (p′
FA −pFA) cannot be negative. This concludes the
proof of Part (i).
The proof of Part (ii) is nearly identical.
Simply replace “guessing rule” with
“possibly-randomized guessing rule.”
30.4
One Type of Error Is Not Allowed
Suppose no false alarms are allowed. What is then the least probability of missed-
detection p∗
MD(0) we can achieve? In this section we compute this probability and
provide a guessing rule that achieves it. The analogous problem when no missed-
detections are allowed and we seek the least probability of false alarm p∗
FA(0) has
a similar solution.
At ﬁrst sight one might erroneously think that to preclude false alarms we must
never guess “H = 1,” i.e., that we must choose Dc to be the empty set. But this is
not always the case: it suﬃces that we only guess “H = 1” when f0(yobs) is zero,
i.e., that Dc only contain observations yobs for which f0(yobs) is zero. Indeed,

Dc ⊆

y ∈Y : f0(y) = 0

=⇒

pFA = 0

,
(30.21)
because if the LHS of (30.21) holds, then the integrand in (30.3b) is zero over
the range of integration, and pFA is zero. To minimize the probability of missed-
detection, we choose Dc to be the largest we can while satisfying the condition on
the LHS of (30.21). Thus, we choose
Dc =

y ∈Y : f0(y) = 0

(30.22)
with the result that
p∗
MD(0) =

{y∈Y : f0(y)>0}
f1(y) dy.
(30.23)
30.4 One Type of Error Is Not Allowed
749
For example, in the one-dimensional case with the densities f0(y) = I{0 ≤y ≤1}
and f1(y) = I{0.25 ≤y ≤1.25} a rule that achieves p∗
MD(0) is to guess “H = 0”
whenever yobs is in the interval [0, 1], i.e., to choose D to be the interval [0, 1] with
the result that p∗
MD(0) = 0.75.
The optimality of (30.22) is almost self-evident, but not quite. The slight technical
diﬃculty is that the LHS of (30.21) is suﬃcient to guarantee no false alarms, but
it is not necessary. Are there other decision rules that also guarantee zero false
alarms? As we shall see in the next lemma, there are, but they diﬀer from (30.22)
only on sets of Lebesgue measure zero, which do not inﬂuence the probability of
missed-detection. The lemma also shows that randomized guessing rules are no
better for this problem than the best deterministic ones.
Lemma 30.4.1 (Optimal Rule with No False Alarms). Among all randomized
guessing rules whose probability of false alarm is zero, the deterministic guessing
rule (30.22) minimizes the probability of missed-detection and yields the missed-
detection probability in (30.23). Thus,
p∗
MD(0) = p∗
MD,rnd(0) =

{y∈Y : f0(y)>0}
f1(y) dy.
(30.24)
Moreover,

0, p∗
MD(0)

is Pareto-optimal
(30.25)
both with respect to the class of deterministic rules and of randomized rules.
Proof. Consider any randomized decision rule that is determined by a function
b: Y →[0, 1] and whose false-alarm probability is zero, i.e., (see (30.12))

Y

1 −b(y)

f0(y) dy = 0.
(30.26)
The integral of a nonnegative function can be zero only if the set over which the
function is positive is of Lebesgue measure zero (Proposition 2.5.3 (i)).
Thus,
(30.26) implies that the set

y ∈Y : 1 −b(y) > 0 and f0(y) > 0

(30.27)
must be of Lebesgue measure zero. Consequently, the integral of any function—
and in particular the function y 	→

1 −b(y)

f1(y)—over this set’s complement is
identical to the integral over all Y, and

Y

1 −b(y)

f1(y) dy =

{y∈Y : 1 −b(y) = 0 or f0(y) = 0}

1 −b(y)

f1(y) dy
=

{y∈Y : 1 −b(y) > 0 and f0(y) = 0}

1 −b(y)

f1(y) dy
≤

{y∈Y : f0(y)=0}

1 −b(y)

f1(y) dy
≤

{y∈Y : f0(y)=0}
f1(y) dy,
(30.28)
750
The Radar Problem
where the second line holds because when 1 −b(y) is zero so is the integrand;
the third line holds because the integrand is nonnegative, so inﬂating the set over
which we integrate cannot decrease the integral; and the fourth line follows by
upper-bounding 1 −b(y) by 1.
Since the LHS of (30.28) is the probability of
detection of the rule deﬁned by b(·), if we subtract both sides of (30.28) from 1 we
obtain
pMD(b) ≥1 −

{y∈Y : f0(y)=0}
f1(y) dy
=

{y∈Y : f0(y)>0}
f1(y) dy.
(30.29)
Since this inequality holds for every randomized rule b(·) whose false-alarm prob-
ability is zero, it establishes the optimality of the guessing rule (30.22) whose
false-alarm probability is zero and whose missed-detection probability is the RHS
of (30.29).
The Pareto-optimality of

0, p∗
MD(0)

is almost self-evident. To prove it for deter-
ministic rules using (30.7), we consider any achievable pair (p′
FA, p′
MD) satisfying
p′
FA ≤0 and p′
MD ≤p∗
MD(0) and prove that both inequalities must hold with equal-
ity. Clearly p′
FA ≤0 implies equality, i.e., p′
FA = 0, because probabilities are non-
negative. And the achievability of (p′
FA, p′
MD)—which by the above is (0, p′
MD)—
implies (by the deﬁnition of p∗
MD(0) as the lowest missed-detection probability that
can be achieved by a deterministic guessing rule of zero false-alarm probability)
that p′
MD ≥p∗
MD(0), which, in combination with the assumption p′
MD ≤p∗
MD(0),
implies equality.
The proof for randomized rules is almost identical: simply replace “deterministic”
with “randomized” and p∗
MD(0) with p∗
MD,rnd(0) (which, by (30.24), is identical to
it).
We can analyze the case where no missed-detections are allowed in a similar way.
In this case it is optimal to guess “H = 0” whenever f1(yobs) is zero. This leads to
D =

y ∈Y : f1(y) = 0

(30.30)
with the result that
p∗
FA(0) =

{y∈Y : f1(y)>0}
f0(y) dy.
(30.31)
And like Lemma 30.4.1 we have:
Lemma 30.4.2 (Optimal Rule with No Missed-Detections). Among all random-
ized guessing rules whose probability of missed-detection is zero, the deterministic
guessing rule (30.30) minimizes the probability of false alarm and yields the false-
alarm probability in (30.31). Thus,
p∗
FA(0) = p∗
FA,rnd(0) =

{y∈Y : f1(y)>0}
f0(y) dy.
(30.32)
30.5 Likelihood-Ratio Tests
751
Moreover,

p∗
FA(0), 0

is Pareto-optimal
(30.33)
both with respect to the class of deterministic rules and of randomized rules.
Note 30.4.3. In view of Lemma 30.4.1, we shall no longer distinguish between
p∗
MD(0) and p∗
MD,rnd(0). Likewise, in view of Lemma 30.4.2, we shall no longer
distinguish between p∗
FA(0) and p∗
FA,rnd(0).
The achievability of the pair (p∗
FA(0), 0) using a deterministic guessing rule shows
that p∗
MD(πFA) is zero whenever πFA ≥p∗
FA(0).
And since p∗
MD,rnd(πFA) never
exceeds p∗
MD(πFA), we conclude:
Corollary 30.4.4. If πFA is larger than p∗
FA(0), then both p∗
MD(πFA) and p∗
MD,rnd(πFA)
are zero:
p∗
MD(πFA) = p∗
MD,rnd(πFA) = 0,
πFA ≥p∗
FA(0).
(30.34)
Corollary 30.4.5. There are no Pareto-optimal pairs (p∗
FA, p∗
MD) (be it with respect
to deterministic or randomized rules) with p∗
FA larger than p∗
FA(0).
Proof. Let (p∗
FA, p∗
MD) be Pareto-optimal.
Then the achievability of (p∗
FA(0), 0)
using a deterministic guessing rule implies that if both p∗
FA(0) ≤p∗
FA and 0 ≤p∗
MD
then both must hold with equality (cf. (30.7)). But the inequality 0 ≤p∗
MD always
holds. We thus conclude that if the inequality p∗
FA(0) ≤p∗
FA holds, then it must
hold with equality. Thus p∗
FA cannot exceed p∗
FA(0).
30.5
Likelihood-Ratio Tests
A key role in the radar problem is played by likelihood-ratio tests, which we deﬁne
next.
Deﬁnition 30.5.1 (Likelihood-Ratio Test). We say that a deterministic guessing
rule φGuess(·) speciﬁed by the set D is a likelihood-ratio test with the threshold
0 ≤Υ ≤∞if it guesses “H = 0” whenever LR(yobs) is strictly larger than Υ, and
it guesses “H = 1” whenever LR(yobs) is strictly smaller than Υ, i.e, if

LR(y) > Υ

=⇒

y ∈D

(30.35a)

LR(y) < Υ

=⇒

y /∈D

.
(30.35b)
(There are no restrictions on φGuess(yobs) when LR(yobs) is equal to Υ.)
A similar deﬁnition holds for randomized guessing rules with (30.35) replaced by

LR(y) > Υ

=⇒

b(y) = 1

(30.36a)

LR(y) < Υ

=⇒

b(y) = 0

.
(30.36b)
Although there is no prior in the radar problem, from every radar problem and
every α ∈(0, 1) we can construct a binary hypothesis-testing problem with a
nondegenerate prior (of the kind we encountered in Chapter 20) by setting
Pr[H = 0] = α,
Pr[H = 1] = 1 −α,
(30.37a)
752
The Radar Problem
and by setting for every y ∈Y
fY|H=0(y) = f0(y),
fY|H=1(y) = f1(y).
(30.37b)
Any guessing rule for the radar problem can also be used for the hypothesis-
testing problem (30.37) and vice versa.
If φGuess(·) is a guessing rule that on
the hypothesis-testing problem yields the error probabilities p(error|H = 0) and
p(error|H = 1), then on the radar problem it will yield
pMD = p(error|H = 1),
pFA = p(error|H = 0).
(30.37c)
As we saw in Chapter 20 (see Theorem 20.5.2 and the discussion of randomized
decision rules in Section 20.6), if the prior is nondegenerate, then
Pr[H = 0] p(error|H = 0) + Pr[H = 1] p(error|H = 1)
is minimized by a guessing rule that compares the likelihood-ratio function LR(yobs)
to the ratio Pr[H = 1]/ Pr[H = 0] and guesses H accordingly. This rule minimizes
the probability of error irrespective of how it resolves ties. In view of (30.37a), the
ratio Pr[H = 1]/ Pr[H = 0] can be expressed as (1 −α)/α. We can also turn this
around: a likelihood-ratio test of threshold Υ ∈(0, ∞) minimizes
α p(error|H = 0) + (1 −α) p(error|H = 1)
whenever α is such that
1 −α
α
= Υ.
(30.38)
Solving (30.38) for α in terms of Υ, we conclude that among all guessing rules
(deterministic or randomized) any likelihood-ratio test of threshold Υ ∈(0, ∞)
minimizes
1
Υ + 1 p(error|H = 0) +
Υ
Υ + 1 p(error|H = 1).
Stated in terms of the radar problem using (30.37c): irrespective of how it resolves
ties, a likelihood-ratio test of threshold 0 < Υ < ∞minimizes
1
Υ + 1 pFA +
Υ
Υ + 1 pMD
(30.39)
among all randomized guessing rules. It thus follows from Proposition 30.3.2 (ii)
that likelihood-ratio tests are Pareto-optimal. This result, which we state next,
is sometimes called the Neyman-Pearson Lemma after the statisticians Jerzy
Neyman (1894–1981) and Egon Pearson (1895–1980).
Proposition 30.5.2 (Likelihood-Ratio Tests Are Pareto-Optimal). Irrespective of
how it resolves ties, any likelihood-ratio test of ﬁnite positive threshold Υ minimizes
1
Υ + 1 pFA +
Υ
Υ + 1 pMD
(30.40)
among all randomized guessing rules and is thus Pareto-optimal with respect to the
class of randomized guessing rules.
30.5 Likelihood-Ratio Tests
753
1
1
1
1
pMD
pMD
pFA
pFA
(a)
(b)
Figure 30.3: The ﬁgure on the left depicts a generic achievable region and lines of
slope −1/Υ. The lines that do not contain achievable pairs are dashed, and those
that do are solid. In this example there is a unique achievable pair that minimizes
1
Υ+1 pFA +
Υ
Υ+1 pMD among all achievable pairs. It is Pareto-optimal and marked
by a bullet. The line through it is thick solid. In the ﬁgure on the right the point
marked by a bullet is Pareto-optimal, and there is a unique value of Υ for which
it minimizes
1
Υ+1 pFA +
Υ
Υ+1 pMD among all achievable pairs. But for this value of
Υ it is not the unique minimizer: all the (Pareto-optimal) points that lie on the
solid line segment going through it are also minimizers. For no value of Υ is the
point marked by a bullet the unique minimizer. Incidentally, the point marked by
a square is Pareto-optimal, and it is the unique minimizer of
1
Υ+1 pFA +
Υ
Υ+1 pMD
for a whole range of values of Υ.
To gain some geometric insight into the minimization of (30.40), recall that given
any number c, the loci of pairs (pFA, pMD) satisfying
1
Υ + 1 pFA +
Υ
Υ + 1 pMD = c
is a straight line of slope −1/Υ. Depending on the value of c, the line may or
may not contain pairs (pFA, pMD) that are achievable. For example, if c is zero,
then the line contains an achievable pair only if we can simultaneously achieve
zero false alarms and zero missed-detections. For larger values of c, the line may
contain achievable pairs even if (0, 0) is not achievable. Figure 30.3 (a) depicts an
achievable region and some such lines. The lines that do not contain achievable
pairs are dashed, and those that do are solid. Of all the lines that contain achievable
pairs, the one whose value of c is smallest is drawn thick. This line intersects (in
this example) the achievable region at a single point, which is marked by a bullet
and at which the line is tangent to the curve of Pareto-optimal pairs. But, as
Figure 30.3 (b) shows, the minimizing line may intersect the achievable region at
more than one point. Moreover, the curve of Pareto-optimal pairs may contain
“corners” at which the tangent is undeﬁned.
754
The Radar Problem
Having shown that every likelihood-ratio test of ﬁnite positive threshold yields a
Pareto-optimal pair, we now ask the reverse question: is every Pareto-optimal pair
(p∗
FA, p∗
MD) achievable by some likelihood-ratio test of some ﬁnite threshold? As
we shall see—in the range 0 < p∗
FA < p∗
FA(0)—the answer is “yes.” To prove this
requires a bit more than just showing that every Pareto-optimal pair minimizes
some functional of the form α pFA + β pMD with α, β > 0: it is possible that a pair
that we wish to achieve minimizes α pFA + β pMD but that another undesired pair
also minimizes it. In this case we need to show that a likelihood-ratio test exists
that achieves the desired and not the undesired pair. This will require a judicious
choice of the mechanism for resolving ties.
Figure 30.3 (b) depicts a situation
where there is a unique value of Υ for which
1
Υ+1 pFA +
Υ
Υ+1 pMD is minimized at
the (desired) point marked by a bullet, but for this value of Υ all the points on
the thick line segment containing the bullet are also minimizers. Incidentally, this
ﬁgure also shows that a given point (in this example the one marked by a square)
may be the minimizer of (30.40) for a whole range of values of Υ.
To overcome these diﬃculties, we shall adopt a somewhat less geometric and more
algebraic approach.
To this end we ﬁrst introduce the cumulative distribution
function of LR(Y) under the null hypothesis and study some of its properties. This
function is key to the study of the performance of likelihood-ratio tests. Deﬁne
P0

LR(Y) ≤ξ

=

Y
I

LR(y) ≤ξ

f0(y) dy,
0 ≤ξ ≤∞,
(30.41a)
P0

LR(Y) < ξ

=

Y
I

LR(y) < ξ

f0(y) dy,
0 ≤ξ ≤∞,
(30.41b)
P0

LR(Y) = ξ

=

Y
I

LR(y) = ξ

f0(y) dy,
0 ≤ξ ≤∞,
(30.41c)
and, more generally,
P0

Y ∈B

=

Y
I

y ∈B

f0(y) dy.
(30.41d)
Lemma 30.5.3 (On the Distribution of LR(Y) under the Null Hypothesis). The
function ξ 	→P0[LR(Y) ≤ξ] is monotonically nondecreasing and continuous from
the right. Moreover,
P0

LR(Y) = 0

= 0
(30.42)
and
P0

LR(Y) < ∞

= p∗
FA(0).
(30.43)
Proof. Being the distribution function of the random variable LR(Y) (when Y
is drawn according to f0(·)), the function ξ 	→P0[LR(Y) ≤ξ] is monotonically
nondecreasing and continuous from the right (Billingsley, 1995, Chapter 2, Sec-
tion 14).4
4Strictly speaking, LR(Y) is not a random variable, because it can take on the value +∞. As
such, it is a “generalized random variable,” but the distribution function of a generalized random
variable is also nondecreasing and continuous from the right.
30.5 Likelihood-Ratio Tests
755
To establish (30.42) we note that the convention 0/0 = 1—which we adopted when
we deﬁned the likelihood-ratio function in (20.39) and (30.18)—implies that

y : LR(y) = 0} =

y : f0(y) = 0

\

y : f0(y) = f1(y) = 0

.
(30.44)
Consequently,
P0

LR(Y) = 0

= P0

Y ∈{y : LR(y) = 0}

= P0

Y ∈{y : f0(y) = 0} \ {y : f0(y) = f1(y) = 0}

≤P0

Y ∈{y : f0(y) = 0}

= 0.
We next turn to proving (30.43). Our convention (30.18b) implies that

y : LR(y) < ∞} =

y : f1(y) > 0

∪

y : f0(y) = f1(y) = 0

.
(30.45)
Since
P0

Y ∈{y : f0(y) = f1(y) = 0}

≤P0

Y ∈{y : f0(y) = 0}

= 0,
(30.46)
it follows from (30.45) and (30.46) that
P0

LR(Y) < ∞

= P0

Y ∈{y : LR(y) < ∞}

= P0

Y ∈{y : f1(y) > 0} ∪{y : f0(y) = f1(y) = 0}

= P0

Y ∈{y : f1(y) > 0}

= p∗
FA(0),
where the last equality follows from (30.32).
With the aid of Lemma 30.5.3 we can now analyze likelihood-ratio tests and prove
that any Pareto-optimal pair whose false-alarm probability is (strictly) between 0
and p∗
FA(0) can be achieved by such a test.
Lemma 30.5.4. For every 0 < πFA < p∗
FA(0), the pair

πFA, p∗
MD,rnd(πFA)

is
Pareto-optimal with respect to the class of randomized guessing rules, and it can be
achieved using a randomized likelihood-ratio test that bases its guess on LR(yobs)
only, i.e., for which
b(yobs) =
⎧
⎪
⎨
⎪
⎩
1
if LR(yobs) > Υ,
γ
if LR(yobs) = Υ,
0
if LR(yobs) < Υ,
(30.47)
for some threshold 0 < Υ < ∞and for some γ ∈[0, 1].
Proof. We will show that for every πFA satisfying
0 < πFA < p∗
FA(0)
(30.48)
756
The Radar Problem
there is a choice of the threshold Υ ∈(0, ∞) and of γ ∈[0, 1] that results in
the test (30.47) having false-alarm probability πFA. Denoting this test’s missed-
detection probability by p∗
MD, it will then follow from the Pareto-optimality of
all likelihood-ratio tests (Proposition 30.5.2) that the pair (πFA, p∗
MD) is Pareto-
optimal with respect to the class of randomized guessing rules and hence that p∗
MD
equals p∗
MD,rnd(πFA) (Proposition 30.1.1).
Using the notation we introduced in (30.41), the probability of false alarm of the
likelihood-ratio test (30.47) can be expressed as
P0

LR(Y) < Υ

+ (1 −γ) P0

LR(Y) = Υ

.
(30.49)
We shall conclude the proof by showing that for every πFA in the range (30.48)
there exist Υ > 0 and γ ∈[0, 1] for which this expression equals πFA.
We begin with Υ, which we would like to choose so that (30.52) and (30.54) ahead
will hold. To this end, we deﬁne Υ as
Υ = min
'
ξ ∈R : P0

LR(Y) ≤ξ

≥πFA
(
.
(30.50)
Here the set whose smallest element we seek is not empty because, by (30.43),
lim
ξ→∞P0

LR(Y) ≤ξ

= P0

LR(Y) < ∞

= p∗
FA(0),
(30.51)
and we are only considering the case (30.48) where πFA is smaller than p∗
FA(0). This
set has a smallest element because ξ 	→P0[LR(Y) ≤ξ] is continuous from the right
(Lemma 30.5.3). The minimum is thus deﬁned and ﬁnite. Also, since the smallest
element is in the set, it cannot be zero, because—by (30.42) and (30.48)—zero is
not in the set. Thus, indeed, 0 < Υ < ∞.
With this deﬁnition, Υ has the following properties: Since Υ is in the set,
P0

LR(Y) ≤Υ

≥πFA.
(30.52)
And since it is the smallest element in the set, no number smaller than Υ is in the
set, so
P0

LR(Y) ≤Υ −ϵ

< πFA,
ϵ > 0.
(30.53)
From (30.53) we obtain upon letting ϵ tend to zero from above that
P0

LR(Y) < Υ

≤πFA.
(30.54)
Having deﬁned Υ and derived its salient properties, we next describe our choice of γ.
It depends on whether or not P0[LR(Y) = Υ] is zero. If P0[LR(Y) = Υ] is zero,
then the left-hand sides of (30.52) and (30.54) are equal, so one can be smaller-or-
equal πFA and the other greater-or-equal πFA only if they are both equal to πFA. In
this case, the likelihood-ratio test with threshold Υ has false-alarm probability πFA
irrespective of how we resolve ties (see (30.49)).
We can choose γ as zero for
concreteness.
Suppose now that P0[LR(Y) = Υ] is not zero. In this case we choose 1 −γ as
1 −γ = πFA −P0

LR(Y) < Υ

P0

LR(Y) = Υ

.
(30.55)
30.5 Likelihood-Ratio Tests
757
The number on the RHS is nonnegative by (30.54), and it does not exceed one
because
πFA −P0

LR(Y) < Υ

= πFA −P0

LR(Y) ≤Υ

+ P0

LR(Y) = Υ

≤P0

LR(Y) = Υ

,
(30.56)
where the inequality follows from (30.52). Substituting this choice of γ in (30.49)
shows that the false-alarm probability of the likelihood-ratio test deﬁned by Υ and
by γ is πFA.
We next show that randomization is not essential for obtaining the Pareto-optimal
pairs: if we do not insist that the likelihood-ratio test depend on the observable yobs
only via LR(yobs), then randomization is not needed. This result relies heavily on
the assumption that under both hypotheses Y has a density. Had we allowed for
point masses, this result would not have held.
Proposition 30.5.5. For every 0 < πFA < p∗
FA(0), the pair

πFA, p∗
MD,rnd(πFA)

can be achieved using a deterministic likelihood-ratio test.
Proof. The randomized likelihood-ratio test in (30.47) is not deterministic when γ
is strictly between 0 and 1. Inspecting the proof of Lemma 30.5.4 we see that this
is the case whenever P0

LR(Y) = Υ

is nonzero and the RHS of (30.55) is strictly
between 0 and 1, i.e., whenever
0 < πFA −P0

LR(Y) < Υ

< P0

LR(Y) = Υ

,
(30.57)
where the threshold Υ is deﬁned in (30.50).
We shall thus conclude the proof
by showing that when (30.57) holds, we can ﬁnd an alternative deterministic
likelihood-ratio test of false-alarm probability πFA.
Our proposed deterministic likelihood-ratio test will also be of threshold Υ. But it
will not treat all the observations yobs for which LR(yobs) equals Υ in the same way:
some such observations will result in the guess “H = 0,” and others in “H = 1.”
To be more speciﬁc, denote the ﬁrst component of the observed vector Y by Y (1)
and the ﬁrst component of its realization yobs by y(1)
obs. The deterministic rule we
propose guesses “H = 0” whenever yobs is in the set
D =
'
yobs : LR(yobs) > Υ
(
∪
'
yobs : LR(yobs) = Υ and y(1)
obs ≥ξ
(
(30.58)
for some judicious choice of ξ that will guarantee a false-alarm probability πFA.
The false-alarm probability of this rule is
P0

LR(Y) < Υ

+ P0

LR(Y) = Υ and Y (1) < ξ

.
(30.59)
For this to equal πFA, we need to choose ξ so that
P0

LR(Y) = Υ and Y (1) < ξ

= πFA −P0

LR(Y) < Υ

.
(30.60)
We next show that such a ξ exists. To this end, deﬁne the mapping g: R →[0, 1]
g: α 	→P0

LR(Y) = Υ and Y (1) < α

.
(30.61)
758
The Radar Problem
This function is monotonically nondecreasing;
lim
α→∞g(α) = P0

LR(Y) = Υ

(30.62a)
(as can be proved using the Monotone Convergence Theorem); and
lim
α→−∞g(α) = 0
(30.62b)
(as can be proved by noting that g(·) is bounded from above by the cumulative
distribution function of Y (1), which tends to zero as its argument tends to −∞).
Once we show that g is continuous, the existence of a ξ satisfying (30.60) will
follow from (30.62) and (30.57) using the Intermediate Value Theorem (Rudin,
1976, Theorem 4.23, p. 93), (Royden and Fitzpatrick, 2010, Section 1.6).
It thus only remains to prove that g is continuous.
Since g is monotonically
nondecreasing, |g(α + h) −g(α)| is upper-bounded by g(α + |h|) −g(α −|h|),
and to prove continuity at α it suﬃces to prove that g(α + h) −g(α −h) tends
to zero when h tends to zero from above. To establish this, we note that for any
positive h,
0 ≤g(α + h) −g(α −h)
= P0

LR(Y) = Υ and Y (1) < α + h

−P0

LR(Y) = Υ and Y (1) < α −h

= P0

LR(Y) = Υ and α −h ≤Y (1) < α + h

≤P0

α −h ≤Y (1) < α + h

,
and the RHS tends to zero as h ↓0 because the hypothesis that Y has a density
implies that so does its ﬁrst component Y (1), and consequently the cumulative
distribution function of Y (1) is continuous.
Combining this result with (30.24) and (30.34) we obtain:
Corollary 30.5.6. The functions p∗
MD(·) and p∗
MD,rnd(·) are identical:
p∗
MD(πFA) = p∗
MD,rnd(πFA),
0 ≤πFA ≤1.
(30.63)
Corollary 30.5.7. Every Pareto-optimal pair with respect to the class of randomized
guessing rules can be also achieved using a deterministic guessing rule.
Proof. The pair

0, p∗
MD(0)

is achievable by a deterministic rule by Lemma 30.4.1;
all pairs of the form

πFA, p∗
MD,rnd(πFA)

for 0 < πFA < p∗
FA(0) are achievable with
deterministic rules by Proposition 30.5.5; and the pair

p∗
FA(0), 0

is achievable
with a deterministic rule by Lemma 30.4.2. There are no more Pareto-optimal
pairs (Corollary 30.4.5).
30.6 A Gaussian Example
759
30.6
A Gaussian Example
How do we guess whether an observed random variable Y was generated from a
centered or from a mean-A Gaussian distribution? Here we answer this question
for the case where the observed random variable has the same variance under the
two hypotheses. We denote this variance by σ2, and we assume that σ > 0 so
f0(y) =
1
√
2πσ2 e−y2
2σ2 ,
y ∈R,
(30.64a)
f1(y) =
1
√
2πσ2 e−(y−A)2
2σ2
,
y ∈R.
(30.64b)
We further assume that A is positive. (If it is zero, then f0(·) and f1(·) are identical
and the observation is of no use. And if it is negative, we can consider −Y as our
observation.)
Since f0(·) is positive, it follows from Lemma 30.4.1 that p∗
MD(0) = 1. Likewise,
since f1(·) is positive, it follows from Lemma 30.4.2 that p∗
FA(0) = 1. The pairs
(0, 1) and (1, 0) are thus Pareto-optimal. We next ﬁnd the more interesting pairs.
Any guessing rule that guesses “H = 0” whenever LR(yobs) ≥Υ is Pareto-optimal
for every value of Υ ∈(0, ∞) (Proposition 30.5.2). Consequently, so is the rule
that guesses “H = 0” whenever LLR(yobs) ≥log Υ (because the logarithm is a
monotonic function, so the two rules are, in fact, identical). As Υ varies over the
positive reals, its logarithm varies over all the reals. In other words, every υ in R is
the logarithm of some positive Υ. Hence, the rule that guesses “H = 0” whenever
LLR(yobs) ≥υ is Pareto-optimal for every υ ∈R (because, it is identical to the rule
that guesses “H = 0” whenever LR(yobs) ≥eυ, and the latter is Pareto-optimal
because it is a likelihood-ratio test with positive ﬁnite threshold). We next explore
such rules and show that with a judicious choice of υ ∈R they can achieve any
prespeciﬁed false-alarm probability that is strictly between zero and one. There is
no need to randomize or scrutinize ties.
For the densities in (30.64) the log likelihood-ratio function is
LLR(y) = −Ay
σ2 + A2
2σ2 ,
y ∈R,
(30.65)
so our guessing rule can be described as
Guess “H = 0” ⇐⇒LLR(yobs) ≥υ
⇐⇒−Ayobs
σ2
+ A2
2σ2 ≥υ
⇐⇒Ayobs
σ2
≤A2
2σ2 −υ
⇐⇒yobs ≤σ2
A
	 A2
2σ2 −υ

⇐⇒yobs ≤˜υ,
(30.66)
where in the fourth line we have used the hypothesis that A is positive, and where
in the ﬁfth line we deﬁned
˜υ = σ2
A
	 A2
2σ2 −υ

.
(30.67)
760
The Radar Problem
We thus see that to every υ ∈R there corresponds some ˜υ (which is given in (30.67))
such that the rule of guessing “H = 0” whenever yobs ≤˜υ is Pareto-optimal. But
we can strengthen the statement and claim that this rule is Pareto-optimal for every
˜υ ∈R, because to every ˜υ ∈R there corresponds some υ ∈R for which (30.67)
holds.
The probability of false alarm of the rule (30.66) is
pFA = P0

Y ≥˜υ

= Q
 ˜υ
σ

,
(30.68)
where in the second line we have noted that under the null hypothesis Y has a
N(0, σ) distribution and hence exceeds ˜υ with the probability speciﬁed in (19.12a).
Since the Q-function maps R onto (0, 1), for each prespeciﬁed false-alarm probabil-
ity pFA (strictly) between 0 and 1 we can ﬁnd some ˜υ ∈R for which (30.68) holds.
For this choice of ˜υ ∈R the decision rule (30.66) is not only Pareto-optimal, but
also of the desired false-alarm probability. Its missed-detection probability is
pMD = 1 −Q
 ˜υ −A
σ

,
(30.69)
because, under the alternative hypothesis the observation Y has a N(A, σ) distribu-
tion, and the probability that it is smaller than ˜υ can be computed using (19.12b).
We can now characterize the Pareto-optimal pairs of error probabilities. Those
comprise the pair (0, 1), the pair (1, 0), and all the pairs that have the form

p∗
FA, p∗
MD

=
	
Q
 ˜υ
σ

, 1 −Q
 ˜υ −A
σ

(30.70)
for some ˜υ ∈R. Those are depicted in Figure 30.4.
30.7
Detecting a Signal in White Gaussian Noise
In our next example the observation consists of a stochastic process

Y (t), t ∈R

.
Under the null hypothesis, it is the realization of white Gaussian noise N of double-
sided PSD N0/2 with respect to the bandwidth W, whereas under the alternative
hypothesis it is the sum of such noise N and some integrable signal s that is
bandlimited to W Hz. Given the signal s and the PSD SNN of the noise, our task
is to design a guessing rule that bases its decision on the observed SP.5
We ﬁrst argue that there is no loss of optimality in basing our guess on ⟨Y, φ⟩,
where
φ =
s
∥s∥2
(30.71)
is a unit-vector collinear with s. We do so by showing that, given any guessing
rule that bases its guess on Y, we can construct a randomized rule of identical
performance that bases its decision on ⟨Y, φ⟩. The argument is very similar to the
5We limit ourselves to guessing rules that are measurable with respect to the σ-algebra gen-
erated by the cylindrical sets of Y (Deﬁnition 25.2.2).
30.7 Detecting a Signal in White Gaussian Noise
761
1
1
1
1
pMD
pMD
pFA
pFA
(a)
(b)
Figure 30.4: The Pareto-optimal pairs (30.70) for the Gaussian problem. In (a)
A/σ is 1, and in (b) it is 2.
one we employed in proving Theorem 26.3.1 and which was illustrated in Figure 26.1
on Page 623.
The rule we construct uses ⟨Y, φ⟩and local randomness to generate a SP Y′, which
is then fed to the given guessing rule. Since Y′ will be generated in a way that will
guarantee that its FDDs equal those of Y under both hypotheses, the constructed
rule and the given rule will have identical performance.
To generate Y′, the rule we construct ﬁrst uses the local randomness to generate
a SP N′ of the same law as N but independent of it. In terms of ⟨Y, φ⟩and N′,
we can express Y′ as
Y′ = ⟨Y, φ⟩φ + N′ −⟨N′, φ⟩φ
(30.72)
=

⟨N, φ⟩φ + N′ −⟨N′, φ⟩φ
if Y = N,
⟨N, φ⟩φ + N′ −⟨N′, φ⟩φ + s
if Y = s + N,
(30.73)
where in the last line we have used the fact that ⟨s + N, φ⟩= ⟨s, φ⟩+ ⟨N, φ⟩
and ⟨s, φ⟩φ = s. We see from (30.73) that, under both hypotheses, Y′ and Y
have identical laws, because N and ⟨N, φ⟩φ + N′ −⟨N′, φ⟩φ have identical laws
(Theorem 25.15.7).
Having established that there is no loss of optimality in basing our guess on ⟨Y, φ⟩,
we next proceed to study the guessing problem when this is our observation. Us-
ing the properties of WGN (Proposition 25.15.2), we conclude that ⟨Y, φ⟩has a
N

0, σ2
distribution under H = 0 and a N

∥s∥2, σ2
distribution under H = 1,
where
σ2 = N0
2 .
(30.74a)
The problem of guessing based on ⟨Y, φ⟩is thus exactly the problem we studied
in Section 30.6. Using the results from that section (with ∥s∥2 substituted for A),
762
The Radar Problem
we obtain that the Pareto-optimal pairs are the trivial pairs (0, 1) and (1, 0) and
all the pairs that can be expressed as

p∗
FA, p∗
MD

=
-
Q
	 ˜υ
σ

, 1 −Q
	 ˜υ −∥s∥2
σ

.
(30.74b)
for some ˜υ ∈R.
30.8
Suﬃcient Statistics
We have seen that, when randomized guessing rules are allowed, any Pareto-
optimal pair (p∗
FA, p∗
MD) with 0 < p∗
FA < p∗
FA(0) can be achieved by a (random-
ized) likelihood-ratio test that bases its decision on the likelihood ratio LR(yobs)
(Lemma 30.5.4). It therefore stands to reason that if T(·) is a suﬃcient statistic for
f0(·) and f1(·) so LR(yobs) is computable from T(yobs) (Deﬁnition 20.12.2), then
basing our (randomized) guess on T(yobs) should incur no loss of optimality. This
is true, but before we can prove this we need to show that also the Pareto-optimal
pairs (0, p∗
MD(0)) and (p∗
FA(0), 0) can be achieved using guessing rules that depend
only on LR(yobs). (The rules in Lemma 30.4.1 and Lemma 30.4.2 are not quite of
this kind because of the convention we adopted to handle the case of 0/0 in the
deﬁnition of the likelihood-ratio function (30.18).)
Proposition 30.8.1 (Rules Based on LR(yobs) for pFA = 0 and for pMD = 0).
(i) Guessing “H = 0” whenever LR(yobs) > 0 achieves the pair (0, p∗
MD(0)).
(ii) Guessing “H = 0” whenever LR(yobs) = ∞achieves the pair (p∗
FA(0), 0).
Proof. We begin with Part (i). In view of our convention that 0/0 is 1,

y : LR(y) > 0

=

y : f0(y) > 0

∪

y : f0(y) = f1(y) = 0

.
(30.75)
Consequently, since

Y
f0(y) I
'
y ∈

y′ : f0(y′) = f1(y′) = 0
(
dy = 0
(30.76a)
and

Y
f1(y) I
'
y ∈

y′ : f0(y′) = f1(y′) = 0
(
dy = 0,
(30.76b)
we conclude that the events Y ∈{y : LR(y) > 0} and Y ∈{y : f0(y) > 0} have
identical probabilities under both hypotheses. Consequently, the rule of guessing
“H = 0” when yobs ∈{y : f0(y) > 0} and the rule of guessing “H = 0” when
yobs ∈{y : LR(y) > 0} have identical performance. And since the former achieves
(0, p∗
MD(0)) (Lemma 30.4.1), so does the latter.
The proof of Part (ii) follows similarly from

y : LR(y) = ∞

=

y : f1(y) = 0

\

y : f0(y) = f1(y) = 0

,
(30.77)
from (30.76), and from Lemma 30.4.2.
30.9 A Noncoherent Detection Problem
763
We are now ready to state the main result on suﬃcient statistics.
Proposition 30.8.2 (Suﬃcient Statistics and the Radar Problem). If the mapping
T : Rd →Rd′ forms a suﬃcient statistic for the densities f0(·) and f1(·), then any
Pareto-optimal pair that can be achieved by a possibly-randomized guessing rule
that is based on Y can also be achieved by a randomized guessing rule that is based
on T(Y).
Proof. Let (p∗
FA, p∗
MD) be Pareto-optimal with respect to the class of randomized
guessing rules that are based on Y.
By Corollary 30.4.5, Lemma 30.5.4, and
Proposition 30.8.1 the pair (p∗
FA, p∗
MD) is achievable by a guessing rule that forms
its guess based on local randomness and the value of LR(yobs) that it is fed. Since
T : Rd →Rd′ is a suﬃcient statistic for the densities f0(·) and f1(·), there exists a
set Y0 ⊂Rd of Lebesgue measure zero and a Borel measurable function ζ : Rd′ →
[0, ∞] such that for all yobs ∈Rd satisfying
yobs /∈Y0
and
f0(yobs) + f1(yobs) > 0
(30.78)
we can express LR(yobs) in terms of T(yobs) as
LR(yobs) = ζ

T(yobs)

.
(30.79)
Suppose now that instead of LR(yobs) we feed ζ

T(yobs)

to the above rule. This
would result in a randomized rule that bases its decision on T(yobs). We claim
that this rule has the same performance as the one that is fed LR(yobs) and hence
also achieves (p∗
FA, p∗
MD). Indeed, under both hypotheses, the probability that the
observable yobs violates (30.78) is zero (cf. Note 20.12.1), so under both hypotheses
ζ

T(Y)

and LR(Y) are equal with probability one.
30.9
A Noncoherent Detection Problem
Our next example is a variation on a theme from Chapter 27 on noncoherent
detection. The observation is a SP

Y (t)

, and we wish to guess whether it is noise
(the “null hypothesis”) or the sum of noise and a passband signal of random phase
(the “alternative hypothesis”). We model the random passband signal S as
S(t) = 2 Re

sBB(t) ei(2πfct+Θ)
= 2 Re

sBB(t) ei2πfct
cos Θ −2 Im

sBB(t) ei2πfct
sin Θ
= 2 Re

sBB(t) ei2πfct
cos Θ + 2 Re

i sBB(t) ei2πfct
sin Θ
= sc(t) cos Θ + ss(t) sin Θ,
t ∈R,
(30.80)
where sBB is a deterministic (known), possibly-complex, integrable signal that
is bandlimited to W/2 Hz; fc is the carrier frequency, which is assumed to be
deterministic (known) and to satisfy fc > W/2; the RV Θ (unknown) is uniformly
distributed over the interval [−π, π) independently of the noise; and where we
deﬁned
sc(t) ≜2 Re

sBB(t) ei2πfct
,
t ∈R,
(30.81a)
ss(t) ≜2 Re

i sBB(t) ei2πfct
,
t ∈R.
(30.81b)
764
The Radar Problem
The noise is assumed to be WGN of double-sided PSD N0/2 with respect to the
bandwidth W around the carrier frequency fc (Deﬁnition 25.15.9). Notice that, by
Theorem 7.6.10,
⟨sc, ss⟩= 0.
(30.82)
Also, if we deﬁne Es as
Es = 2 ∥sBB∥2
2 ,
(30.83)
then, by Theorem 7.6.10 on the energy in baseband and passband,
Es = ∥S∥2
2 = ∥sc∥2
2 = ∥ss∥2
2
(30.84)
for every realization of Θ. The signals sc/√Es and ss/√Es are thus orthonormal.
The motivation for studying this problem is similar to the one in Section 27.1: We
envision a scenario where the distance to the target is known only approximately,
and we therefore only have an estimate of the arrival time of the reﬂected signal.
Our model is plausible when our estimation error is small compared to 1/W but
large compared to 1/fc.
As in Section 30.7, here too we can reduce the problem to a ﬁnite-dimensional one:
there is no loss of optimality in basing our guess on the tuple (Tc, Ts), where
Tc ≜
#
Y, sc
√Es
$
,
Ts ≜
#
Y, ss
√Es
$
.
(30.85)
Indeed, the performance of any given guessing rule that bases its decision on the
waveform Y can also be achieved by a randomized decision rule that bases its
decision on (Tc, Ts) in the following way.
It uses local randomness to generate
noise N′ of the same law as N; it uses (Tc, Ts) and N′ to synthesize Y′, where
Y′ = Tc
sc
√Es
+ Ts
ss
√Es
+ N′ −
#
N′, sc
√Es
$ sc
√Es
−
#
N′, ss
√Es
$ ss
√Es
;
(30.86)
and it then feeds Y′ to the given rule. Using arguments very similar to those in
Section 30.7, it can be shown that the laws of Y′ and Y are identical both under
the null hypothesis and under the alternative hypothesis.
To derive Pareto-optimal rules based on the tuple (Tc, Ts), we next derive the
tuple’s densities under the two hypotheses.
To simplify the typesetting, let us
deﬁne
σ2 ≜N0
2
(30.87)
and
t ≜t2
c + t2
s
σ2
.
(30.88)
(We deﬁne t not only to simplify the typesetting but also for ulterior motives that
have to do with the further reduction of the suﬃcient statistic from a random
vector of two components (Tc, Ts) to a scalar.)
Since the signals tuple

sc/√Es, ss/√Es

is orthonormal (by (30.82) and (30.84)),
it follows that under the null hypothesis Tc and Ts are independent variance-σ2
30.9 A Noncoherent Detection Problem
765
centered Gaussians (Note 25.15.10), and hence
fTc,Ts|H=0(tc, ts) =
1
2πσ2 exp

−1
2σ2

t2
c + t2
s

=
1
2πσ2 exp

−t
2

.
(30.89)
To compute the density under the alternative hypothesis, we next condition on Θ
and then integrate it out like we did in Section 27.4. Conditional on Θ = θ we
have under the alternative hypothesis that Tc and Ts are independent variance-σ2
Gaussians with Tc having mean √Es cos θ and with Ts having mean √Es sin θ. Thus,
fTc,Ts|H=1,Θ=θ(tc, ts)
=
1
2πσ2 exp
	
−1
2σ2

tc −

Es cos θ
2 +

ts −

Es sin θ
2
=
1
2πσ2 exp

−Es
2σ2 −t
2

exp
 1
σ2

Es tc cos θ + 1
σ2

Es ts sin θ

.
(30.90)
As in (27.32), we now integrate Θ out:
fTc,Ts|H=1(tc, ts) =
 π
−π
fΘ|H=1(θ) fTc,Ts|H=1,Θ=θ(tc, ts) dθ
=
 π
−π
fΘ(θ) fTc,Ts|H=1,Θ=θ(tc, ts) dθ
= 1
2π
 π
−π
fTc,Ts|H=1,Θ=θ(tc, ts) dθ
=
1
2πσ2 e−Es/(2σ2) e−t/2
× 1
2π
 π
−π
exp
	 1
σ2

Es tc cos θ + 1
σ2

Es ts sin θ

dθ
=
1
2πσ2 e−Es/(2σ2) e−t/2
× 1
2π
 π
−π
exp
-4
Es
σ2
√
t cos

θ −arctan(ts/tc)
.
dθ
=
1
2πσ2 e−Es/(2σ2) e−t/2
× 1
2π
 π−arctan(ts/tc)
−π−arctan(ts/tc)
exp
-4
Es
σ2
√
t cos ψ
.
dψ
=
1
2πσ2 e−Es/(2σ2) e−t/2 1
2π
 π
−π
exp
-4
Es
σ2
√
t cos ψ
.
dψ
=
1
2πσ2 e−Es/(2σ2) e−t/2 I0
-4
Es
σ2
√
t
.
,
(30.91)
where the ﬁrst equality follows by averaging out Θ; the second because the law of Θ
does not depend on the hypothesis; the third because Θ is uniform; the fourth by
766
The Radar Problem
the explicit form of fTc,Ts|Θ=θ,H=1(·) (30.90); the ﬁfth by the trigonometric identity
α cos θ + β sin θ =

α2 + β2 cos

θ −arctan(β/α)

(30.92)
and the deﬁnition of t (30.88); the sixth by the change of variable ψ ≜θ −
arctan(ts/tc); the seventh from the periodicity of the cosine function; and the ﬁnal
equality by recalling the deﬁnition of the zeroth-order modiﬁed Bessel function
I0(·) (27.34).
Having computed the density of (Tc, Ts) under both the null hypothesis (30.89) and
the alternative hypothesis (30.91), we can now derive the Pareto-optimal decision
rules. But before rushing to do so, we pause to note that both fTc,Ts|H=0(tc, ts)
and fTc,Ts|H=1(tc, ts) are computable from t (30.88). Consequently, so is their ratio,
and we conclude that

Tc, Ts

	→T 2
c + T 2
s
σ2
(30.93)
is a suﬃcient statistic for the two densities. By Proposition 30.8.2 there is therefore
no loss of optimality in basing our decision on
T ≜T 2
c + T 2
s
σ2
.
(30.94)
In analogy to the two methods we presented in Section 27.5, we can now proceed to
derive the Pareto-optimal pairs using two diﬀerent methods. The ﬁrst is to ignore
the suﬃciency of T and to work with the likelihood-ratio function
LR(tc, ts) = fTc,Ts|H=0(tc, ts)
fTc,Ts|H=1(tc, ts)
=
1
2πσ2 exp

−t
2

1
2πσ2 e−Es/(2σ2) exp

−t
2

I0
3
Es
σ2
√
t

=
eEs/(2σ2)
I0
3
Es
σ2
√
t
.
(30.95)
The second method is to compute the density of T under the two hypotheses and de-
rive the Pareto-optimal pairs via the likelihood-ratio function fT |H=0(t)/fT |H=1(t).
We shall use the ﬁrst method, but we shall nonetheless ﬁrst compute the densities
fT |H=0(·), fT |H=1(·), because those will be needed when we analyze the perfor-
mance of the proposed guessing rules. Under the null hypothesis, T is the sum of
the squares of two independent standard Gaussians (Tc/σ and Tc/σ). As such, it
has a central χ2 distribution with two degrees of freedom (Section 19.8.1),
T ∼χ2
2
(the null hypothesis),
(30.96)
i.e., a mean-2 exponential distribution (Note 19.8.1):
fT |H=0(t) = 1
2 e−t/2 I{t ≥0},
t ∈R.
(30.97)
30.9 A Noncoherent Detection Problem
767
To compute the distribution of T under the alternative hypothesis, we ﬁrst condi-
tion on Θ = θ, with the result that T is distributed as the sum of the squares
of two independent unit-variance Gaussians, where the ﬁrst, Tc/σ, is of mean
σ−1√Es cos θ and the second, Ts/σ, is of mean σ−1√Es sin θ.
Thus, under the
alternative hypothesis, the conditional distribution of T given Θ = θ is a non-
central χ2 distribution with two degrees of freedom and noncentrality parameter
Es/σ2 (Note 19.8.2). Since the parameters of this distribution do not depend on θ,
we conclude that, under the alternative hypothesis, T is independent of Θ and
T ∼χ2
2,Es/σ2
(the alternative hypothesis).
(30.98)
This concludes the derivation of the distribution of T under the two hypotheses.
We next derive the Pareto-optimal pairs using the ﬁrst method, i.e., by ignoring
the suﬃciency of T and by considering our observation as being the pair (Tc, Ts)
and using the likelihood-ratio function (30.95).
Under both hypotheses the joint density of (Tc, Ts) is positive ((30.89) and (30.91)),
so p∗
MD(0) = 1 (Lemma 30.4.1) and p∗
FA(0) = 1 (Lemma 30.4.2). The pairs (0, 1)
and (1, 0) are thus Pareto-optimal. Any guessing rule that guesses “H = 0” when-
ever LR(y) ≥Υ is Pareto-optimal, irrespective of the value of Υ ∈(0, ∞) (Propo-
sition 30.5.2). Using the explicit form of the likelihood-ratio function (30.95), we
thus conclude that the rule
guess “H = 0” whenever
eEs/(2σ2)
I0
3
Es
σ2
√
t
 ≥Υ
(30.99a)
is Pareto-optimal for every Υ ∈(0, ∞). Deﬁning Υ′ = eEs/(2σ2)/Υ, noting that
every Υ′ ∈(0, ∞) corresponds to some Υ ∈(0, ∞) (i.e., that the mapping Υ 	→Υ′
is onto (0, ∞)), and noting that I0(·) is positive (27.34), we conclude that the rule
guess “H = 0” whenever
I0
	4
Es
σ2
√
t

≤Υ′
(30.99b)
is Pareto-optimal for every choice of Υ′ ∈(0, ∞). Given any Υ′′ ∈(0, ∞), we can
deﬁne Υ′ to be I0(Υ′′) with the result that Υ′ ∈(1, ∞) and that the rule (30.99b)
is thus Pareto-optimal. Substituting I0(Υ′′) for Υ′ in (30.99b) thus shows that the
rule
guess “H = 0” whenever
I0
	4
Es
σ2
√
t

≤I0(Υ′′)
(30.99c)
is Pareto-optimal for every Υ′′ ∈(0, ∞). Since the zeroth-order modiﬁed Bessel
function I0(·) is strictly increasing on the nonnegative reals, the rule (30.99c) is
equivalent to the rule
guess “H = 0” whenever
4
Es
σ2
√
t ≤Υ′′,
(30.99d)
which is thus also Pareto-optimal for every Υ′′ ∈(0, ∞). Deﬁning υ to be the
square of Υ′′/

Es/σ2 (and noting that the mapping from Υ′′ to υ is onto the
positive reals) we conclude that
guess “H = 0” whenever
t ≤υ
(30.99e)
768
The Radar Problem
is Pareto-optimal for every choice of υ ∈(0, ∞).
The false-alarm probability of this rule is, by (30.96), the probability that a mean-2
exponential exceeds υ, so
pFA = exp

−υ
2

,
υ ∈(0, ∞).
(30.100)
To each prespeciﬁed pFA ∈(0, 1), we can thus ﬁnd a corresponding υ ∈(0, ∞)
so that the Pareto-optimal rule (30.99e) will be of this false-alarm probability.
Consequently, as we vary υ over (0, ∞) we swipe over all Pareto-optimal rules of
nontrivial false alarm.
The probability of missed-detection of the rule (30.99e) is, by (30.98), the proba-
bility that a χ2
2,Es/σ2 RV does not exceed υ. Equivalently, it is the probability that
the square root of a χ2
2,Es/σ2 RV (which has a Rice distribution) does not exceed
√υ. Using (19.51) (with the substitution of Es/σ2 for λ and of √υ for x) we thus
obtain
pMD = 1 −Q1
	4
Es
σ2 , √υ

,
υ ∈(0, ∞),
(30.101)
where Q1(·) is the Marcum Q-function.
To conclude, the Pareto-optimal pairs are the pairs (0, 1), (1, 0), and all the pairs
that have the form

p∗
FA, p∗
MD

=
-
exp

−υ
2

, 1 −Q1
	4
Es
σ2 , √υ

.
(30.102)
for υ ∈(0, ∞). Pairs of the form (30.102) are achieved by the guessing rule (30.99e).
30.10
Randomization Is Not Needed
We have seen that all the pairs that are Pareto-optimal with respect to the class
of randomized guessing rules can also be achieved by deterministic rules (Corol-
lary 30.5.7). Here we present a stronger result: any pair—be it Pareto-optimal
or not—that is achievable by a randomized guessing rule can also be achieved by
a deterministic one. Like the aforementioned result on Pareto-optimal pairs, this
result too relies heavily on the assumption that the observable Y has a density
under both hypotheses. This section’s main result can be stated as follows:
Proposition 30.10.1 (Rrnd = R). Every pair (pFA, pMD) that can be achieved by
some randomized rule can also be achieved by a deterministic rule:
Rrnd = R.
(30.103)
Since Rrnd ⊇R (30.15), to prove (30.103) it remains to prove that
Rrnd ⊆R,
(30.104)
i.e., that every pair that is achievable by a randomized rule can also be achieved
by a deterministic rule. The proof of this result relies on an important theorem
30.10 Randomization Is Not Needed
769
due to Alexey Liapunov (1911–1973) on the convexity of vector-valued measures.
To state the theorem we ﬁrst recall that a subset K of R2 is a convex set if for
every 0 < λ < 1 and every pair of tuples r1, r2 in K, the tuple λr1 + (1 −λ)r2 is
also in K. We can now state Liapunov’s theorem for our setting as follows:
Theorem 30.10.2 (Liapunov). Let f0(·) and f1(·) be two densities on Rn. The
subset of R2 comprising all pairs that can be expressed as
	
A
f0(y) dy,

A
f1(y) dy

for some (Lebesgue measurable) subset A of Rn is convex.
If a set K ⊆R2 is convex, then so is the set
'
1 −r(1), r(2)
:

r(1), r(2)
∈K
(
.
By using this observation, by substituting in Liapunov’s Theorem the set D (the
subset of Rn where we guess “H = 0”) for A, and by recalling the representa-
tions (30.3b) and (30.4b) of pFA and pMD we obtain:
Corollary 30.10.3 (R Is Convex). The set of pairs (pFA, pMD) that can be achieved
by deterministic guessing rules is convex.
Before we can prove the inclusion (30.104) using Corollary 30.10.3, we need some
more groundwork. Recall that in (30.9) we deﬁned p∗
MD(πFA) to be the least proba-
bility of missed-detection that can be achieved by a deterministic rule whose false-
alarm probability does not exceed πFA. We did not require that (πFA, p∗
MD(πFA))
be achievable. But we did show that (πFA, p∗
MD(πFA)) is in R (i.e., is achievable)
whenever πFA is in the range 0 ≤πFA ≤p∗
FA(0). (For πFA = 0 the result fol-
lows from Lemma 30.4.1; for 0 < πFA < p∗
FA(0) it follows from Proposition 30.5.5;
and for πFA = p∗
FA(0) it follows from Lemma 30.4.2.) We next use the convexity
of R to show that this also holds for p∗
FA(0) < πFA ≤1, i.e., that it holds for all
0 ≤πFA ≤1.6
Lemma 30.10.4. For every 0 ≤πFA ≤1, the pair

πFA, p∗
MD(πFA)

is achievable
by a deterministic guessing rule:

πFA, p∗
MD(πFA)

∈R,
0 ≤πFA ≤1.
(30.105)
Proof. For πFA = 0 the result follows from Lemma 30.4.1; for 0 < πFA < p∗
FA(0) it
follows from Proposition 30.5.5; and for πFA = p∗
FA(0) it follows from Lemma 30.4.2.
The remaining case is when πFA is in the range
p∗
FA(0) < πFA ≤1.
(30.106)
For such πFA we can express (πFA, 0) as a convex combination of (p∗
FA(0), 0) and
(1, 0), both of which are in R (the former by Lemma 30.4.2 and the latter by
6This result does not need the full power of Liapunov’s Theorem. We could have also proved
it using the technique we used in the proof of Proposition 30.5.5.
770
The Radar Problem
considering the deterministic guessing rule of always guessing “H = 1”).
The
convexity of R thus implies that (πFA, 0) is in R:
(πFA, 0) ∈R,
p∗
FA(0) < πFA ≤1.
(30.107)
For πFA in the range (30.106), p∗
MD(πFA) is zero (30.34), so (30.107) is equivalent
to

πFA, p∗
MD(πFA)

∈R,
p∗
FA(0) < πFA ≤1,
(30.108)
thus verifying the lemma in this range too.
Before we can prove (30.104) we also need the following symmetry result:
Proposition 30.10.5. Both R and Rrnd are invariant under rotation by 180 degrees
around the pair (1/2, 1/2):

(pFA, pMD) ∈R

⇐⇒

(1 −pFA, 1 −pMD) ∈R

,
(30.109)

(pFA, pMD) ∈Rrnd

⇐⇒

(1 −pFA, 1 −pMD) ∈Rrnd

.
(30.110)
Proof. Consider ﬁrst Rrnd. To establish (30.110) it suﬃces to prove that

(pFA, pMD) ∈Rrnd

=⇒

(1 −pFA, 1 −pMD) ∈Rrnd

,
(30.111)
because the reverse implication will then follow by applying (30.111) to the pair
(1 −pFA, 1 −pMD). To prove (30.111) suppose that (pFA, pMD) is achieved by the
randomized guessing rule determined by the function b(·). Consider now a new
rule that is determined by the function ¯b(·), where
¯b(y) = 1 −b(y),
y ∈Y.
Denoting the probability of false alarm of this new guessing rule by ¯pFA,
¯pFA =

Y

1 −¯b(y)

f0(y) dy
=

Y
f0(y) dy −

Y
¯b(y) f0(y) dy
= 1 −

Y

1 −b(y)

f0(y) dy
= 1 −pFA.
Likewise, denoting the probability of missed-detection of the new guessing rule by
¯pMD,
¯pMD =

Y
¯b(y) f1(y) dy
=

Y
f1(y) dy −

Y

1 −¯b(y)

f1(y) dy
= 1 −

Y
b(y) f1(y) dy
= 1 −pMD.
30.10 Randomization Is Not Needed
771
1
1
1
1
p∗
MD(0)
p∗
MD(0)
p∗
FA(0)
p∗
FA(0)
pMD
pMD
pFA
pFA
(a)
(b)
Figure 30.5: A generic achievable region. In (a) the dashed line is the graph of
p∗
MD,rnd(·). In (b), we have added the graph of pFA 	→1 −p∗
MD,rnd(1 −pFA), which
is obtained from the graph of p∗
MD,rnd(·) by rotating it around the point (1/2, 1/2)
by 180 degrees.
Thus, the probabilities of error of the new rule are (1−pFA, 1−pMD), and the pair
is thus achievable. This concludes the proof of (30.111) and hence of (30.110).
The proof for R is very similar. The only diﬀerence is that the function b(·) and
hence also ¯b(·) are zero-one valued.
We next use Proposition 30.10.5 to show that if (pFA, pMD) ∈Rrnd then we can
bound pMD from below and from above using the function p∗
MD,rnd(·) as follows:
p∗
MD,rnd(pFA) ≤pMD ≤1 −p∗
MD,rnd(1 −pFA),
(pFA, pMD) ∈Rrnd.
(30.112)
The lower bound pMD ≥p∗
MD,rnd(pFA) follows directly from the deﬁnition (30.16)
of p∗
MD,rnd(pFA), which implies that no achievable pair of false alarm pFA can have
a missed-detection probability smaller than p∗
MD,rnd(pFA). More interesting is the
upper bound, i.e., the statement that

(pFA, pMD) ∈Rrnd

=⇒

pMD ≤1 −p∗
MD,rnd(1 −pFA)

.
(30.113)
This relation is illustrated in Figure 30.5. The shadowed region corresponds to
the achievable region Rrnd. The dashed line is the graph of p∗
MD,rnd(·), and the
irregularly-dashed line is the graph of pFA 	→1 −p∗
MD,rnd(1 −pFA). (It is obtained
from the dashed line by rotating it by 180 degrees around the point (1/2, 1/2).)
No achievable pair lies above the irregularly-dashed line.
To prove (30.113), suppose that (pFA, pMD) ∈Rrnd.
Proposition 30.10.5 then
implies that also (1 −pFA, 1 −pMD) ∈Rrnd. This and the deﬁnition of p∗
MD,rnd(1 −
pFA) imply that p∗
MD,rnd(1−pFA) ≤1−pMD and hence pMD ≤1−p∗
MD,rnd(1−pFA).
Having established (30.112), we now recall that the functions p∗
MD,rnd(·) and p∗
MD(·)
are identical (Corollary 30.5.6), and we can thus substitute p∗
MD(·) for p∗
MD,rnd(·)
772
The Radar Problem
in (30.112) to obtain
p∗
MD(pFA) ≤pMD ≤1 −p∗
MD(1 −pFA),
(pFA, pMD) ∈Rrnd.
(30.114)
The inequalities in (30.114) imply that every (pFA, pMD) ∈Rrnd can be written as
a convex combination of the pairs

pFA, p∗
MD(pFA)

and

pFA, 1 −p∗
MD(1 −pFA)

.
Once we show that these two pairs are both in R, it will follow from the convexity
of R (Corollary 30.10.3) that this convex combination is in R, and (30.104) will
be established.
Beginning with the ﬁrst pair, we note that

pFA, p∗
MD(pFA)

∈R,
0 ≤πFA ≤1,
(30.115)
by Lemma 30.10.4, i.e., by (30.105). As to the second pair, we note that substi-
tuting 1 −pFA for pFA in (30.115) yields

1 −pFA, p∗
MD(1 −pFA)

∈R,
0 ≤πFA ≤1,
(30.116)
from which the achievability of the second pair, namely,

pFA, 1 −p∗
MD(1 −pFA)

∈R,
0 ≤πFA ≤1
(30.117)
follows using Proposition 30.10.5. We have thus established the achievability of
the two pairs using deterministic guessing rules and have thus concluded the proof
of (30.104).
As we have noted, the assumption that Y has a density under both hypotheses is
essential for R to equal Rrnd. (We used this assumption in the proof of Proposi-
tion 30.5.5, and it is also key in Liapunov’s Theorem.) To see this, consider the
case where under both hypotheses the observation is deterministically equal to 17.
In this case the observation is clearly useless. Depending on whether or not we
guess “H = 0” after observing 17, we either achieve (pFA, pMD) equal to (1, 0) or
to (0, 1). There are no other pairs that are achievable with a deterministic rule.
However, with randomized guessing rules we can guess “H = 0” with probability b
and thus achieve all pairs of the form (1 −b, b) where b is any number satisfying
0 ≤b ≤1.
30.11
The Big Picture
To see the big picture, we collect here the key results on Pareto-optimal pairs
and the rules that achieve them. In some places we also expand. For example,
Theorem 30.11.1 (ii) explains how we drew the achievable regions in the diﬀerent
ﬁgures in this chapter. As noted earlier, some of the results rely heavily on the
assumption that the observable has a density under both hypotheses.
Theorem 30.11.1 (Achievable Pairs and Pareto-Optimal Pairs).
(i) Every pair of error probabilities that is achievable by a randomized guessing
rule can also be achieved by a deterministic one:
Rrnd = R.
(30.118)
30.11 The Big Picture
773
(ii) The set R is convex and comprises all probability pairs (pFA, pMD) satisfying
p∗
MD(pFA) ≤pMD ≤1 −p∗
MD(1 −pFA).
(30.119)
(iii) The set of Pareto-optimal pairs comprises all pairs of the form

pFA, p∗
MD(pFA)

,
0 ≤pFA ≤p∗
FA(0).
(30.120)
(iv) Every likelihood-ratio test of positive ﬁnite threshold is Pareto-optimal.
(v) Every Pareto-optimal pair can be achieved by a deterministic likelihood-ratio
test. It can also be achieved by a randomized likelihood-ratio test that bases
its decision on LR(yobs) only.
Proof. Part (i) is from Proposition 30.10.1. As to Part (ii), the convexity result
is from Corollary 30.10.3. That every pair (pFA, pMD) in R must satisfy (30.119)
follows from (30.114). And that every pair (pFA, pMD) satisfying (30.119) must be
in R follows from the convexity of R, because every such pair can be written as
a convex combination of the pairs

pFA, p∗
MD(pFA)

and

pFA, 1 −p∗
MD(1 −pFA)

,
both of which are in R by (30.115) and (30.117).
As to Part (iii), all the pairs in (30.120) are Pareto-optimal by Lemma 30.4.1,
Lemma 30.5.4, and Lemma 30.4.2. There are no additional Pareto-optimal pairs
of false alarm in the interval [0, p∗
FA(0)] by Note 30.3.1, and there are none outside
this interval by Corollary 30.4.5.
Part (iv) is from Proposition 30.5.2. We ﬁnally prove Part (v). The achievability of
every Pareto-optimal pair using a deterministic likelihood-ratio test follows from
Part (iii), Proposition 30.5.5, and Proposition 30.8.1. The achievability using a
randomized likelihood-ratio test that bases its decision only on LR(yobs) follows
from Part (iii), Lemma 30.5.4, and Proposition 30.8.1.
Theorem 30.11.2 (The Function p∗
MD(·)).
(i) The functions p∗
MD(·) and p∗
MD,rnd(·) are identical:
p∗
MD(πFA) = p∗
MD,rnd(πFA),
0 ≤πFA ≤1.
(30.121)
(ii) For every πFA ∈[0, 1], the pair

πFA, p∗
MD(πFA)

is achievable:

πFA, p∗
MD(πFA)

∈R,
0 ≤πFA ≤1.
(30.122)
(iii) For πFA = 0,
p∗
MD(0) =

{y∈Y : f0(y)>0}
f1(y) dy.
(30.123)
It is achieved by guessing “H = 0” whenever f0(yobs) is positive.
(iv) For 0 < πFA < p∗
FA(0), the pair

πFA, p∗
MD(πFA)

is achievable by a determin-
istic likelihood-ratio test of positive ﬁnite threshold. It is also achievable by a
randomized likelihood-ratio test of the same threshold that bases its decision
only on LR(yobs).
774
The Radar Problem
(v) For p∗
FA(0) ≤πFA ≤1,
p∗
MD(πFA) = 0.
(30.124)
(vi) The function p∗
MD(·) is monotonically nonincreasing, convex, and continuous
over the closed interval [0, 1].
Proof. Part (i) is a restatement of Corollary 30.5.6; see (30.63).
Part (ii) is
a restatement of Lemma 30.10.4; see (30.105).
Part (iii) is a restatement of
Lemma 30.4.1; see (30.24). Part (iv) is a combination of Lemma 30.5.4 and Propo-
sition 30.5.5. Part (v) is a restatement of Corollary 30.4.4; see (30.34).
We next turn to Part (vi). Monotonicity is proved by noting that increasing the
maximal-allowed false-alarm probability enlarges (or leaves unchanged) the class
of allowed decision rules in (30.8b) and hence can only decrease (or not change) the
least probability of missed-detection in the class (30.9). Convexity is a consequence
of the convexity of R and the achievability of (πFA, p∗
MD(πFA)) (Part (ii)): If π(1)
FA
and π(2)
FA are two diﬀerent false-alarm probabilities, then Part (ii) implies that

π(ν)
FA, p∗
MD

π(ν)
FA

∈R,
ν ∈{1, 2},
(30.125)
and the convexity of R (Corollary 30.10.3) thus implies

λ π(1)
FA+(1−λ) π(2)
FA, λ p∗
MD

π(1)
FA

+(1−λ) p∗
MD

π(2)
FA

∈R,
0 ≤λ ≤1. (30.126)
The latter implies
p∗
MD

λ π(1)
FA+(1−λ) π(2)
FA

≤λ p∗
MD

π(1)
FA

+(1−λ) p∗
MD

π(2)
FA

,
0 ≤λ ≤1, (30.127)
and thus establishes convexity.
The proof of continuity is a bit technical, so we will be terse.
Convexity over
the closed interval [0, 1] implies continuity over the open interval (0, 1) (see, for
example, (Royden and Fitzpatrick, 2010, Section 6.6, Corollary 17), so it remains
to establish continuity at the end points. We ﬁrst consider the endpoint πFA = 1,
where continuity follows from monotonicity and convexity: Monotonicity implies
that
lim
πFA↑1 p∗
MD(πFA) ≥p∗
MD(1).
(30.128)
And convexity implies that (30.128) must hold with the reverse inequality, because
if we substitute in (30.127) πFA for π(1)
FA, the value 1 for π(2)
FA, and 1/2 for λ we
obtain
p∗
MD
πFA + 1
2

≤1
2 p∗
MD(πFA) + 1
2 p∗
MD(1),
and hence
2 p∗
MD
πFA + 1
2

−p∗
MD(πFA) ≤p∗
MD(1),
(30.129)
from which it follows upon taking the limit πFA ↑1 that
lim
πFA↑1 p∗
MD(πFA) ≤p∗
MD(1).
(30.130)
30.11 The Big Picture
775
This combines with (30.128) to establish continuity at πFA = 1.
It thus remains to establish continuity at the endpoint πFA = 0, i.e., to prove
lim
πFA↓0 p∗
MD(πFA) = p∗
MD(0).
(30.131)
Monotonicity implies that the above limit exists, and we now proceed to show
that it equals p∗
MD(0). To this end, consider the likelihood-ratio test that guesses
“H = 0” whenever LR(yobs) > ϵ, and denote its false-alarm and missed-detection
probabilities by pFA(ϵ) and pMD(ϵ):
pFA(ϵ) = P0

0 ≤LR(Y) ≤ϵ

,
(30.132)
pMD(ϵ) = P1

LR(Y) > ϵ

,
(30.133)
where P1[·] is deﬁned as in (30.41d) but with the density f0 replaced by f1. Since
this is a likelihood-ratio test, the pair

pFA(ϵ), pMD(ϵ)

is Pareto-optimal (Propo-
sition 30.5.2), and hence, by (30.10),
p∗
MD

pFA(ϵ)

= pMD(ϵ),
ϵ > 0.
(30.134)
We next study the limits of pFA(ϵ) and pMD(ϵ) as ϵ ↓0. Starting with the former,
lim
ϵ↓0 pFA(ϵ) = lim
ϵ↓0 P0

0 ≤LR(Y) ≤ϵ

= P0

LR(Y) = 0

= 0,
(30.135)
where the last equality follows from (30.42). As to the latter,
lim
ϵ↓0 pMD(ϵ) = lim
ϵ↓0 P1

LR(Y) > ϵ

= P1

LR(Y) > 0

= p∗
MD(0),
(30.136)
where the last equality follows because the rule of guessing “H = 0” whenever
LR(yobs) > 0 achieves the pair

0, p∗
MD(0)

(Proposition 30.8.1).
To conclude the proof of (30.131) we now compute:
lim
πFA↓0 p∗
MD(πFA) = lim
ϵ↓0 p∗
MD

pFA(ϵ)

(30.137)
= lim
ϵ↓0 pMD(ϵ)
(30.138)
= p∗
MD(0),
(30.139)
where the ﬁrst equality follows from the existence of the limit on the LHS and
from (30.135); the second equality from (30.134); and where the last equality follows
from (30.136).
776
The Radar Problem
30.12
Relative Entropy
Any useful lower bound on the missed-detection probability must also take into
account the false-alarm probability, because we can always achieve zero pMD by
ignoring the observation and guessing that the target is present. In this section we
shall derive a necessary condition that all achievable pairs (pFA, pMD) must satisfy.
The condition will help us to understand the trade-oﬀbetween the two types of
error, and it will often allow us to rule out the possibility that both are very low.
The condition will be expressed in terms of the relative entropy between the
densities of the observable under the two hypotheses.
Relative entropy, also known as Kullback-Leibler divergence or divergence,
is central to Statistics, Probability, and Information Theory. It can be deﬁned
between any pair of probability distributions, but we will focus on two special
cases only: when both distributions have densities, and when both are deﬁned over
a ﬁnite set. In the next deﬁnition and, in fact, throughout, we adopt the convention

0 ln 0
β = 0,
β ≥0

and

α ln α
0 = +∞,
α > 0

.
(30.140)
Deﬁnition 30.12.1 (Relative Entropy).
(i) Let p = (p1, . . . , pn) and q = (q1, . . . , qn) be n-tuples having nonnegative
entries that sum to one. The relative entropy is deﬁned as
D(p∥q) ≜
n

j=1
pj ln pj
qj
.
(30.141)
(ii) Let f(·) and g(·) be probability density functions on Rn. The relative entropy
is deﬁned as7
D(f∥g) ≜

f(x) ln f(x)
g(x) dx.
(30.142)
Relative entropy is not symmetric: D(p∥q) is typically diﬀerent from D(q∥p).
Example 30.12.2 (The Relative Entropy between Equi-Variance Gaussians).
Suppose that f(·) is the density of the N

μ1, σ2
distribution, and g(·) is the
density of the N

μ2, σ2
distribution, where σ > 0 and μ1, μ2 ∈R. Then
D(f∥g) =
 ∞
−∞
1
√
2πσ2 e−(x−μ1)2
2σ2
ln
1
√
2πσ2 e−(x−μ1)2
2σ2
1
√
2πσ2 e−(x−μ2)2
2σ2
dx
7Some clariﬁcation is needed, because the integrand could be +∞(though not −∞). More
generally, the integral

A
f(x) ln f(x)
g(x) dx
should be interpreted as follows: If the integrand is +∞on a subset of A of positive Lebesgue
measure, then the integral is deﬁned as being +∞. (This is reasonable, because the contribution
of the other points in A to the integral is lower-bounded by −1; see Exercise 30.14.) Otherwise,
the integral is computed by excluding from A the points where the integrand is inﬁnite.
30.12 Relative Entropy
777
=
 ∞
−∞
1
√
2πσ2 e−(x−μ1)2
2σ2
1
2σ2

(x −μ2)2 −(x −μ1)2
dx
=
1
2σ2
 ∞
−∞
1
√
2πσ2 e−(x−μ1)2
2σ2

2x(μ1 −μ2) + μ2
2 −μ2
1

dx
=
1
2σ2

2μ1(μ1 −μ2) + μ2
2 −μ2
1

.
Thus,
D

N(μ1, σ2)
N(μ2, σ2)

= (μ1 −μ2)2
2σ2
,
σ > 0.
(30.143)
A key property of D(p∥q) is that it is nonnegative, and that it is zero if, and only
if, the tuples p and q are identical. Likewise for densities, D(f∥g) is nonnegative
and is equal to zero if, and only if, the densities f(·) and g(·) diﬀer on a set of
Lebesgue measure zero. We shall only prove nonnegativity:
Theorem 30.12.3 (Relative Entropy Is Nonnegative).
(i) If p and q are as in Deﬁnition 30.12.1, then
D(p∥q) ≥0.
(30.144)
(ii) If f(·) and g(·) are probability densities on Rn, then
D(f∥g) ≥0.
(30.145)
Proof. We shall only prove Part (ii); the proof of the discrete version merely re-
quires replacing the integrals with sums. The key is the inequality
ln ξ ≤ξ −1,
ξ > 0,
(30.146)
which also holds when ξ is zero provided that we interpret ln 0 as −∞. Starting
from (30.142) and using the convention (30.140),
−D(f∥g) = −

{x: f(x)>0}
f(x) ln f(x)
g(x) dx
=

{x: f(x)>0}
f(x) ln g(x)
f(x) dx
≤

{x: f(x)>0}
f(x)
 g(x)
f(x) −1

dx
=

{x: f(x)>0}
g(x) dx −

{x: f(x)>0}
f(x) dx
=

{x: f(x)>0}
g(x) dx −

Rn f(x) dx
≤

Rn g(x) dx −1
= 0,
778
The Radar Problem
where the second line follows from the identity −ln(a/b) = ln(b/a); the third
from (30.146) by substituting g(x)/f(x) for ξ; the fourth from the linearity of
integration; the ﬁfth because extending the domain of integration does not change
the integral if we only add points where the integrand is zero; the sixth because
f(·) is a density (and hence integrates to one) and because g(·) is nonnegative so
extending the domain of integration cannot decrease the integral; and the ﬁnal
equality because, being a density, g(·) integrates to one.
We call the next inequality the “Log-Integral Inequality” because the standard
name for its discrete counterpart is the “Log-Sum Inequality” (Cover and Thomas,
2006, Chapter 2, Section 7, Theorem 2.7.1).
Theorem 30.12.4 (Log-Integral Inequality). Let f(·) and g(·) be densities on Rn,
and let A be any (Lebesgue measurable) subset of Rn. Then,

A
f(x) ln f(x)
g(x) dx ≥
	
A
f(x) dx

ln

A f(x) dx


A g(x) dx
 .
(30.147)
Proof. Deﬁne the constants
a =

A
f(x) dx,
b =

A
g(x) dx.
(30.148)
We shall prove the inequality for the case where both a and b are positive. (If a is
zero, then both sides of the inequality are zero, and if a is positive and b is zero,
then both sides are inﬁnite.) Deﬁne the two densities
˜f(x) = 1
a f(x) I

x ∈A

,
˜g(x) = 1
b g(x) I

x ∈A

.
(30.149)
By the nonnegativity of D(˜f∥˜g) (Theorem 30.12.3),
0 ≤D(˜f∥˜g)
=

A
1
a f(x) ln a−1 f(x)
b−1 g(x) dx
=

A
1
a f(x) ln b
a dx +

A
1
a f(x) ln f(x)
g(x) dx
= ln b
a +

A
1
a f(x) ln f(x)
g(x) dx
= 1
a

A
f(x) ln f(x)
g(x) dx −ln a
b ,
from which the result follows by multiplying both sides of the inequality by a.
We next apply the Log-Integral Inequality to the radar problem. We ﬁrst consider
deterministic decision rules8 and derive two necessary conditions that their error
8Since R = Rrnd (Proposition 30.10.1), we need not consider randomized rules but, for the
beneﬁt of readers who have skipped the proposition’s proof, those will be considered later too.
30.12 Relative Entropy
779
probabilities (pFA, pMD) must satisfy irrespective of the set D that deﬁnes them.
One necessary condition is in terms of D(f0∥f1) and the other in terms of D(f1∥f0).
To derive the ﬁrst necessary condition we apply the Log-Integral Inequality twice.
In both applications we substitute in (30.147) the densities f0(·) for f(·) and f1(·)
for g(·). In the ﬁrst application, we substitute the decision region D for A to obtain

D
f0(y) ln f0(y)
f1(y) dy ≥

D
f0(y) dy

ln

D f0(y) dy


D f1(y) dy

=

1 −pFA

ln 1 −pFA
pMD
,
(30.150)
where the equality follows from the expressions (30.3b) and (30.4b) for pFA and pMD.
In the second application of the Log-Integral Inequality, we substitute Dc for A to
obtain

Dc f0(y) ln f0(y)
f1(y) dy ≥

Dc f0(y) dy

ln

Dc f0(y) dy


Dc f1(y) dy

= pFA ln
pFA
1 −pMD
.
(30.151)
Summing (30.150) and (30.151) yields the ﬁrst necessary condition
D(f0∥f1) ≥

1 −pFA

ln 1 −pFA
pMD
+ pFA ln
pFA
1 −pMD
.
(30.152)
Notice that the RHS of the above has the form of a relative entropy between two
discrete probability vectors. We can thus also express the condition as
D

f0
f1

≥D

(1 −pFA, pFA)
(pMD, 1 −pMD)

.
(30.153)
Before turning to the second necessary condition, we next argue that (30.153)
also holds for randomized guessing rules. As we have seen in Section 20.6, any
randomized guessing rule that guesses “H = 0” with probability b(yobs) can be
viewed as a deterministic rule that bases its decision on an observation consisting of
the pair (yobs, θobs) ∈Rn×[0, 1] and that guesses “H = 0” whenever θobs ≤b(yobs),
where Θ ∼U

[0, 1]

is the outcome of a uniform random-number generator that is
independent of the observation so the corresponding two densities are
f0(y) I{0 ≤θ ≤1},
f1(y) I{0 ≤θ ≤1}.
(30.154)
The relative entropy between these two densities is
 1
0

Rn f0(y) I{0 ≤θ ≤1} ln f0(y) I{0 ≤θ ≤1}
f1(y) I{0 ≤θ ≤1} dy dθ
=
 1
0

Rn f0(y) ln f0(y)
f1(y) dy dθ
=
 1
0
D(f0∥f1) dθ
= D(f0∥f1).
(30.155)
780
The Radar Problem
Applying the result on deterministic rules to the setting where the densities are
those in (30.154) and using (30.155) establishes the bound for randomized rules.
Condition (30.152) was derived from the Log-Integral Inequality by substituting
the densities f0(·) for f(·) and f1(·) for g(·). A diﬀerent condition can be derived
in a similar way by swapping f0(·) and f1(·), i.e., by substituting in (30.147) f1(·)
for f(·) and f0(·) for g(·). If we substitute the decision region D for A we obtain

D
f1(y) ln f1(y)
f0(y) dy ≥

D
f1(y) dy

ln

D f1(y) dy


D f0(y) dy

= pMD ln
pMD
1 −pFA
,
(30.156)
and if we substitute Dc for A we obtain

Dc f1(y) ln f1(y)
f0(y) dy ≥

Dc f1(y) dy

ln

Dc f1(y) dy


Dc f0(y) dy

= (1 −pMD) ln 1 −pMD
pFA
.
(30.157)
Summing (30.156) and (30.157) leads to our second necessary condition
D(f1∥f0) ≥pMD ln
pMD
1 −pFA
+ (1 −pMD) ln 1 −pMD
pFA
= D

(pMD, 1 −pMD)
(1 −pFA, pFA)

.
(30.158)
Like (30.152) this bound too can be extended to randomized rules. To summarize:
Theorem 30.12.5 (Necessary Conditions on (pFA, pMD)). If (pFA, pMD) is achiev-
able by some possibly-randomized decision rule, then

1 −pFA

ln 1 −pFA
pMD
+ pFA ln
pFA
1 −pMD
≤D(f0∥f1),
(30.159)
and
pMD ln
pMD
1 −pFA
+ (1 −pMD) ln 1 −pMD
pFA
≤D(f1∥f0).
(30.160)
As an example of the use of Theorem 30.12.5, we next revisit the Gaussian problem
we discussed in Section 30.6. Using the expression for the relative entropy between
two Gaussians (30.143) we conclude that in this example D(f0∥f1) happens to equal
D(f1∥f0) and both are equal to A2/(2σ2). We thus conclude from the theorem that,
for this problem,

1 −pFA

ln 1 −pFA
pMD
+ pFA ln
pFA
1 −pMD
≤A2
2σ2
(30.161a)
and
pMD ln
pMD
1 −pFA
+ (1 −pMD) ln 1 −pMD
pFA
≤A2
2σ2 .
(30.161b)
30.13 Additional Reading
781
1
1
1
1
pMD
pMD
pFA
pFA
(a)
(b)
Figure 30.6: The achievable pairs for the Gaussian problem of Section 30.6. Lightly
shaded in (a) are the pairs that satisfy (30.161a), and lightly shaded in (b) are those
that satisfy (30.161b).
These necessary conditions are depicted in Figure 30.6. The darkly-shaded area cor-
responds to the achievable pairs. Lightly shaded are the pairs that satisfy (30.161a)
(Figure 30.6 (a)) and those that satisfy (30.161b) (Figure 30.6 (b)). In this very
special case, D(f1∥f0) is equal to D(f0∥f1), so Condition (30.161b) is identical to
the result of swapping pMD and pFA in (30.161a). The lightly-shaded regions in
Figure 30.6 can thus be obtained from each other by reﬂection with respect to the
slope-1 line passing through the origin.
30.13
Additional Reading
We have not considered the radar problem for signals with unknown parameters.
Good starting points for the literature on this are (Poor, 1994) and (Helstrom,
1995). For more on the Knapsack Problem see (Cormen, Leiserson, Rivest, and
Stein, 2009, Chapter 16, Section 2). For various extensions of Liapunov’s Theorem
see (Karlin and Studden, 1966, Chapter VIII, Section 12), and for a simpliﬁed
proof see (Ross, 2005). An accessible account of the role of relative entropy in
Information Theory, Statistics, and Probability Theory can be found in (Cover
and Thomas, 2006).
30.14
Exercises
Exercise 30.1 (A Degenerate Radar Problem). Consider a degenerate radar problem
where the two densities f0(·) and f1(·) are identical.
Find all the achievable pairs
(pFA, pMD) and all the Pareto-optimal pairs.
782
The Radar Problem
Exercise 30.2 (Unimpressive Achievable Pairs). Show that—irrespective of the densi-
ties f0(·) and f1(·)—any pair (pFA, pMD) satisfying pFA + pMD = 1 is achievable by a
randomized guessing rule.
Exercise 30.3 (Testing between Exponentials). Find all the Pareto-optimal pairs for the
one-dimensional radar problem with the exponential densities
f0(y) = e−y I

y ≥0

,
f1(y) = 2e−2y I

y ≥0

.
Exercise 30.4 (A Diﬀerent Optimality Criterion). For given positive numbers α, β > 0,
which guessing rule minimizes α pFA + β pMD?
Exercise 30.5 (An Odd Radar Problem). Consider the one-dimensional radar problem
with the uniform densities
f0(y) = I

0 < y < 1

,
f1(y) = 2 I
 1
2 < y < 1
!
.
(i) Find all achievable pairs (pFA, pMD).
(ii) Find all Pareto-optimal pairs (p∗
FA, p∗
MD).
(iii) Compute p∗
MD(0.75).
(iv) Is the pair

0.75, p∗
MD(0.75)

Pareto optimal?
Illustrate your answers pictorially.
Hint: How do pFA and pMD depend on the set D?
Exercise 30.6 (On the Optimality Criteria). Prove that the RHS of (30.10) does not
imply its LHS by providing an example of a radar problem where for some πFA, say 0.1,
the pair

πFA, p∗
MD(πFA)

is not Pareto optimal.
Hint: Consider a case where f0(·) and f1(·) are both uniform, but on disjoint subsets of R.
Exercise 30.7 (Convex Combinations of Achievable Pairs). Let φ(1)
Guess(·) and φ(2)
Guess(·)
be deterministic guessing rules achieving the pairs (p(1)
FA, p(1)
MD) and (p(2)
FA, p(2)
MD), and let
λ ∈(0, 1) be arbitrary. Find a randomized guessing rule achieving the pair
	
λ p(1)
FA + (1 −λ) p(2)
FA, λ p(1)
MD + (1 −λ) p(2)
MD

.
Repeat when φ(1)
Guess and φ(2)
Guess are randomized guessing rules determined by b(1)(·) and
b(2)(·).
Exercise 30.8 (Two Pairs Minimize A Linear Functional). Suppose that α and β are posi-
tive and that both

p(1)
FA, p(1)
MD

and

p(2)
FA, p(2)
MD

minimize α pFA +β pMD over all achievable
pairs (pFA, pMD). Prove that both pairs are Pareto optimal and that so is every pair on
the line segment connecting them, i.e., every pair of the form
	
λ p(1)
FA + (1 −λ) p(2)
FA, λ p(1)
MD + (1 −λ) p(2)
MD

,
λ ∈(0, 1).
30.14 Exercises
783
Exercise 30.9 (Achievable Regions with Corners). Find a one-dimensional radar problem
with f0(y) = I{0 ≤y ≤1} whose achievable region is the one depicted in Figure 30.3 (b).
Assume that the point marked by a square is (0.375, 0.25) or, more generally, (α, β). Are
there other (correct) solutions?
Exercise 30.10 (The Minimax Criterion). Show that if (p⋆, p⋆) is a Pareto-optimal pair
of equal false-alarm and missed-detection probabilities, then it minimizes max{pFA, pMD}
over all achievable pairs in the sense that for every achievable pair (pFA, pMD)
p⋆= max{p⋆, p⋆} ≤max{pFA, pMD}.
Argue that there exists a likelihood-ratio test that minimizes max{pFA, pMD} over all
achievable pairs and that (p⋆, p⋆) is the intersection of the curve (πFA, p∗
MD(πFA)) with
the unit-slope line through the origin.
Exercise 30.11 (Giving Up). Suppose we allow guessing rules that can guess “H = 0,”
“H = 1”, or “I give up,” where “I give up” counts as an error irrespective of whether or
not the target is present. Characterize the achievable pairs (pFA, pMD).
Exercise 30.12 (The Relative Entropy between Exponentials). Compute the relative
entropy between the mean-μ1 and the mean-μ2 exponential distributions.
Exercise 30.13 (Invariance under Rotation). The set of achievable pairs (pFA, pMD) is
invariant under rotation by 180 degrees around the point (1/2, 1/2) (Proposition 30.10.5).
Is the same true about the pairs that satisfy the necessary conditions of Theorem 30.12.5?
Exercise 30.14 (A Technicality Related to Relative Entropy). Prove that if f(·) and g(·)
are densities on Rn and A ⊂Rn is measurable, then

A
f(x) ln f(x)
g(x) dx ≥−1.
In fact, the above integral is lower-bounded by −1/e.
Exercise 30.15 (Relative R´enyi Entropy Is Nonnegative). Let α ̸= 1 be positive. The
relative R´enyi entropy of order α between two probability vectors p and q (as in Deﬁni-
tion 30.12.1) is
Dα(p∥q) ≜
1
α −1 log

n

j=1
pα
j q1−α
j

.
(30.162)
Between two densities f(·) and g(·) it is
Dα(f∥g) ≜
1
α −1 log

f(x)α g(x)1−α dx

.
(30.163)
Prove that both are always nonnegative.
Hint: Treat the cases 0 < α < 1 and α > 1 separately using H¨older’s Inequality.
Exercise 30.16 (Relative R´enyi Entropy and Relative Entropy). Show that, as α tends
to 1, the relative R´enyi entropy of order α (30.162) converges to the relative entropy:
lim
α→1 Dα(p∥q) = D(p∥q).
(30.164)
784
The Radar Problem
Exercise 30.17 (Log-Integral Inequality for Relative R´enyi Entropy). Let f(·) and g(·) be
densities on Rn, and let α be positive but not equal to 1. Prove that for any (Lebesgue
measurable) A ⊆Rn
1
α −1 log

A
f(x)α g(x)1−α dx

≥
1
α −1 log
	
A
f(x) dx

α	
A
g(x) dx

1−α
.
(30.165)
Hint: Recall Exercise 30.15 and the proof of Theorem 30.12.4.
Exercise 30.18 (Relative R´enyi Entropy and the Radar Problem). Show that if (pFA, pMD)
is achievable then for every positive α ̸= 1,
Dα

f0
f1

≥Dα

(1 −pFA, pFA)
(pMD, 1 −pMD)

(30.166)
and
Dα(f1∥f0) ≥Dα

(pMD, 1 −pMD)
(1 −pFA, pFA)

.
(30.167)
Hint: Recall Exercise 30.17 and mimic the derivations of (30.153) and (30.158).
Exercise 30.19 (Errors and Erasures). Consider the multi-hypothesis testing setting of
Theorem 21.3.1 but where the guessing rule is a mapping φGuess : Rd →M ∪{0} with
the understanding that if φGuess(yobs) is some m ∈M then the guess is “M = m”,
whereas if φGuess(yobs) is zero then the decoder declares “erasure”. Given such a guessing
rule, deﬁne for every m ∈M the set Dm = {y ∈Rd : φGuess(y) = m}, and deﬁne
D0 = {y ∈Rd : φGuess(y) = 0}.
(i) Show that
Pr(erasure) =

m∈M
πm

Rd fY|M=m(y) I

y ∈D0

dy,
and
Pr(error) = 1 −

m∈M
πm

Rd fY|M=m(y) I

y ∈Dm

dy
−

m∈M
πm

Rd fY|M=m(y) I

y ∈D0

dy.
(ii) Fix some λ ≥0, and let “M = m” be the guess produced by the MAP rule
(Section 21.3.4). Show that a rule that minimizes Pr(error) + λ Pr(erasure) is to
guess like the MAP if
πmfY|M=m(y)
πmfY|M=m(y) + 
m′̸=m πm′fY|M=m′(y) ≥1 −λ
and to declare “erasure” otherwise.
Exercise 30.20 (The Radar Problem and Multi-Hypothesis Testing). Consider the multi-
hypothesis testing setup of Section 21.2. Let φGuess be any guessing rule and Dm comprise
the observables that result in the guess “M = m”, i.e., Dm =

y ∈Rd : φGuess(y) = m

.
Let q(·) be any probability density function on Rd.
30.14 Exercises
785
(i) Show that for some ˜m ∈M

D ˜
m
q(y) dy ≤1
M.
(ii) Let Pr(error|M = ˜m) be the error probability associated with φGuess and ˜m. Con-
sider now the radar problem with f0(·) being q(·) and with f1(·) being fY|M= ˜
m.
Show that, for this radar problem, the guessing rule that guesses “H = 1” whenever
y ∈D ˜
m and guesses “H = 0” otherwise has
pFA =

D ˜
m
q(y) dy,
pMD = Pr(error|M = ˜m).
(iii) Conclude that
Pr(error|M = ˜m) ≥p∗
MD
 1
M

,
where p∗
MD is calculated for the above radar problem.
Chapter 31
A Glimpse at Discrete-Time Signal
Processing
Once we have reduced the detection problem from one where the observable is a
continuous-time SP to one where it is a random vector, we can harness the power of
digital signal processing to form our guess. This approach is particularly suitable
for intersymbol interference channels—“ISI channels”—which we shall encounter in
Chapter 32, and where the required digital signal processing is quite sophisticated.
The more basic and more general elements of discrete-time signal processing are
treated here. The main concepts are similar to their continuous-time counterparts,
so the reading should be fairly easy.
31.1
Discrete-Time Filters
In Chapter 5 we discussed the convolution between waveforms and the result of ﬁl-
tering them. Here we discuss the analogous results for bi-inﬁnite sequences. In view
of the notation we adopted for discrete-time stochastic processes (Section 12.2), we
denote the bi-inﬁnite sequence . . . , a−2, a−1, a0, a1, a2, . . . by

aν, ν ∈Z

or simply
by (aν). But sometimes we use ν 	→aν because a bi-inﬁnite sequence is a mapping
from the integers to the real or complex ﬁeld. For this reason we shall also some-
times denote the sequence by a. In that case the sequence ~a maps ν to a−ν and is
thus the “mirror image” of a.
We say that a sequence (aν) is absolutely summable if
∞

ν=−∞
|aν| < ∞.
Here |·| denotes the absolute value for real sequences and the modulus for complex
sequences. The class of all absolutely summable sequences is denoted ℓ1. This
notation does not make it explicit whether the sequences are real or complex. A
sequence (aν) is square summable if
∞

ν=−∞
|aν|2 < ∞,
786
31.1 Discrete-Time Filters
787
and the class of square-summable sequences is denoted ℓ2. Note that
ℓ1 ⊂ℓ2,
(31.1)
because if a sequence is in ℓ1 then all but a ﬁnite number of its elements are of
modulus smaller than one, and if a modulus is smaller than one, then it cannot
exceed its square:
|ξ|2 ≤|ξ|,

|ξ| < 1

.
The inclusion in (31.1) is strict: there are sequences, such as ν 	→1/(|ν|+1), which
are in ℓ2 but not in ℓ1.
The convolution of the bi-inﬁnite sequences (aν) and (bν) is a bi-inﬁnite sequence
that we denote (aν) ⋆(bν) or a ⋆b. Its η-th symbol is denoted

(aν) ⋆(bν)

η
or
(a ⋆b)η
and is formally given by
(a ⋆b)η =
∞

ν=−∞
aν bη−ν.
(31.2)
The convolution of two square-summable sequences (aν), (bν) ∈ℓ2 is deﬁned at
every η ∈Z and is a bounded sequence. Indeed, the Cauchy-Schwarz Inequality
for sequences (Exercise 3.12) shows that for sequences (aν), (bν) ∈ℓ2 and any η ∈Z
the sequence ν 	→aν bη−ν is absolutely summable and its sum, which is (a ⋆b)η,
can be bounded by
(a ⋆b)η
 ≤
	 
ν∈Z
|aν|2

1/2	 
ν∈Z
|bν|2

1/2
,
η ∈Z.
(31.3)
Since ℓ1 ⊂ℓ2, the convolution of two absolutely summable sequences is also always
deﬁned. In fact, as we next show, the result is absolutely summable with

η∈Z
(a ⋆b)η
 ≤
	 
ν∈Z
|aν|

	 
ν∈Z
|bν|

.
(31.4)
This can be proved using Fubini’s Theorem, because

η∈Z
(a ⋆b)η
 =

η∈Z


ν∈Z
aν bη−ν

≤

η∈Z

ν∈Z
|aν| |bη−ν|
=

ν∈Z
|aν|

η∈Z
|bη−ν|
=
	 
ν∈Z
|aν|

	 
η′∈Z
|bη′|

,
where in the last equality we deﬁned η′ ≜η −ν.
788
A Glimpse at Discrete-Time Signal Processing
All the properties listed in Theorem 5.6.1 hold whenever the sequences involved
are in ℓ1. To those we add that if (eν) is the sequence deﬁned by
eν ≜I{ν = 0},
ν ∈Z,
(31.5)
then
(aν) ⋆(eν) = (aν).
(31.6)
In analogy to Deﬁnition 11.2.1, we deﬁne the self-similarity sequence as follows:
Deﬁnition 31.1.1 (Self-Similarity Sequence). The self-similarity sequence Raa(·)
of a (possibly-complex) square-summable sequence a is deﬁned as
Raa(η) ≜

ν∈Z
aν+η a∗
ν,
η ∈Z.
(31.7)
A
discrete-time ﬁlter of impulse response (hν) is a device that when fed
the input sequence (aν) produces the output sequence (aν) ⋆(hν). A discrete-time
ﬁlter is said to be stable if its impulse response is absolutely summable. It is said
to be causal if hν is zero whenever ν < 0.
The Discrete-Time Fourier Transform (DTFT) of an absolutely summable
sequence a ∈ℓ1 is discussed in Appendix B. It is denoted ua and is the mapping
from R to C
ua: θ 	→

ν∈Z
aν e−i2πνθ.
The frequency response of a stable discrete-time ﬁlter of impulse response (hν)
is the Discrete-Time Fourier Transform of its impulse response: it is the mapping
from R to C
uh: θ 	→

ν∈Z
hν e−i2πνθ.
(31.8)
This function is continuous and periodic with its value at θ + 1 being equal to its
value at θ. Consequently, it is often restricted to the interval I, where, as in (A.1),
I ≜
'
θ ∈R : −1
2 ≤θ < 1
2
(
.
(31.9)
From its restriction to I we can, of course, recover its value everywhere via the
periodic extension: The periodic extension SP of a function S: I →C, is a
function that agrees with S on I and that satisﬁes SP(θ + 1) = SP(θ) for every
θ ∈R. Thus,
SP(ν + θ) = S(θ),

θ ∈I, ν ∈Z

.
(31.10)
We also deﬁne
¯I ≜
'
θ ∈R : −1
2 ≤θ ≤1
2
(
.
(31.11)
A stable discrete-time ﬁlter of impulse response (hν) is said to have a stable
inverse if there exists a sequence (aν) ∈ℓ1 such that (hν) ⋆(aν) =

eν

, i.e.,

(hν) ⋆(aν)

η = I{η = 0},
η ∈Z.
The following theorem on stable inverses is due to Norbert Wiener (1894–1964):
31.2 Processing Discrete-Time Stochastic Processes
789
Theorem 31.1.2 (Existence of a Stable Inverse). If the frequency response of a
stable discrete-time ﬁlter is never zero, then the ﬁlter has a stable inverse.
Proof. See (Rudin, 1987, Chapter 18, Theorem 18.21).
31.2
Processing Discrete-Time Stochastic Processes
We next discuss linear functionals of discrete-time stochastic processes as well as the
result of ﬁltering such processes. We treat the real and complex cases separately,
with the latter addressed in Section 31.4.
31.2.1
Linear Functionals of Discrete-Time SPs
If an absolutely summable sequence is multiplied term-by-term by a bounded se-
quence, then the result is also absolutely summable. But what if the latter is only
bounded in some probabilistic sense? The next proposition shows that this is often
enough. In this proposition we require that the SP (Xν) have a bounded second
moment, i.e., that there exist some γ > 0 that upper-bounds E[X2
ν] for every
ν ∈Z:
E

X2
ν

≤γ,
ν ∈Z.
(31.12)
Note 31.2.1. Every WSS SP is also of bounded second moment.
Proof. If

Xν

is WSS, then
E

X2
ν

=

E[Xν]
2 + Var[Xν]
=

E[X0]
2 + Var[X0] ,
and (31.12) thus holds (with equality) with
γ =

E[X0]
2 + Var[X0] .
The following proposition is the discrete-time counterpart of Proposition 25.10.1.
Proposition 31.2.2 (Linear Functional of a Discrete-Time SP). Let (Xν) be a SP
of bounded second moment, and let (aν) be an absolutely summable real sequence.
(i) The set
N =
1
ω ∈Ω :

ν∈Z
aν Xν(ω)
 = ∞
2
is an event of probability zero.
(ii) The mapping from Ω to R
Y (ω) =
⎧
⎨
⎩

ν∈Z
aν Xν(ω)
if ω /∈N,
0
otherwise
(31.13)
790
A Glimpse at Discrete-Time Signal Processing
is a random variable of mean
E[Y ] =
∞

ν=−∞
aν E[Xν]
(31.14)
and of second moment
E

Y 2
=

ν′∈Z

ν′′∈Z
aν′ aν′′ E[Xν′Xν′′] .
(31.15)
(iii) If

Xν

is WSS and of autocovariance function KXX(·), then the mean and
variance of the RV Y above are given by
E
 
ν∈Z
aν Xν

= E[X0]

ν∈Z
aν,
(31.16)
Var
 
ν∈Z
aν Xν

=

ν′∈Z

ν′′∈Z
aν′ aν′′KXX(ν′ −ν′′)
(31.17)
=

η∈Z
KXX(η) Raa(η),
(31.18)
where Raa is the self-similarity sequence of (aν) (Deﬁnition 31.1.1).
(iv) If

Xν

is WSS and of PSD SXX(·) (Deﬁnition 13.6.1), then the variance
can be expressed as
Var
 
ν∈Z
aνXν

=

1
2
−1
2
SXX(θ)

∞

ν=−∞
aν e−i2πθν

2
dθ
(31.19)
=

1
2
−1
2
SXX(θ)
ua(θ)
2 dθ.
(31.20)
Proof. The proof is almost identical to the proof of Proposition 25.10.1 and mostly
requires replacing integrals with sums. It is thus omitted.
We will need the following corollary when we discuss the time-η value

(Xν)⋆(hν)

η
of the convolution (Xν) ⋆(hν). It shows that we can ﬁnd an event of probability
zero outside of which the convolution sum converges at all (integer) epochs: “one
null event ﬁts all.” It has no continuous-time counterpart because it relies heavily
on the fact that, when time is discrete, the set of epochs is countable.
Corollary 31.2.3. If

Xν

is of bounded second moment, and if h ∈ℓ1, then

η∈Z
1
ω ∈Ω :

ν∈Z
Xν(ω) hη−ν
 = ∞
2
(31.21)
is an event of probability zero.
31.2 Processing Discrete-Time Stochastic Processes
791
Proof. For any η ∈Z we can substitute ν 	→hη−ν for

aν

in Proposition 31.2.2
to obtain from Part (i) that the event
1
ω ∈Ω :

ν∈Z
Xν(ω) hη−ν
 = ∞
2
has probability zero. And since the union of a countable collection of events having
probability zero is also of probability zero (Corollary 21.5.2 (i)), the event in (31.21)
must also have probability zero.
31.2.2
Filtering Discrete-Time SPs
With the aid of Corollary 31.2.3, we can now deﬁne the convolution between a WSS
SP and a deterministic absolutely summable sequence, i.e., the response of a stable
ﬁlter to a WSS input. This is the discrete-time counterpart to Deﬁnition 25.13.1.
Deﬁnition 31.2.4 (Filtering a Discrete-Time SP). The convolution (Xν) ⋆(hν)
between a discrete-time SP of bounded second moment (Xν) and an absolutely
summable sequence (hν) is the SP (Yν) that is deﬁned as
Yη(ω) =
⎧
⎨
⎩

ν∈Z
Xν(ω) hη−ν
if ω /∈N,
0
otherwise,
(31.22a)
where
N =

η∈Z
1
ω ∈Ω :

ν∈Z
Xν(ω) hη−ν
 = ∞
2
.
(31.22b)
As in Deﬁnition 25.13.1, the deﬁnition of the convolution for ω in N is arbitrary
and was chosen for concreteness. Ignoring the minor technical issue of how the
convolution is deﬁned when ω ∈N, the convolution is “almost linear” in the sense
of Lemma 25.10.3. This minor technicality will be ignored in the future.
As we next argue, the convolution is “almost” associative. By this we mean that for
ω’s outside an event whose probability is zero, the following two calculations yield
identical sequences. In the ﬁrst calculation we convolve the sequence ν 	→Xν(ω)
with (hν) and then convolve the result with the sequence (gν).
In the second
we convolve ν 	→Xν(ω) with the result of convolving (hν) with (gν), i.e., with
(hν) ⋆(gν).
Proposition 31.2.5 (The Convolution Is “almost” Associative). Let

Xν

be of
bounded second moment, and let the deterministic sequences h and g be absolutely
summable. Then there exists an event N of probability zero such that for every
ω /∈N

ν 	→Xν(ω)

⋆h

⋆g =

ν 	→Xν(ω)

⋆(h ⋆g).
(31.23)
Proof. Since h and g are absolutely summable, so are the sequences ν 	→|hν| and
ν 	→|gν|, which we denote |h| and |g|. Consequently, their convolution is also
absolutely summable, i.e.,
|h| ⋆|g| ∈ℓ1,
(31.24)
792
A Glimpse at Discrete-Time Signal Processing
(because the convolution of absolutely summable sequences is absolutely summable;
see (31.4)). It therefore follows from Corollary 31.2.3 (upon substituting |h| ⋆|g|
for h there) that the event
N =

η∈Z
1
ω ∈Ω :

ν∈Z
Xν(ω)



|h| ⋆|g|

η−ν
 = ∞
2
(31.25)
is of probability zero.1 For ω /∈N we have

ν∈Z
Xν(ω)
 
|h| ⋆|g|

η−ν < ∞,
η ∈Z,
i.e.,

ν∈Z
Xν(ω)
 
m∈Z
hm
gη−ν−m
 < ∞,

η ∈Z, ω /∈N

,
or

ν∈Z

m∈Z
Xν(ω) hm gη−ν−m
 < ∞,

η ∈Z, ω /∈N

.
Replacing the summation variable m with k, where k = m + ν, we obtain

ν∈Z

k∈Z
Xν(ω) hk−ν gη−k
 < ∞,

η ∈Z, ω /∈N

.
(31.26)
This and Fubini’s Theorem justify swapping the sums in the following calculation:
	
ν 	→Xν(ω)

⋆h

⋆g

η
=

k∈Z

ν 	→Xν(ω)

⋆h

k gη−k
=

k∈Z
	
ν∈Z
Xν(ω) hk−ν

gη−k
=

ν∈Z

k∈Z
Xν(ω) hk−ν gη−k
=

ν∈Z
Xν(ω)

k∈Z
hk−ν gη−k
=

ν∈Z
Xν(ω)

m∈Z
hm gη−ν−m
=

ν∈Z
Xν(ω)

h ⋆g

η−ν
=

ν 	→Xν(ω)

⋆(h ⋆g)

η,

η ∈Z, ω /∈N

,
where we have changed the summation argument by deﬁning m ≜k −ν.
The following corollary is very important. It says that passing a SP through a
stable ﬁlter that has a stable inverse is essentially a lossless operation.
1The convolution of nonnegative sequences is nonnegative, so taking the absolute value of
|h| ⋆|g|

η−ν in (31.25) is superﬂuous.
31.2 Processing Discrete-Time Stochastic Processes
793
Corollary 31.2.6. If (Xν) is of bounded second moment and (hν) is the impulse
response of a stable ﬁlter that has a stable inverse, then (Xν) can be recovered from
(Xν) ⋆(hν) with probability one.
Proof. Apply Proposition 31.2.5 with (gν) being the impulse response of the inverse
ﬁlter, i.e., with (gν) chosen so (hν) ⋆(gν) = (eν).
31.2.3
Discrete-Time Gaussian SPs
Discrete-time Gaussian stochastic processes are deﬁned like their continuous-time
counterparts (Deﬁnition 25.3.1):
Deﬁnition 31.2.7 (Discrete-Time Gaussian SP). A discrete-time SP

Xν

is said
to be Gaussian if for every positive integer m and every choice of the integers
ν1, . . . , νm, the random vector

Xν1, . . . , Xνm
T
is a Gaussian vector.
In analogy to Proposition 25.11.1 and Theorem 25.12.1 we have:
Theorem 31.2.8 (Linear Functionals of a Discrete-Time Gaussian SP). Let

Xν

be a Gaussian SP of bounded second moment.
(i) If (aν) is an absolutely summable real sequence, then

ν∈Z
aν Xν
is a Gaussian RV.
(ii) If the m bi-inﬁnite real sequences

a1,ν

, . . . ,

am,ν

are absolutely summable,
then the m random variables

ν∈Z
a1,ν Xν, . . . ,

ν∈Z
am,ν Xν
are jointly Gaussian.
Proof. To prove Part (i) we could mimic the proof of Proposition 25.11.1, but
there is a simpler proof: For any n ∈N deﬁne
Sn ≜
n

ν=−n
aν Xν.
Since

Xν

is a Gaussian SP, the vector (X−n, . . . , Xn)T is Gaussian (Deﬁni-
tion 31.2.7), and consequently Sn—being a linear functional of a Gaussian vector—
is a Gaussian RV (Theorem 23.6.17). Since, by Proposition 31.2.2, Sn converges
(almost surely) to

ν∈Z
aν Xν,
794
A Glimpse at Discrete-Time Signal Processing
the latter must also be Gaussian (Theorem 19.9.1).
Part (ii) can be proved from Part (i) in much the same way that Theorem 25.12.1
was proved using Proposition 25.11.1. Alternatively it can be proved as we proved
Part (i) but using Proposition 23.6.3 (instead of Theorem 23.6.17) and Theo-
rem 23.9.1 (instead of Theorem 19.9.1).
Theorem 31.2.9 (Filtering a Discrete-Time SP). Let (Xν) be a centered WSS
discrete-time SP of autocovariance function KXX, and let (hν) be an absolutely
summable real bi-inﬁnite sequence. Let

Yν

=

Xν

⋆

hν

.
(31.27)
(i) The discrete-time SP (Yν) is a centered WSS SP whose autocovariance func-
tion KYY is
KYY = KXX ⋆Rhh,
(31.28)
where Rhh is the self-similarity sequence of

hν

.
(ii) If (Xν) is of PSD SXX, then (Yν) is of PSD
SYY (θ) = SXX(θ)

∞

ν=−∞
hν e−i2πνθ

2
(31.29)
= SXX(θ)
uh(θ)
2,
θ ∈R.
(31.30)
(iii) If (Xν) is a Gaussian SP, then so is (Yν).
Proof. The proof is similar to the proof of Theorem 25.13.2 with all integrals
replaced by sums. It is thus omitted.
31.3
Discrete-Time Whitening Filters
A whitening ﬁlter is lossless in the sense of Corollary 31.2.6 and allows us to trans-
form a Gaussian SP with memory to one that is memoryless (IID). For this to
hold, the magnitude of the ﬁlter’s frequency response must be inversely propor-
tional to the square root of the SP’s PSD; the phase is immaterial. Consequently,
the whitening ﬁlter, when it exists, is not unique.
Deﬁnition 31.3.1 (Discrete-Time Whitening Filter). A discrete-time whiten-
ing ﬁlter for a WSS SP (Xν) of PSD SXX is a stable ﬁlter whose frequency re-
sponse uh satisﬁes
uh(θ)
 = SXX(θ)−1/2
(31.31)
(outside a set of θ’s of Lebesgue measure zero) and that has a stable inverse.
31.3 Discrete-Time Whitening Filters
795
The existence of a whitening ﬁlter is a technical issue. It is natural to assume that
the PSD of the SP be positive so that the square root of its reciprocal be ﬁnite.
But we typically require a bit more, because we need to guarantee that this square
root correspond to the magnitude of some stable ﬁlter.
The next theorem provides suﬃcient conditions for the existence of a whitening
ﬁlter. In its statement we shall require that some function be absolutely continuous
on ¯I.2 Most readers have probably not encountered this notion before, and it is
inessential here. Interested readers can read up on this topic, for example, in (Roy-
den and Fitzpatrick, 2010, Section 6.4). Here we merely note that every function
that is continuously diﬀerentiable on [a, b] is also absolutely continuous on [a, b].
And to deal with functions that are continuous but only piecewise continuously
diﬀerentiable, we note that if a < b < c and if g(·) is absolutely continuous on both
[a, b] and [b, c], then g(·) is absolutely continuous on [a, c]. This extends to any
ﬁnite number of intervals [a, b1], [b1, b2], . . . , [bn, c].
Finally, we note that if g(·) is absolutely continuous on [a, b] then it is continuous
on [a, b] and that, excluding a set of Lebesgue measure zero, it is diﬀerentiable at
every point in [a, b]. Moreover, irrespective of how we deﬁne its derivative g′ in the
exception set
g(ξ) = g(a) +
 ξ
a
g′(α) dα,
ξ ∈[a, b].
Theorem 31.3.2 (Existence of a Discrete-Time Whitening Filter). Suppose the
mapping S: I →R is positive
S(θ) > 0,
θ ∈I;
its periodic extension is absolutely continuous on ¯I; and its derivative S′(·) satisﬁes

1
2
−1
2

S′(θ)
2 dθ < ∞.
Then there exists a stable discrete-time ﬁlter with a stable inverse whose impulse
response

hν

satisﬁes

∞

ν=−∞
hν e−i2πνθ
 =
1

S(θ)
,
θ ∈I.
If S is symmetric in the sense that S(−θ) = S(θ) whenever |θ| < 1/2, then such a
ﬁlter exists that is additionally real.
2A real-valued function g(·) is said to be absolutely continuous on the interval [a, b] if it is
deﬁned on [a, b], and for every ϵ > 0 there exists some δ > 0 such that

n

i=1
(bi −ai) < δ

=⇒

n

i=1
g(bi) −g(ai)
 < ϵ

,
for every ﬁnite number of pairwise-disjoint subintervals (a1, b1), . . . , (an, bn) of [a, b].
796
A Glimpse at Discrete-Time Signal Processing
Proof. Before presenting the details, we give an overview of the proof. Our ap-
proach is to deﬁne hν to be the (−ν)-th Fourier Series Coeﬃcient of the mapping
θ 	→
1

S(θ)
,
(31.32)
so
hν =

1
2
−1
2
1

S(θ)
ei2πνθ dθ,
ν ∈Z.
(31.33)
(When S is symmetric the ﬁlter is thus real.) From this it will then be merely a
technicality to show that this ﬁlter has the desired discrete-time frequency response,
i.e., that
∞

ν=−∞
hν e−i2πνθ =
1

S(θ)
,
θ ∈I.
(31.34)
(Some technical assumptions are needed for (31.33) to imply (31.34).)
The other technical issue will be to show that the ﬁlter is stable, i.e., that
∞

ν=−∞
|hν| < ∞.
(31.35)
The existence of a stable inverse will then follow from (31.34) using Theorem 31.1.2,
because the RHS of (31.34) is never zero.
To address some of the above technicalities, we begin by showing that the theorem’s
hypotheses guarantee that the mapping (31.32) is absolutely continuous on ¯I. Since
S(θ) is positive and continuous on ¯I, we can ﬁnd constants smin and smax such that
0 < smin ≤S(θ) ≤smax,
θ ∈¯I.
(31.36)
Since smin is positive, the function ξ 	→ξ−1
2 is continuously diﬀerentiable on
[smin, smax]. And S(·) is absolutely continuous on ¯I by assumption. Consequently,
since the composition of a Lipschitz function with an absolutely continuous function
is absolutely continuous (Royden and Fitzpatrick, 2010, Section 6.4, Exercise 44),
(Bogachev, 2007, Section 5.3, Lemma 5.3.2), the mapping (31.32) is absolutely
continuous on ¯I.
Having established that the mapping (31.32) is absolutely continuous on ¯I, we next
show that its derivative is square integrable. By the chain rule for diﬀerentiation
(Rudin, 1976, Chapter 5, Theorem 5.5), the derivative of the mapping (31.32) is
−1
2 S(θ)−3
2 dS(θ)
dθ
(31.37)
at every θ at which S(·) is diﬀerentiable. Since, by assumption, S(·) is absolutely
continuous, it is diﬀerentiable for all θ ∈¯I outside a set of Lebesgue measure zero,
and the derivative of the mapping (31.32) is thus given by (31.37) for all θ ∈¯I
outside a set of Lebesgue measure zero.
31.4 Processing Discrete-Time Complex Processes
797
Using (31.36) we thus conclude that, outside a subset of ¯I of Lebesgue measure
zero,

d
dθ
1

S(θ)
 ≤1
2 s
−3
2
min

dS(θ)
dθ
.
This, combined with the theorem’s hypothesis that the derivative of S(·) is square
integrable over ¯I implies that the derivative of the mapping (31.32) is also square
integrable.
We are now ready to tackle the technicalities. Since the mapping (31.32) is abso-
lutely continuous and its derivative square integrable, its Fourier Series Coeﬃcients
are absolutely summable (Katznelson, 2004, the theorem in Chapter I, Section 6.2)
and (31.35) is thus established.
As to (31.34), it simply follows from the absolute continuity of the mapping (31.32)
(Katznelson, 2004, the corollary in Chapter II, Section 2.2).
The existence of a stable inverse follows from Theorem 31.1.2.
31.4
Processing Discrete-Time Complex Processes
Linear functionals of complex stochastic processes are very similar to their real
counterparts. But proper complex stochastic processes have no real counterparts
and thus require some attention. The same applies to the ﬁltering of such complex
stochastic processes.
As in the real case, we shall need an assumption on the second moment of the
CSP. We say that a CSP (Zν) is of bounded second moment if there exists
some γ > 0 that upper-bounds E

|Zν|2
for every ν ∈Z:
E

|Zν|2
≤γ,
ν ∈Z.
(31.38)
Every WSS CSP satisﬁes this condition because if (Zν) is WSS then
E

|Zν|2
=
E[Zν]
2 + Var[Zν]
=
E[Z0]
2 + Var[Z0] ,
and (31.38) thus holds with
γ =
E[Z0]
2 + Var[Z0] .
Note also that (31.38) implies
E

(Re(Zν))2
, E

(Im(Zν))2
≤γ,
(31.39)
so the real and imaginary parts of

Zν

satisfy our underlying assumption for real
stochastic processes (31.12).
Finally note that, by the Cauchy-Schwarz Inequality for complex random variables
(Exercise 17.9), Inequality (31.38) implies
E[ZνZ∗
ν′]
 ≤γ,
ν, ν′ ∈Z.
(31.40)
798
A Glimpse at Discrete-Time Signal Processing
The deﬁnition of a Gaussian SP requires little change. It builds on the notion of a
complex Gaussian vector (Deﬁnition 24.3.6):
Deﬁnition 31.4.1 (Discrete-Time Gaussian CSP). A discrete-time CSP (Zν) is
said to be Gaussian if for every positive integer m and every choice of the integers
ν1, . . . , νm, the random vector

Zν1, . . . , Zνm
T
is a complex Gaussian vector.
Also recall the deﬁnitions of a wide-sense stationary (Deﬁnition 17.5.3) and of a
proper (Deﬁnition 17.5.4) discrete-time CSP.
The following theorem is the complex counterpart of Proposition 31.2.2 and The-
orem 31.2.8. Note the additional part pertaining to proper CSPs.
Theorem 31.4.2 (Linear Functional of a CSP). Let (aν) be an absolutely summable
complex sequence, and let the CSP (Zν) be of bounded second moment.
(i) The set
N =
1
ω ∈Ω :

ν∈Z
aν Zν(ω)
 = ∞
!
is an event of probability zero.
(ii) The mapping
W(ω) =
⎧
⎨
⎩

ν∈Z
aν Zν(ω)
if ω /∈N,
0
otherwise
(31.41)
is a complex random variable of mean
E[W] =
∞

ν=−∞
aν E[Zν]
(31.42)
and of second moment
E

|W|2
=

ν′∈Z

ν′′∈Z
aν′ a∗
ν′′ E

Zν′Z∗
ν′′

.
(31.43)
(iii) The partial sums converge to W in the sense that
lim
n→∞E
5 W −
n

ν=−n
aν Zν

26
= 0.
(iv) If

Zν

is a proper CSP, then W is a proper CRV.
(v) If

Zν

is a Gaussian CSP, then W is a Gaussian CRV.
31.4 Processing Discrete-Time Complex Processes
799
(vi) If

Zν

is WSS and of autocovariance function KZZ(·), then the mean and
variance of the CRV W above are given by
E
 
ν∈Z
aνZν

= E[Z0]

ν∈Z
aν,
(31.44)
Var
 
ν∈Z
aνZν

=

ν′∈Z

ν′′∈Z
aν′ a∗
ν′′ KZZ(ν′ −ν′′)
(31.45)
=

η∈Z
KZZ(η) Raa(η),
(31.46)
where Raa is the self-similarity sequence of (aν) (Deﬁnition 31.1.1).
(vii) If

Zν

is WSS and of PSD SZZ(·), then the variance of W is also given by
Var[W] =

1
2
−1
2
SZZ(θ)

∞

ν=−∞
aν e−i2πθν

2
dθ
=

1
2
−1
2
SZZ(θ) |ua(θ)|2 dθ.
(31.47)
Proof. The proof is almost identical to the proof of Proposition 31.2.2. New are
Parts (iii), (iv), and (v), so we focus on those. Let γ > 0 be such that (31.38) and
hence also (31.39) hold. To prove (iii) we note that for ω /∈N we have by (31.41)
W −
n

ν=−n
aν Zν =

|ν|>n
aν Zν
=

ν∈Z
bν Zν,
where
bν =

0
if |ν| ≤n,
aν
otherwise,
ν ∈Z.
Substituting the sequence

bν

for

aν

in Part (ii) we obtain
E
5W −
n

ν=−n
aν Zν

26
=

|ν′|>n

|ν′′|>n
aν′ a∗
ν′′ E[Zν′Z∗
ν′′]
≤

|ν′|>n

|ν′′|>n
|aν′ aν′′|
E[Zν′Z∗
ν′′]

≤

|ν′|>n

|ν′′|>n
|aν′ aν′′| γ
= γ

|ν′|>n
|aν′|

|ν′′|>n
|aν′′|,
(31.48)
where the second inequality follows from (31.40). The RHS of (31.48) tends to
zero, because (aν) was assumed to be absolutely summable. Consequently, since
its LHS is nonnegative, it too must tend to zero as n tends to inﬁnity.
800
A Glimpse at Discrete-Time Signal Processing
Part (iv) can be proved as follows: Since (Zν) is proper, the vector

Z−n, . . . , Z−1, Z0, Z1, . . . , Zn
T
(31.49)
is a proper complex random vector (Deﬁnition 17.5.4). Consequently, if we de-
ﬁne Wn as the partial sum
Wn =
n

ν=−n
aν Zν,
then, being a linear functional of the above proper vector, Wn must be a proper
CRV (Proposition 17.4.2). This combines with the fact that E

|W −Wn|2
→0
(Part (iii)) to imply that W must also be proper (Proposition 17.6.1).
As to (v), we note that since (Zν) is a Gaussian CSP, the real SP
. . . , Re(Z−1), Im(Z−1), Re(Z0), Im(Z0), Re(Z1), Im(Z1), . . .
(31.50)
must be Gaussian. By (31.39), it is of bounded second moment. And since Re(Z)
and Im(Z) are linear functionals of this Gaussian SP, they must be jointly Gaussian
by Theorem 31.2.8 (ii).
We can deﬁne the ﬁltering of a discrete-time CSP like we did in the real case in
Deﬁnition 31.2.4. In analogy to Theorem 31.2.9 we have:
Theorem 31.4.3 (Filtering a Discrete-Time CSP). Let (Zν) be a centered WSS
discrete-time CSP of autocovariance function KZZ, and let (hν) be an absolutely
summable complex bi-inﬁnite sequence. Let

Yν

=

Zν

⋆

hν

.
(31.51)
(i) The discrete-time CSP (Yν) is a centered WSS CSP whose autocovariance
function KYY is
KYY = KZZ ⋆Rhh,
(31.52)
where Rhh is the self-similarity sequence of

hν

.
(ii) If (Zν) is of PSD SZZ, then (Yν) is of PSD
SYY (θ) = SZZ(θ)

∞

ν=−∞
hν e−i2πνθ

2
(31.53)
= SZZ(θ)
uh(θ)
2,
θ ∈R.
(31.54)
(iii) If (Zν) is a Gaussian CSP, then so is (Yν).
(iv) If (Zν) is a proper CSP, then so is (Yν).
31.5 Additional Reading
801
Proof. Parts (i), (ii), and (iii) are similar to those in Theorem 31.2.9, so we shall
only prove Part (iv). Recalling Deﬁnition 17.5.4, we set out to prove that E[Yν Yν′]
is zero for all integers ν and ν′. Starting from the deﬁnition of the convolution,
E

Yν Yν′
= E
 
m∈Z
Zm hν−m

η∈Z
Zη hν′−η

=

m∈Z

η∈Z
hν−m hν′−η E

Zm Zη

= 0,
ν, ν′ ∈Z,
where the last equality follows from the assumption that (Zν) is proper, which
implies that E[Zm Zη] is zero for all integers m and η.
31.5
Additional Reading
Digital Signal Processing is a huge ﬁeld, and we have only presented the very
basics that we shall need in Chapter 32 to study the Linearly-Dispersive Channel.
Interested readers can consult (Porat, 2008) and (Pourahmadi, 2001) for much
more. Our treatment of the whitening ﬁlter is a bit diﬀerent from the standard
one because we require that the ﬁlter be stable, whereas other authors often only
require that its impulse response be square summable.
31.6
Exercises
Exercise 31.1 (Time- and Frequency-Limited Sequences). Prove the discrete-time coun-
terpart of Theorem 6.8.2: If a sequence (aν) is such that aν is zero whenever |ν| > η,
where η is some positive integer, and if its DTFT ua(θ) is zero whenever ξ ≤|θ| < 1/2,
where ξ ∈[0, 1/2), then the sequence must be the all-zero sequence.
Hint: Recall the proof of Theorem 15.4.1.
Exercise 31.2 (On the Convolution of Causal Sequences). Suppose that (aν), (bν) ∈ℓ1
are both causal and their convolution is (eν) of (31.5). Show that a0 cannot be zero.
Exercise 31.3 (Uniqueness of the Inverse Filter). Show that if a stable ﬁlter has a stable
inverse then this stable inverse is unique.
Exercise 31.4 (Verifying the Inverse Filter). Consider the discrete-time ﬁlter whose im-
pulse response (hν) is given for every ν ∈Z by hν = αν I{ν ≥0}, where α is some real
number.
(i) Under what conditions on α is this ﬁlter stable?
(ii) Show that, for such values of α, the sequence (bν) with b0 = 1, with b1 = −α, and
with bν being zero for all other integers ν is the inverse of (hν).
802
A Glimpse at Discrete-Time Signal Processing
Exercise 31.5 (The Inverse of an FIR Filter). A ﬁnite impulse response (FIR) ﬁlter
is one whose impulse response (hν) is such that there exists some η ∈N for which hν is
zero whenever |ν| > η. Show that if (hν) is the impulse response of an FIR ﬁlter and hν
is nonzero for at least two values of ν, then the ﬁlter cannot have an inverse that is FIR.
Exercise 31.6 (Is the Convolution always Associative?). Consider the bi-inﬁnite sequences
aν =
⎧
⎪
⎨
⎪
⎩
1
if ν = 0,
−1
if ν = 1,
0
otherwise,
bν = I{ν ≥0},
cν = −I{ν ≤−1},
ν ∈Z.
Compare (b ⋆a) ⋆c with b ⋆(a ⋆c) and reﬂect on Proposition 31.2.5.
Exercise 31.7 (Reﬂecting a Whitening Filter). Show that if (hν) is the impulse response
of a whitening ﬁlter for (Xν) then so is ν →h∗
−ν.
Exercise 31.8 (Gauss-Markov). Let (Uν) be IID N

0, σ2
u

, where σu > 0. Let α ∈(0, 1)
be a constant, and deﬁne the SP (Xν) as
Xν =
∞

η=0
αη Uν−η.
(31.55)
Show that (Xν) is a WSS Gaussian SP satisfying the recursion
Xν+1 = α Xν + Uν+1,
ν ∈Z.
(31.56)
This is the Gauss-Markov SP. Under what conditions on σu and α is Xν of unit variance
for every integer ν?
Exercise 31.9 (Delayless Causal Whitening Filters). Show that if there exists a causal
whitening ﬁlter for (Xν), then there also exists one whose impulse response at time zero
is nonzero.
Exercise 31.10 (Causal Whitening Filters and Prediction). Let (Xν) be a centered Gaus-
sian SP having a whitening ﬁlter whose impulse (hν) is causal and nonzero at time zero.
(i) Show that (Xν) can be represented as
Xν =
∞

η=1
aη Xν−η + σ Uν,
ν ∈Z,
(31.57)
where (Uν) are IID N(0, 1); where σ2 = (1/h0)2; and where
aη = −hη
h0 ,
η ∈Z.
(31.58)
(ii) Suppose now additionally that the stable inverse of (hν) is causal. Prove that for
every ν ∈Z the RV Xν is independent of all future U’s, i.e., of all the random
variables Uν′ with ν′ > ν.
(iii) Argue that, subject to the above conditions, the inﬁnite sum on the RHS of (31.57)
is the predictor of Xν based on Xν−1, Xν−2, . . . of least mean squared-error.
Hint: For Part (ii) express (Xν) as the convolution of (Uν) with the inverse of (hν).
Exercise 31.11 (From a CSP to a SP). Let (Zν) be a WSS CSP. Show that the SP in
(31.50) need not be WSS, even if (Zν) is also proper (Deﬁnition 17.5.4).
Chapter 32
Intersymbol Interference
32.1
The Linearly-Dispersive Channel
The linearly-dispersive channel—also known as the intersymbol interfer-
ence channel or ISI channel—ﬁlters its input and adds white Gaussian noise to
the result. It serves, inter alia, as a model for communication over copper wires
as in communications over telephone lines.
Such a channel is depicted in Fig-
ure 32.1. In this chapter we shall discuss PAM (Sections 32.2 and 32.3) and QAM
(Section 32.4) over such channels with special emphasis on the receiver structure.
In Figure 32.1 and throughout this chapter we use the subscript “c” for continuous-
time waveforms, e.g., xc for the channel input and hc for the ﬁlter’s impulse re-
sponse. This is to avoid confusion with a related discrete-time channel that we
shall encounter later. We assume throughout that the ﬁlter is stable, i.e., that hc
is integrable:
hc ∈L1.
(32.1)
xc(t)
hc
Yc(t)
Nc(t)
(xc ⋆hc)(t)
Figure 32.1: A linearly-dispersive channel Yc(t) = (xc ⋆hc)(t) + Nc(t).
32.2
PAM on the ISI Channel
Consider k data bits D1, . . . , Dk, which are stacked in the vector D as in (28.3), and
that are mapped by the encoding function ϕ(·) to the n real symbols X1, . . . , Xn,
which are stacked in a vector X as in (28.4), so X = ϕ(D) as in (28.5).
The
803
804
Intersymbol Interference
transmitted PAM signal Xc is
Xc(t) = A
n

ℓ=1
Xℓgc(t −ℓTs),
t ∈R,
(32.2)
where gc is the pulse shape, which is assumed to be a real integrable signal that is
bandlimited to W Hz. Since ﬁltering a PAM signal is tantamount to ﬁltering its
pulse shape (Section 15.4 and particularly (15.21)), the received signal Yc can be
written as
Yc(t) = A
n

ℓ=1
Xℓpc(t −ℓTs) + Nc(t),
t ∈R,
(32.3)
where
pc = gc ⋆hc
(32.4)
is a real integrable signal that is bandlimited to W Hz (Proposition 6.5.2), and
the stochastic process

Nc(t)

is WGN of double-sided PSD N0/2 with respect to
the bandwidth W. Rather than guessing D based on

Yc(t)

, there is no loss of
optimality in guessing it based on the inner products between

Yc(t)

and the time
shifts of pc by Ts, 2Ts, . . . , nTs (Proposition 28.2.1). We prefer to scale the inner
products and deﬁne
Tℓ=

1
N0/2

Yc, t 	→pc(t −ℓTs)

(32.5)
=

2A2
N0
n

ℓ′=1
Xℓ′ Rpcpc

(ℓ−ℓ′)Ts

+ ˜Zℓ,
ℓ∈{1, . . . , n},
(32.6)
where Rpcpc is the self-similarity function of pc, and where ( ˜Z1, . . . , ˜Zn)T is a
centered Gaussian vector that is independent of D and that has the covariance
matrix
Cov
 ˜Zℓ′, ˜Zℓ′′
= Rpcpc

(ℓ′ −ℓ′′)Ts

,
ℓ′, ℓ′′ ∈{1, . . . , n}.
(32.7)
To avoid edge eﬀects and to harness the machinery of WSS SPs, it is convenient
to extend the symbols to a bi-inﬁnite sequence by deﬁning Xℓas zero for every
integer ℓoutside the range {1, . . . , n}:
Xℓ= 0,
ℓ∈Z \ {1, . . . , n}.
(32.8)
And it is convenient to deﬁne Tℓalso for ℓ/∈{1, . . . , n} by extending (32.5) also to
such ℓ’s:
Tℓ=

1
N0/2

Yc, t 	→pc(t −ℓTs)

,
ℓ∈Z,
(32.9)
so
Tℓ=

2A2
N0
n

ℓ′=1
Xℓ′ Rpcpc

(ℓ−ℓ′)Ts

+ ˜Zℓ,
ℓ∈Z,
(32.10)
=

2A2
N0
∞

ℓ′=−∞
Xℓ′ Rpcpc

(ℓ−ℓ′)Ts

+ ˜Zℓ,
ℓ∈Z,
(32.11)
32.2 PAM on the ISI Channel
805
where
 ˜Zℓ, ℓ∈Z

is now a centered discrete-time WSS Gaussian SP that is
independent of D and that has the autocovariance function
K ˜
Z ˜
Z(η) = Rpcpc(ηTs),
η ∈Z.
(32.12)
As a technical matter, we note here that
∞

η=−∞
Rpcpc(ηTs)
 < ∞.
(32.13)
This can be seen by noting that pc is an integrable signal that is bandlimited to
W Hz, so Rpcpc—which equals pc ⋆~pc (Section 11.4)—must also be an integrable
signal that is bandlimited to W Hz (Proposition 6.5.2), and η 	→Rpcpc(ηTs) must
therefore be absolutely summable (Exercise 8.10).
The PSD of
 ˜Zℓ

(Deﬁnition 13.6.1) can be readily computed from its autocovari-
ance function (32.12) by expressing K ˜
Z ˜
Z(η) as
K ˜
Z ˜
Z(η) = Rpcpc(ηTs)
=
 ∞
−∞
|ˆpc(f)|2 ei2πfηTs df
=
∞

j=−∞

j
Ts +
1
2Ts
j
Ts −
1
2Ts
|ˆpc(f)|2 ei2πfηTs df
= 1
Ts
∞

j=−∞

1
2
−1
2
ˆpc
 θ
Ts
+ j
Ts

2
ei2π(θ+j)η dθ
= 1
Ts
∞

j=−∞

1
2
−1
2
ˆpc
 θ
Ts
+ j
Ts

2
ei2πθη dθ
=

1
2
−1
2
	 1
Ts
∞

j=−∞
ˆpc
 θ
Ts
+ j
Ts

2
ei2πθη dθ
=

1
2
−1
2
	 1
Ts
∞

j=−∞
ˆgc
 θ
Ts
+ j
Ts

ˆhc
 θ
Ts
+ j
Ts

2
ei2πθη dθ,
η ∈Z, (32.14)
where the ﬁrst equality follows from (32.12); the second because Rpcpc is of FT
f 	→|ˆpc(f)|2 (Section 11.4) and is an integrable signal that is bandlimited to W
Hz; the third by replacing the integral over the reals with the sum of integrals
over the nonoverlapping intervals of the form

j/Ts −1/(2Ts), j/Ts + 1/(2Ts)

; the
fourth by changing the integration variable from f to θ ≜Ts(f −j/Ts); the ﬁfth
because exp (i2πjη) is one; the sixth by swapping the summation and the integra-
tion; and the seventh by (32.4). From (32.14) and the symmetry of the integrand,
we conclude that the PSD of
 ˜Zℓ

is
S˜
Z˜
Z(θ) = 1
Ts
∞

j=−∞
ˆgc
 θ
Ts
+ j
Ts

ˆhc
 θ
Ts
+ j
Ts

2
,
θ ∈
%
−1
2, 1
2

.
(32.15)
Note that the sum in (32.15) is actually ﬁnite, because gc is bandlimited to W Hz.
806
Intersymbol Interference
We shall now assume that
 ˜Zℓ

has a whitening ﬁlter, e.g., that S˜
Z˜
Z of (32.15)
satisﬁes the assumptions of Theorem 31.3.2. To whiten the noise, let hDT-Wh be
the impulse response of a whitening ﬁlter for ( ˜Zℓ) (Section 31.3). Rather than
basing our guess of D on (Tℓ), let us base it on the result

Yℓ

of feeding

Tℓ

through the whitening ﬁlter
(Yℓ) = (Tℓ) ⋆hDT-Wh.
(32.16)
This incurs no loss of optimality, because the ﬁlter is stable with a stable inverse so
(Tℓ) can be recovered from (Yℓ) with probability one (Corollary 31.2.6). Starting
with (32.16), we can express Yℓas follows:
Yℓ=
∞

ν=−∞
Tν hDT-Wh
ℓ−ν
(32.17a)
=
∞

ν=−∞
α
n

ℓ′=1
Xℓ′ Rpcpc

(ν −ℓ′)Ts

hDT-Wh
ℓ−ν
+

( ˜Zν) ⋆(hDT-Wh
ν
)

(ℓ)
(32.17b)
= α
n

ℓ′=1
Xℓ′
∞

ν=−∞
Rpcpc

(ν −ℓ′)Ts

hDT-Wh
ℓ−ν
+ Zℓ
(32.17c)
= α
n

ℓ′=1
Xℓ′
∞

η=−∞
Rpcpc

(ℓ−ℓ′ −η)Ts

hDT-Wh
η
+ Zℓ
(32.17d)
= α
n

ℓ′=1
Xℓ′ hℓ−ℓ′ + Zℓ,
(32.17e)
= α
∞

ℓ′=−∞
Xℓ′ hℓ−ℓ′ + Zℓ,
(32.17f)
where
α =

2A2
N0
,
(32.18)
. . . , Z−1, Z0, Z1, . . . ∼IID N(0, 1),
(32.19)
and
hκ =
∞

ν=−∞
hDT-Wh
ν
Rpcpc

(κ −ν)Ts

,
κ ∈Z.
(32.20)
Here (32.17b) follows from the expression (32.10) for Tℓand from the deﬁnition
of the convolution; (32.17c) follows by swapping the sums and because the ﬁlter
whitens the noise; (32.17d) follows by deﬁning η as ℓ−ν; (32.17e) follows from the
deﬁnition (32.20); and (32.17f) follows from (32.8).
Notice that the sequence (hκ) in (32.20) is absolutely summable
(hκ) ∈ℓ1,
(32.21)
because it is the result of convolving the sequence η 	→Rpcpc(ηTs), which is abso-
lutely summable by (32.13), with the sequence η 	→hDT-Wh
η
, which is the impulse
response of a stable ﬁlter, namely the whitening ﬁlter.
32.3 Guessing the Data Bits
807
We have thus reduced the continuous-time linearly-dispersive channel with PAM
signaling to a discrete-time ISI channel whose time-ℓoutput Yℓis
Yℓ= α
∞

ℓ′=−∞
Xℓ′ hℓ−ℓ′ + Zℓ,
ℓ∈Z,
(32.22)
where α is a positive constant (32.18); the sequence (hk) (32.20) is absolutely
summable; the noise sequence (Zℓ) is IID N(0, 1) and independent of D; the sym-
bol Xℓis zero whenever ℓ∈Z \ {1, . . . , n}; and all the quantities are real. We
refer to hκ as the κ-th ISI coeﬃcient, and to the sequence (hκ) as the ISI
coeﬃcients.
We emphasize that the ISI coeﬃcients (hκ) of (32.20) are determined not only
by the pulse shape gc and the channel response hc (through pc): they are also
inﬂuenced by the choice of the whitening ﬁlter, which is not unique.
Diﬀerent
choices of the whitening ﬁlter lead to diﬀerent ISI coeﬃcients.
The coeﬃcients (hκ) corresponding to positive values of κ are called postcursor
coeﬃcients because through them the symbol Xℓ′ inﬂuences Yℓfor ℓ> ℓ′, i.e.,
for values of ℓthat are in the “future” relative to ℓ′. For example, if h1 is nonzero,
then Xℓ′ inﬂuences the future output Yℓ′+1 because, by (32.22),
α−1
Yℓ′+1 −Zℓ′+1

= · · · + h1Xℓ′ + h0Xℓ′+1 + · · · .
(32.23)
Similarly, the coeﬃcients (hκ) corresponding to negative values of κ are called
precursor coeﬃcients because through them the symbol Xℓ′ inﬂuences Yℓfor
ℓ< ℓ′, i.e., for values of ℓthat are in the “past” relative to ℓ′. For example, if h−1
is nonzero, then Xℓ′ inﬂuences the past output Yℓ′−1 because, by (32.22),
α−1
Yℓ′−1 −Zℓ′−1

= · · · + h0Xℓ′−1 + h−1Xℓ′ + · · · .
(32.24)
These terms can also be described using the time frame with respect to which ℓ
corresponds to the present by rewriting (32.22) as
α−1
Yℓ−Zℓ

=
· · · + h2Xℓ−2 + h1Xℓ−1



contribution of past symbols due to postcursor coeﬃcients
+ h0Xℓ+
h−2Xℓ+2 + h−1Xℓ+1 + · · · .



contribution of future symbols due to precursor coeﬃcients
(32.25)
32.3
Guessing the Data Bits
32.3.1
Postulating a Model
Engineers are sometimes reluctant to implement the MAP rule, either because the
implementation is too complex or because they seek a one-for-all solution that
works in many diﬀerent scenarios. To ﬁnd a reasonable guessing rule, they some-
times postulate a diﬀerent set of densities { ˜fY|M=m(·)} (perhaps simpler) and
808
Intersymbol Interference
a diﬀerent prior {˜πm}, and they then implement the MAP for the “postulated
model,” i.e., the guessing rule that guesses “M = ˜m” whenever ˜m achieves
max
m′∈M ˜πm′ ˜fY|M=m′(yobs)
(32.26)
(with ties being resolved arbitrarily). In other words, they pretend that the densi-
ties and prior are { ˜fY|M=m(·)} and {˜πm}—knowing full-well that they are not—
and they then implement the MAP for this “pretend model.”
For example, if ˜fY|M=m(·) is the same as fY|M=m(·) for every m ∈M and if {˜πm}
is uniform, then this approach leads to the ML rule (Sections 20.8 and 21.3.4).
More interesting, of course, is when the postulated densities are diﬀerent from the
true ones.
We shall refer to the postulated densities and prior as the postulated model and
to the true densities {fY|M=m(·)} and the true prior {πm} as the true model. We
emphasize that the postulated model need not be consistent with the mathematical
assumptions we made in deriving the true model, and it may not resemble the
truth. But we hope that if it does, then using the MAP rule corresponding to the
postulated model may yield good results on the true model.
Instead of providing the densities explicitly, we sometimes describe them by how
the observables are generated. For example, instead of saying that we pretend that
“under H = 0 the observable is N(A, σ2) and that under H = 1 it is N(−A, σ2),”
we could say that we pretend that “the observable Y is given by Y = (1−2H)A+Z,
where Z ∼N(0, σ2) is independent of H.”
For the ISI channel we shall postulate a model that is also an ISI channel, but
one whose ISI coeﬃcients

ℏκ

are simpler in the sense that ℏκ is zero for every
integer κ that is either negative or exceeds some κmax ∈N:
ℏκ = 0,
κ /∈{0, . . . , κmax}.
(32.27)
The ISI coeﬃcients (hκ) of the true model, which are given in (32.20), do not
typically have this simple form. However, as we next show, if κmax is allowed to
be suﬃciently large, then a judicious choice of the phase of the whitening ﬁlter
can lead to the true coeﬃcients being “nearly” of this simple form: hκ will be
approximately zero whenever κ is negative or exceeds κmax.
To this end, we ﬁrst note that the sequence (hκ) of ISI coeﬃcients corresponding
to any whitening ﬁlter hDT-Wh is absolutely summable (32.21), so it tends to
zero as κ tends to ±∞. Consequently, hκ is approximately zero whenever κ /∈
{κ1, . . . , κ2}, where κ1, κ2 are appropriately chosen integers. If κ1 is zero, we set
κmax to be κ2 and we are done. Otherwise, we change our whitening ﬁlter so that
the new ISI coeﬃcients be equal to the result of shifting (hκ) by κ1. To be more
explicit, if hDT-Wh is the original whitening ﬁlter that yielded (hκ), we consider
the new whitening ﬁlter ν 	→hDT-Wh
ν+κ1
. (This is also a whitening ﬁlter because the
Discrete-Time Fourier Transform of ν 	→hDT-Wh
ν+κ1
has the same magnitude as that
of hDT-Wh.) Substituting ν 	→hDT-Wh
ν+κ1
for hDT-Wh in (32.20) shows that employing
ν 	→hDT-Wh
ν+κ1
instead of hDT-Wh yields the ISI coeﬃcients κ 	→hκ+κ1, which are
approximately zero whenever κ is negative or exceeds κmax, where κmax = κ2 −κ1.
32.3 Guessing the Data Bits
809
To complete the description of the postulated model we need to address the prior.
Recall that X = ϕ(D), where ϕ: {0, 1}k →Rn is a one-to-one mapping that maps
the k data bits to the n real symbols; see (28.3), (28.4), and (28.5). Let X denote
the constellation of ϕ (Section 10.8), so ϕ(D) ∈X n. For the purpose of deriving
our decision rule, we shall postulate that
D1, . . . , Dk ∼IID random bits
(32.28a)
and that the system is uncoded (Section 10.9), i.e., that the range of ϕ is X n

ϕ(d) : d ∈{0, 1}k
= X n.
(32.28b)
Since we are assuming throughout that ϕ is one-to-one, postulating that it is also
onto is equivalent to postulating that it is a one-to-one mapping of {0, 1}k onto X n
(Section 10.9). Such a mapping maps uniform inputs to uniform outputs. And
since (32.28a) is equivalent to (D1, . . . , Dk) being uniform over {0, 1}k, we can
conclude that (32.28) (together with the assumption that ϕ is one-to-one) implies
that
(X1, . . . , Xn) ∼U (X n) ,
(32.29)
i.e., that the symbols X1, . . . , Xn are independent with each being uniformly dis-
tributed over X:
X1, . . . , Xn ∼IID U (X).
(32.30)
(In fact, since ϕ is one-to-one, the conditions (32.30), (32.29), and (32.28) are
equivalent.)
32.3.2
States and Dynamic Programming
We next derive a guessing rule for the postulated model. We pretend that our
observations came from this model, and we ﬁnd the guessing rule that minimizes
the probability of a message error (28.16).
We begin by noting that, since the postulated ISI coeﬃcients (ℏν) satisfy (32.27),
the outputs . . . , Y−1, Y0 and the outputs Yn+κmax+1, . . . are not inﬂuenced by the
symbols (X1, . . . , Xn), and they are merely independent noise. Under the postu-
lated model we can therefore ignore these outputs and base our guess of D only on
the observed y1, . . . , yn+κmax. For typographical reasons, it is convenient to deﬁne
n′ = n + κmax.
(32.31)
Since X is a deterministic function of the data D,
˜fY1,...,Yn′|D=d(y1, . . . , yn′) = ˜fY1,...,Yn′|X=ϕ(d)(y1, . . . , yn′).
Hence, given y1, . . . , yn′, the Maximum-Likelihood decoder for the postulated model
searches over all d ∈{0, 1}k for the data vector that maximizes the value of
˜fY1,...,Yn′|X=ϕ(d)(y1, . . . , yn′). Since the range of ϕ is X n (32.28b), we can per-
form this search in two steps: ﬁrst search over all sequences in X n for some ˜x for
which
˜fY1,...,Yn′|X=˜x(y1, . . . , yn′) = max
x′∈X n ˜fY1,...,Yn′|X=x′(y1, . . . , yn′)
(32.32)
810
Intersymbol Interference
(resolving ties arbitrarily), and then produce the data sequence ϕ−1(˜x) that ϕ
maps to it.
To ﬁnd a maximizer ˜x, we need the postulated densities in an explicit form. Sub-
stituting in (32.22) the postulated ISI coeﬃcients (ℏν) for the true ones (hν) and
recalling that the Zℓ’s are IID N(0, 1), we obtain the explicit form
˜fY1,...,Yn′|X1=x1,...,Xn=xn(y1, . . . , yn′)
= (2π)−n′/2 exp
-
−1
2
n′

ℓ=1
	
yℓ−α ℏ0 xℓ−α
κmax

ℓ′=1
ℏℓ′ xℓ−ℓ′

2.
with xℓtaken as zero if ℓ/∈{1, . . . n}.
(32.33)
Taking logarithms, discarding the term −n′
2 log(2π) (which is common to all the
hypotheses), and scaling by −2α−2, we conclude that the maximization of the
likelihood under the postulated model is equivalent to the minimization of Υ(x; y),
where x and y stand for x1, . . . , xn and y1, . . . , yn′, and
Υ(x; y) =
n′

ℓ=1
	
α−1yℓ−ℏ0 xℓ−
κmax

ℓ′=1
ℏℓ′ xℓ−ℓ′

2
,
(32.34)
with xℓtaken as zero if ℓ/∈{1, . . . n}. We refer to Υ(x; y) as the “cost of x1, . . . , xn.”
(More precise but too cumbersome would have been “cost of x1, . . . , xn given
y1, . . . , yn′.”)
The diﬃculty in minimizing the cost Υ(x; y) over x1, . . . , xn is that each symbol xℓ
inﬂuences a number of diﬀerent terms in the sum, so we cannot minimize the sum
term-by-term. To address this issue, we introduce the notion of a state. We will
deﬁne the states so that the time-ℓstate sℓwill be a deterministic function of the
time-(ℓ−1) state sℓ−1 and the symbol xℓ
sℓ= ψ(sℓ−1, xℓ)
(32.35a)
(where the time-zero state s0 is chosen arbitrarily) and so that the cost Υ(x; y)
will be expressible as
Υ(x; y) =
n

ℓ=1
γℓ(xℓ, sℓ−1; yℓ) + γn+1(sn; yn+1, . . . , yn′)
(32.35b)
for some functions γ1, . . . , γn+1. Once we have cast the problem in this form, we
will be able to use Dynamic Programming or the Viterbi Algorithm to minimize
the cost.
Ideally, we would like the time-(ℓ−1) state sℓ−1 to correspond to the symbols
xℓ−1, . . . , xℓ−κmax, which are needed in addition to xℓin order to compute the ℓ-th
term in (32.34) (for 1 ≤ℓ≤n). This would allow us to express this term as a
function of xℓand sℓ−1 (and yℓ). But there are some slight technicalities stemming
from edge eﬀects. We must therefore be a bit more formal.
A state s is a κmax-tuple in X κmax of components s(1), s(2), . . . , s(κmax):
s =

s(1), s(2), . . . , s(κmax)
∈X κmax.
(32.36)
32.3 Guessing the Data Bits
811
The collection of all states, namely X κmax, is denoted S:
S = X κmax.
(32.37)
We choose from S some arbitrary state s0 ∈S and call it “the state at time zero.”
We next deﬁne the state transition law. As in a shift-register, feeding the symbol x1
at time 1 results in the transition of the state from s0 to the time-1 state
s1 =

x1, s(1)
0 , s(2)
0 , . . . , s(κmax−1)
0

.
(32.38)
We express this by writing
s0
x1
⇝s1.
(32.39)
Feeding x2 after x1 leads to the time-2 state
s2 =

x2, s(1)
1 , s(2)
1 , . . . , s(κmax−1)
1

=

x2, x1, s(1)
0 , . . . , s(κmax−2)
0

.
Thus,
s0
x1
⇝s1
x2
⇝s2.
In general, if the time-(ℓ−1) state is sℓ−1, then feeding xℓleads to the time-ℓstate
sℓ= ψ(sℓ−1, xℓ),
(32.40a)
where
ψ(sℓ−1, xℓ) =

xℓ, s(1)
ℓ−1, s(2)
ℓ−1, . . . , s(κmax−1)
ℓ−1

.
(32.40b)
Note that feeding the inputs x1, . . . , xℓconsecutively results in the time-ℓstate sℓ
being
sℓ=
⎧
⎨
⎩

xℓ, xℓ−1, . . . , xℓ−κmax+1

if ℓ∈{κmax, . . . , n},

xℓ, xℓ−1, . . . , x1, s(1)
0 , . . . , s(κmax−ℓ)
0

if ℓ∈{1, . . . , κmax −1}.
(32.41)
(The second case accounts for the edge eﬀect we mentioned earlier.) From sℓwe
can thus recover xℓ, xℓ−1, . . . , xℓ−min{κmax−1,ℓ−1}:
xℓ−ν = s(ν+1)
ℓ
,
ν ∈

0, . . . , min{κmax −1, ℓ−1}

,
ℓ∈{1, . . . , n}.
(32.42)
And from sℓ−1 we can recover xℓ−1, . . . , xℓ−min{κmax,ℓ−1}:
xℓ−ν = s(ν)
ℓ−1,
ν ∈

1, . . . , min{κmax, ℓ−1}

,
ℓ∈{2, . . . , n + 1}.
(32.43)
Finally, by substituting n for ℓin (32.42) we obtain that from sn we can recover
xn, xn−1, . . . , xn−min{κmax−1,n−1}:
xn−ν = s(ν+1)
n
,
ν ∈

0, . . . , min{κmax −1, n −1}

.
(32.44)
812
Intersymbol Interference
We next express the cost Υ(x, y) (32.34) as in (32.35b) and without the need for
the interpretation of xℓas being zero when ℓ/∈{1, . . . , n}. To avoid this need,
we rewrite (32.34) in a somewhat more cumbersome form, but one that does not
involve any xℓwith ℓ/∈{1, . . . n}:
Υ(x; y) =
n

ℓ=1
	
α−1yℓ−ℏ0 xℓ−
min{κmax,ℓ−1}

ℓ′=1
ℏℓ′ xℓ−ℓ′

2
+
n′

ℓ=n+1
	
α−1yℓ−
min{κmax,ℓ−1}

ℓ′=ℓ−n
ℏℓ′ xℓ−ℓ′

2
,
(32.45)
where we have set the upper limits in the two sums over ℓ′ above to min{κmax, ℓ−1}
to avoid ℓ−ℓ′ being nonpositive; and in the last line of (32.45) we discarded the
term ℏ0 xℓ(because ℓexceeds n), and we set the lower limit on ℓ′ to ℓ−n so as to
guarantee that ℓ−ℓ′ not exceed n.
To express the RHS of (32.45) as in (32.35b) in terms of the states, we can
use (32.43) and (32.44) to obtain
Υ(x; y) =
n

ℓ=1
	
α−1yℓ−ℏ0 xℓ−
min{κmax,ℓ−1}

ℓ′=1
ℏℓ′ s(ℓ′)
ℓ−1

2



γℓ(xℓ,sℓ−1;yℓ)
+
n′

ℓ=n+1
	
α−1yℓ−
min{κmax,ℓ−1}

ℓ′=ℓ−n
ℏℓ′ s(n−ℓ+ℓ′+1)
n

2



γn+1(sn;yn+1,...,yn+κmax)
(32.46)
=
n

ℓ=1
γℓ(xℓ, sℓ−1; yℓ) + γn+1(sn; yn+1, . . . , yn′),
(32.47)
where for every ℓ∈{1, . . . , n} we deﬁne the ℓ-th intermediate cost
γℓ(xℓ, sℓ−1; yℓ) =
	
α−1yℓ−ℏ0 xℓ−
min{κmax,ℓ−1}

ℓ′=1
ℏℓ′ s(ℓ′)
ℓ−1

2
,
(32.48)
and we deﬁne the ﬁnal cost as
γn+1(sn; yn+1, . . . , yn′) =
n′

ℓ=n+1
	
α−1yℓ−
min{κmax,ℓ−1}

ℓ′=ℓ−n
ℏℓ′ s(n−ℓ+ℓ′+1)
n

2
. (32.49)
Note that (32.43) and (32.44) also provide alternative forms for the costs (in terms
of the symbols instead of the states):
γℓ(xℓ, sℓ−1; yℓ) =
	
α−1yℓ−ℏ0 xℓ−
min{κmax,ℓ−1}

ℓ′=1
ℏℓ′ xℓ−ℓ′

2
,
ℓ∈{1, . . . , n},
(32.50)
32.3 Guessing the Data Bits
813
and
γn+1(sn; yn+1, . . . , yn′) =
n′

ℓ=n+1
	
α−1yℓ−
min{κmax,ℓ−1}

ℓ′=ℓ−n
ℏℓ′ xℓ−ℓ′

2
.
(32.51)
Having established that our problem of minimizing (32.34) can be cast as in (32.35),
we next propose an algorithm to solve it. For typographical reasons we shall no
longer make the dependence of the cost on y1, . . . , yn′ explicit as in (32.35b), and
we shall write the cost instead as
n

ℓ=1
γℓ(xℓ, sℓ−1) + γn+1(sn).
Let us deﬁne the cost-to-go of xj, . . . , xn from the state s at time j −1 as
n

ℓ=j
γℓ(xℓ, sℓ−1) + γn+1(sn),
(32.52)
where the state evolves according to (32.40a) with sj−1 taken to be s. Let us denote
by cj−1(s) the minimum of (32.52) over all choices of xj, xj+1, . . . , xn. We refer to
cj−1(s) as the minimum cost-to-go from s at time j −1. With this notation the
minimum of (32.35b) that we seek can be expressed as c0(s0)
min
x∈X n Υ(x; y) = c0(s0).
(32.53)
Moreover,
cn(s) = γn+1(s),
s ∈S.
(32.54)
To be formal, we say that cn(s) is achieved by the empty string of symbols.
To get a sense of what the next proposition is about, let us imagine that we did not
know that the shortest path between two points is a straight line. Even without
knowing this, it would be fairly easy to convince ourselves that if the shortest
path from A to B goes through C, then its segment from C to B ought to be along
the shortest path from C to B. This is the analog of the following proposition,
which can also be stated extremely informally as follows: “Whether or not a path
achieving cν(˜s) goes at time j through the state s we do not know. But if it does,
then it must continue from there along a path that achieves cj(s), and any path
achieving cj(s) will do.”
Proposition 32.3.1 (Principle of Optimality: Cost-to-Go). Let s,˜s ∈S be ar-
bitrary states, and let ν, j ∈{0, . . . , n} be intermediate times with ν < j.
Let
xν+1, . . . , xn achieve cν(˜s) and go through the state s at time j
˜s
xν+1
⇝· · ·
xj
⇝s
xj+1
⇝· · ·
xn
⇝.
(32.55)
Then:
(i) xj+1, . . . , xn must achieve cj(s).
814
Intersymbol Interference
(ii) If x⋆
j+1, . . . , x⋆
n achieves cj(s), then xν+1, . . . , xj, x⋆
j+1, . . . , x⋆
n achieves cν(˜s).
Proof. Let x⋆
j+1, . . . , x⋆
n achieve cj(s). The proof is based on a comparison between
the cost-to-go of xν+1, . . . , xn and the cost-to-go of xν+1, . . . , xj, x⋆
j+1, . . . , x⋆
n. De-
note by sν+1, . . . , sn the states through which xν+1, . . . , xn goes, and deﬁne sν
as ˜s:
˜s = sν
xν+1
⇝sν+1
xν+2
⇝· · ·
xn
⇝sn.
The assumption (32.55) that xν+1, . . . , xn goes through s at time j implies that
n

ℓ=j+1
γℓ(xℓ, sℓ−1) + γn+1(sn) ≥cj(s).
(32.56)
Consequently, the cost-to-go of xν+1, . . . , xn from ˜s at time ν can be lower-bounded
as follows:
j

ℓ=ν+1
γℓ(xℓ, sℓ−1) +
n

ℓ=j+1
γℓ(xℓ, sℓ−1) + γn+1(sn) ≥
j

ℓ=ν+1
γℓ(xℓ, sℓ−1) + cj(s).
(32.57)
In comparison, the cost-to-go of xν+1, . . . , xj, x⋆
j+1, . . . , x⋆
n from ˜s at time ν is
j

ℓ=ν+1
γℓ(xℓ, sℓ−1) +
n

ℓ=j+1
γℓ(x⋆
ℓ, sℓ−1) + γn+1(sn) =
j

ℓ=ν+1
γℓ(xℓ, sℓ−1) + cj(s).
(32.58)
Our assumption that xν+1, . . . , xn minimizes the cost-to-go from ˜s at time ν rules
out the possibility that xν+1, . . . , xj−1, x⋆
j, . . . , x⋆
n has a lower cost, and (32.56)
must hence hold with equality, thus proving that xj+1, . . . , xn must achieve cj(s)
and hence establishing Part (i). Moreover, the fact that (32.56) holds with equality
implies that the cost-to-go of xν+1, . . . , xj, x⋆
j+1, . . . , x⋆
n from ˜s at time ν is the same
as that of xν+1, . . . , xn, so it too achieves cν(˜s), thus establishing Part (ii).
We have already seen in (32.54) how to compute cn(s) for every state s ∈S. We
will now use Proposition 32.3.1 to show that knowing cj(s) for every state s ∈S
allows us to compute cj−1(˜s) eﬃciently for every state ˜s ∈S. This will allow us to
compute c0(s0) as follows: start by computing cn(s) for every state s ∈S; proceed
to compute cn−1(s) for every state s ∈S; and continue in this fashion until you
have computed c0(s0). We will also see that with little additional work we can also
ﬁnd inputs x1, . . . , xn that achieve c0(s0).
Assume then that we have computed cj(s) for every state s ∈S, and ﬁx some
state ˜s for which we would like to compute cj−1(˜s).
Here j ∈{1, . . . , n}.
To
compute cj−1(˜s) we need to ﬁnd some sequence x′
j, . . . , x′
n that achieves it. There
are # X n−j+1 such sequences, but our search turns out to be easier: once we have
found x′
j, the rest is easy, and there are only # X possible choices for x′
j. To see
why x′
j holds the key, suppose that x′
j, . . . , x′
n achieves cj−1(˜s). Since
˜s
x′
j
⇝ψ(˜s, x′
j),
32.3 Guessing the Data Bits
815
we can infer from Proposition 32.3.1 (with the substitution of j −1 for ν and of
ψ(˜s, x′
j) for s) that x′
j+1, . . . , x′
n must achieve cj

ψ(˜s, x′
j)

. It thus follows that the
cost-to-go of x′
j, . . . , x′
n from ˜s at time j −1 is γj(x′
j,˜s) + cj

ψ(˜s, x′
j)

. And since
x′
j, . . . , x′
n minimizes the cost-to-go, x′
j must minimize this cost. Hence
cj−1(˜s) = min
x∈X
'
γj(x,˜s) + cj

ψ(˜s, x)
(
.
(32.59)
As promised, the minimization in (32.59) is over x in X, and the latter contains
only # X elements.
The algorithm for ﬁnding c0(s0) can be thus described as
follows: For each s ∈S compute cn(s). Then use (32.59) to compute cn−1(˜s) for
every ˜s ∈S. Continue using (32.59) until you have computed c0(s0). (In the last
step one need not compute c0(s) for every state: it suﬃces to compute c0(s0).)
To guess the symbols, we also need to ﬁnd a sequence achieving c0(s0) and not
just the latter’s value. This requires just a simple change to our algorithm. Again
we start out with the calculation of cn(s) for every s ∈S. But we also keep track
of the sequence that achieves it: in this case the empty sequence. Suppose now
that we have computed cj

s

for every s ∈S and that we have found a sequence
x⋆
j+1, . . . , x⋆
n that achieves it. For each ˜s ∈S we next calculate cj−1(˜s) according
to (32.59). If the minimum in (32.59) is achieved by some x⋆, then we record that
cj−1(˜s) is achieved by the concatenation of x⋆with the sequence that achieves
cj

ψ(˜s, x⋆)

.
We continue until we have computed c0(s0) and a sequence that
achieves it.
The complexity of this procedure is roughly proportional to n (# S) (# X), i.e., to
n (# X)κmax+1,
(32.60)
because in going from j to j −1 we use (32.59) for every ˜s ∈S, and in each use we
search over a set of cardinality # X.
This algorithm is not very suitable for real-time applications, because it is a “back-
ward algorithm” that can only begin once all the symbols y1, . . . , yn′ have been
received. Instead, engineers usually implement the Viterbi Algorithm, which is a
“forward algorithm,” that we describe next. It is very similar to the algorithm we
described above, except that, instead of working down from n, it works up to n.
32.3.3
The Viterbi Algorithm
Instead of studying the cost-to-go, we shall study the cost-to-reach, which we
deﬁne next. The cost-to-reach of x1, . . . , xj (from s0) is
j

ℓ=1
γℓ(xℓ, sℓ−1) + γn+1(sn) I{j = n},
(32.61)
where the state evolves according to (32.40a), and the last term γn+1(sn) is added
only if j is equal to n. Let us denote by ˜cj(s) the minimum cost-to-reach over
all sequences x1, . . . , xj reaching s at time j, i.e., satisfying s0
x1
⇝· · ·
xj
⇝s. If no
such sequence exists, then we say that s cannot be reached from s0 in j steps, and
816
Intersymbol Interference
we set ˜cj(s) to be +∞. For j = 0 we deﬁne
˜c0(s) =

0
if s = s0,
+∞
otherwise,
(32.62)
and we say that ˜c0(s0) is achieved by the empty string of inputs.
In terms of the minimal cost-to-reach, the minimum of (32.35b) that we seek is
min
x∈X n Υ(x; y) = min
s∈S ˜cn(s),
(32.63)
and our task is to ﬁnd this minimum and a sequence x1, . . . , xn that achieves it.
The following proposition is the analog of Proposition 32.3.1 for the cost-to-reach.
Informally, it can be stated as follows: “Whether or not a path achieving ˜cν(˜s)
goes at time j < ν through the state s we do not know. But if it does, then it does
so along a path that achieves ˜cj(s), and any path achieving ˜cj(s) will do.” The
formal version is as follows:
Proposition 32.3.2 (Principle of Optimality: Cost-to-Reach). Let s,˜s ∈S be
arbitrary states, and let ν, j ∈{0, . . . , n} be intermediate times with j < ν. Let
x1, . . . , xν achieve ˜cν(˜s) and go through the state s at time j
s0
x1
⇝· · ·
xj
⇝s
xj+1
⇝· · ·
xν
⇝˜s.
(32.64)
Then:
(i) x1, . . . , xj must achieve ˜cj(s).
(ii) If x⋆
1, . . . , x⋆
j achieves ˜cj(s), then x⋆
1, . . . , x⋆
j, xj+1, . . . , xν achieves ˜cν(˜s).
Proof. Let x⋆
1, . . . , x⋆
j achieve ˜cj(s).
The proof is very similar to the proof of
Proposition 32.3.1 and compares the cost-to-reach of x1, . . . , xν with the cost-to-
reach of x⋆
1, . . . , x⋆
j, xj+1, . . . , xν. The details are omitted.
With the aid of Proposition 32.3.2 (with the substitution of j + 1 for ν) we shall
next show how knowing ˜cj(s) for every state s ∈S allows us to compute ˜cj+1(˜s)
eﬃciently. We ﬁrst show that
˜cj+1(˜s) =
min
s,x: s
x
⇝˜s
'
˜cj(s) + γj+1(x, s) + γn+1(˜s) I{j + 1 = n}
(
,
(32.65)
where the minimum is over all states s ∈S and symbols x ∈X such that s
x⇝˜s.
Later we will characterize all such s and x.
To see why (32.65) holds, let x′
1, . . . , x′
j+1 be a sequence achieving ˜cj+1(˜s), and
let s′ be the state it reaches at time j, so
s0
x′
1
⇝s1
x′
2
⇝· · ·
x′
j
⇝s′ x′
j+1
⇝˜s.
(32.66)
32.3 Guessing the Data Bits
817
We do not know s′, but we do know from (32.66) that
s′ x′
j+1
⇝˜s.
(32.67)
Since the inputs x′
1, . . . , x′
j+1 achieve ˜cj+1(˜s) and go through s′ at time j, it follows
from Proposition 32.3.2 (i) that the cost-to-reach of x′
1, . . . , x′
j is ˜cj(s′). Conse-
quently, ˜cj+1(˜s), which is the cost-to-reach of x′
1, . . . , x′
j+1, can also be expressed
as
˜cj+1(˜s) = ˜cj(s′) + γj+1(x′
j+1, s′) + γn+1(˜s) I{j + 1 = n}.
(32.68)
By (32.67), the pair s′, x′
j+1 are in the feasible set of (32.65), and we thus conclude
that ˜cj+1(˜s) is lower-bounded by the RHS of (32.65). (The RHS of (32.65), which
is the minimum over all choices of s and x satisfying s
x⇝˜s cannot be higher than
when we make a particular choice for s to be s′ and of x to be x′
j+1.) However,
equality must hold, because if s⋆and x⋆achieve the minimum in (32.65), and if
x⋆
1, . . . , x⋆
j achieve ˜cj(s⋆), then the concatenated sequence x⋆
1, . . . , x⋆
j, x⋆reaches ˜s
at time j + 1 and is of cost-to-reach ˜cj

s⋆
+ γj+1(x⋆, s⋆) + γn+1(˜s) I{j + 1 = n},
which is the RHS of (32.65).
The minimization in (32.65) is over all states s and symbols x satisfying s
x⇝˜s.
Given some state ˜s ∈S, under what conditions on x ∈X and s ∈S does this hold?
In other words, given ˜s ∈S, from which states s and with which inputs x can we
go from s to ˜s? Using (32.40b) it is readily seen that s
x⇝˜s holds if, and only if,
the following two conditions hold:
1) The state s has the form
s =

˜s(2), ˜s(3), . . . ˜s(κmax−1), ξ

(32.69a)
for some ξ ∈X.
2) The input x is given by
x = ˜s(1).
(32.69b)
Here ξ can be viewed as the right-most entry in the shift register s, i.e., as the
entry that is “forgotten” as x enters the shift-register from the left.
Thus, in the minimization on the RHS of (32.65) we can ﬁx x to be ˜s(1), and we
can restrict the minimization over s to states s that are of the form (32.69a), of
which there are only # X.
We can thus describe the Viterbi Algorithm as follows: Initialize using (32.62).
Assume now that you have computed ˜cj(s) for every state s ∈S. Compute ˜cj+1(˜s)
for every ˜s ∈S as follows: for each ξ ∈X compute
˜cj(s) + γj+1(x, s) + γn+1(˜s) I{j + 1 = n},
where x and s are given in (32.69), and ﬁnd one, say ξ⋆, that among all those ξ’s
minimizes this expression. If s⋆is the result of substituting ξ⋆for ξ in (32.69a),
then
˜cj+1(˜s) = ˜cj(s⋆) + γj+1

˜s(1), s⋆
+ γn+1(˜s) I{j + 1 = n}.
(32.70)
818
Intersymbol Interference
Continue until ˜cn(s) has been computed for every state s and then take its minimum
over all s. This is the minimum of (32.35b).
A minor alteration to the algorithm allows us to ﬁnd not only the minimum of ˜cn(s)
but also a path that achieves it. The idea is to compute not only ˜cj(s) but also
a path that achieves it. Such a path is called a survivor for the state s at
time j. The survivor for the state s0 at time 0 is the empty path. Suppose now
that for some j ∈{0, . . . , n −1} we have found both ˜cj(s) and a survivor achieving
it for every s ∈S. To compute ˜cj+1(˜s) we use (32.65). If the minimum in (32.65)
is achieved by s⋆and x⋆(with the latter being equal to ˜s(1) by (32.69b)), then a
survivor for the state ˜s at time j + 1 is the concatenation of the survivor for the
state s⋆at time j and the symbol x⋆.
At the heart of the Viterbi Algorithm is (32.65), which engineers sometimes call
the add-compare-select step: For each choice of s and x satisfying s
x⇝˜s, we
add to ˜cj

s

the term γj+1(x, s) + γn+1(˜s) I{j + 1 = n} and we compare the
results. We then select the pair s⋆and x⋆for which the result is minimal. We
ﬁnally set ˜cj+1(˜s) to be this minimum, and we set the survivor of ˜cj+1(˜s) to be the
concatenation of the survivor of ˜cj

s⋆
with x⋆.
We have chosen to derive the Viterbi Algorithm via Dynamic Programming. A
diﬀerent approach is graph theoretic. We imagine a directed graph with weights
whose vertices are pairs of times and states (j, s). More speciﬁcally, it has the
vertex (0, s0) and all the vertices of the form (j, s) where j ∈{1, . . . , n} and s ∈S.
There is an edge from the vertex (j, s) to the vertex (j + 1,˜s) if s
x⇝˜s, in which
case its weight is
γj+1(x, s) + γn+1(˜s) I{j + 1 = n},
(32.71)
i.e.,
γj+1

˜s(1), s

+ γn+1(˜s) I{j + 1 = n}.
(32.72)
Note that the graph contains no cycles. In this setting the Viterbi Algorithm ﬁnds
the path of least weight from s0 to each of the vertices that has the form (n, s)
for some s ∈S (Cormen, Leiserson, Rivest, and Stein, 2009, Section 24.2). The
graph, without the weights, is sometimes called a trellis. An example of a trellis
is depicted in Figure 32.2. (Only the vertices (j, s) that are reachable from (0, s0)
are depicted.)
Our description of the Viterbi Algorithm has us decode only after n steps. Since n
is typically very large (corresponding to the total number of symbols transmitted
during the transmitter’s lifetime), this delay is intolerable. Instead engineers often
decode Xℓafter running the Viterbi Algorithm for ℓ+ j steps, where j is chosen to
be “suﬃciently large.” They pick some arbitrary state s and look at its time-(ℓ+j)
survivor. This survivor’s time-ℓsymbol is then their guess of Xℓ.
32.4
QAM on the ISI Channel
We next consider QAM for the ISI channel. As in Section 28.5, we envision that k
data bits D1, . . . , Dk, which we stack in a vector D, are mapped by ϕ: {0, 1}k →Cn
to n complex symbols C1, . . . , Cn, which we stack in a vector C, so ϕ(D) = C.
32.4 QAM on the ISI Channel
819
(+1, +1, +1)
(+1, +1, −1)
(+1, −1, +1)
(+1, −1, −1)
(−1, +1, +1)
(−1, +1, −1)
(−1, −1, +1)
(−1, −1, −1)
Figure 32.2: An example of a trellis. Here κmax = 3, the constellation is {±1},
and only the vertices that are reachable from (0, s0) are depicted. The time-zero
state s0 here is (+1, −1, +1).
The transmitted signal is
XPB(t) = 2 Re

XBB(t) ei2πfct
,
t ∈R,
(32.73)
where its baseband representation XBB(·) is
XBB(t) = A
n

ℓ=1
Cℓgc(t −ℓTs),
t ∈R.
(32.74)
Here A > 0, the pulse shape gc is some integrable complex signal that is band-
limited to W/2 Hz, and the carrier frequency fc exceeds W/2. For convenience
(cf. (32.8)), we deﬁne Cℓas zero whenever ℓ/∈{1, . . . , n}
Cℓ= 0,
ℓ∈Z \ {1, . . . , n}.
(32.75)
As we have seen in Section 16.9 (Proposition 16.9.1), passing the QAM signal XPB
through the real ﬁlter of impulse response hc is tantamount to replacing its pulse
shape gc with the pulse shape pc, where
pc(t) =

W
2
−W
2
ˆgc(f) ˆhc(f + fc) ei2πfct df,
t ∈R
(32.76)
is an integrable complex signal that is bandlimited to W/2 Hz and whose FT is
ˆpc(f) = ˆgc(f) ˆhc(f + fc),
f ∈R.
(32.77)
Thus, the baseband representation of XPB ⋆hc is
(XPB ⋆hc)BB(t) = A
n

ℓ=1
Cℓpc(t −ℓTs),
t ∈R.
(32.78)
820
Intersymbol Interference
We can hence view the received waveform as the sum of a QAM signal (of pulse
shape pc) and WGN. This is the setting we studied in Section 28.5, where we
also identiﬁed a real random vector (Section 28.5.2) and a complex random vector
(Section 28.5.3) on which we could base our guess without any loss of optimality.
Here we shall use the latter, but we shall scale the inner products. We ﬁrst deﬁne
pc,I,ℓ(t) = 2 Re
 1
√
2 pc(t −ℓTs) ei2πfct
,
t ∈R,
(32.79a)
pc,Q,ℓ(t) = 2 Re
 1
√
2 i pc(t −ℓTs) ei2πfct
,
t ∈R,
(32.79b)
and then1
Tℓ= T (ℓ)
I
+ i T (ℓ)
Q ,
ℓ∈Z,
(32.80a)
where
T (ℓ)
I
=
4
1
N0
 ∞
−∞
Yc(t) pc,I,ℓ(t) dt,
ℓ∈Z,
(32.80b)
T (ℓ)
Q
=
4
1
N0
 ∞
−∞
Yc(t) pc,Q,ℓ(t) dt,
ℓ∈Z.
(32.80c)
It follows from Proposition 28.5.2 that there is no loss of optimality in guessing D
based on T1, . . . , Tn, and that Tℓcan be expressed as
Tℓ=

2A2
N0
n

ℓ′=1
Cℓ′ Rpcpc

(ℓ−ℓ′)Ts

+ ˜Zℓ,
ℓ∈{1, . . . , n},
(32.81)
where Rpcpc is the self-similarity function of pc, and where ( ˜Z1, . . . , ˜Zn)T is a
circularly-symmetric complex Gaussian vector that is independent of D and that
has the covariance matrix
Cov
 ˜Zℓ′, ˜Zℓ′′
= Rpcpc

(ℓ′ −ℓ′′)Ts

,
ℓ′, ℓ′′ ∈{1, . . . , n}.
(32.82)
If we also consider ℓ/∈{1, . . . , n}, then we can express Tℓas
Tℓ=

2A2
N0
n

ℓ′=1
Cℓ′ Rpcpc

(ℓ−ℓ′)Ts

+ ˜Zℓ,
ℓ∈Z,
(32.83)
where
 ˜Zℓ

is a circularly-symmetric complex Gaussian SP that is independent
of D and that has the autocovariance function
K ˜
Z ˜
Z(η) = Rpcpc(ηTs),
η ∈Z.
(32.84)
To whiten the noise, we can proceed as in Section 32.2: Let hDT-Wh be the (com-
plex) impulse response of a whitening ﬁlter for
 ˜Zℓ, ℓ∈Z

(Section 31.3). And
1In Section 28.5.3 we denoted T (ℓ)
I
+ i T (ℓ)
Q
by T (ℓ), but here we prefer Tℓin order to be
consistent with our notation for sequences.
32.4 QAM on the ISI Channel
821
rather than basing our guess of D on

Tℓ, ℓ∈Z

, let us base it on the result

Yℓ

of feeding

Tℓ

through the whitening ﬁlter
(Yℓ) = (Tℓ) ⋆hDT-Wh.
(32.85)
Proceeding as in (32.17), this can be written in view of (32.83) as
Yℓ= α
∞

ℓ′=−∞
Cℓ′ hℓ−ℓ′ + Zℓ,
ℓ∈Z,
(32.86a)
where
Cℓ= 0,
ℓ∈Z \ {1, . . . , n},
(32.86b)
. . . , Z−1, Z0, Z1, . . . ∼IID NC(0, 1),
(32.86c)
α =

2A2
N0
,
(32.86d)
and
hk =
∞

ν=−∞
hDT-Wh
ν
Rpcpc

(k −ν)Ts

,
k ∈Z.
(32.86e)
The similarities between the discrete-time model for PAM (32.22) and for QAM
(32.86) are striking. The only diﬀerence is that in the former the symbols (Xℓ)
and ISI coeﬃcients (hκ) are real, whereas in the latter the symbols (Cℓ) and ISI
coeﬃcients (hκ) are complex, and that the Gaussians in the former are real and in
the latter complex.
As we did in the real case, we shall now assume that the data D1, . . . , Dk are IID
random bits and that transmission is uncoded, so
(C1, . . . , Cn) ∼U (Cn) ,
(32.87)
where C denotes the constellation (Section 16.7). We shall also postulate simpliﬁed
complex ISI coeﬃcients

ℏκ

for which
ℏκ = 0,
κ /∈{0, . . . , κmax}.
(32.88)
Under the postulated model the outputs Yj for j < 0 or j > n + κmax can thus
be ignored, and the decision can be based on Y1, . . . , Yn′, where n′ = n + κmax.
Since the Gaussians in (32.86c) are standard complex Gaussians (Section 24.2.1),
the postulated densities are
˜fY1,...,Yn′|C1=c1,...,Cn=cn(y1, . . . , yn′)
= π−n′ exp
-
−
n′

ℓ=1
yℓ−α ℏ0 cℓ−α
κmax

ℓ′=1
ℏℓ′ cℓ−ℓ′

2.
with cℓtaken as zero if ℓ/∈{1, . . . n}.
(32.89)
822
Intersymbol Interference
Taking logarithms, discarding the term −n′ log π (which is common to all the
hypotheses), and scaling by −α−2, we conclude that the maximization of the like-
lihood under the postulated model is equivalent to the minimization of Υ(c; y),
where c and y stand for c1, . . . , cn and y1, . . . , yn′, and
Υ(c; y) =
n′

ℓ=1
α−1yℓ−ℏ0 cℓ−
κmax

ℓ′=1
ℏℓ′ cℓ−ℓ′

2
,
(32.90)
with cℓtaken as zero if ℓ/∈{1, . . . n}. We refer to this quantity as the “cost of
c1, . . . , cn.” Notice that Υ(c; y) is very similar to its PAM counterpart (32.34):
the only diﬀerence is that the symbols are denoted cℓinstead of xℓand that the
squared parentheses are replaced with the squared magnitude.
As for PAM, we next deﬁne states so that the time-ℓstate sℓwill be a deterministic
function of the time-(ℓ−1) state sℓ−1 and the symbol cℓ
sℓ= ψ(sℓ−1, cℓ)
(32.91a)
(where the time-zero state s0 is chosen arbitrarily); and so that the cost (32.90)
will be expressible as
Υ(c; y) =
n

ℓ=1
γℓ(cℓ, sℓ−1; yℓ) + γn+1(sn; yn+1, . . . , yn′).
(32.91b)
Once we have cast the problem in this form, we will be able to use Dynamic
Programming or the Viterbi Algorithm to minimize the cost just as we did for
PAM.
The states are now elements of Cκmax (instead of elements of X κmax in the PAM
case), and the transition law analogous to (32.40) is
sℓ= ψ(sℓ−1, cℓ),
(32.92a)
where
ψ(sℓ−1, cℓ) =

cℓ, s(1)
ℓ−1, s(2)
ℓ−1, . . . , s(κmax−1)
ℓ−1

.
(32.92b)
The deﬁnitions of γℓ(cℓ, sℓ−1; yℓ) and γn+1(sn; yn+1, . . . , yn′) are nearly identical to
their real counterparts, except that the symbols are denoted cj instead of xj and
the squared parentheses are replaced with the squared magnitude:
γℓ(cℓ, sℓ−1; yℓ) =
α−1yℓ−ℏ0 cℓ−
min{κmax,ℓ−1}

ℓ′=1
ℏℓ′ cℓ−ℓ′

2
(32.93)
=
α−1yℓ−ℏ0 cℓ−
min{κmax,ℓ−1}

ℓ′=1
ℏℓ′ s(ℓ′)
ℓ−1

2
,
ℓ∈{1, . . . , n}, (32.94)
and
γn+1(sn; yn+1, . . . , yn′) =
n′

ℓ=n+1
α−1yℓ−
min{κmax,ℓ−1}

ℓ′=ℓ−n
ℏℓ′ cℓ−ℓ′

2
(32.95)
=
n′

ℓ=n+1
α−1yℓ−
min{κmax,ℓ−1}

ℓ′=ℓ−n
ℏℓ′ s(n−ℓ+ℓ′+1)
n

2
.
(32.96)
32.5 From Passband to Baseband
823
With these new deﬁnitions, the Dynamic Programming algorithm and the Viterbi
Algorithm require no change.
32.5
From Passband to Baseband
The QAM receiver that we described in Section 32.4 is mathematically elegant but
not very practical. The diﬃculty is in computing T (ℓ)
I
and T (ℓ)
Q , which are needed to
calculate Tℓ(32.80). In principle, we could compute them using matched ﬁlters, but
the impulse responses of these ﬁlters would depend on the carrier frequency. For
this frequency to be tunable, we would need a whole set of matched ﬁlters for all the
diﬀerent carrier frequencies, and this would have dire implications on the required
hardware. Also, designing such ﬁlters can be very tricky when fc is large. Instead,
engineers prefer to convert the received waveform to baseband (Section 7.6) or to
some intermediate frequency, and to then perform the required signal processing
on the converted signal, which does not depend on the carrier frequency.2 There
are numerous diﬀerent designs along these lines, and we cannot possibly analyze
them all. Instead, we shall focus on one and present the mathematical tools that
are needed to analyze it and many of its ilk.
e−i2πfct
Front-End
(nonideal BPF)
hFE
Baseband
(nonideal LPF)
h
Yc(t)
˜Yc(t)
Figure 32.3: Converting the received signal to baseband. The diagram uses com-
plex notation; if h is real (which it usually is), then a real implementation would
be reminiscent of Figure 7.11.
Our design is depicted with complex notation in Figure 32.3. The received signal
is denoted Yc and is assumed to be the sum of a mean signal sc and centered
noise Nc.
For the ISI channel the mean signal is xc ⋆hc, but for the sake of
generality we prefer to denote it sc.
We shall assume that sc is an integrable
signal that is bandlimited to W Hz around the carrier frequency fc. The received
signal Yc is fed to a stable real nonideal bandpass ﬁlter of impulse response hFE.
This ﬁlter serves two purposes. The ﬁrst is to serve as a front-end ﬁlter of the
kind we discussed in Section 26.7 and brieﬂy in Section 26.8. For this purpose it
2Applying the results of Chapter 7 directly is, however, mathematically dubious, because the
noise component of the received signal is neither energy-limited nor integrable. Nevertheless, the
results of Chapter 7 do provide the right intuition; see Appendix D.
824
Intersymbol Interference
is typically required to satisfy
ˆhFE(f) = 1,
|f| −fc
 ≤W
2
(32.97)
so as to pass the mean signal sc unaltered; cf. (26.60). The second is to convert the
received waveform Yc into a (random) passband signal that can be then converted
to baseband. For this purpose we typically require that
ˆhFE(f) = 0,
|f| −fc
 > WFE
2
(32.98a)
for some
0 < WFE
2
< fc.
(32.98b)
The front-end ﬁlter does depend on the carrier frequency, but only very coarsely.
And it is typically not required to reject all the out-of-band noise, so its impulse
response need not be very precise.
The output Yc ⋆hFE of the front-end ﬁlter is next converted to baseband using
the following procedure: it is multiplied by t 	→e−i2πfct, and the result is then fed
to a stable ﬁlter—possibly-complex, but usually real— of impulse response h. We
denote the CSP that the procedure produces ˜Yc so
˜Yc =

t 	→e−i2πfct(Yc ⋆hFE)(t)

⋆h.
(32.99)
This is reminiscent of the procedure described in Proposition 7.6.3 with ˇg0 replaced
by h. (In practice, engineers often choose h to be the impulse response of a stable
ﬁlter that closely resembles an ideal unit-gain lowpass ﬁlter of cutoﬀfrequency
W/2. The ideal unit-gain lowpass ﬁlter itself is, alas, not stable.) For the ﬁlter to
“reject the image,” we shall assume that
ˆh(f) = 0,
|f| > WBB,
(32.100a)
where WBB, in analogy to (7.35b), satisﬁes
0 < WBB ≤2fc −WFE
2
.
(32.100b)
And for the procedure to produce the baseband representation of sc in the absence
of noise, we require that
ˆh(f) = 1,
|f| ≤W
2 .
(32.101)
Our main result is that from the baseband ﬁlter’s output ˜Yc we can recover Tℓusing
a (complex) matched ﬁlter. More precisely, if the front-end ﬁlter satisﬁes (32.97)
and the baseband lowpass ﬁlter satisﬁes (32.101), then
Tℓ=
4
2
N0
#
˜Yc, t 	→pc(t −ℓTs)
$
,
ℓ∈Z.
(32.102)
32.5 From Passband to Baseband
825
32.5.1
Computing Inner Products in Baseband
To derive (32.102) we shall begin by showing that—as long as the front-end ﬁlter
is of unit gain over the frequency band of interest (32.97)— we can compute the
passband inner products T (ℓ)
I
and T (ℓ)
Q
of (32.80) from Yc ⋆hFE.
Proposition 32.5.1 (

Y, gPB

and

Y ⋆hFE, gPB

). Let gPB be a real integrable
signal that is bandlimited to W Hz around the carrier frequency fc, and let hFE be
the real impulse response of a stable ﬁlter satisfying (32.97). If Y is a SP satisfying
sup
t∈R
E

|Y (t)|

< ∞,
(32.103)
then

Y, gPB

=

Y ⋆hFE, gPB

with probability 1.
(32.104)
Proof. Using Lemma 15.7.1 on the associativity of the convolution with the sub-
stitution of hFE for r; of Y for

X(t)

; of ~gPB for h; and of 0 for t, we obtain that
with probability one, the two integrals
 ∞
−∞
Y (−τ)

hFE ⋆~gPB

(τ) dτ
and
 ∞
−∞

Y ⋆hFE

(−σ)~gPB(σ) dσ
(32.105)
are both deﬁned and are equal. The integral on the right in (32.105) is the inner
product on the right in (32.104). And since the front-end ﬁlter has unit gain over
the bandwidth of interest (32.97), hFE ⋆~gPB equals ~gPB, and the integral on the
left in (32.105) is the inner product on the left in (32.104).
Applying Proposition 32.5.1 twice—once by substituting pc,I,ℓfor gPB and once
by substituting pc,Q,ℓfor gPB—we obtain that, with probability one, T (ℓ)
I
and T (ℓ)
Q
of (32.80) can be expressed as
T (ℓ)
I
=
4
1
N0
#
Yc ⋆hFE, pc,I,ℓ
$
,
ℓ∈Z,
(32.106a)
T (ℓ)
Q
=
4
1
N0
#
Yc ⋆hFE, pc,Q,ℓ
$
,
ℓ∈Z,
(32.106b)
where pc,I,ℓand pc,Q,ℓare deﬁned in (32.79). We now proceed to relate these inner
products to the inner products in baseband.
Proposition 32.5.2. Let gBB be the baseband representation of some real integrable
passband signal gPB that is bandlimited to W Hz around the carrier frequency fc.
Let the SP YPB satisfy the condition
sup
t∈R
E
YPB(t)

< ∞,
(32.107)
and let the CSP YBB be given by
YBB =

t 	→e−i2πfct YPB(t)

⋆h,
(32.108)
where h ∈L1 is possibly complex. Then, with probability one,
2 Re

⟨YBB, gBB⟩

=
#
YPB, t 	→2 Re

ei2πfct
gBB ⋆~h∗
(t)
$
.
(32.109)
826
Intersymbol Interference
In particular, since gBB ⋆~h∗is of FT f 	→ˆgBB(f) ˆh∗(f),
	
ˆh(f) = 1,
|f| ≤W
2

=⇒
	
⟨YPB, gPB⟩= 2 Re

⟨YBB, gBB⟩

.
(32.110)
Proof. To prove this result we shall justify the following formal calculation
⟨YBB, gBB⟩=
#
t 	→e−i2πfct YPB(t)

⋆h, gBB
$
(32.111)
=
#
t 	→e−i2πfct YPB(t), gBB ⋆~h∗$
(32.112)
=
#
YPB, t 	→ei2πfct
gBB ⋆~h∗
(t)
$
,
(32.113)
which implies that
2 Re

⟨YBB, gBB⟩

= 2 Re
#
YPB, t 	→ei2πfct
gBB ⋆~h∗
(t)
$
=
#
YPB, t 	→2 Re

ei2πfct
gBB ⋆~h∗
(t)
$
,
where the second equality holds because YPB is real.
In the above calculation (32.111) follows from the deﬁnition of YBB (32.108),
and (32.113) follows from the deﬁnition of the inner product. The second equal-
ity (32.112) follows from the analogue of Lemma 26.10.7 for complex stochastic
processes and can be justiﬁed as follows:
#
t 	→e−i2πfct YPB(t)

⋆h, gBB
$
=
 ∞
−∞
	 ∞
−∞
e−i2πfcτ YPB(τ) h(t −τ) dτ

g∗
BB(t) dt
=
 ∞
−∞
e−i2πfcτ YPB(τ)
 ∞
−∞
h(t −τ) g∗
BB(t) dt dτ
=
 ∞
−∞
e−i2πfcτ YPB(τ)
	 ∞
−∞
h∗(t −τ) gBB(t) dt

∗
dτ
=
 ∞
−∞
e−i2πfcτ YPB(τ)
	 ∞
−∞
~h
∗(τ −t) gBB(t) dt

∗
dτ
=
#
τ 	→e−i2πfcτ YPB(τ), gBB ⋆~h∗$
,
where the swapping of the integration order can be justiﬁed using Fubini’s The-
orem, because—using arguments similar to those that we used in the proof of
Proposition 25.10.1 (ii)—one can show that, with probability one,
 ∞
−∞
 ∞
−∞
YPB(τ) h(t −τ) gBB(t)
 dτ dt < ∞.
We next apply Proposition 32.5.2 with Yc⋆hFE corresponding to YPB and with ˜Yc
therefore corresponding to YBB. Starting from (32.106) and using the explicit form
(32.79) of the baseband representations of pc,I,ℓand pc,Q,ℓ(namely, the mapping
32.6 Additional Reading
827
t 	→pc(t−ℓTs)/
√
2 for the former and t 	→i pc(t−ℓTs)/
√
2 for the latter), we obtain
that, if (32.97) and (32.101) hold, then with probability one
T (ℓ)
I
=
4
1
N0
2 Re
#
˜Yc, t 	→
1
√
2 pc(t −ℓTs)
$
,
ℓ∈Z,
(32.114)
and
T (ℓ)
Q
=
4
1
N0
2 Re
#
˜Yc, t 	→
1
√
2 i pc(t −ℓTs)
$
(32.115)
=
4
1
N0
2 Im
#
˜Yc, t 	→
1
√
2 pc(t −ℓTs)
$
,
(32.116)
so
Tℓ=
4
2
N0
#
˜Yc, t 	→pc(t −ℓTs)
$
,
ℓ∈Z.
(32.117)
This expression for Tℓis the one we promised in (32.102).
We state this as a
theorem:
Theorem 32.5.3. Let pc be some integrable complex signal that is bandlimited to
W/2 Hz, and let the passband signals pc,I,ℓand pc,Q,ℓbe given in (32.79). Let Yc
be some SP satisfying
sup
t∈R
E

|Yc(t)|

< ∞,
and let ˜Yc be given by (32.99), where hFE ∈L1 is real and satisﬁes (32.97) and
h ∈L1 is possibly complex and satisﬁes (32.101). Then Tℓof (32.80) is given
by (32.102).
In some applications the baseband signal ˜Yc is used not only to compute inner
products but also for timing recovery or for estimating other parameters. For such
applications it is important to characterize the distribution of the baseband noise.
This is explored in Appendix D.
32.6
Additional Reading
The literature on communication over linearly-dispersive channels is vast. Good
starting points are (Proakis and Salehi, 2007) and (Barry, Lee, and Messerschmitt,
2004). These texts deal inter alia with suboptimal receiver designs for scenarios
where κmax is too large for the implementation of the Viterbi Algorithm. They
also deal with additional properties of the whitening ﬁlter (such as minimum-
phase) that are sometimes desirable. But please note that some of the results in
the literature do not guarantee the existence of a stable ﬁlter with these additional
properties but only one whose impulse response is square summable.
For more on Dynamic Programming, see (Bertsekas, 2005) and (Cormen, Leiserson,
Rivest, and Stein, 2009).
828
Intersymbol Interference
32.7
Exercises
Exercise 32.1 (The Matched-Filter Bound). Consider uncoded PAM communication with
antipodal signaling over an ISI channel (32.2), (32.3), (32.4). Prove that if ˆ
Xj is any guess
of Xj based on the received waveform Yc, then
Pr
$ ˆ
Xj ̸= Xj
%
≥Q
⎛
⎝
&
2A2 ∥gc ⋆hc∥2
2
N0
⎞
⎠.
(32.118)
Exercise 32.2 (Edges of the Trellis). Consider the trellis of Section 32.3.3 when n > 4,
the constellation X is {±1}, and κmax = 3. Which edges are incident from (leave) the
node (j, s), where j = 4 and s = (+1, −1, −1)? Which edges are incident to (enter) this
node?
Exercise 32.3 (A Known Symbol). Just before you are about to implement the ML
guessing rule for the postulated model of Section 32.3.1, a genie reveals to you that the
symbol Xj⋆is equal to xj⋆. How would you guess the remaining symbols?
Exercise 32.4 (Terminating Symbols). How would you modify the Viterbi Algorithm for
the case where the data D1, . . . , Dk are mapped to X1, . . . , Xk using antipodal signaling
(10.2) and the symbols Xk+1, . . . , Xk+κmax are all +1?
Exercise 32.5 (A Known Data Bit). Consider uncoded PAM over an ISI channel where
the data bits are mapped to symbols using the mapping (10.3) (4-PAM). How would you
modify the guessing rule of Section 32.3 if just before you started decoding you were told
that the ﬁrst data bit is zero?
Exercise 32.6 (No Cursor). We never assumed that in the postulated model ℏ0 is nonzero.
Does the Viterbi Algorithm also work when ℏ0 is zero? How does this aﬀect the algorithm?
Exercise 32.7 (A LPF instead of a Matched Filter). Consider PAM communication over
an ISI channel as in (32.2), (32.3), (32.4) but with a diﬀerent receiver structure: the
received waveform Yc is fed to a stable ﬁlter whose frequency response closely resembles
"
LPFW (see (6.39) and Figure 6.2), and the ﬁlter’s output is sampled at all integer multiples
of Ts to form the sequence
 ˜Tℓ

. Describe the discrete-time channel from the symbols (Xℓ)
to
 ˜Tℓ

.
Exercise 32.8 (Zero-Forcing Equalization). Consider uncoded antipodal PAM commu-
nication over an ISI channel as in (32.2), (32.3), (32.4) with the additional assumption
that the time shifts of the pulse shape gc by integer multiples of Ts are orthonormal. A
zero-forcing equalizer passes the received signal Yc through a stable ﬁlter whose frequency
response over the band [−W, W ] is the reciprocal of ˆhc. It then guesses Xℓby comparing
to zero the inner product between the ﬁlter’s output and t →gc(t −ℓTs). Calculate the
probability of error of the zero-forcing equalizer. Explain why this approach is unsuitable
when the frequency response of hc has a small magnitude at some frequencies inside the
band [−W, W ].
32.7 Exercises
829
Exercise 32.9 (ISI with Colored Noise). Propose a decoder for uncoded PAM communi-
cation over an ISI channel that is similar to the one described in Section 32.1 but where
the noise

Nc(t)

is not white: it is a centered measurable stationary Gaussian SP of
PSD SNN that can be whitened with respect to W (Deﬁnition 26.10.1).
Exercise 32.10 (Two ISI Channels). Suppose the waveform xc is transmitted over two
linearly-dispersive channels, each of which is as depicted in Figure 32.1. The noises on the
two channels are independent, and the two ﬁlters are diﬀerent: their impulse responses
are hc,1 and hc,2. Based on the two outputs Yc,1, Yc,2 we wish to guess the data. How
would you adapt Section 32.3 to this setting?
Exercise 32.11 (Allpass Channels). For the setting of Section 32.2, show that if the
frequency response of the channel ﬁlter is of unit magnitude over the bandwidth of interest,
i.e.,
ˆhc(f)
 = 1,
|f| ≤W,
then any performance that can be achieved on the ISI channel can also be achieved on
the channel where Yc = xc + Nc and vice versa.
Hint: Consider ﬁltering Yc.
Appendix A
On the Fourier Series
A.1
Introduction and Preliminaries
We survey here some of the results on the Fourier Series that are used in the book.
The Fourier Series has numerous other applications that we do not touch upon.
For those we refer the reader to (Katznelson, 2004), (Dym and McKean, 1972),
and (K¨orner, 1988).
To simplify typography, we denote the half-open interval [−1/2, 1/2) by I:
I ≜
'
θ ∈R : −1
2 ≤θ < 1
2
(
.
(A.1)
Deﬁnition A.1.1 (Fourier Series Coeﬃcient). The η-th Fourier Series Coef-
ﬁcient of an integrable function g: I →C is denoted by ˆg(η) and is deﬁned for
every integer η by
ˆg(η) ≜

I
g(θ) e−i2πηθ dθ.
(A.2)
The periodic extension of the function g: I →C is denoted by gP : R →C and
is deﬁned as
gP(n + θ) = g(θ),

n ∈Z, θ ∈I

.
(A.3)
We say that g: I →C is periodically continuous if its periodic extension gP is
continuous, i.e., if g(·) is continuous in I and if, additionally,
lim
θ↑1/2 g(θ) = g(−1/2).
(A.4)
A degree-n trigonometric polynomial is a function of the form
θ 	→
n

η=−n
aη ei2πηθ,
θ ∈R,
(A.5)
where an and a−n are not both zero. Note that if p(·) is a trigonometric polynomial,
then p(θ + 1) = p(θ) for all θ ∈R.
830
A.1 Introduction and Preliminaries
831
If g: I →C is integrable, and if p(·) is a trigonometric polynomial, then we deﬁne
the convolution g ⋆p at every θ ∈R as
(g ⋆p)(θ) =

I
g(ϑ) p(θ −ϑ) dϑ
(A.6)
=

I
p(ϑ) gP(θ −ϑ) dϑ.
(A.7)
Lemma A.1.2 (Convolution with a Trigonometric Polynomial). The convolution
of an integrable function g: I →C with the trigonometric polynomial
θ 	→
n

η=−n
aη ei2πηθ
(A.8)
is the trigonometric polynomial
θ 	→
n

η=−n
ˆg(η) aη ei2πηθ,
θ ∈R.
(A.9)
Proof. Denote the trigonometric polynomial in (A.8) by p(·). By swapping sum-
mation and integration we obtain
(g ⋆p)(θ) =

I
g(ϑ) p(θ −ϑ) dϑ
=

I
g(ϑ)
n

η=−n
aη ei2πη(θ−ϑ) dϑ
=
n

η=−n

I
g(ϑ) aη ei2πη(θ−ϑ) dϑ
=
n

η=−n
aη ei2πηθ

I
g(ϑ) e−i2πηϑ dϑ
=
n

η=−n
aη ei2πηθˆg(η),
θ ∈R.
Deﬁnition A.1.3 (Fej´er’s Kernel). Fej´er’s degree-n kernel kn is the trigono-
metric polynomial
kn(θ) =
n

η=−n
	
1 −
|η|
n + 1

ei2πηθ
(A.10a)
=
⎧
⎪
⎨
⎪
⎩
n + 1
if θ ∈Z,
1
n+1
	
sin((n+1)πθ)
sin(πθ)

2
if θ ∈R \ Z.
(A.10b)
The key properties of Fej´er’s kernel are that it is nonnegative
kn(θ) ≥0,
θ ∈R;
(A.11a)
832
On the Fourier Series
that it integrates over I to one

I
kn(θ) dθ = 1;
(A.11b)
and that for every ﬁxed 0 < δ < 1/2
lim
n→∞

δ<|θ|< 1
2
kn(θ) dθ = 0.
(A.11c)
Here (A.11a) follows from (A.10b); (A.11b) follows from (A.10a) by term-by-term
integration over I; and (A.11c) follows from the inequality
kn(θ) ≤
1
n + 1
	
1
sin πδ

2
,
δ ≤|θ| ≤1
2,
which follows from (A.10b) by upper-bounding the numerator by 1 and by using
the monotonicity of sin2(πθ) in |θ| ∈[0, 1/2].
For an integrable function g: I →C, we deﬁne for every n ∈N and θ ∈R
σn(g, θ) ≜(g ⋆kn)(θ)
(A.12)
=
n

η=−n
	
1 −
|η|
n + 1

ˆg(η) ei2πηθ,
(A.13)
where the second equality follows from (A.10a) and Lemma A.1.2. We also deﬁne
for g: R →C or g: I →C
∥g∥I,1 =

I
|g(θ)| dθ.
(A.14)
Finally, for every function h: I →C and ϑ ∈R we deﬁne the mapping hϑ : R →C
as
hϑ : θ 	→hP(θ −ϑ).
(A.15)
A.2
Reconstruction in L1
Lemma A.2.1. If g: R →C is integrable over I and g(θ + 1) = g(θ) for every
θ ∈R, then
lim
ϑ→0

I
g(θ) −g(θ −ϑ)
 dθ = 0.
(A.16)
Proof. This is easy to see if g is continuous, because in this case it is also uniformly
continuous (due to its periodicity). The general result follows from this case by
picking a periodic continuous function h that approximates g in the sense that
∥g −h∥I,1 < ϵ/2; by computing
∥g −gϑ∥I,1 = ∥g −h + h −gϑ∥I,1
= ∥g −h + h −hϑ + hϑ −gϑ∥I,1
≤∥g −h∥I,1 + ∥h −hϑ∥I,1 + ∥hϑ −gϑ∥I,1
= ∥g −h∥I,1 + ∥h −hϑ∥I,1 + ∥h −g∥I,1
≤ϵ + ∥h −hϑ∥I,1 ;
(A.17)
and by then applying the result to h, which is continuous.
A.2 Reconstruction in L1
833
Theorem A.2.2 (Reconstruction in L1). If g: I →C is integrable, then
lim
n→∞

I
g(θ) −σn(g, θ)
 dθ = 0.
(A.18)
Proof. Let gP be the periodic extension of g. Then for every δ ∈(0, 1/2),

I
g(θ) −σn(g, θ)
 dθ =

I
gP(θ) −

I
gP(θ −ϑ) kn(ϑ) dϑ
 dθ
=

I


I
kn(ϑ)

gP(θ) −gP(θ −ϑ)

dϑ
 dθ
=

I

 δ
−δ
+

δ<|ϑ|< 1
2
kn(ϑ)

gP(θ) −gP(θ −ϑ)

dϑ
 dθ, (A.19)
where the ﬁrst equality follows from the deﬁnition of σn(g, θ) (A.12), and where the
second equality follows from (A.11b). We now bound the two integrals in (A.19)
separately:

I

 δ
−δ
kn(ϑ)

gP(θ) −gP(θ −ϑ)

dϑ
 dθ
≤

I
 δ
−δ
kn(ϑ)
gP(θ) −gP(θ −ϑ)
 dϑ dθ
(A.20)
=
 δ
−δ

I
kn(ϑ)
gP(θ) −gP(θ −ϑ)
 dθ dϑ
=
 δ
−δ
kn(ϑ)

I
gP(θ) −gP(θ −ϑ)
 dθ dϑ
≤
 δ
−δ
kn(ϑ) max
|ϑ′|≤δ
1
I
gP(θ) −gP(θ −ϑ′)
 dθ
2
dϑ
≤

I
kn(ϑ) max
|ϑ′|≤δ
1
I
gP(θ) −gP(θ −ϑ′)
 dθ
2
dϑ
= max
|ϑ|≤δ

I
gP(θ) −gP(θ −ϑ)
 dθ,
(A.21)
where the ﬁrst inequality follows from the Triangle Inequality for Integrals (Propo-
sition 2.4.1) and the nonnegativity of kn(·) (A.11a), and where the last equality
follows because kn(·) integrates to one (A.11b).
The second integral in (A.19) is bounded as follows:

I

δ<|ϑ|< 1
2
kn(ϑ)
gP(θ) −gP(θ −ϑ)
 dϑ dθ
=

δ<|ϑ|< 1
2
kn(ϑ)

I
gP(θ) −gP(θ −ϑ)
 dθ dϑ
≤max
ϑ′∈I
1
I
gP(θ) −gP(θ −ϑ′)
 dθ
2 
δ<|ϑ|< 1
2
kn(ϑ) dϑ
≤2 ∥g∥I,1

δ<|ϑ|< 1
2
kn(ϑ) dϑ.
(A.22)
834
On the Fourier Series
From (A.19), (A.21), and (A.22) we obtain

I
g(θ) −σn(g, θ)
 dθ ≤max
|ϑ|≤δ

I
gP(θ −ϑ) −gP(θ)
 dθ
+ 2 ∥g∥I,1

δ<|ϑ|< 1
2
kn(ϑ) dϑ.
(A.23)
Inequality (A.23) establishes the theorem as follows. For every ϵ > 0 we can ﬁnd
by Lemma A.2.1 some δ > 0 such that
max
|ϑ|≤δ

I
gP(θ −ϑ) −gP(θ)
 dθ < ϵ,
(A.24)
and keeping this δ > 0 ﬁxed we have by (A.11c)
lim
n→∞2 ∥g∥I,1

δ<|ϑ|< 1
2
kn(ϑ) dϑ = 0.
(A.25)
It thus follows from (A.23), (A.24), and (A.25) that
lim
n→∞

I
g(θ) −σn(g, θ)
 dθ < ϵ,
(A.26)
from which the theorem follows because ϵ > 0 was arbitrary.
From Theorem A.2.2 we obtain:
Theorem A.2.3 (Uniqueness Theorem). Let g1, g2 : I →C be integrable. If
ˆg1(η) = ˆg2(η),
η ∈Z,
(A.27)
then g1 and g2 are equal except on a set of Lebesgue measure zero.
Proof. Let g = g1 −g2. By (A.27)
ˆg(η) = 0,
η ∈Z,
(A.28)
and consequently, by (A.13), σn(g, θ) = 0 for every n ∈N and θ ∈I. By Theo-
rem A.2.2
lim
n→∞

I
g(θ) −σn(g, θ)
 dθ = 0,
(A.29)
which combines with (A.28) to establish that

I
|g(θ)| dθ = 0.
Thus, g is zero except on a set of Lebesgue measure zero (Proposition 2.5.3 (i)),
and the result follows by recalling that g = g1 −g2.
Theorem A.2.4 (Riemann-Lebesgue Lemma). If g: I →C is integrable, then
lim
|η|→∞ˆg(η) = 0.
(A.30)
A.3 Geometric Considerations
835
Proof. Given any ϵ > 0, let p be a degree-n trigonometric polynomial satisfying

I
g(θ) −p(θ)
 dθ < ϵ.
(A.31)
(Such a trigonometric polynomial exists for some n ∈N by Theorem A.2.2). Ex-
pressing g as (g −p) + p and using the linearity of the Fourier Series Coeﬃcients
we obtain for every integer η whose magnitude exceeds the degree n of p
ˆg(η)
 =
 
(g −p)(η) + ˆp(η)

=
 
(g −p)(η)

≤

I
g(θ) −p(θ)
 dθ
< ϵ,
(A.32)
where the equality in the ﬁrst line follows from the linearity of the Fourier Series
Coeﬃcient; the equality in the second line holds because |η| is larger than the
degree n of p; the inequality in the third line holds because for every integrable
h: I →C we have
ˆh(η)
 =


I
h(θ) e−i2πηθ dθ

≤

I
h(θ) e−i2πηθ dθ
=

I
|h(θ)| dθ,
η ∈Z;
and where the inequality in the last line of (A.32) follows from (A.31).
A.3
Geometric Considerations
Every square-integrable function that is zero outside the interval [−1/2, 1/2] is also
integrable (Proposition 3.4.3). For such functions we can discuss the inner product
and some of the related geometry. The main result is the following.
Theorem A.3.1 (Complete Orthonormal System). The bi-inﬁnite sequence of
functions . . . , φ−1, φ0, φ1, . . . deﬁned for every η ∈Z by
φη(θ) = ei2πηθ I

θ ∈I

,
θ ∈R
forms a complete orthonormal system (CONS) for the subspace of L2 consisting of
those energy-limited functions that are zero outside the interval I.
Proof. The orthonormality follows by direct calculation

I
ei2πηθ e−i2πη′θ dθ = I{η = η′},
η, η′ ∈Z.
(A.33)
836
On the Fourier Series
To show completeness it suﬃces by Proposition 8.6.5 (ii) to show that a square-
integrable function g: I →C that satisﬁes
⟨g, φη⟩= 0,
η ∈Z
(A.34)
must be equal to the all-zero function except on a subset of I of Lebesgue measure
zero. To show this, we note that
⟨g, φη⟩= ˆg(η),
η ∈Z,
(A.35)
so (A.34) is equivalent to
ˆg(η) = 0,
η ∈Z,
(A.36)
and hence, by Theorem A.2.3, g must be zero except on a set of Lebesgue measure
zero.
Recalling Deﬁnition 8.2.1 and Proposition 8.2.2 (d) we obtain that, because the
functions . . . , φ−1, φ0, φ1, . . . form a CONS and because ⟨g, φη⟩= ˆg(η), the fol-
lowing holds:
Theorem A.3.2. Let g, h: I →C be square integrable. Then

I
|g(θ)|2 dθ =
∞

η=−∞
|ˆg(η)|2
(A.37)
and

I
g(θ) h∗(θ) dθ =
∞

η=−∞
ˆg(η) ˆh∗(η).
(A.38)
There is nothing special about the interval I, and, indeed, by scaling we obtain:
Theorem A.3.3. Let S be positive.
(i) The bi-inﬁnite sequence of functions deﬁned for every η ∈Z by
s 	→
1
√
S
ei2πηs/S I
1
−S
2 ≤s < S
2
2
,
s ∈R
(A.39)
forms a CONS for the class of square-integrable functions that are zero out-
side the interval [−S/2, S/2).
(ii) If g is square integrable and zero outside the interval [−S/2, S/2), then
 S/2
−S/2
|g(ξ)|2 dξ =
∞

η=−∞

 S/2
−S/2
g(s) 1
√
S
e−i2πηs/S ds

2
.
(A.40)
(iii) If g, h: R →C are square integrable and zero outside the interval [−S/2, S/2),
then
 S/2
−S/2
g(ξ) h∗(ξ) dξ
=
∞

η=−∞
	 S/2
−S/2
g(s) 1
√
S
e−i2πηs/S ds

	 S/2
−S/2
h(s) 1
√
S
e−i2πηs/S ds

∗
.
A.3 Geometric Considerations
837
Note A.3.4. The theorem continues to hold if we replace the half-open interval
with the open interval (−S/2, S/2) or with the closed interval [−S/2, S/2], because
the integrals are insensitive to these replacements.
Note A.3.5. We refer to
 S/2
−S/2
g(s) 1
√
S
e−i2πηs/S ds
as the η-th Fourier Series Coeﬃcient of g with respect to the interval
[−S/2, S/2).
Lemma A.3.6 (A Mini Parseval Theorem).
(i) If
x(t) =
 W
−W
g(f) ei2πft df,
t ∈R,
(A.41)
where g: R →C satisﬁes
 W
−W
|g(f)|2 df < ∞,
(A.42)
then
 ∞
−∞
|x(t)|2 dt =
 W
−W
|g(f)|2 df.
(A.43)
(ii) If for both ν = 1 and ν = 2
xν(t) =
 W
−W
gν(f) ei2πft df,
t ∈R,
(A.44)
where the functions g1, g2 : R →C satisfy
 W
−W
|gν(f)|2 df < ∞,
ν = 1, 2,
(A.45)
then
 ∞
−∞
x1(t) x∗
2(t) dt =
 W
−W
g1(f) g∗
2(f) df.
(A.46)
Proof. We ﬁrst prove Part (i). We begin by expressing the energy in x in the form
 ∞
−∞
|x(t)|2 dt =
∞

ℓ=−∞
 −
ℓ
2W +
1
2W
−
ℓ
2W
|x(t)|2 dt
=
∞

ℓ=−∞

1
2W
0
x

α −
ℓ
2W

2
dα
=

1
2W
0
∞

ℓ=−∞
x

α −
ℓ
2W

2
dα,
(A.47)
838
On the Fourier Series
where in the second equality we changed the integration variable to α ≜t+ℓ/(2W);
and where the third equality follows from Fubini’s Theorem and the nonnegativity
of the integrand. The proof of Part (i) will follow from (A.47) once we show that
for every α ∈R
∞

ℓ=−∞
x

α −
ℓ
2W

2
= 2W
 W
−W
|g(f)|2 df.
(A.48)
This can be shown by noting that by (A.41)
1
√
2W
x

α −
ℓ
2W

=
 W
−W
1
√
2W
e−i2πf
ℓ
2W ei2πfαg(f) df,
so (2W)−1/2x

α −ℓ/(2W)

is the ℓ-th Fourier Series Coeﬃcient of the mapping
f 	→ei2πfαg(f) with respect to the interval [−W, W) and consequently
∞

ℓ=−∞

1
√
2W
x

α −
ℓ
2W

2
=
 W
−W
ei2πfαg(f)
2 df
=
 W
−W
|g(f)|2 df,
where the ﬁrst equality follows from Theorem A.3.3 (ii) and the second because
the magnitude of ei2πfα is one.
To prove Part (ii) we note that by opening the square and then applying Part (i)
to the function βx1 + x2 we obtain for every β ∈C
|β|2
 ∞
−∞
x1(t)
2 dt +
 ∞
−∞
x2(t)
2 dt + 2 Re
	
β
 ∞
−∞
x1(t) x∗
2(t) dt

=
 ∞
−∞
βx1(t) + x2(t)
2 dt
=
 W
−W
βg1(f) + g2(f)
2 df
= |β|2
 W
−W
g1(f)
2 df +
 W
−W
g2(f)
2 df + 2 Re
	
β
 W
−W
g1(f) g∗
2(f) df

.
Consequently, upon applying Part (i) to x1 and to x2, we obtain
Re
	
β
 ∞
−∞
x1(t) x∗
2(t) dt

= Re
	
β
 W
−W
g1(f) g∗
2(f) df

,
β ∈C,
which implies that
 ∞
−∞
x1(t) x∗
2(t) dt =
 W
−W
g1(f) g∗
2(f) df.
A.4 Pointwise Reconstruction
839
Corollary A.3.7.
(i) Let y: R →C be of ﬁnite energy, and let T > 0 be arbitrary. Let
˜g(f) =
 T
−T
y(t) e−i2πft dt,
f ∈R.
Then
 T
−T
|y(t)|2 dt =
 ∞
−∞
|˜g(f)|2 df.
(ii) Let the signals x1, x2 : R →C be of ﬁnite energy, and let T > 0. Deﬁne
gν(f) =
 T
−T
xν(t) e−i2πft dt,

ν = 1, 2,
f ∈R

.
Then
 T
−T
x1(t) x∗
2(t) dt =
 ∞
−∞
g1(f) g∗
2(f) df.
Proof. Part (i) follows from Lemma A.3.6 (i) by substituting T for W; by substi-
tuting ~y for g; and by swapping the dummy variables f and t. Part (ii) follows
analogously.
A.4
Pointwise Reconstruction
If g: I →C is periodically continuous, then we can reconstruct its value at every
point from its Fourier Series Coeﬃcients:
Theorem A.4.1 (Reconstructing Periodically Continuous Functions). Let the
function g: I →C be periodically continuous. Then
lim
n→∞max
θ∈I
g(θ) −σn(g, θ)

= 0.
(A.49)
Proof. Let gP denote the periodic extension of g. Then for every θ ∈I,
g(θ) −σn(g, θ) = g(θ) −

I
kn(ϑ) gP(θ −ϑ) dϑ
=

I
kn(ϑ)

gP(θ) −gP(θ −ϑ)

dϑ,
(A.50)
where the ﬁrst equality follows from the deﬁnition of σn(g, θ) (A.12) and the second
from (A.11b). Consequently, for every θ ∈I,
g(θ) −σn(g, θ)

≤

I
kn(ϑ)
gP(θ) −gP(θ −ϑ)
 dϑ
=
 δ
−δ
+

δ<|ϑ|< 1
2
kn(ϑ)
gP(θ) −gP(θ −ϑ)
 dϑ,
0 ≤δ < 1
2.
(A.51)
840
On the Fourier Series
We next treat the two integrals separately. For the ﬁrst we have for every θ ∈I
and every 0 ≤δ < 1/2,
 δ
−δ
kn(ϑ)
gP(θ) −gP(θ −ϑ)
 dϑ ≤max
|ϑ′|≤δ
gP(θ) −gP(θ −ϑ′)
  δ
−δ
kn(ϑ) dϑ
≤max
|ϑ|≤δ
gP(θ) −gP(θ −ϑ)

,
(A.52)
where the ﬁrst inequality follows from the the nonnegativity of kn(·) (A.11a), and
where the second inequality follows because kn(·) is nonnegative and integrates
over I to one (A.11b). For the second integral in (A.51) we have for every θ ∈I
and every 0 ≤δ < 1/2,

δ<|ϑ|< 1
2
kn(ϑ)
gP(θ) −gP(θ −ϑ)
 dϑ ≤2 max
θ′∈I {|g(θ′)|}

δ<|ϑ|< 1
2
kn(ϑ) dϑ, (A.53)
where the maximum on the RHS is ﬁnite because g is periodically continuous.
Combining (A.51), (A.52), and (A.53) we obtain for every 0 ≤δ < 1/2
max
θ∈I
g(θ) −σn(g, θ)

≤max
θ∈I max
|ϑ|≤δ
gP(θ) −gP(θ −ϑ)

+ 2 max
θ′∈I
g(θ′)
 
δ<|ϑ|< 1
2
kn(ϑ) dϑ.
(A.54)
Because g(·) is periodically continuous, it follows that its periodic extension gP is
uniformly continuous. Consequently, for every ϵ > 0 we can ﬁnd some δ > 0 such
that
max
|ϑ|≤δ
gP(θ) −gP(θ −ϑ)
 < ϵ,
θ ∈I.
(A.55)
By letting n tend to inﬁnity in (A.54), we obtain from (A.11c) and (A.55)
lim
n→∞max
θ∈I
g(θ) −σn(g, θ)

< ϵ,
which establishes the result because ϵ > 0 was arbitrary.
As a corollary we obtain:
Corollary A.4.2 (Weierstrass’s Approximation Theorem). Every periodically con-
tinuous function from I to C can be approximated uniformly using trigonometric
polynomials.
Appendix B
On the Discrete-Time Fourier Transform
The Discrete-Time Fourier Transform (DTFT) is to absolutely summable bi-inﬁnite
sequences what the Fourier Transform is to integrable functions. Here we summa-
rize its basic properties.
Recall that a bi-inﬁnite sequence . . . , a−1, a0, a1, . . . is absolutely summable if
∞

ν=−∞
|aν| < ∞,
and that the set containing all such sequences is denoted ℓ1. Recall also that I
denotes the half-closed interval
I =
'
θ ∈R : −1
2 ≤θ < 1
2
(
and that its closure ¯I is thus
¯I =
'
θ ∈R : −1
2 ≤θ ≤1
2
(
.
Deﬁnition B.1 (The Discrete-Time Fourier Transform). The Discrete-Time
Fourier Transform (DTFT) of a complex absolutely summable bi-inﬁnite se-
quence (aν) is the mapping ua: R →C deﬁned by
ua: θ 	→
∞

ν=−∞
aν e−i2πνθ.
(B.1)
The key properties of the DTFT are summarized below.
Theorem B.2 (Properties of the DTFT). Let (aν) be a complex absolutely summable
bi-inﬁnite sequence.
(i) The sum in (B.1) converges uniformly on R.
(ii) The function ua(·) is continuous and periodic with
ua(θ + 1) = ua(θ),
θ ∈R.
841
842
On the Discrete-Time Fourier Transform
(iii) The sequence (aν) can be recovered from its DTFT ua via
aν =

I
ua(θ) ei2πνθ dθ,
ν ∈Z.
(B.2)
Thus, a−ν is the ν-th Fourier Series Coeﬃcient of ua(·)
a−ν = ˆua(ν),
ν ∈Z.
(B.3)
(iv) If (bν) is another absolutely summable bi-inﬁnite sequence, then
∞

ν=−∞
aν b∗
ν =

I
ua(θ) ub∗(θ) dθ
(B.4)
and, in particular,
∞

ν=−∞
|aν|2 =

I
ua(θ)
2 dθ.
(B.5)
(v) If (bν) is as above, then the DTFT of (aν)⋆(bν) is the mapping θ 	→ua(θ)ub(θ):
Ő
a ⋆b(θ) = ua(θ) ub(θ),
θ ∈R.
(B.6)
Proof. Part (i) follows by observing that, since the complex exponentials are of
modulus one,
aν e−i2πνθ ≤|aν|,
θ ∈R
and, since the RHS of the above is summable, the uniform convergence of the sum
follows from Weierstrass’s test (Rudin, 1976, Ch. 7, Theorem 7.10).
As to Part (ii), the continuity of ua follows from the continuity of the complex
exponentials and from Part (i) as follows: the former implies that the partial sums
are continuous, the latter implies that they converge uniformly, and if a sequence
of continuous functions converges uniformly, then the limit must be a continuous
function (Rudin, 1976, Theorem 7.12). The periodicity follows from the periodicity
of the complex exponentials:
e−i2πν(θ+1) = e−i2πνθ,
(θ ∈R, ν ∈Z).
To verify Part (iii) we calculate:

I
ua(θ) ei2πνθ dθ =

I
	
∞

ν′=−∞
aν′ e−i2πν′θ

ei2πνθ dθ
=

I
∞

ν′=−∞
aν′ ei2π(ν−ν′)θ dθ
=
∞

ν′=−∞
aν′

I
ei2π(ν−ν′)θ dθ
=
∞

ν′=−∞
aν′ I{ν′ = ν}
= aν,
ν ∈Z,
On the Discrete-Time Fourier Transform
843
where the term-by-term integration is justiﬁed by the uniform convergence (Rudin,
1976, Corollary to Theorem 7.16), and where we have also used (A.33).
We next turn to Part (iv). By Part (iii) we can express a−ν as the ν-th Fourier
Series Coeﬃcient of ua, i.e.,
a−ν = ˆua(ν),
ν ∈Z.
Likewise,
b−ν = ˆub(ν),
ν ∈Z,
so Part (iv) follows from Theorem A.3.2.
As to Part (v), we calculate the DTFT of the convolution as follows:
∞

η=−∞

(aν) ⋆(bν)

η e−i2πηθ =
∞

η=−∞
	
∞

ν=−∞
aνbη−ν

e−i2πηθ
=
∞

m=−∞
bm
∞

ν=−∞
aν e−i2π(m+ν)θ
=
∞

m=−∞
bm e−i2πmθ
∞

ν=−∞
aν e−i2πνθ
= ub(θ) ua(θ),
θ ∈R,
where we have deﬁned m as η −ν.
Every absolutely summable sequence is square summable (31.1), but some se-
quences are square summable but not absolutely summable.1 The next theorem
allows us to deﬁne the discrete-time analog of the L2-Fourier Transform (Sec-
tion 6.2.3): it allows us to deﬁne the DTFT of any square-summable sequence,
even if is not absolutely summable.
Theorem B.3 (L2-DTFT).
(i) If (aν) is a square-summable bi-inﬁnite complex sequence, then there exists a
function g: R →C such that
g(θ + 1) = g(θ),
θ ∈R,
(B.7a)

I
|g(θ)|2 dθ < ∞,
(B.7b)
and
aν =

I
g(θ) ei2πνθ dθ,
ν ∈Z.
(B.7c)
Moreover,
lim
n→∞

I

n

ν=−n
aν e−i2πνθ −g(θ)

2
dθ = 0.
(B.7d)
1An example of a sequence in ℓ2 but not ℓ1 is ν →1/(|ν| + 1).
844
On the Discrete-Time Fourier Transform
The function g is unique in the sense that two functions that satisfy (B.7)
can diﬀer only on a set of Lebesgue measure zero. Consequently, if (aν) is
additionally absolutely summable, then g(·) must be indistinguishable from
the mapping
θ 	→
∞

ν=−∞
aν e−i2πνθ.
The equivalence class of g(·) is called the L2-Discrete-Time Fourier Trans-
form or L2-DTFT of (aν).
(ii) If g: R →C is any function satisfying (B.7a) and (B.7b), and if we deﬁne
the sequence (aν) as in (B.7c), then (aν) is square summable.
(iii) If g(·) is the L2-DTFT of the square-summable bi-inﬁnite sequence (aν), and
if h(·) is the L2-DTFT of the square-summable bi-inﬁnite sequence (bν), then
∞

ν=−∞
aνb∗
ν =

I
g(θ) h∗(θ) dθ.
Proof. See (Katznelson, 2004, Chapter I, Section 5.5).
Appendix C
Positive Deﬁnite Functions
Complex positive deﬁnite functions appear inter alia as the characteristic func-
tions of random variables (Sections 19.7.1 and 17.3.6) and as the autocovariance
functions of complex stochastic processes. Real positive deﬁnite functions appear
as the characteristic functions of symmetric RVs or as the autocovariance functions
of (real) SPs.
Deﬁnition C.1 (Positive Deﬁnite Function). A function h: R →C is said to be
positive deﬁnite if for every positive integer n and all choices of x1, . . . , xn ∈R
and α1, . . . , αn ∈C
n

j=1
n

k=1
αj α∗
k h(xj −xk) ≥0.
(C.1)
Allowing the coeﬃcients α1, . . . , αn in (C.1) to be complex is not very natural when
h(·) is real. The following proposition shows that if h(·) is real and symmetric, then
it suﬃces to establish (C.1) for real α’s.
Proposition C.2 (Real Positive Deﬁnite Functions). A real function h: R →R
is positive deﬁnite if, and only if, it satisﬁes the following two conditions: it is
symmetric (i.e., ~h = h) and (C.1) holds for all x1, . . . , xn ∈R and α1, . . . , αn ∈R.
The basic properties of positive deﬁnite functions are summarized below (Feller,
1971, Chapter XIX, Section 2, Lemma 3).
Proposition C.3 (Basic Properties). If h: R →C is positive deﬁnite, then
(i) it is conjugate symmetric
h(−x) = h∗(x),
x ∈R;
(ii) its value at zero h(0) is nonnegative (and hence real)
h(0) ≥0;
(iii) it attains its maximum at zero in the sense that
|h(x)| ≤h(0),
x ∈R;
845
846
Positive Deﬁnite Functions
(iv) if h(·) is continuous at the origin, then it is continuous everywhere; and
(v) it satisﬁes
h(x) −h(y)
2 ≤2 h(0)

h(0) −Re(h(x −y))

,
x, y ∈R.
Theorem C.4 (Bochner). If h: R →C is a continuous positive deﬁnite function,
then h is proportional to a characteristic function of some RV V in the sense that
h(x) = h(0) · E

ei2πxV 
,
x ∈R.
If additionally h is real-valued, then it can be expressed as above with the RV V
having a symmetric distribution.
Since the Inverse Fourier-Stieltjes Transform is injective (Rudin, 1962, Chapter 1,
Section 1.3, Theorem 1.3.6), Bochner’s Theorem has the following corollary.
Corollary C.5. If h: R →C is a positive deﬁnite function that is equal to the IFT
of some integrable function S: R →C, then S(·) must be nonnegative outside a set
of Lebesgue measure zero.
Theorem C.6 (Riesz-Crum).
(i) Every Lebesgue measurable positive deﬁnite function h: R →C can be ex-
pressed uniquely as
h(x) = hc(x) + hs(x),
x ∈R,
where hc(·) is a positive deﬁnite function that is continuous, and hs(·) is a
positive deﬁnite function that is zero outside a set of Lebesgue measure zero.
(ii) If h is real valued, then so are hc and hs; if h is integrable, then so is hc
(and hs); if h is symmetric (i.e., ~h = h) then so are hc and hs.
Proof. See Crum (1956).
If a nonzero positive deﬁnite function is continuous and also integrable, then it is
proportional to the characteristic function of a RV with a density, and the density
can be computed using the Fourier Transform (Feller, 1971, Chapter XV, Section 3,
Theorem 3, and Chapter XIX, Section 2, Lemma 1). Hence:
Proposition C.7 (Integrable Positive Deﬁnite Functions). A continuous integrable
positive deﬁnite function is equal to the IFT of its FT.
Dropping the continuity assumption yields a slightly weaker conclusion:
Positive Deﬁnite Functions
847
Corollary C.8. Let h: R →C be a positive deﬁnite integrable function, and let hc
and hs be as in Theorem C.6 (i).
(i) The IFT of the FT of h is equal to hc
ˇˆh = hc
(C.2)
and is thus indistinguishable from h
ˇˆh ≡h.
(C.3)
(ii) The FT of h is nonnegative
ˆh(f) ≥0,
f ∈R.
(C.4)
Proof. Since h = hc+hs and hs is zero outside a set of Lebesgue measure zero, the
functions h and hc are indistinguishable and thus of identical Fourier Transforms
ˆh = ˆhc.
(C.5)
To prove (C.2) we note that (C.5) implies that the IFT of the FT of h is equal to
the IFT of the FT of hc, and the latter is equal to hc by Proposition C.7.
From (C.5) we infer that to establish (C.4) it suﬃces to establish that
ˆhc(f) ≥0,
f ∈R.
(C.6)
This is trivial when hc is zero. When it is not, we can establish (C.6) by applying
Bochner’s Theorem to hc and by using the inversion formula for characteristic func-
tions (Shiryaev, 1996, Chapter II, § 12, Theorem 3) or (Feller, 1971, Chapter XV,
Section 3, Theorem 3).
Appendix D
The Baseband Representation of Passband
Stochastic Processes
Many of the results of Chapter 7 on the baseband representation of passband signals
also apply to stationary stochastic processes. But the proofs must be altered to
avoid applying the Fourier Transform to sample-paths, because those are typically
neither integrable nor of ﬁnite energy.
To state the results we need to extend the deﬁnition of a Gaussian CSP (Deﬁni-
tion 31.4.1) and of a proper CSP (Deﬁnition 17.5.4) from discrete time to contin-
uous time. The extensions are obvious but are provided nonetheless.
Deﬁnition D.1 (Gaussian, Proper, and Circularly-Symmetric CSP). A continuous-
time CSP

Z(t), t ∈R

is said to be Gaussian if every complex random vector
of the form

Z(t1), . . . Z(tn)
T,
where n ∈N and t1, . . . , tn ∈R, is a complex Gaussian vector (Deﬁnition 24.3.6).
It is said to be proper if every such complex random vector is proper (Deﬁni-
tion 17.4.1). And it is said to be circularly-symmetric if every such complex
random vector is circularly-symmetric (Deﬁnition 24.3.2).
Since a complex Gaussian vector is proper if, and only if, it is circularly-symmetric
(Proposition 24.3.11) we have:
Note D.2. A Gaussian CSP is proper if, and only if, it is circularly-symmetric.
We shall also need the deﬁnition of a WSS CSP in continuous time (the counterpart
to Deﬁnition 17.5.6).
Deﬁnition D.3 (WSS Continuous-Time CSP). A continuous-time CSP

Z(t)

is
said to be wide-sense stationary if it is of ﬁnite variance; the mean E[Z(t)] does
not depend on t; and Cov[Z(t + τ), Z(t)] does not depend on t. Its autocovariance
function KZZ is the mapping τ 	→Cov[Z(t + τ), Z(t)]. The CSP or the autocovari-
ance function is said to be of PSD SZZ if SZZ is nonnegative, integrable, and its
IFT is KZZ.
Using Proposition 24.3.7 we can mimick the proof of Proposition 25.5.1 to conclude:
848
The Baseband Representation of Passband Stochastic Processes
849
Note D.4 (For Proper Gaussians, WSS = stationary). A proper Gaussian CSP
is wide-sense stationary if, and only if, it is stationary.
The ﬁrst result is on the synthesis of a passband SP from a baseband CSP. It is
particularly useful for computer simulations of passband systems.
Theorem D.5 (Synthesizing a Passband SP from a Baseband CSP). Let

XBB(t)

be a proper WSS CSP of autocovariance function KBB, and let the SP

X(t)

be
given by
X(t) = 2 Re

XBB(t) ei2πfct
,
t ∈R,
(D.1)
where fc > 0 is arbitrary.
(i) The SP

X(t)

is centered and WSS with autocovariance function
τ 	→2 Re

KBB(τ) ei2πfcτ
.
(D.2)
(ii) If

XBB(t)

is of PSD SBB, then

X(t)

is of PSD
f 	→SBB(f −fc) + SBB(−f −fc).
(D.3)
(iii) If

XBB(t)

is Gaussian, then so is

X(t)

.
Proof. We begin by rewriting (D.1) in the form
X(t) = XBB(t) ei2πfct + X∗
BB(t) e−i2πfct,
t ∈R
(D.4)
to emphasize that X(t) is a linear functional of the pair XBB(t) and X∗
BB(t). We
now proceed to prove Part (i). Since

XBB(t)

is WSS, it is of ﬁnite variance and
therefore so is

X(t)

, because (D.1) implies that
X(t)
 ≤2
XBB(t) ei2πfct
and hence that
E

X2(t)

≤4 E

|XBB(t)|2
.
Since

XBB(t)

is proper, it is centered, and therefore so is

X(t)

(see (D.4)).
To complete the proof of Part (i) we next compute E[X(t + τ) X(t)].
We ﬁrst
substitute t + τ for t in (D.4), to obtain
X(t + τ) = XBB(t + τ) ei2πfc(t+τ) + X∗
BB(t + τ) e−i2πfc(t+τ).
(D.5)
Using (D.5) and (D.4) we can now compute
E

X(t + τ) X(t)

= E

XBB(t + τ) XBB(t)

ei2πfc(2t+τ) + E

XBB(t + τ) X∗
BB(t)

ei2πfcτ
+ E

X∗
BB(t + τ) XBB(t)

e−i2πfcτ + E

X∗
BB(t + τ) X∗
BB(t)

e−i2π(2fc+τ)
= E

XBB(t + τ) X∗
BB(t)

ei2πfcτ + E

X∗
BB(t + τ) XBB(t)

e−i2πfcτ
= E

XBB(t + τ) X∗
BB(t)

ei2πfcτ +

E

XBB(t + τ) X∗
BB(t)

ei2πfcτ∗
= KBB(τ) ei2πfcτ +

KBB(τ) ei2πfcτ∗
= 2 Re

KBB(τ) ei2πfcτ
,
t, τ ∈R,
850
The Baseband Representation of Passband Stochastic Processes
where the second equality follows from the hypothesis that

XBB(t)

is proper.
To prove Part (ii) we note that since KBB is the IFT of SBB, the mapping in (D.2)
is the IFT of the mapping f 	→SBB(f −fc)+S∗
BB(−f −fc), and the latter is equal
to the mapping in (D.3) because SBB is real.
Part (iii) follows from (D.4) and the fact that linear transformations map Gaussian
vectors to Gaussian vectors (Proposition 24.3.9).
The second result is on the conversion from passband to baseband.
Theorem D.6 (From Passband to Baseband (and back)). Let

XPB(t)

be a
measurable centered WSS SP of PSD SPB that is “passband” in the sense that
SPB(f) = 0,
|f| −fc
 > W
2 ,
(D.6)
where
fc > W
2 > 0.
(D.7)
Let the (possibly complex) impulse response h be integrable
h ∈L1;
(D.8a)
of unit gain in the band [−W/2, W/2]
ˆh(f) = 1,
|f| ≤W
2 ;
(D.8b)
and image rejecting in the sense that
ˆh(f) = 0,
|f| ≥2fc −W
2 .
(D.8c)
Let the CSP

XBB(t)

be given by (cf. Proposition 7.6.3)
XBB =

t 	→e−i2πfctXPB(t)

⋆h.
(D.9)
(i) The CSP XBB is proper, WSS, and of PSD
f 	→SPB(f + fc) I
'
|f| ≤W
2
(
.
(D.10)
(ii) The power in

XPB(t)

is twice the power in

XBB(t)

, and
E

X2
PB(t)

= 2 E

|XBB(t)|2
,
t ∈R.
(D.11)
(iii) At every ﬁxed epoch t ∈R
XPB(t) = 2 Re

XBB(t) ei2πfct
,
with probability one.
(D.12)
(iv) If vPB is an integrable passband signal that is bandlimited to W Hz around
the carrier frequency fc, and if vBB is its baseband representation, then

XPB, vPB

= 2 Re

XBB, vBB

,
with probability one.
(D.13)
The Baseband Representation of Passband Stochastic Processes
851
(v) If

XPB(t)

is Gaussian, then so is

XBB(t)

.
(vi) If

XPB(t)

is Gaussian and if its PSD SPB, in addition to satisfying (D.6),
is also symmetric around the carrier frequency fc in the sense that
SPB

fc −˜f

= SPB

fc + ˜f

,
 ˜f
 ≤W
2 ,
(D.14)
then the stochastic processes

Re(XBB(t)), t ∈R

and

Im(XBB(t)), t ∈R

are independent centered stationary Gaussian stochastic processes, each of
which is of PSD
f 	→1
2 SPB(f + fc) I
'
|f| ≤W
2
(
.
(D.15)
Proof. Let KPB be the autocovariance function of XPB. To establish that XBB is
proper and WSS, we need to show inter alia that it is of bounded variance. This
can be established using Fubini’s Theorem as in the proof of Proposition 25.10.1.
The details are therefore omitted.
In the course of the proof we shall repeatedly use the fact that the theorem’s
hypotheses about SPB and ˆh imply that for every f ∈R,
SPB(f + fc) ˆh(f) = SPB(f + fc) ˆh(f) I
'
|f| ≤W
2
(
(D.16)
= SPB(f + fc) I
'
|f| ≤W
2
(
,
(D.17)
where the ﬁrst equality follows from (D.6) and (D.8c), and where the second equal-
ity follows from (D.8b).
To prove Part (i) we begin by showing that XBB is proper. It is clearly centered,
because XPB is centered and by (D.9)
XBB(t) =
 ∞
−∞
e−i2πfcσXPB(σ) h(t −σ) dσ,
t ∈R.
(D.18)
To show that E

XBB(t1) XBB(t2)

is zero for all t1, t2 ∈R we ﬁrst note that,
by (D.18),
XBB(t1) XBB(t2)
=
 ∞
−∞
e−i2πfcσXPB(σ) h(t1 −σ) dσ
 ∞
−∞
e−i2πfcμXPB(μ) h(t2 −μ) dμ
=
 ∞
−∞
 ∞
−∞
e−i2πfc(σ+μ) h(t1 −σ) h(t2 −μ) XPB(σ) XPB(μ) dσ dμ,
(D.19)
so
E

XBB(t1) XBB(t2)

=
 ∞
−∞
 ∞
−∞
e−i2πfc(σ+μ) h(t1 −σ) h(t2 −μ) E

XPB(σ)XPB(μ)

dσ dμ
=
 ∞
−∞
 ∞
−∞
e−i2πfc(σ+μ) h(t1 −σ) h(t2 −μ) KPB(σ −μ) dσ dμ.
(D.20)
852
The Baseband Representation of Passband Stochastic Processes
Replacing the integration variables (σ, μ) with (σ, τ) where τ = σ −μ we obtain
E

XBB(t1) XBB(t2)

=
 ∞
−∞
 ∞
−∞
e−i2πfc(2σ−τ) h(t1 −σ) h(t2 + τ −σ) KPB(τ) dτ dσ
=
 ∞
−∞
e−i4πfcσ h(t1 −σ)
 ∞
−∞
ei2πfcτ h(t2 + τ −σ) KPB(τ) dτ dσ.
(D.21)
We next use Proposition 6.2.4 (

xˇg∗=

ˆxg∗) to compute the inner integral
 ∞
−∞
ei2πfcτ h(t2 + τ −σ) KPB(τ) dτ.
(D.22)
Indeed, τ 	→ei2πfcτ KPB(τ) is the complex conjugate of τ 	→e−i2πfcτ KPB(τ), and
the latter is the IFT of f 	→SPB(f + fc), which thus plays the role of g. And the
mapping τ 	→h(t2 + τ −σ), which plays the role of x is of FT f 	→ei2π(t2−σ) ˆh(f).
Hence, by Proposition 6.2.4 (and the fact that SPB is real)
 ∞
−∞
ei2πfcτ h(t2 + τ −σ) KPB(τ) dτ
=
 ∞
−∞
SPB(f + fc) ei2π(t2−σ)f ˆh(f) df
=

W
2
−W
2
SPB(f + fc) ei2π(t2−σ)f df,
(D.23)
where the second equality follows from (D.17). Substituting (D.23) in (D.21) yields
E

XBB(t1) XBB(t2)

=
 ∞
−∞
e−i4πfcσ h(t1 −σ)

W
2
−W
2
SPB(f + fc) ei2π(t2−σ)f df dσ
=

W
2
−W
2
ei2πt2f SPB(f + fc)
 ∞
−∞
e−i2πσ(2fc+f) h(t1 −σ) dσ df.
(D.24)
The inner integral is the FT of σ 	→h(t1 −σ) evaluated at 2fc + f. Since the FT
of σ 	→h(t1 −σ) is ˜f 	→e−i2π ˜
ft1 ˆh(−˜f) we obtain, upon substituting 2fc + f for ˜f,
 ∞
−∞
e−i2πσ(2fc+f) h(t1 −σ) dσ = e−i2π(2fc+f)t1 ˆh(−2fc −f).
(D.25)
Substituting (D.25) in (D.24) we obtain
E

XBB(t1) XBB(t2)

=

W
2
−W
2
ei2πt2f SPB(f + fc) e−i2π(2fc+f)t1 ˆh(−2fc −f) df
= 0,
(D.26)
where the second equality holds because over the range of integration, i.e., for
|f| ≤W/2, the integrand is zero because, by (D.8c), ˆh(−2fc −f) is zero. Since
The Baseband Representation of Passband Stochastic Processes
853
we have already established that XBB is centered, and since (D.26) holds for all
t1, t2 ∈R, it follows that XBB is proper.
We next show that XBB is WSS. To this end, we compute E[XBB(t1) X∗
BB(t2)] for
arbitrary epochs t1, t2 ∈R. As in (D.19) and (D.20),
XBB(t1) X∗
BB(t2)
=
 ∞
−∞
e−i2πfcσXPB(σ) h(t1 −σ) dσ ·
	 ∞
−∞
e−i2πfcμXPB(μ) h(t2 −μ) dμ

∗
=
 ∞
−∞
 ∞
−∞
e−i2πfc(σ−μ) h(t1 −σ) h∗(t2 −μ) XPB(σ) XPB(μ) dσ dμ,
(D.27)
so
E

XBB(t1) X∗
BB(t2)

=
 ∞
−∞
 ∞
−∞
e−i2πfc(σ−μ) h(t1 −σ) h∗(t2 −μ) KPB(σ −μ) dσ dμ.
(D.28)
Replacing the integration variables (σ, μ) with (σ, τ) where τ = σ −μ we obtain
E

XBB(t1) X∗
BB(t2)

=
 ∞
−∞
 ∞
−∞
e−i2πfcτ h(t1 −σ) h∗(t2 + τ −σ) KPB(τ) dτ dσ
=
 ∞
−∞
h(t1 −σ)
 ∞
−∞
e−i2πfcτ h∗(t2 + τ −σ) KPB(τ) dτ dσ.
(D.29)
We can now use Proposition 6.2.4 (

xˇg∗=

ˆxg∗or, upon conjugation,

x∗ˇg =

ˆx∗g) to compute the inner integral
 ∞
−∞
e−i2πfcτ h∗(t2 + τ −σ) KPB(τ) dτ.
Indeed, τ 	→e−i2πfcτ KPB(τ) is the IFT of f 	→SPB(f + fc) and τ 	→h(t2 + τ −σ)
is of FT f 	→ei2π(t2−σ)f ˆh(f), so
 ∞
−∞
e−i2πfcτ h∗(t2 + τ −σ) KPB(τ) dτ
=
 ∞
−∞
e−i2π(t2−σ)f ˆh∗(f) SPB(f + fc) df
=

W
2
−W
2
e−i2π(t2−σ)f SPB(f + fc) df,
(D.30)
where the second equality follows from (D.17). Substituting (D.30) in (D.29) yields
E

XBB(t1) X∗
BB(t2)

=
 ∞
−∞
h(t1 −σ)

W
2
−W
2
e−i2π(t2−σ)f SPB(f + fc) df dσ
=

W
2
−W
2
e−i2πt2f SPB(f + fc)
 ∞
−∞
ei2πσf h(t1 −σ) dσ df.
854
The Baseband Representation of Passband Stochastic Processes
The inner integral is the FT of σ 	→h(t1 −σ) evaluated at −f. Since this FT is
˜f 	→e−i2π ˜
ft1 ˆh(−˜f), the inner integral evaluates to
 ∞
−∞
ei2πσf h(t1 −σ) dσ = ei2πft1 ˆh(f),
and thus
E

XBB(t1) X∗
BB(t2)

=

W
2
−W
2
e−i2πt2f SPB(f + fc) ei2πft1 ˆh(f) df
=

W
2
−W
2
ei2πf(t1−t2) SPB(f + fc) df,
(D.31)
where the last equality follows from (D.8b). The RHS of (D.31) depends on t1 and
on t2 only via their diﬀerence t1 −t2, so XBB is WSS. From (D.31) we also see
that the autocovariance function of XBB is the IFT of the mapping in (D.10), so
the latter is the PSD of XBB.
To prove Part (ii) we shall establish (D.11), which implies that the power in XPB
is twice the power in XBB (Proposition 25.9.2 and its complex version). The LHS
of (D.11) is the integral of SPB over all the frequencies f ∈R. The RHS is twice
the integral of the mapping in (D.10) (because this mapping is by Part (i) the PSD
of XBB). The two are equal because XPB is real and its PSD is hence symmetric.
We next turn to Part (iii). For every epoch t ∈R we deﬁne the error
e(t) = E

XPB(t) −2 Re

XBB(t) ei2πfct2
(D.32)
and proceed to show that it is zero. Using the identity
2 Re

XBB(t) ei2πfct
= XBB(t) ei2πfct + X∗
BB(t) e−i2πfct
(D.33)
and opening the square, we obtain that
e(t) = E

X2
PB(t)

−2 E

XPB(t) XBB(t)

ei2πfct −2 E

XPB(t) X∗
BB(t)

e−i2πfct
+ E

X2
BB(t)

ei4πfct + E

(X∗
BB(t))2
e−i4πfct + 2 E

|XBB(t)|2
= E

X2
PB(t)

−2 E

XPB(t) XBB(t)

ei2πfct −2 E

XPB(t) X∗
BB(t)

e−i2πfct
+ 2 E

|XBB(t)|2
= E

X2
PB(t)

+ 2 E

|XBB(t)|2
−4 Re

E

XPB(t) XBB(t)

ei2πfct
= 4 E

|XBB(t)|2
−4 Re

E

XPB(t) XBB(t)

ei2πfct
,
(D.34)
where the second equality holds because XBB is proper (Part (i)), and the ﬁnal
equality follows from (D.11). To conclude the proof that e(t) is zero, it thus remains
to establish that
Re

E

XPB(t) XBB(t)

ei2πfct
= E

|XBB(t)|2
,
(D.35)
The Baseband Representation of Passband Stochastic Processes
855
which is what we proceed to do. Beginning with (D.18),
E

XPB(t) XBB(t)

= E

XPB(t)
 ∞
−∞
e−i2πfcσXPB(σ) h(t −σ) dσ

=
 ∞
−∞
E

XPB(t) XPB(σ)

e−i2πfcσ h(t −σ) dσ
=
 ∞
−∞
KPB(t −σ) e−i2πfcσ h(t −σ) dσ
= e−i2πfct
 ∞
−∞
KPB(t −σ) ei2πfc(t−σ) h(t −σ) dσ
= e−i2πfct
 ∞
−∞
KPB(τ) ei2πfcτ h(τ) dτ
= e−i2πfct
 ∞
−∞
SPB(f) ˆh(f −fc) df
= e−i2πfct
 ∞
−∞
SPB( ˜f + fc) ˆh( ˜f) d ˜f
= e−i2πfct

W
2
−W
2
SPB( ˜f + fc) d ˜f
= e−i2πfct E

|XBB(t)|2
,
(D.36)
from which (D.35) follows. Here the sixth equality follows from Proposition 6.2.4,
the eighth from (D.17), and the last because the power in XBB is the integral of
its PSD, and the latter is the mapping in (D.10).
The proof of Part (iv) is based on the identity
⟨Z ⋆h, v⟩=

Z, ~h∗⋆v

with probability one,
(D.37)
which is the complex counterpart of (26.113).
Here

Z(t)

is a CSP, and the
signals h and v are integrable complex signals. Using this identity we have with
probability one
2 Re

XBB, vBB

= 2 Re

t 	→XPB(t) e−i2πfct
⋆h, vBB

= 2 Re

t 	→XPB(t) e−i2πfct, ~h∗⋆vBB

= 2 Re

t 	→XPB(t) e−i2πfct, vBB

= 2 Re
	 ∞
−∞
XPB(t)

vBB(t) ei2πfct∗dt

=
 ∞
−∞
XPB(t) 2 Re

vBB(t) ei2πfct∗
dt
=

XPB, vPB

,
where the ﬁrst equality follows from the deﬁnition of XBB (D.9); the second from
the identity (D.37); the third holds because vBB is bandlimited to W/2 Hz, the FT
of ~h∗is the complex conjugate of ˆh, and h is of unit-gain in the band [−W/2, W/2]
(D.8b); the fourth follows from the deﬁnition of the inner product; the ﬁfth holds
856
The Baseband Representation of Passband Stochastic Processes
because XPB is real; and the sixth holds because Re(z∗) equals Re(z) and because
vBB is the baseband representation of vPB.
To establish Part (v) note that XPB is a Gaussian SP so

e−i2πfct XPB(t), t ∈R

is a Gaussian CSP. Consequently, XBB, which is the result of ﬁltering this CSP, is
also a Gaussian CSP.1
We ﬁnally proceed to Part (vi). To establish independence and Gaussianity we need
to show that, for every n ∈N and epochs t1, . . . , tn ∈R, the random n-vectors

Re

XBB(t1)

, . . . , Re

XBB(tn)
T
and

Im

XBB(t1)

, . . . , Im

XBB(tn)
T
are independent Gaussian vectors (Deﬁnitions 25.2.3 and 25.3.1). They are jointly
Gaussian (Deﬁnition 23.7.1) because, by Part (v) and Deﬁnition 24.3.6, the random
2n-vector

Re

XBB(t1)

, . . . , Re

XBB(tn)

, Im

XBB(t1)

, . . . , Im

XBB(tn)
T
is Gaussian. Being jointly Gaussian, each of them is Gaussian (Corollary 23.6.5).
Moreover, to establish their independence it suﬃces to establish that they are
uncorrelated (Proposition 23.7.3). Since XPB is centered so is XBB, and to establish
independence it remains to show that
E
%
Re

XBB(t)

Im

XBB(t′)
&
= 0,
t, t′ ∈R.
(D.38)
Using the identities
Re

XBB(t)

= XBB(t) + X∗
BB(t)
2
,
Im

XBB(t′)

= XBB(t′) −X∗
BB(t′)
2i
,
and the fact that XBB is proper we obtain
E
%
Re

XBB(t)

Im

XBB(t′)
&
= E

X∗
BB(t) XBB(t′)

−E

X∗
BB(t′) XBB(t)

4i
= KBB(t′ −t) −K∗
BB(t′ −t)
4i
= 1
2 Im

KBB(t′ −t)

= 0,
t, t′ ∈R,
where KBB is the autocovariance function of XBB, and where the last equality holds
because the symmetry hypothesis (D.14) on SPB implies that the mapping (D.10)
(which is real) is also symmetric and hence that its IFT, namely KBB, is real. We
1We are relying here on the complex version of Theorem 25.13.2 (iv). To prove this it suﬃces
to notice that the proof of Proposition 25.11.1 goes through almost verbatim also for the complex
case.
The Baseband Representation of Passband Stochastic Processes
857
ﬁnally turn to the autocovariance functions.
E
%
Re

XBB(t)

Re

XBB(t′)
&
= E

XBB(t) X∗
BB(t′)

+ E

X∗
BB(t) XBB(t′)

4
= KBB(t −t′) + K∗
BB(t −t′)
4
= 1
2 Re

KBB(t −t′)

= 1
2 KBB(t −t′),
t, t′ ∈R,
where the last equality holds because, as we have seen, the symmetry assumption
(D.14) implies that KBB is real. The real part of XBB is thus WSS, and its PSD
is thus the mapping in (D.15). The calculation for the imaginary part of XBB is
nearly identical and is omitted.
Bibliography
Adams, R. A., and J. J. F. Fournier (2003): Sobolev Spaces. Elsevier B.V.,
Amsterdam, The Netherlands, second edn.
Adler, R. J. (1990): An Introduction to Continuity, Extrema, and Related Topics
for General Gaussian Processes. Institute of Mathematical Statistics, Hayward,
CA.
Axler, S. (2015): Linear Algebra Done Right. Springer-Verlag New York, Inc.,
New York, third edn.
Barry, J. R., E. A. Lee, and D. G. Messerschmitt (2004): Digital Commu-
nication. Springer-Verlag New York, Inc., New York, third edn.
Bertsekas, D. P. (2005): Dynamic Programming and Optimal Control, vol. 1.
Athena Scientiﬁc, Nashua, NH, third edn.
Billingsley, P. (1995): Probability and Measure. John Wiley & Sons, Inc., New
York, third edn.
Blackwell, D., and R. V. Ramamoorthi (1982): “A Bayes but not classically
suﬃcient statistic,” The Annals of Statistics, 10(3), 1025–1026.
Blahut, R. E. (2003): Algebraic Codes for Data Transmission. Cambridge Uni-
versity Press, New York.
Boas, Jr., R. P. (1954): Entire Functions. Academic Press Inc., New York.
Bogachev, V. I. (1998): Gaussian Measures. American Mathematical Society,
Providence, RI.
(2007): Measure Theory: Volume I. Springer-Verlag, Berlin, Germany.
Bryc, W. (1995): The Normal Distribution: Characterizations with Applications.
Springer-Verlag New York, Inc., New York.
Chung, K. L. (2001): A Course in Probability Theory. Academic Press, San Diego,
CA, third edn.
Cormen, T. H., C. E. Leiserson, R. L. Rivest, and C. Stein (2009): Intro-
duction to Algorithms. The MIT Press, Cambridge, MA, third edn.
858
Bibliography
859
Cover, T. M., and J. A. Thomas (2006): Elements of Information Theory.
Wiley, Hoboken, NJ, second edn.
Cram´er, H., and M. R. Leadbetter (2004): Stationary and Related Stochastic
Processes: Sample Function Properties and Their Applications. Dover Publica-
tions, Inc., Mineola, NY.
Crum, M. M. (1956): “On positive deﬁnite functions,” Proc. London Math. Soc.,
6(3), 548–560.
Davenport, Jr., W. B., and W. L. Root (1987): An Introduction to the Theory
of Random Signals and Noise. IEEE Press, New York.
de Caen, D. (1997): “A lower bound on the probability of a union,” Discrete
Mathematics, 169, 217–220.
Doob, J. L. (1990): Stochastic Processes. John Wiley & Sons, Inc., New York.
Dudley, R. M. (2003): Real Analysis and Probability. Cambridge University
Press, New York, second edn.
Dym, H., and H. P. McKean (1972): Fourier Series and Integrals. Academic
Press, Inc., San Diego.
Farebrother, R. W. (1988):
Linear Least Squares Computations. Marcel
Dekker, Inc., New York.
Feller, W. (1971): An Introduction to Probability Theory and Its Applications,
vol. II. John Wiley & Sons, Inc., New York, second edn.
Finner, H., and M. Roters (1997): “Log-concavity and inequalities for chi-
square, F and beta distributions with applications in multiple comparisons,”
Statistica Sinica, 7(3), 771–787.
Forney, Jr., G. D. (1991): “Geometrically uniform codes,” IEEE Transactions
on Information Theory, 37(5), 1241–1260.
Gallager, R. G. (2008): Principles of Digital Communication. Cambridge Uni-
versity Press, New York.
Gikhman, I. I., and A. V. Skorokhod (1996): Introduction to the Theory of
Random Processes. Dover Publications, Inc., Mineola, NY.
Golub, G. H., and C. F. van Loan (1996): Matrix Computations. The John
Hopkins University Press, Baltimore, MD, third edn.
Gray, R. M. (2006): “Toeplitz and circulant matrices: a review,” Foundations
and Trends in Communications and Information Theory, 2(3), 155–239.
Grimmett, G., and D. Stirzaker (2001): Probability and Random Processes.
Oxford University Press, New York, third edn.
Halmos, P. R. (1950): Measure Theory. D. Van Nostrand Company, Inc., Prince-
ton, NJ.
860
Bibliography
(1958): Finite-Dimensional Vector Spaces. D. Van Nostrand Company,
Inc., Princeton, NJ.
Halmos, P. R., and L. J. Savage (1949): “Application of the Radon-Nikodym
theorem to the theory of suﬃcient statistics,” The Annals of Mathematical Sta-
tistics, 20(2), 225–241.
Helstrom, C. W. (1995): Elements of Signal Detection and Estimation. Prentice
Hall, Englewood Cliﬀs, NJ.
Herstein, I. N. (1975): Topics in Algebra. John Wiley & Sons, Inc., New York,
second edn.
Horn, R. A., and C. R. Johnson (2013): Matrix Analysis. Cambridge University
Press, New York, second edn.
Johnson, N. L., and S. Kotz (1972): Distributions in Statistics: Continuous
Multivariate Distributions. John Wiley & Sons, Inc., New York.
Johnson, N. L., S. Kotz, and N. Balakrishnan (1994): Continuous Univari-
ate Distributions, vol. 1. John Wiley & Sons, Inc., New York, second edn.
(1995): Continuous Univariate Distributions, vol. 2. John Wiley & Sons,
Inc., New York, second edn.
Kailath, T., A. H. Sayed, and B. Hassibi (2000): Linear Estimation. Prentice
Hall, Inc., Upper Saddle River, NJ.
Kallenberg, O. (2002): Foundations of Modern Probability. Springer, New York,
second edn.
Karatzas, I., and S. E. Shreve (1991): Brownian Motion and Stochastic Cal-
culus. Springer-Verlag New York, Inc., New York, second edn.
Karlin, S., and W. J. Studden (1966): TchebycheﬀSystems: With Applications
in Analysis and Statistics. John Wiley & Sons, Inc., New York.
Katznelson, Y. (2004): An Introduction to Harmonic Analysis. Cambridge Uni-
versity Press, New York, NY, third edn.
K¨orner, T. W. (1988): Fourier Analysis. Cambridge University Press, New York,
NY.
Kwakernaak, H., and R. Sivan (1991): Modern Signals and Systems. Prentice
Hall, Inc., Englewood Cliﬀs, NJ.
Lehmann, E. L., and J. P. Romano (2005): Testing Statistical Hypotheses.
Springer-Verlag New York, Inc., New York, third edn.
Loeliger, H.-A. (1991): “Signal sets matched to groups,” IEEE Transactions on
Information Theory, 37(6), 1675–1682.
Lo`eve, M. (1963): Probability Theory. D. Van Nostrand Company, Inc., Prince-
ton, NJ, third edn.
Bibliography
861
Logan, Jr., B. F. (1978): “Theory of analytic modulation systems,” The Bell
System Technical Journal, 57(3), 491–576.
MacWilliams, F. J., and N. J. A. Sloane (1977): The Theory of Error-
Correcting Codes. North-Holland, Amsterdam, The Netherlands.
Marshall, A. W., I. Olkin, and B. C. Arnold (2011): Inequalities: Theory
of Majorization and Its Applications. Springer, New York, NY, second edn.
Nehari, Z. (1975): Conformal Mapping. Dover Publications, Inc., Mineola, NY.
Neveu, J. (1968): Processus Al´eatoires Gaussiens. Les Presses de l’Universit´e de
Montr´eal.
Oppenheim, A. V., and A. S. Willsky (1997): Signals & Systems. Prentice
Hall, Inc., Upper Saddle River, NJ, second edn.
Pinsky,
M.
A. (2002):
Introduction to Fourier Analysis and Wavelets.
Brooks/Cole, Paciﬁc Grove, CA.
Poor, H. V. (1994):
An Introduction to Signal Detection and Estimation.
Springer-Verlag New York, Inc., New York, second edn.
Porat, B. (2008): Digital Processing of Random Signals: Theory and Methods.
Dover Publications, Inc., Mineola, NY.
Pourahmadi, M. (2001): Foundations of Time Series Analysis and Prediction
Theory. John Wiley & Sons, Inc., New York.
Proakis, J., and M. Salehi (2007): Digital Communications. McGraw-Hill Ed-
ucation, 5th edn.
Requicha, A. A. G. (1980): “The zeros of entire functions: theory and engineer-
ing applications,” Proceedings of the IEEE, 68(3), 308–328.
Richardson, T., and R. Urbanke (2008): Modern Coding Theory. Cambridge
University Press, New York.
Riesz, F., and B. Sz.-Nagy (1990): Functional Analysis. Dover Publications,
Inc., Mineola, NY.
Romano, J. P., and A. F. Siegel (1986): Counterexamples in Probability and
Statistics. Chapman & Hall, New York.
Ross, D. A. (2005): “An elementary proof of Lyapunov’s theorem,” The American
Mathematical Monthly, 112(7), 651–653.
Roth, R. M. (2006): Introduction to Coding Theory. Cambridge University Press,
New York.
Royden, H. L., and P. M. Fitzpatrick (2010): Real Analysis. Prentice Hall,
Inc., Upper Saddle River, NJ, fourth edn.
Rudin, W. (1962): Fourier Analysis on Groups. John Wiley & Sons, New York.
862
Bibliography
(1976): Principles of Mathematical Analysis. McGraw-Hill, Inc., New
York, third edn.
(1987): Real & Complex Analysis. McGraw-Hill, Inc., New York, third
edn.
Ryan, W., and S. Lin (2009): Channel Codes: Classical and Modern. Cambridge
University Press.
Sason, I., and S. Shamai (2006): “Performance analysis of linear codes under
maximum-likelihood decoding: a tutorial,” Foundations and Trends in Commu-
nications and Information Theory, 3(1/2), 1–225.
Shiryaev, A. N. (1996): Probability. Springer-Verlag New York, Inc., New York,
second edn.
Simon, B. (2005): Orthogonal Polynomials on the Unit Circle. Part 1: Classical
Theory. American Mathematical Society, Providence RI.
Simon, M. K. (2002): Probability Distributions Involving Gaussian Random Vari-
ables: A Handbook for Engineers and Scientists. Kluwer Academic Publishers,
Norwell, MA.
Slepian, D. (1976): “On bandwidth,” Proceedings of the IEEE, 64(3), 292–300.
Slepian, D., and H. O. Pollak (1961): “Prolate spheroidal wave functions,
Fourier analysis and uncertainty—I,” The Bell System Technical Journal, 40(1),
43–63.
Steele, J. M. (2004): The Cauchy-Schwarz Master Class: An Introduction to the
Art of Mathematical Inequalities. Cambridge University Press, New York.
Stein, E. M., and G. Weiss (1971): Introduction to Fourier Analysis on Eu-
clidean Spaces. Princeton University Press, Princeton, NJ.
Tong, Y. L. (1990): The Multivariate Normal Distribution. Springer-Verlag New
York, Inc., New York.
Unser, M. (2000): “Sampling—50 years after Shannon,” Proceedings of the IEEE,
88(4), 569–587.
van Lint, J. H. (1998): Introduction to Coding Theory. Springer-Verlag New
York, Inc., New York, third edn.
Verd´u, S. (1998): Multiuser Detection. Cambridge University Press, New York.
Viterbi, A. J., and J. K. Omura (1979): Principles of Digital Communication
and Coding. McGraw-Hill, Inc., New York.
Williams, D. (1991): Probability with Martingales. Cambridge University Press,
New York.
Yaglom, A. (1986): Correlation Theory of Stationary and Related Random Func-
tions: Volume I: Basic Results. Springer-Verlag.
Bibliography
863
Zhang, F. (2011): Matrix Theory: Basic Results and Techniques. Springer-Verlag
New York, Inc., New York, second edn.
Zvonkin, A. (1997): “Matrix integrals and map enumeration: an accessible in-
troduction,” Mathematical and Computer Modelling, 26(8–10), 281–304.
Theorems Referenced by Name
Bernstein’s Inequality
Theorem 6.7.1
Bochner’s Theorem
Theorem 25.8.1
Cauchy-Schwarz Inequality
Theorem 3.3.1
Cauchy-Schwarz Inequality for Random Variables
Theorem 3.5.1
Characterization of Shift-Orthonormal Pulses
Corollary 11.3.4
Covariance Inequality
Corollary 3.5.2
Dominated Convergence Theorem
(Rudin, 1987, Theorem 1.34)
Factorization Theorem
Theorem 22.3.1
Fubini’s Theorem
See Section 2.6
H¨older’s Inequality
Theorem 3.3.2
Kolmogorov’s Existence Theorem
Theorem 25.2.1
L2-Sampling Theorem
Theorem 8.4.3
Liapunov’s Theorem
Theorem 30.10.2
Minimum Bandwidth Theorem
Corollary 11.3.5
Nyquist’s Criterion
Theorem 11.3.2
Parseval’s Theorem
Theorem 6.2.9
Pointwise Sampling Theorem
Theorem 8.4.5
Pythagorean Theorem
Theorem 4.5.2
Riesz-Fischer Theorem
Theorem 8.6.3
Sandwich Theorem
Chapter 8, Footnote 5
Triangle Inequality for Complex Numbers
(2.11) and (2.12)
Triangle Inequality in L2
(4.12) and (4.14)
Union-of-Events Bound (or Union Bound)
Theorem 21.5.1
Wiener-Khinchin Theorem
Theorem 25.14.1
864
Abbreviations
Abbreviations in Mathematics
CDF
Cumulative Distribution Function
CONS
Complete Orthonormal System
CRV
Complex Random Variable
CSP
Complex Stochastic Process
DTFT
Discrete-Time Fourier Transform
FDD
Finite-Dimensional Distribution
FT
Fourier Transform
IFT
Inverse Fourier Transform
IID
Independent and Identically Distributed
LHS
Left-Hand Side
MGF
Moment Generating Function
PDF
Probability Density Function
PMF
Probability Mass Function
PSD
Power Spectral Density
RHS
Right-Hand Side
RV
Random Variable
SP
Stochastic Process
WSS
Wide-Sense Stationary
Abbreviations in Communications
BER
Bit Error Rate
BPF
Bandpass Filter
ISI
Intersymbol Interference
LPF
Lowpass Filter
M-PSK
M-ary Phase Shift Keying
PAM
Pulse Amplitude Modulation
PSK
Phase Shift Keying
QAM
Quadrature Amplitude Modulation
QPSK
Quadrature Phase Shift Keying
WGN
White Gaussian Noise
865
List of Symbols
General
A ⇒B
Statement B is true whenever Statement A is true.
A ⇔B
Statement A is true if, and only if, Statement B is true.

Summation.
?
Product.
≜
Equal by deﬁnition.
□
End of proof.
Sets
∅
Empty set.
{−: −}
The set of all objects described before the colon that satisfy
the condition stated after the colon.
# A
Number of elements of the set A.
a ∈A
Set membership: a is an element of A.
a /∈A
Exclusion: a is not an element of A.
A ⊂B
Proper subset: every element of A is an element of B but some
elements of B are not elements of A.
A ⊆B
Subset: every element of A is also an element of B.
B \ A
Setminus: {b ∈B : b /∈A}.
Ac
Set-complement.
A △B
Symmetric Set Diﬀerence: (A \ B) ∪(B \ A).
A × B
Cartesian product:

(a, b) : a ∈A, b ∈B

.
An
n-fold Cartesian product: A × A × · · · × A



n times
.
A ∩B
Intersection: {ξ ∈A : ξ ∈B}.
A ∪B
Union: elements of A or B.
Speciﬁc Sets
N
Natural Numbers: {1, 2, . . .}.
Z
Integers: {. . . , −2, −1, 0, 1, 2, . . .}.
R
Real Numbers.
C
Complex Numbers.
F2
Binary Field (Section 29.2).
I
Unit interval [−1/2, 1/2); see (A.1).
866
List of Symbols
867
Intervals and Some Functions
≤, <, ≥, >
Inequality signs.
+∞, −∞, ∞
Inﬁnities.
[a, b]
Closed Interval: {ξ ∈R : a ≤ξ ≤b}.
[a, b)
Interval open on the right: {ξ ∈R : a ≤ξ < b}.
(a, b]
Interval open on the left: {ξ ∈R : a < ξ ≤b}.
(a, b)
Open interval: {ξ ∈R : a < ξ < b}.
[0, ∞]
Nonnegative reals including inﬁnity: {ξ ∈R : ξ ≥0} ∪{∞}.
⌊ξ⌋
Floor: the largest integer not larger than ξ.
⌈ξ⌉
Ceiling: the smallest integer not smaller than ξ.
max
Maximum.
min
Minimum.
sup
Least upper bound.
inf
Greatest lower bound.
Complex Numbers
C
Complex ﬁeld.
i
i = √−1.
Re(z)
Real part of z.
Im(z)
Imaginary part of z.
|z|
Modulus of z.
z∗
Complex conjugate of z.
D(z0, r)
Open disc: {z ∈C : |z −z0| < r}.
Limits
an →a
Convergence: the sequence a1, a2, . . . converges to a.
limn→∞an
Limit: the limit of an as n tends to inﬁnity.
→
Converges to.
limn→∞an
Upper limit (limit superior).
limn→∞an
Lower limit (limit inferior).
Deﬁning and Operating on Functions
g: D →R
Function of name g, domain D, and codomain R.
g: t 	→t2
Function of name g mapping t to t2. (Domain & codomain
unspeciﬁed.)
g ◦h
Composition: ξ 	→g

h(ξ)

.
d
Diﬀerentiation operator.
∂g(x)
∂x(j)
Partial derivative of g(·) with respect to x(j).
∂g(x)
∂x
Jacobian matrix.

D
Integral over the region D.
g(ξ)

ξ=a
The evaluation of the function ξ 	→g(ξ) at a.
868
List of Symbols
g(ξ)
b
a
The evaluation g(b) −g(a).
Function Norms, Relations, and Equivalence Classes
∥x∥1
See (2.6).
∥x∥2
See (3.12).
∥x∥I,1
See (A.14).
x ≡y
x and y are indistinguishable; see Deﬁnition 2.5.2.
[u]
The equivalence class of u; see (4.61).
Function Spaces
L1
Integrable functions from R to C or R to R (depending on
context); see Sections 2.2 and 2.3.
L2
Square-integrable functions from R to C or R to R (depending
on context); see Section 3.1.
L2
Collection of equivalence classes of square-integrable functions;
see Section 4.7.
Special Functions
I{statement}
Indicator function. Its value is 1 if the statement is true and 0
otherwise.
0
All-zero function: t 	→0.
n!
n factorial: 1 × 2 × · · · × n.
n
k

Number of subsets of {1, . . . , n} containing k (distinct) ele-
ments (= n!/(k!(n −k)!)).
√ξ
Nonnegative square root of ξ.
cos(·)
Cosine function (argument in radians).
sin(·)
Sine function (argument in radians).
sinc(·)
Sinc function; see (5.20).
arctan(·)
Inverse tangent.
Q(·)
Q-function; see (19.9).
Γ(·)
Gamma function; see (19.39).
I0(·)
The zeroth-order modiﬁed Bessel function; see (27.34).
ln(·)
Natural logarithm (base e).
exp(·)
Exponential function: exp(ξ) = eξ.
ξ mod [−π, π)
element of [−π, π) that diﬀers from ξ by an integer multiple
of 2π.
List of Symbols
869
Operations on Signals
~x
The mirror image of x; see (5.1).
ˆx
The Fourier Transform of the signal x; see (6.1).
ˇx
Inverse Fourier Transform of x; see (6.4).
⟨x, y⟩
Inner product between the signals x and y; see (3.1) and (3.4).
x ⋆y
Convolution of x with y; see (5.2).
x + y
The signal t 	→x(t) + y(t).
αx
The scaling of the signal x by complex or real number α, i.e.,
the signal t 	→αx(t).
Rxx
Self-similarity function of the signal x; see (11.2).
ˆg(η)
The η-th Fourier Series Coeﬃcient; see (A.2).
Filters
"
LPFWc(·)
Frequency response of a unit-gain lowpass ﬁlter of cutoﬀfre-
quency Wc. That is, "
LPFWc(f) = I{|f| ≤Wc}.
LPFWc(·)
Impulse response of a unit-gain lowpass ﬁlter of cutoﬀfre-
quency Wc. That is, LPFWc(t) = 2Wc sinc(2Wct).

BPFW,fc(·)
Frequency response of a unit-gain bandpass ﬁlter of band-
width W around the carrier frequency fc. That is, the mapping
of f to I
|f| −fc
 ≤W/2

. It is assumed that fc > W/2.
BPFW,fc(·)
Impulse response of a unit-gain bandpass ﬁlter of band-
width W around the carrier frequency fc. That is, the mapping
of t to 2W cos(2πfct) sinc(Wt). It is assumed that fc > W/2.
PAM Signaling
g or φ
Pulse shape; see Section 10.7.
Ts
Baud period; see Section 10.7.
1/Ts
Baud rate.
X
Constellation; see Section 10.8.
δ
Minimum distance of a constellation; see Section 10.8.
enc(·)
Block encoder; see Deﬁnition 10.4.1 and (18.3).
x(t; d)
Transmitted signal at time t when the data are d; see (28.7).
870
List of Symbols
QAM Signaling
g or φ
Pulse shape; see Sections 16.3 & 16.5.
Ts
Baud period; see Section 16.3.
1/Ts
Baud rate.
C
Constellation; see Section 16.7.
δ
Minimum distance of a constellation; see Section 16.7.
enc(·)
Block encoder; see (18.3).
x(t; d)
The transmitted signal at time t when the data are d; see
(28.36).
Matrices
n × m matrix
A matrix with n rows and m columns.
0
The all-zero matrix.
In
The n × n identity matrix.
a(k,ℓ)
The Row-k Column-ℓcomponent of the matrix A.
A∗
Componentwise complex conjugate.
AT
Transpose of A.
A†
Hermitian conjugate of A.
tr(A)
Trace of A.
det(A)
Determinant of A.
Re(A)
Componentwise real part of A.
Im(A)
Componentwise imaginary part of A.
A ⪰0
A is a positive semideﬁnite matrix.
A ≻0
A is a positive deﬁnite matrix.
Vectors
Rn
Set of column vectors of n real components.
Cn
Set of column vectors of n complex components.
0
The all-zero vector.
a(j)
The j-component of the column vector a.
aT
The transpose of the vector a.
∥a∥
Euclidean norm of a; see (20.85).
⟨a, b⟩E
Euclidean inner product; see (20.84).
dE(a, b)
Euclidean distance between a and b, i.e., ∥a −b∥.
Linear Algebra
span(v1, . . . , vn)
Linear subspace spanned by the n-tuple (v1, . . . , vn); see (4.8).
dim(V)
Dimension of the subspace V.
ker(T)
Kernel of the linear mapping T(·).
range(T)
Range of the linear mapping T(·).
List of Symbols
871
Probability
(Ω, F, P)
Probability triple; see Page 3.
PX(·)
Probability Mass Function (PMF) of X.
PX,Y (·, ·)
Joint PMF of (X, Y ).
PX|Y (·|·)
Conditional PMF of X given Y .
fX(·)
Probability density function (PDF) of X.
fX,Y (·, ·)
Joint PDF of (X, Y ).
fX|Y (·|·)
Conditional PDF of X given Y .
fX|A
Conditional PDF of X given the event A.
FX(·)
Cumulative distribution function of X.
ΦX(·)
Characteristic function of X.
MX(·)
Moment generating function of X; see (19.23).
E[X]
Expectation of X; see (17.9).
Var[X]
Variance of X; see (17.14a).
Cov[X, Y ]
Covariance between X and Y ; see (17.17).
E[·|·]
Conditional expectation.
Pr(·)
Probability of an event.
Pr(·|·)
Conditional probability of an event.
Pr[·]
Probability that a RV satisﬁes some condition.
Pr[·|·]
Conditional version of Pr[·].
L=
Equal in law.
X⊸−Y ⊸−Z
X and Z are conditionally independent given Y .
{Xk}
Sequence of random variables . . . , X−1, X0, X1, . . .
X ∼Distribution
X has the speciﬁed distribution.
χ2
n,λ
Noncentral χ2 distribution with n degrees of freedom
and noncentrality parameter λ.
Bernoulli(p)
Bernoulli distribution (takes on the values 0 and 1 with
probabilities p and 1 −p).
U (A)
Uniform distribution over the set A.
NC(0, K)
Multivariate proper complex Gaussian distribution of
covariance K; see Note 24.3.13.
N(μ, K)
Multivariate real Gaussian distribution of mean μ and
covariance K.
Stochastic Processes

X(n)

,

Xn, n ∈Z

Discrete-time stochastic process.

X(t)

,

X(t), t ∈R

Continuous-time stochastic process.
X
Shorthand for

X(t), t ∈R

.
KXX
Autocovariance function.
SXX
Power spectral density (PSD).
ρXX(·)
Correlation function.
⟨X, s⟩
Shorthand for
 ∞
−∞X(t) s(t) dt.
872
List of Symbols
Hypothesis Testing
Bm,m′
The subset of Rd deﬁned in (21.34).
H
RV to be guessed in binary hypothesis testing.
LLR(·)
Log likelihood-ratio function; see (20.41).
LR(·)
Likelihood-ratio function; see (20.38).
M
Number of hypotheses in multi-hypothesis testing.
M
Set of hypotheses {1, . . . , M}.
M
RV to be guessed in multi-hypothesis testing.
φGuess
Generic guessing rule; see Sections 20.2 & 21.2.
φ∗
Guess
Generic optimal guessing rule.
φMAP
MAP Decision Rule.
φML
Maximum-Likelihood Rule.
p∗(error)
Optimal probability of error.
pMAP(error|·)
Conditional probability of error of MAP rule.
The Binary Field and Binary Tuples
F2
Binary ﬁeld {0, 1}.
Fκ
2
The set of binary κ-tuples.
⊕
Addition in F2; see (29.3).
·
Multiplication in F2; see (29.4).
dH(u, v)
Hamming distance; see Section 29.2.4.
wH(u)
Hamming weight; see Section 29.2.4.
Υ and Υη
Antipodal mappings (29.14) and (29.17).
Coding
Aκ,0
Binary N-tuples whose κ-th component is zero; see (29.61).
Aκ,1
Binary N-tuples whose κ-th component is one; see (29.64).
c
Generic element of range(T).
dmin,H
Minimum Hamming distance; see (29.54).
enc
Encoder.
p∗
κ
Optimal probability of error in guessing the κ-th data bit.
pMAP(error|D = d)
Conditional probability of error of the MAP rule designed
to minimize block errors.
ψd(·)
See (29.77).
x
Generic element of range(enc).
xη(d)
The η-th symbol in the N-tuple enc(d).
The Radar Problem
φGuess
Generic deterministic guessing rule (30.1).
D
Observations leading to accepting the null hypothesis (30.2).
pFA
Probability of false alarm (30.3c).
pMD
Probability of missed detection (30.4c).
pD
Probability of detection (30.6), i.e., 1 −pMD.
List of Symbols
873
p∗
D(πFA)
Highest detection probability for the allowed false-alarm prob-
ability (30.8).
p∗
MD(πFA)
Least missed-detection probability for the allowed false-alarm
probability (30.9).
R
All pairs (pFA, pMD) that are achievable by deterministic rules.
pFA(b)
Probability of false alarm of the randomized rule determined
by b(·) (30.12).
pMD(b)
Probability of missed detection of the randomized rule deter-
mined by b(·) (30.13).
pD(b)
Probability of detection of the randomized rule determined by
b(·) (30.14).
Rrnd
All pairs (pFA, pMD) that are achievable by randomized rules.
p∗
MD,rnd(πFA)
Like p∗
MD(·) but when randomization is allowed (30.16). See
also Note 30.4.3.
P0

LR(Y) ≤ξ

See (30.41a).
P0

LR(Y) < ξ

See (30.41b).
P0

LR(Y) = ξ

See (30.41c).
P0

Y ∈B

See (30.41d).
D(p∥q)
Relative entropy between two probability vectors (30.141).
D(f∥g)
Relative entropy between two densities (30.142).
Bi-Inﬁnite Sequences
a or (aν)
Generic bi-inﬁnite sequence.
(eν)
The identity sequence eν = I{ν = 0},
ν ∈Z.
ℓ1
Collection of absolutely summable bi-inﬁnite sequences.
ℓ2
Collection of square-summable bi-inﬁnite sequences.
a ⋆b
The convolution between (aν) and (bν).
(aν) ⋆(bν)
Same as a ⋆b.
(a ⋆b)η
The convolution at time η.
Raa
Self-similarity of (aν); see (31.7).
ua
Discrete-Time Fourier Transform of (aν); see Appendix B.
Intersymbol Interference (ISI)
hc
Channel’s impulse response (continuous time).
xc
Transmitted waveform.
Yc
Received waveform.

Nc(t)

Additive noise (after the ﬁlter).
gc
Pulse shape (PAM or QAM).
pc
gc ⋆hc.
hDT-Wh
Whitening ﬁlter (discrete-time).
(hκ)
ISI coeﬃcients (discrete-time).
(ℏκ)
Postulated ISI coeﬃcients (discrete-time).
κmax
Positive integer for which (32.27) holds.
n′
See (32.31).
˜fY1,...,Yn′|D=d
Postulated density.
874
List of Symbols
Υ(x; y)
Cost of x1, . . . , xn (given y1, . . . , yn′) (32.34).
s
Generic state
s0
Time-zero state.
S
Collection of all states.
ψ(s, x)
Next-state function (32.40).
γℓ(xℓ, sℓ−1)
Cost function; also denoted γℓ(xℓ, sℓ−1; yℓ).
γn+1(sn)
Final-state cost; also denoted γn+1(sn; yn+1, . . . , yn′).
s′
x⇝s′′
Feeding x causes the state to change from s′ to s′′.
cj−1(s)
minimum cost-to-go from s at time j −1.
˜cj(s)
minimum cost-to-reach s at time j.
Index
8-PPM, 193, 294
a fortiori, xxii
a posteriori, xxii
a priori, xxii
absolute value, 2, 16
absolutely continuous, 795n
absolutely summable, 786
achievable (pFA, pMD), 742
add-compare-select, 818
adjoint
of a convolution, 654, 655
aﬃne transformation
of a multivariate Gaussian, 513
of a scalar, 375
of a univariate Gaussian, 375
of a vector, 513
all-zero
function, 3
matrix, 496
signal, 3
all-zero codeword assumption, 729–734
almost sure convergence
of random variables, 390
of random vectors, 528
alternative, 741
ampliﬁcation, 3, 27
analytic continuation, 384, 385n
analytic function, 61
analytic representation, 112, 137
of an energy-limited signal, 137–
138
characterization, 137
deﬁnition, 137
of an integrable signal, 112–119
characterization, 113
deﬁnition, 112
inner products, 117
recovering from, 116
analytic signal, see analytic representa-
tion
ancillary statistic, 493
antenna selection, 687
antipodal mapping, 707, 710
argument, 66n
Arithmetic-Geometric Inequality, 458
assuming the all-zero codeword, 729–734
Aubel, C´eline, xxvi
autocorrelation function, 222, see also
self-similarity function
autocovariance function
of a continuous-time CSP, 848
of a continuous-time SP, 565
of a discrete-time CSP, 330
of a discrete-time SP, 222
average autocovariance function, 277,
294, 619
and operational PSD, 276–282
and PAM, 277
and power, 277
deﬁnition, 277
properties, 279
average probability of a bit error, 691
band-edge symmetry, 203
bandlimited stochastic process, 266
bandpass ﬁlter, 62, see also ideal unit-
gain bandpass ﬁlter
bandwidth, 734
around a carrier, 104, 107
of a product, 91–93
of a stochastic process, 266
of baseband representation
energy-limited signal, 139
integrable signal, 125
of energy-limited signal, 82
of integrable signal, 90
Barker code, 294
baseband CSP, 848–857
baseband representation, 104, 112, 119,
138, 169
FT of, 120
875
876
Index
inner product, 128, 140, 305–308
of convolution, 129, 140
of energy-limited signal, 138–141
characterization, 139
deﬁnition, 138
inner product, 140
properties, 140
recovering from, 139
sampling of, see complex sam-
pling
of ﬁlter’s output, 131, 140
of integrable signal, 119–133
characterization, 123, 126
deﬁnition, 119
FT of, 120
inner product, 128
recovering from, 126
of QAM, 297
of stochastic processes, 848–857
sampling of, see complex sampling
basis, 30, 148, 148n
baud period, 734
in PAM, 185, 185n
in QAM, 298
baud rate
in PAM, 185, 185n
in QAM, 298
Baudot, Jean-Maurice-´Emil, 185n
BER, 691
Bernoulli, 480
Bernstein’s Inequality, 94, 671
Bessel function, 389, 677
Bhattacharyya Bound, 408, 434, 456–
458
bi-inﬁnite block-mode
with PAM
deﬁnition, 241
operational PSD, 268
power, 241
with QAM
operational PSD, 352
power, 347
bi-orthogonal code, 738
bi-orthogonal keying, 648–652
BIBO stable, see stable
Biglieri, Ezio, xxvii
binary ﬁeld, 708
binary hypothesis testing, 395–440
binary-input/output-symmetric, 730
binary-to-complex block encoder, 341
binary-to-reals block encoder, 180, 241
Binomial Expansion, 644
bit error, 708
bit error rate, 691, 728
bit rate, 734
block error, 708
block-encoder
binary-to-complex
deﬁnition, 341
rate, 342
binary-to-reals
deﬁnition, 180
rate, 180
block-mode, 179–181, 347
blocklength, 711
Boche, Holger, xxvii
Bochner’s Theorem, 574, 846
B¨olcskei, Helmut, xxvi, xxvii
Bonferroni-Type Inequalities, 466
Boole’s Inequality, 451n
Borel measurable, 185, 185n
Borel σ-algebra, 185n
Borgmann, Moritz, xxvii
bounded variance, 276
bounded-input/bounded-output stable,
see stable
Box-Muller transform, 393
Boyd, Stephen, xxvii
Bracher, Annina, xxvi
Braendle, Samuel, xxvii
Br¨andle, Marion, xxvii
Brickwall function, 76
FT of, 68, 76–77
IFT of, 68, 76–77
Bross, Shraga, xxvii
Bunte, Christoph, xxvi
Bussgang’s Theorem, 538, 615
C, 1
Cantor set, 8n
carrier frequency, 106, 168, 168n
Cauchy, Augustin Louis, 45
Cauchy-Riemann equations, 322
Cauchy-Schwarz Inequality, 18–22, 25
for d-tuples, 25
for complex random variables, 338
for random variables, 23
for sequences, 25
causal ﬁlter, 59, 788
causal sequence, 788, 801
causality, 192
centered
complex Gaussian
random variable, 546
Index
877
random vector, 550
Gaussian
random variable, 375
random vector, 494
PAM constellation, 186
QAM constellation, 305
stochastic process, 215
central chi-square distribution, 386–390
Central Limit Theorem, 373
change of variable
complex random variable, 322
complex vector, 327, 339
real vector, 320
characteristic function
of a central χ2, 387
of a complex Gaussian random vec-
tor, 555–556
of a complex random vector, 326
of a CRV, 320
of a Gaussian random vector, 515
of a Gaussian RV, 385
of a pair of random variables, 320
of a random vector, 508–509
of a RV, 384
of a squared Gaussian, 386
of a standard complex Gaussian,
555
charge density, 258
circular symmetry, 540–559
of a complex Gaussian, 548
of a complex Gaussian vector, 553–
557
of a complex random vector
and linear functionals, 549
and linear transformations, 549
and properness, 550
deﬁnition, 548
of a CRV
and expectation, 541
and properness, 545
characterization, 543
deﬁnition, 541
of a CSP, 848
clock, 670
closed subspace, 148n, 157
code property, 713
Coding Theory, 707
codomain, 2
coherent decoder, 682
colored noise, 652–662, 829
compact support
function of, 288, 364
complete (space), 72, 157, 166
complete
orthonormal
system,
see
CONS
complex conjugate
of a matrix, 315
of a scalar, 15
complex dimensions per second, 296, 300
complex Gaussian
random variable, 545–548
centered, 546
circularly-symmetric, 548
deﬁnition, 546
proper, 546
random vector, 550–559
and linear transformations, 551
centered, 550
characteristic function, 555–556
characterization, 551
circularly-symmetric, 553
deﬁnition, 550
proper, 551, 553–557
complex magnitude, see absolute value
complex modulus, see absolute value
complex positive semideﬁnite matrix,
337, 553
complex random variable, see CRV
complex random vector
characteristic function, 326
circularly-symmetric, see circular
symmetry
covariance matrix, 323
deﬁnition, 323
expectation, 323
ﬁnite variance, 323
proper, 324–325
transforming, 326, 327, 339
complex sampling, 125, 169–170
reconstruction from, 170–173
complex signal, 2
complex stochastic process, see CSP
complex symbols per second, 296
complex-valued signal, 2
componentwise antipodal mapping, 710
composite hypothesis testing, 468n, 671
composition (of functions), 2
conditional
distribution, 398–399, 523
independence, 414, 529–533
probability, 443
conjugate (of a matrix), 315
conjugate-symmetric, 111
function, 66, 111
878
Index
matrix, 315
CONS, 147–166
characterization, 149
deﬁnition, 148
for closed subspaces, 159
for energy-limited signals that are
bandlimited to W Hz, 152, 153
for energy-limited signals that van-
ish outside an interval, 151
Prolate
Spheroidal
Wave
Func-
tions, 161
consistency property (of FDDs), 561
constellation
M-PSK, 304
of PAM, 185–191
centered, 186
deﬁnition, 185
minimum distance, 186
normalization, 186
number of points, 186
second moment, 186
of QAM, 303–305
centered, 305
deﬁnition, 303
minimum distance, 304
number of points, 303
second moment, 305
QPSK, 303
square 4-QAM, 303
convergence of random variables
almost surely, 390
in distribution, 391
in mean square, 390
in probability, 390
with probability one, 390
convergence of random vectors
almost surely, 528
in distribution, 528
in mean square, 528
in probability, 528
with probability one, 528
convex set, 769
convolution, 54–64, 69, 141
and delay, 62
and inner products, 63
and reﬂection, 62
baseband representation of,
129,
140
between real and complex signals,
124
continuity of, 63
derivative of, 63
FT of, 78
in Probability Theory, 63
is associative, 58, 283, 802
is commutative, 58
is distributive, 58
limits of, 361
of sequences, 63, 787
the adjoint of, 654, 655
the self-similarity of, 212
uniform continuity, 56
correlation coeﬃcient, 23
cost-to-go, 813
cost-to-reach, 815
covariance
between two CRVs, 319
between two RVs, 23
Covariance Inequality, 23
covariance matrix
and positive semideﬁnite matrices,
511
of a complex random vector, 323
of a real random vector, 504
singular, 506–508
covariance stationary, see WSS
Craig’s formula, 379
Crum, M. M., 846
CRV, 314–340
argument, 321
characteristic function, 320
circularly-symmetric, see circular
symmetry
covariance, 319
deﬁnition, 316
density, 316
distribution, 316
expectation, 314, 317
magnitude, 321
proper, 318–319
transforming, 320, 322
variance, 314, 317
cryptography, 542
CSP
centered, 328
continuous time
measurable, 349n
operational PSD, 349
deﬁnition, 328
discrete-time, 328–340
autocovariance function, 330
bounded second moment, 797
covariance stationary, see WSS
ﬁltering, 800
Index
879
Gaussian, 798–801
linear processing, 797–801
proper, 329, 798, 800, 802
PSD, 331
second-order
stationary,
see
WSS
spectral
distribution
function,
334
stationary, see stationary
strict-sense stationary, see sta-
tionary
strongly stationary, see station-
ary
weakly stationary, see WSS
wide-sense stationary, see WSS
WSS, see WSS
ﬁnite variance, 328
Gaussian, 800, 848
Cuﬀ, Paul, xxvi
cumulative distribution function, 377
cyclostationary, 257n
de Caen’s Inequality, 466
decision rule, see guessing rule
decoding rule, see guessing rule
degree-n trigonometric polynomial, 830
degrees of freedom
of a central χ2 distribution, 387
of a noncentral χ2 distribution, 388
of a signal, 100
delay
in PAM, 191–192
introduced by channel, 670
of an equivalence class, 53
Dembo, Amir, xxvii
dense subset of L1, 364
detection
probability of
(deterministic rule), 741
(randomized rule), 745
detection in WGN, 620–669
M-PSK, 640–642
antipodal signaling, 638–639
bi-orthogonal keying, 648–652
binary signaling, 638–640
from SP to vector, 621–625
in passband, 637–638
optimal decision rule, 628–632
orthogonal keying, 642–645
probability of error, 632–634
signals of inﬁnite bandwidth, 664
simplex, 645–648
diﬀerentiable complex function, 321
digital implementation, 192
dimension, 31, 711
Dirac’s Delta, 3
discrete-time
causality, 788
convolution, 787
ﬁlter, 786, 788
frequency response, 788
impulse response, 788
self-similarity, 788
stability, 788
stable inverse, 788, 789, 793, 794
discrete-time ISI channel, 807
discrete-time single-block model, 696
distance spectrum, see weight enumera-
tor
domain, 2
Dominated Convergence Theorem, 864
DTFT, 788, 841–844
dual code, 738
duality, 155
Durisi, Giuseppe, xxvii
Dynamic Programming, 809, 810, 822,
823
dynamic range, 634
e.g., xxii
early-late gate, 667
eigenvalue, 499
eigenvector, 499
encoder property, 713
energy
in baseband and passband, 128, 141
in PAM, 232–235
in QAM, 341–344
of a complex signal, 16
of a real signal, 14
energy per bit
in PAM, 234
in QAM, 344
energy per complex symbol
in PAM, 371
in QAM, 344
energy per symbol
in PAM, 234
in QAM, 344
energy-limited
passband
signal,
see
passband signal
energy-limited signal, 16
that is bandlimited, 47, 80–88
bandwidth of, 82
880
Index
continuity of, 85
deﬁnition, 81
of zero energy, 81
through a stable ﬁlter, 90
through an ideal unit-gain LPF, 86
entire function, 61, 94, 96, 97
of exponential type, 97
Ephraim, Yariv, xxvii
equal law
complex random variables, 316
complex random vectors, 323, 326
random variables, 219
random vectors, 219
equalization, 703
equivalence class, 49–51, 70
equivalence relation, 49
erasures, 784
essential supremum, 51n
estimation
and conditional expectation, 526
jointly Gaussian vectors, 526
Estimation Theory, 526
Euler’s Identity, 124
event, 3, 213
excess bandwidth, 203, 206, 300, 734
exclusive-or, 488, 708
expectation
of a complex random vector, 323
of a CRV, 317
of a random matrix, 504
of a random vector, 503
expected energy, 233
experiment outcome, 3, 213
exponential distribution, 387, 783
F2, 708
Factorization Theorem, 471–473
false alarm, 740
probability of
(deterministic rule), 741
(randomized rule), 745
FDD, 216, 560–563
consistency property, 561
of a continuous-time Gaussian SP,
563
symmetry property, 561
Fej´er’s kernel, 831
ﬁeld, 708
ﬁlter, 59–62
baseband representation of output,
131, 140
causal, 59
continuous time
impulse response, 59
output’s
baseband
representa-
tion, 131
stable, 59
discrete time, 786
causal, 788
minimum-phase, 827
stable, 788
whitening ﬁlter, 794
front-end, see front-end ﬁlter
stable, 59
whitening, see whitening ﬁlter
ﬁnite impulse response, 802
ﬁnite-dimensional distribution, see FDD
ﬁnite-variance
complex random vector, 323
complex stochastic process, 328
continuous-time real SP, 560
random vector, 504
FIR ﬁlter, 802
Fisher, R. A., 491
for example, xxii
Forney, David Jr., xxvii
Fourier Series, 151–152, 830–840
CONS, 835
pointwise reconstruction, 839
reconstruction in L1, 832
Fourier Series Coeﬃcient, 151, 830, 837
Fourier Transform, 65–103
boundedness, 74
conjugate-symmetric, 66, 104, 111–
112
continuity, 74
deﬁnition
for elements of L2, 72
for signals in L1, 65
diﬀerentiation of, 102
of sinc(·), 71, 76
of a product, 91
of baseband representation, 120
of convolution, 78
of real signals, 66
of symmetric signals, 66
of the Brickwall function, 68
preserves inner products, 66, 68–70
properties, 68
reconstructing from, 75
reconstructing using IFT, 75
frequency response
continuous time, 78
discrete time, 788
Index
881
of ideal unit-gain BPF, 80
of ideal unit-gain LPF, 79
with respect to a band, 132
front-end ﬁlter, 634–637
FT, see Fourier Transform
Fubini’s Theorem, 10, 11, 69
function, 14
all-zero, 3
codomain, 2, 14
domain, 2, 14
energy-limited, 15, 16
injective, 2, 178
integrable, 5
Lebesgue measurable, 4
notation, 2
one-to-one, 2, 178
onto, 2
range, 2
surjective, 2
Fundamental Theorem of Algebra, 188
Gaehwiler, Samuel, xxvi
Gallager, Robert, xxvii
Galois Field, 708
Gamma function, 387
Gauss-Markov, 802
Gaussian
complex random vector, see com-
plex Gaussian
continuous-time SP, 563–564, 566–
568, 572, 585–600, 603–613
deﬁnition, 563
FDDs, 563
ﬁltering, 594–600
linear functionals, 585–594
PSD, 572
stationary, 566
white, xx, 603–613
CRV, see complex Gaussian
CSP, 800, 848
radar problem, 759–760
random variable, 375
and aﬃne transformations, 375
characteristic function, 385
convergence, 390–392
density, 376
generating a, 393
MGF, 383
standard, see standard Gaussian
random vector, 495
a canonical representation, 518–
520
and aﬃne transformations, 513
and pairwise independence, 517
Bussgang’s Theorem, 538
centered, 494
characteristic function, 515
conditional independence, 529–
533
convergence, 528–529
density, 521–523
linear functionals of, 523
moments, 527
standard, see standard Gaussian
generalized random variable, 754n
generalized Rayleigh distribution, 388
generalized Rice distribution, 389
generator matrix, 712, 713
GF(2)
addition, 708
multiplication, 708
Gram, Jørgen Pedersen, 45
Gram-Schmidt procedure, 45–48
and bandwidth, 101
guessing rule
deﬁnition, 396, 442
deterministic (radar problem), 741
MAP, see MAP
maximum a posteriori, see MAP
maximum likelihood, see ML
ML, see ML
optimal, 397, 442
probability of error, 397, 442
randomized, 403–405, 446
randomized (radar problem), 745
with random parameter, 431–432
Hadamard code, 738
half-normal, 385n
Hamel basis, 148n
Hamming
and Euclidean distance, 711
code, 737
distance, 710
minimum distance, 724
weight, 710
hard decisions, 735
Hellinger distance, 440
Herglotz’s Theorem, 228
Hermite functions, 100
Hermitian conjugate, 315
Hermitian matrix, 315
Hilbert space, 49, 51, 166
Hilbert Transform, 142
882
Index
Hilbert Transform kernel, 142
Ho, Minnie, xxvii
H¨older’s Inequality, 18, 20, 57
holomorphic function, see analytic func-
tion
H¨osli, Daniel, xxvii
Huang, Qiuting, xxvi
Huber, Johannes, xxvi
hypothesis testing
M-ary, see multi-hypothesis testing
binary, see binary hypothesis test-
ing
I{·}, 1
i.e., xxii
ideal unit-gain bandpass ﬁlter, 62, 79,
106
frequency response, 62, 80
impulse response, 62
is not causal, 62
is unstable, 62
ideal unit-gain lowpass ﬁlter, 61
cutoﬀfrequency, 61
frequency response, 61, 79
impulse response, 61
is not causal, 61
is unstable, 61
IID random bits, 241
impulse response, 59
discrete time, 788
in particular, xxii
in-phase component, 124, 125, 139
of energy-limited signal, 139
of integrable signal, 125
Inclusion-Exclusion Formula, 466
indeed, xxii
independent random variables, 413, 517
independent stochastic processes
deﬁnition, 563
linear functionals of, 610
indistinguishable, 49
inﬁnite divisibility, 393
injective, 178
inner product, 14–26
and baseband representation, 128
and QAM, 305–309
and
the
analytic
representation,
117
and the baseband representation,
128, 140, 305–308
between complex signals, 15
between real signals, 14
between tuples, 426n
in other texts, 14n
properties, 16
space, 49, 51
integrable
complex functions, 17
complex signal, 5
passband signal, see passband sig-
nal
integrable signal
deﬁnition, 5
that is bandlimited, 88–90
bandwidth of, 90
through a stable ﬁlter, 91
integral
of a complex signal, 5–6
deﬁnition, 5
properties, 6
of a real signal, 4
intersymbol interference, see ISI
inverse ﬁlter, 788, 789, 793, 794, 801
of FIR ﬁlter, 802
uniqueness, 801
Inverse Fourier Transform, 66
deﬁnition, 67
of symmetric signals, 66
of the Brickwall function, 68
properties, 67
irrelevant data, 487–489
and random parameters, 490
ISI, xx, 703
ISI channel, 803
colored noise, 829
discrete-time, 807
PAM, 803–818
QAM, 818–823
ISI coeﬃcients, 807
isomorphism, 161, 173
Itˆo Calculus, 664
joint distribution function, 561n
jointly Gaussian random vectors, 523–
527
and estimation, 526
jointly Gaussian stochastic processes,
618
kernel, 709
Kim, Young-Han, xxvii
Knapsack Problem, 740, 746, 781
0-1, 746
fractional, 746
Index
883
Koch, Tobias, xxvi, xxvii
Koksal, Emre, xxvii
Kolmogorov’s Existence Theorem, 561
Kolmogorov, Andrey N., 398, 491
Kontoyiannis, Ioannis, xxvii
Kubin, Gernot, xxvi
L1, 5
L1-Fourier Transform, 65
L2, 15, 27–51, 70
L2, 44, 49–51, 70
L2-DTFT, 844
L2-Fourier Transform, 70–74
deﬁnition, 72
properties, 72
L2-Inverse Fourier Transform, 72
L2-Sampling Theorem, 155, 169, 171
for passband signals, 172
Laneman, Nicholas, xxvii
Lapidoth, Danielle, xxvi, xxviii
Laplace
distribution, 433
Pierre-Simon, 45
Transform, 383
Lebesgue integral, 4
Lebesgue measurable
complex signal, 5
real signal, 4
Lebesgue null set, see set of Lebesgue
measure zero
length of a vector, 31
Liapunov’s theorem, 769, 781
Liapunov, Alexey A., 769
likelihood-ratio function, 406, 747
likelihood-ratio test, 751–758
linear (K, N) F2 code, 711
linear (K, N) F2 encoder, 711
linear binary code with antipodal signal-
ing, 707–736
deﬁnition, 714
minimizing block errors, 720–725
max-correlation
decision
rule,
721
optimal decoding, 720
probability of a block error, 722–
725
power, 715, 718
PSD, 718
linear binary encoder with antipodal sig-
naling
deﬁnition, 714
minimizing bit errors, 725–729
optimal decoding, 725
probability of a bit error, 726–
729
linear combination, 29
linear functionals
of a Gaussian SP, 585–594
of a SP, 578–594
on Fκ
2, 715
on Rn
deﬁnition, 523
of Gaussian vectors, 523
linear mapping, 709
linear modulation, 181, 185
linear subspace, see subspace
linearly independent, 30
LLR(·), 407
Loeliger, Hans-Andrea, xxvi, xxvii
log likelihood-ratio function, 407
Log-Integral Inequality, 778
Log-Sum Inequality, 778
look-up table, 192
low-density parity-check codes, 736
lowpass ﬁlter, 61, see also ideal unit-gain
lowpass ﬁlter
LR(·), 406, 747
magnitude, see absolute value
MAP, 405–407, 446
mapping, see function
Marcum Q-function, 389, 768
Markov chain, 414, 477–478
mass density, 258
mass line density, 259
Massey, James, xxvii, 196
matched ﬁlter, 59–61, 182, 183
and inner products, 60–61
deﬁnition, 60
matched-ﬁlter bound, 706, 828
matrix
conjugate-symmetric, 315
conjugation, 315
Hermitian, 315
Hermitian conjugate, 315
orthogonal, 498
positive deﬁnite, 501
positive semideﬁnite
complex, 337, 553
real, 501
self-adjoint, 315
symmetric, 315
Toeplitz, 337
transpose, 315
884
Index
matrix representation of an encoder, 712
maximum a posteriori, see MAP
maximum distance separable (MDS),
739
maximum likelihood, see ML
maximum-correlation rule, 461, 630, 631
measurable
complex signal, 5
complex stochastic process, 349n
real signal, 4
stochastic process, 250, 577
memoryless
binary-input/output-symmetric,
730
property of the exponential, 217
message error, 691, 708
MGF, 383
deﬁnition, 383
of a central chi-square, 387
of a Gaussian, 383
of a noncentral chi-square, 388
of a squared Gaussian, 386
of the sum of independent RVs, 388
Miliou, Natalia, xxvii
minimal suﬃcient statistic, 493
minimax criterion, 783
minimum bandwidth, 202
minimum cost-to-go, 813
minimum cost-to-reach, 815
minimum Hamming distance, 724, 736,
739
Minimum Shift Keying, 665
minimum-phase, 827
mirror image, 3, 54, 67
missed-detection, 740
probability of
(deterministic rule), 741
(randomized rule), 745
Mittelholzer, Thomas, xxvii
ML, 407–408, 446
mod 2 addition, 708
modiﬁed zeroth-order Bessel function,
389, 677
modulation, 176
modulator, 176
modulus, see absolute value
Molkaraie, Mehdi, xxvi
moment generating function, see MGF
monotone likelihood ratio, 390, 537
Morgenshtern, Veniamin, xxvii
Moser, Stefan, xxvi, xxvii
M-PSK, 304, 447–451, 455–456, 640–642
multi-dimensional hypothesis testing
M-ary, 458–464
binary, 425–431
multi-hypothesis testing, 441–467
multiple antennas, 662
multiplication by a carrier
doubles the bandwidth, 108
FT of the result of, 108
multivariate Gaussian, 494–539, see also
Gaussian
N, 1
Narayan, Prakash, xxvii
nearest-neighbor decoding,
447,
448,
460, 461
Nefedov, Nikolai, xxvii
Neyman, Jerzy, 752
Neyman-Pearson Lemma, 752
noncentral chi-square distribution, 386–
390
noncoherent detection, 670–685, 763–
768
normal distribution, see Gaussian
n-tuple of vectors, 29
nuisance parameter, see random param-
eter
null hypothesis, 740
number of nearest neighbors, 724
Nyquist pulse, 199
Nyquist’s Criterion, 195–207
observation
in hypothesis testing, 396, 442
in the radar problem, 740
observation space
in hypothesis testing, 396
in the radar problem, 740
one-to-one, 178
open subset, 320, 320n
operational PSD, 257–294
and average autocovariance func-
tion, 276–282
and power, 285–292
and the PSD, 601
deﬁnition, 262–265
of a CSP, 349
of a ﬁltered SP, 283–285
of PAM, 266–294
of QAM, 349–354
uniqueness, 265
optimal guessing rule, 397, 442
orthogonal
Index
885
binary tuples, 738
real passband signals, 129
signals, 33
orthogonal keying, 642–645
noncoherent detection, 670–685
orthogonal matrix, 498
orthonormal
basis, 37–48
construction, 45
deﬁnition, 38
existence, 44
tuple, 37
packet, 693
pairwise independence, 517
pairwise suﬃciency, 473–477
Paley-Wiener, 97
PAM, 184–194, 232–256, 688–696
bandwidth, 188–189
baud period, 185
baud rate, 185
constellation, 185–191
centered, 186
deﬁnition, 185
minimum distance, 186
normalization, 186
number of points, 186
second moment, 186
detection in white noise, 688–696
digital implementation, 192
energy, 232–235
energy per bit, 234
energy per symbol, 234
operational PSD, 266–294
over ISI channel, 803–818
power, 235–256
pulse shape, 185
spectral eﬃciency, 296
Pareto-optimal pair
(deterministic rules), 742
(randomized rules), 745
parity-check matrix, 713
Parseval’s Theorem, 73, 118
Parseval-like theorems, 68–70
passband signal, 104–145
analytic representation of, 112, 137
deﬁnition, 105
energy-limited, 104, 133–141
bandwidth around a carrier, 107
baseband representation of, 138
characterization, 135, 136
deﬁnition, 106
is bandlimited, 136
sampling, 168–175
through BPF, 136
integrable, 104
analytic representation of, 112
bandwidth around a carrier, 107
baseband representation of, 119
characterization, 106
deﬁnition, 105
inner product, 117
is bandlimited, 107
is ﬁnite-energy, 107
through stable ﬁlter, 107
sampling, 168–175
passband SP, 848–857
passband stochastic process, 848–857
Pearson, Egon Sharpe, 752
periodic extension, 788, 830
periodic signal, 64, 103, 254, 294, 619
periodically continuous, 830
Pfander, G¨otz, xxvi
Pﬁster, Christoph, xxvi
phase shift keying, see M-PSK
picket fences, 97–99
picket-fence miracle, 98
π/4-QPSK, 370
Plackett’s Identities, 537
Plancherel’s Theorem, 73
Pointwise Sampling Theorem, 155, 170
for passband signals, 172
Poisson distribution, 389
Poisson summation, 97–99
positive deﬁnite function, 845–847
from R to C, 210
from R to R, 569
from Z to C, 331
from Z to R, 224
positive deﬁnite matrix, 501
positive semideﬁnite matrix
complex, 337, 340, 553
real, 501
postcursor, 807
postulated
densities, 808
model, 808
prior, 808
power
and operational PSD, 285–292
in baseband and passband, 345,
354–361
in PAM, 235–256
in QAM, 344–348
886
Index
of a SP, 250
power spectral density, see PSD
PPM, 193, 294
precursor, 807
prediction, 802
Price’s Theorem, 537
prima facie, xxii
prior
deﬁnition, 396, 441
nondegenerate, 396, 441
uniform, 396, 441
probability density function, 259
probability of detection
(deterministic rule), 741
(randomized rule), 745
probability of error
binary hypothesis testing
Bhattacharyya Bound, 408
general decision rule, 401
in IID Gaussian noise, 427–430
in white noise, 639, 640
no observables, 397
noncoherent, 680–681
optimal, 402, 444
multi-hypothesis testing
M-PSK, 448–451
8-PSK, 642
bi-orthogonal keying, 651
in IID Gaussian noise, 462–464
no observables, 443
noncoherent, 684–685
orthogonal keying, 644
simplex, 648
Union Bound, 453–456
Union-Bhattacharyya
Bound,
456–458
probability space, 3, 213
processing, 411–416, 446–447
projection
as best approximation, 41, 149
of a stochastic process, 607
onto a ﬁnite-dimensional subspace,
41
onto a vector in L2, 35
onto a vector in R2, 35
onto an inﬁnite-dimensional sub-
space, 165
Prolate
Spheroidal
Wave
Functions,
161–162
proper
complex Gaussian RV, 546
complex Gaussian vector, 551, 553–
557
complex random vector, 324–325
limits of, 336
continuous-time CSP, 848
CRV, 318–319
limits of, 334
discrete-time CSP, 329, 800, 802
PSD
of a continuous-time CSP, 848
of a continuous-time SP, 571, 600–
603
of a discrete-time CSP, 331
of a discrete-time SP, 224–229
pulse amplitude modulation, see PAM
pulse position modulation, 193, 294, see
PPM
pulse shape
in PAM, 185
in QAM, 298
Pythagoras’s Theorem, 33
Pythagorean Theorem, 34
QAM, 295–313, 341–372, 696–703
bandwidth, 299
baseband representation of, 297
baud period, 298
baud rate, 298
constellation, 303–305
centered, 305
deﬁnition, 303
minimum distance, 304
M-PSK, 304
number of points, 303
second moment, 305
square 4-QAM, 303
detection in white noise, 696–703
energy, 341–344
energy per bit, 344
energy per symbol, 344
ﬁltering, 309–311
inner products, 305–309
operational PSD, 349–354
over ISI channel, 818–823
power, 344–348
pulse shape, 298
spectral eﬃciency, 303
symbol recovery, 305–309
Q-function, 378–382
QPSK, 303
quadrature amplitude modulation, see
QAM
Index
887
quadrature component, 124, 125, 139
of energy-limited signal, 139
of integrable signal, 125
R, 1
R, 742
Rrnd, 745
radar, 740–781
radar problem
Gaussian, 759–760
signal in white noise, 760–762
suﬃcient statistic, 762–763
radially-symmetric function, 541
Radon-Nikodym Theorem, 442n
raised-cosine, 206
random bits, see IID random bits
random function, see stochastic process
random parameter, 431–432, 489–491,
674
and white noise, 670–685
random process, see stochastic process
random variable, 3, 213
random vector
characteristic function, 508–509
covariance matrix, 504
ﬁnite variance, 504
randomized decision rule, 403–405, 446
randomized guessing rule, see random-
ized decision rule
range
of a linear transformation, 709
of a mapping, 2
rate, 180
in bits per complex symbol, 179,
298
in bits per real symbol, 179
Rayleigh distribution, 388, 393
real dimensions per second, 185
real passband signals
analytic representation, see ana-
lytic representation
baseband representation, see base-
band representation
condition for orthogonality, 129
Sampling Theorem, see Sampling
Theorem
real positive semideﬁnite matrix, 501
real signal, 2
real symbols per second, 185
real-valued signal, 2
receiver operating characteristic, 743
reﬂection, 3, 54, see mirror image
relative entropy, 776–781, 783
between exponentials, 783
between Gaussians, 776
deﬁnition, 776
Log-Integral Inequality, 778
nonnegativity, 777
relative R´enyi entropy, 783, 784
repetition code, 737
representation of an encoder by a ma-
trix, 712
Rice distribution, 389, 768
Riemann
integrable, 4
integral, 4, 6
Riemann-Lebesgue Lemma, 834
Riesz, Frigyes, 846
Riesz-Fischer Theorem, 157
Rimoldi, Bixio, xxvi, xxvii
ROC, 743
Rockefeller Foundation, xxviii
Saengudomlert, Poompat, xxvii
sample function, see sample-path
sample of a stochastic process, 214
sample-and-hold, 614
sample-path, 213
sample-path
realization,
see
sample-
path
sampling as an isomorphism, 161
Sampling Theorem, 76, 147, 152–161
and convolution, 156
for passband signals, 168–175
isomorphism, 161
L2, 155
pointwise, 155
Sandwich Theorem, 159, 159n
Sanjoy, Mitter, xxvii
Sason, Igal, xxvi, xxvii
Schmidt, Erhard, 45
second-order stationary, see WSS
self-adjoint matrix, 315
self-similarity function
of a convolution, 212
of a sequence, 788
of energy-limited signal, 196–198
deﬁnition, 196
properties, 196
of integrable signal
deﬁnition, 208
FT of, 208
set of Lebesgue measure zero, 7–9
Shannon, Claude E., xviii, 178
888
Index
Shrader, Brook, xxvii
σ-algebra
generated by a SP, 562
generated by RVs, 399n
generated by the cylindrical sets,
562
product, 250, 250n
signal
complex, 14
real, 14
signature, 256
simplex, 645–648
simulating observables, 479–481
sinc(·), 76
deﬁnition, 61
FT of, 71, 76
is not integrable, 12
single parity check code, 711
Singleton Bound, 735, 736, 739
singular covariance matrix, 506–508
Slepian’s Inequality, 538
soft decisions, 735
SP, see stochastic process
span, 30
spectral eﬃciency, 296, 303
Spectral Theorem, 500
square summable, 786
stable inverse, 794
stable ﬁlter, 59, 788
stable inverse, 788, 789, 793, 801, 806
standard complex Gaussian
random variable, 540–541
and properness, 541
characteristic function, 555
deﬁnition, 540
density, 540
mean, 541
variance, 541
random vector, 548
covariance matrix, 548
deﬁnition, 548
density, 548
mean, 548
proper, 548
standard deviation, 376
standard Gaussian
complex vector, see standard com-
plex Gaussian
CRV, see standard complex Gaus-
sian
random variable
CDF, 377
deﬁnition, 373
density, 373
moments, 385
random vector
covariance matrix, 510
deﬁnition, 494
density, 509
mean, 510
standard inner product, 426n
state, 810
stationarization argument, 270
stationary
continuous-time SP, 564
discrete-time CSP, 328
discrete-time SP, 219, 220
Stein, Charles, 394
stochastic process, 178, 213–218
bounded variance, 276
centered, 215
complex, see CSP
continuous time
linear functionals, 585
continuous-time, 560–619
autocovariance
function,
565,
568–570
average power, 576–578
bandlimited, 266
bandwidth, 266
centered, 560
covariance stationary, see WSS
deﬁnition, 216
FDD, 560–563
ﬁltering, 594–600
ﬁnite variance, 560
ﬁnite-dimensional
distribution,
see FDD
Gaussian, see Gaussian
independence, 563
linear functionals, 578–594
measurable, 577
path, 560
projection, 607
PSD, 571, 600–603
realization, 560
sample-function, 560
sample-path, 560
second-order
stationary,
see
WSS
spectral
distribution
function,
573–576
state at time-t, 560
stationary, see stationary
Index
889
strict-sense stationary, see sta-
tionary
strongly stationary, see station-
ary
time shift, 564
time-t sample, 560
trajectory, 560
weakly stationary, see WSS
wide-sense stationary, see WSS
WSS, see WSS
deﬁnition, 215
discrete time
bounded second moment, 789
Gaussian, 793–794
linear processing of, 789–794
discrete-time, 219–231
autocorrelation function, 222
autocovariance
function,
222–
229
covariance stationary, see WSS
deﬁnition, 215
one-sided, 216
power spectral density, 224–229
second-order
stationary,
see
WSS
spectral
distribution
function,
228–229
stationary, see stationary
strict-sense stationary, see sta-
tionary
strongly stationary, see station-
ary
weakly stationary, see WSS
wide-sense stationary, see WSS
WSS, see WSS
ﬁnite variance, 215
measurable, 250
power of, 250
zero mean, 215
strict-sense stationary, see stationary
strictly stationary, see stationary
strictly systematic encoder, 713
strongly stationary, see stationary
sub-Nyquist sampling, 162, 164
subspace, 29, 147
closed, see closed subspace
ﬁnite-dimensional, 30, 147
basis for, 30
dimension of, 31
having an orthonormal basis, 41
projection onto, 41
inﬁnite-dimensional, 30, 147
suﬃcient statistics, 416–424, 468–493
and computability of the a posteri-
ori law, 421, 469
and pairwise suﬃciency, 473–477
and random parameters, 489
and simulating observables, 479–
481
and the likelihood-ratio function,
418
factorization criterion, 471–473
for the radar problem, 762–763
in binary hypothesis testing, 416–
424
Markov condition, 477–478
minimal, 493
superposition, 3, 20, 27
support
compact, 364
of a PSD, 362
survivor, 818
symmetric matrix, 315
symmetric random variable, 228
symmetry property (of FDDs), 561
systematic encoder, 713
systematic single parity check encoder,
711
Szeg˝o’s Theorem, 337
Sznitman, Alain-Sol, xxvi, xxvii
Taylor Series, 671
Tchamkerten, Aslan, xxvii
Telatar, ˙I. Emre, xxvi, xxvii
that is, xxii
tie, 403, 443
Tinguely, Stephan, xxvii
Toeplitz matrix, 337
total positivity of order 2, 390, 537
trajectory, see sample-path
Transform Method, 615
transforming
complex random variables, 320, 322
complex random vectors, 326, 327,
339
real random vectors, 320
transpose (of a matrix), 315
trellis, 818
Triangle Inequality
for complex numbers, 6
for complex random variables, 338
for signals, 31
for stochastic processes, 354
trigonometric polynomial, 830
890
Index
Troulis, Markos, xxvi
true model, 808
tuple
of bits, 180
of signals, 29
turbo-codes, 736
uncoded communication, 187, 305, 809
uncoded transmission, 187, 305, 809
uniformly continuous, 56, 56n
Union Bound, 451–458
Union-Bhattacharyya Bound, 456–458
Union-of-Events
Bound,
see
Union
Bound
unit circle, 188
univariate Gaussian, 373–394, see also
Gaussian
variance, 23, 25
variance of a CRV, 317
Varshamov Bound, 735, 736, 739
vector space, 28
Verd´u, Sergio, xxvii
Viterbi Algorithm, 703, 810, 815–818,
822, 823
add-compare-select, 818
cost-to-reach, 815
minimum cost-to-reach, 815
survivor, 818
Vontobel, Pascal, xxvii
Wagner’s rule, 721
Wang, Ligong, xxvii
weak convergence
of random variables, 391
of random vectors, 528
weakly stationary, see WSS
Weierstrass’s Approximation Theorem,
840
weight enumerator, 724
WGN, see white Gaussian noise
wheel-of-fortune, 541
white Gaussian noise, xx, 603–613, 760
deﬁnition, 603
detection in, 620–669
in passband, 613
properties, 604
white noise, see white Gaussian noise
white noise paradigm, 664
whitening ﬁlter, 802
and prediction, 802
causal, 802
deﬁnition, 653
discrete time, 794–797, 806, 807,
820
existence, 662
Wick’s Formula, 527
wide-sense stationary, see WSS
Wiener, Norbert, 788
Wiener-Khinchin criterion, 210
Wiener-Khinchin Theorem, 270, 600
Wigger, Mich`ele, xxvii
worst-case performance, 681
WSS
continuous-time CSP, 848
continuous-time SP, 276n, 565
discrete-time CSP, 328, 329
discrete-time SP, 220–229
Young’s Inequality, 62
Z, 1
Zeitouni, Ofer, xxvii
zero padding, 180
zero-forcing equalization, 828
zeroth-order modiﬁed Bessel function,
389, 677, 766
