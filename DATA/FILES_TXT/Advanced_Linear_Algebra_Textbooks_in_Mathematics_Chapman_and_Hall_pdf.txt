Advanced_Linear_Algebra_Textbooks_in_Mathematics_Chapman_and_Hall.pdf

Advanced Linear Algebra
Designed for advanced undergraduate and beginning graduate students in linear or abstract al-
gebra, Advanced Linear Algebra covers theoretical aspects of the subject, along with examples, 
computations, and proofs. It explores a variety of advanced topics in linear algebra that highlight 
the rich interconnections of the subject to geometry, algebra, analysis, combinatorics, numerical 
computation, and many other areas of mathematics.
The author begins with chapters introducing basic notation for vector spaces, permutations, 
polynomials, and other algebraic structures. The following chapters are designed to be mostly 
independent of each other so that readers with different interests can jump directly to the topic 
they want. This is an unusual organization compared to many abstract algebra textbooks, which 
require readers to follow the order of chapters.
Each chapter consists of a mathematical vignette devoted to the development of one specific 
topic. Some chapters look at introductory material from a sophisticated or abstract viewpoint, 
while others provide elementary expositions of more theoretical concepts. Several chapters offer 
unusual perspectives or novel treatments of standard results.
A wide array of topics is included, ranging from concrete matrix theory (basic matrix compu-
tations, determinants, normal matrices, canonical forms, matrix factorizations, and numerical 
algorithms) to more abstract linear algebra (modules, Hilbert spaces, dual vector spaces, bilinear 
forms, principal ideal domains, universal mapping properties, and multilinear algebra).
The book provides a bridge from elementary computational linear algebra to more advanced, 
abstract aspects of linear algebra needed in many areas of pure and applied mathematics. 
Nicholas A. Loehr received his Ph.D. in mathematics from the University of California at San 
Diego in 2003, studying algebraic combinatorics under the guidance of Professor Jeffrey Rem-
mel. After spending two years at the University of Pennsylvania as an NSF postdoc, Dr. Loehr 
taught mathematics at the College of William and Mary, the United States Naval Academy, and 
Virginia Tech. Dr. Loehr has authored over sixty refereed journal articles and three textbooks on 
combinatorics, advanced linear algebra, and mathematical proofs. He teaches classes in these 
subjects and many others, including cryptography, vector calculus, modern algebra, real analy-
sis, complex analysis, and number theory.
Textbooks in Mathematics
Series editors:
Al Boggess, Kenneth H. Rosen
Classical Vector Algebra
Vladimir Lepetic
Introduction to Number Theory
Mark Hunacek
Probability and Statistics for Engineering and the Sciences with Modeling using R
William P. Fox and Rodney X. Sturdivant
Computational Optimization:  Success in Practice
Vladislav Bukshtynov
Computational Linear Algebra:  with Applications and MATLAB® Computations
Robert E. White
Linear Algebra With Machine Learning and Data
Crista Arangala
Discrete Mathematics with Coding
Hugo D. Junghenn
Applied Mathematics for Scientists and Engineers
Youssef N. Raffoul
Graphs and Digraphs, Seventh Edition
Gary Chartrand, Heather Jordon, Vincent Vatter and Ping Zhang
An Introduction to Optimization with Applications in Data Analytics and Machine Learning
Jeffrey Paul Wheeler
Encounters with Chaos and Fractals, Third Edition
Denny Gulick and Jeff Ford
Differential Calculus in Several Variables
A Learning-by-Doing Approach
Marius Ghergu
Taking the “Oof!” out of Proofs
A Primer on Mathematical Proofs
Alexandr Draganov 
Vector Calculus
Steven G. Krantz and Harold Parks
Intuitive Axiomatic Set Theory
José Luis García
Fundamentals of Abstract Algebra
Mark J. DeBonis
A Bridge to Higher Mathematics
James R. Kirkwood and Raina S. Robeva
Advanced Linear Algebra, Second Edition
Nicholas A. Loehr
https://www.routledge.com/Textbooks-in-Mathematics/book-series/CANDHTEXBOOMTH 
Advanced Linear Algebra
Second Edition
Nicholas A. Loehr
Second edition published 2024
by CRC Press
2385 Executive Center Drive, Suite 320, Boca Raton, FL 33431
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC
© 2024 Nicholas A. Loehr 
First edition published by Taylor and Francis 2014 
Reasonable efforts have been made to publish reliable data and information, but the author and publisher cannot as-
sume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have 
attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders 
if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged, please 
write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or 
utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including pho-
tocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission 
from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.com or contact the 
Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. For works that are 
not available on CCC, please contact mpkbookspermissions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are used only for iden-
tification and explanation without intent to infringe.
ISBN: 978-1-032-76572-3 (hbk)
ISBN: 978-1-032-77742-9 (pbk)
ISBN: 978-1-003-48456-1 (ebk)
DOI: 10.1201/9781003484561
Typeset in CMR10 font 
by KnowledgeWorks Global Ltd.
Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.
Library of Congress Cataloging-in-Publication Data
Names: Loehr, Nicholas A, author.
Title: Advanced linear algebra / authored by Nicholas A Loehr.
Description: Second edition. | Boca Raton, FL : CRC Press, 2024. | Includes
bibliographical references and index.
Identifiers: LCCN 2023057206 | ISBN 9781032765723 (hbk) | ISBN
9781032777429 (pbk) | ISBN 9781003484561 (ebk)
Subjects: LCSH: Algebras, Linear. | BISAC: MATHEMATICS / Algebra / General.
| MATHEMATICS / Combinatorics. | SCIENCE / Physics.
Classification: LCC QA184.2 .L64 2024 | DDC 512/.5--dc23/eng/20240209
LC record available at https://lccn.loc.gov/2023057206
This book is dedicated to Dan Niehaus, T. S. Michael,
and Peter Linnell.
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
Contents
Preface
xvii
Part I
Background on Algebraic Structures
1
1
Overview of Algebraic Systems
3
1.1
Groups
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Rings and Fields
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.4
Subsystems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.5
Product Systems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.6
Quotient Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.7
Homomorphisms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.8
Spanning, Linear Independence, Basis, and Dimension
. . . . . . . . . . .
14
1.9
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.10
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2
Permutations
23
2.1
Symmetric Groups
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.2
Representing Functions as Directed Graphs
. . . . . . . . . . . . . . . . .
24
2.3
Cycle Decompositions of Permutations . . . . . . . . . . . . . . . . . . . .
25
2.4
Composition of Cycles
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.5
Factorizations of Permutations
. . . . . . . . . . . . . . . . . . . . . . . .
27
2.6
Inversions and Sorting
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.7
Signs of Permutations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.8
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.9
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3
Polynomials
35
3.1
Intuitive Definition of Polynomials
. . . . . . . . . . . . . . . . . . . . . .
35
3.2
Algebraic Operations on Polynomials . . . . . . . . . . . . . . . . . . . . .
36
3.3
Formal Power Series and Polynomials
. . . . . . . . . . . . . . . . . . . .
37
3.4
Properties of Degree
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.5
Evaluating Polynomials
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.6
Polynomial Division with Remainder . . . . . . . . . . . . . . . . . . . . .
42
3.7
Divisibility and Associates . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.8
Greatest Common Divisors of Polynomials . . . . . . . . . . . . . . . . . .
44
3.9
GCDs of Lists of Polynomials
. . . . . . . . . . . . . . . . . . . . . . . . .
46
3.10
Matrix Reduction Algorithm for GCDs . . . . . . . . . . . . . . . . . . . .
47
3.11
Roots of Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.12
Irreducible Polynomials
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.13
Unique Factorization of Polynomials
. . . . . . . . . . . . . . . . . . . . .
51
3.14
Prime Factorizations and Divisibility . . . . . . . . . . . . . . . . . . . . .
52
vii
viii
Contents
3.15
Irreducible Polynomials in Q[x]
. . . . . . . . . . . . . . . . . . . . . . . .
53
3.16
Testing Irreducibility in Q[x] via Reduction Modulo a Prime
. . . . . . .
54
3.17
Eisenstein’s Irreducibility Criterion for Q[x]
. . . . . . . . . . . . . . . . .
55
3.18
Lagrange’s Interpolation Formula . . . . . . . . . . . . . . . . . . . . . . .
56
3.19
Kronecker’s Algorithm for Factoring in Q[x] . . . . . . . . . . . . . . . . .
56
3.20
Algebraic Elements and Minimal Polynomials
. . . . . . . . . . . . . . . .
57
3.21
Multivariable Polynomials
. . . . . . . . . . . . . . . . . . . . . . . . . . .
59
3.22
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3.23
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
Part II
Matrices
71
4
Basic Matrix Operations
73
4.1
Formal Definition of Matrices and Vectors
. . . . . . . . . . . . . . . . . .
73
4.2
Vector Spaces of Functions
. . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.3
Matrix Operations via Entries . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.4
Properties of Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . .
77
4.5
Generalized Associativity
. . . . . . . . . . . . . . . . . . . . . . . . . . .
79
4.6
Invertible Matrices
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
4.7
Matrix Operations via Columns . . . . . . . . . . . . . . . . . . . . . . . .
82
4.8
Matrix Operations via Rows . . . . . . . . . . . . . . . . . . . . . . . . . .
83
4.9
Elementary Operations and Elementary Matrices
. . . . . . . . . . . . . .
85
4.10
Elementary Matrices and Gaussian Elimination
. . . . . . . . . . . . . . .
86
4.11
Elementary Matrices and Invertibility
. . . . . . . . . . . . . . . . . . . .
87
4.12
Row Rank and Column Rank
. . . . . . . . . . . . . . . . . . . . . . . . .
88
4.13
Conditions for Invertibility of a Matrix . . . . . . . . . . . . . . . . . . . .
89
4.14
Block Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.15
Tensor Product of Matrices
. . . . . . . . . . . . . . . . . . . . . . . . . .
92
4.16
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
4.17
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
5
Determinants via Calculations
105
5.1
Matrices with Entries in a Ring . . . . . . . . . . . . . . . . . . . . . . . .
105
5.2
Explicit Definition of the Determinant
. . . . . . . . . . . . . . . . . . . .
106
5.3
Diagonal and Triangular Matrices
. . . . . . . . . . . . . . . . . . . . . .
107
5.4
Changing Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
5.5
Transposes and Determinants
. . . . . . . . . . . . . . . . . . . . . . . . .
108
5.6
Multilinearity and the Alternating Property . . . . . . . . . . . . . . . . .
110
5.7
Elementary Row Operations and Determinants
. . . . . . . . . . . . . . .
111
5.8
Determinant Properties Involving Columns
. . . . . . . . . . . . . . . . .
113
5.9
Product Formula via Elementary Matrices . . . . . . . . . . . . . . . . . .
113
5.10
Laplace Expansions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
5.11
Classical Adjoints and Inverses
. . . . . . . . . . . . . . . . . . . . . . . .
117
5.12
Cramer’s Rule
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
5.13
Product Formula via Computations
. . . . . . . . . . . . . . . . . . . . .
119
5.14
Cauchy–Binet Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
5.15
Cayley–Hamilton Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . .
122
5.16
Permanents
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
5.17
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
5.18
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
Contents
ix
6
Comparing Concrete Linear Algebra to Abstract Linear Algebra
134
6.1
Column Vectors versus Abstract Vectors . . . . . . . . . . . . . . . . . . .
134
6.2
Examples of Computing Coordinates . . . . . . . . . . . . . . . . . . . . .
136
6.3
Operations on Column Vectors versus Abstract Vectors
. . . . . . . . . .
138
6.4
Matrices versus Linear Maps
. . . . . . . . . . . . . . . . . . . . . . . . .
140
6.5
Examples of Matrices Associated with Linear Maps . . . . . . . . . . . . .
141
6.6
Vector Operations on Matrices and Linear Maps
. . . . . . . . . . . . . .
144
6.7
Matrix Transpose versus Dual Maps
. . . . . . . . . . . . . . . . . . . . .
145
6.8
Matrix/Vector Multiplication versus Evaluation of Maps . . . . . . . . . .
146
6.9
Matrix Multiplication versus Composition of Linear Maps
. . . . . . . . .
146
6.10
Transition Matrices and Changing Coordinates
. . . . . . . . . . . . . . .
147
6.11
Changing Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
6.12
Algebras of Matrices versus Algebras of Linear Operators
. . . . . . . . .
150
6.13
Similarity of Matrices versus Similarity of Linear Maps . . . . . . . . . . .
151
6.14
Diagonalizability and Triangulability
. . . . . . . . . . . . . . . . . . . . .
151
6.15
Block-Triangular Matrices and Invariant Subspaces . . . . . . . . . . . . .
153
6.16
Block-Diagonal Matrices and Reducing Subspaces
. . . . . . . . . . . . .
154
6.17
Idempotent Matrices and Projections . . . . . . . . . . . . . . . . . . . . .
155
6.18
Bilinear Maps and Matrices
. . . . . . . . . . . . . . . . . . . . . . . . . .
156
6.19
Congruence of Matrices
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
6.20
Real Inner Product Spaces and Orthogonal Matrices
. . . . . . . . . . . .
158
6.21
Complex Inner Product Spaces and Unitary Matrices
. . . . . . . . . . .
160
6.22
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
6.23
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
Part III Matrices with Special Structure
173
7
Hermitian, Positive Definite, Unitary, and Normal Matrices
175
7.1
Conjugate-Transpose of a Matrix
. . . . . . . . . . . . . . . . . . . . . . .
175
7.2
Hermitian Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
7.3
Hermitian Decomposition of a Matrix
. . . . . . . . . . . . . . . . . . . .
179
7.4
Positive Definite Matrices
. . . . . . . . . . . . . . . . . . . . . . . . . . .
180
7.5
Unitary Matrices
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
7.6
Unitary Similarity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
7.7
Unitary Triangularization
. . . . . . . . . . . . . . . . . . . . . . . . . . .
184
7.8
Simultaneous Triangularization
. . . . . . . . . . . . . . . . . . . . . . . .
185
7.9
Normal Matrices and Unitary Diagonalization . . . . . . . . . . . . . . . .
186
7.10
Polynomials and Commuting Matrices
. . . . . . . . . . . . . . . . . . . .
188
7.11
Simultaneous Unitary Diagonalization
. . . . . . . . . . . . . . . . . . . .
189
7.12
Polar Decomposition: Invertible Case . . . . . . . . . . . . . . . . . . . . .
190
7.13
Polar Decomposition: General Case . . . . . . . . . . . . . . . . . . . . . .
191
7.14
Interlacing Eigenvalues for Hermitian Matrices
. . . . . . . . . . . . . . .
192
7.15
Determinant Criterion for Positive Definite Matrices
. . . . . . . . . . . .
194
7.16
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
7.17
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
196
8
Jordan Canonical Forms
202
8.1
Examples of Nilpotent Maps
. . . . . . . . . . . . . . . . . . . . . . . . .
203
8.2
Partition Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
8.3
Partition Diagrams and Nilpotent Maps
. . . . . . . . . . . . . . . . . . .
205
8.4
Computing Images via Partition Diagrams . . . . . . . . . . . . . . . . . .
206
x
Contents
8.5
Computing Null Spaces via Partition Diagrams
. . . . . . . . . . . . . . .
208
8.6
Classification of Nilpotent Maps (Stage 1)
. . . . . . . . . . . . . . . . . .
208
8.7
Classification of Nilpotent Maps (Stage 2)
. . . . . . . . . . . . . . . . . .
209
8.8
Classification of Nilpotent Maps (Stage 3)
. . . . . . . . . . . . . . . . . .
210
8.9
Fitting’s Lemma
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
8.10
Existence of Jordan Canonical Forms . . . . . . . . . . . . . . . . . . . . .
212
8.11
Uniqueness of Jordan Canonical Forms . . . . . . . . . . . . . . . . . . . .
213
8.12
Computing Jordan Canonical Forms
. . . . . . . . . . . . . . . . . . . . .
214
8.13
Application to Differential Equations . . . . . . . . . . . . . . . . . . . . .
216
8.14
Minimal Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
217
8.15
Jordan–Chevalley Decomposition of a Linear Operator . . . . . . . . . . .
218
8.16
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
8.17
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
9
Matrix Factorizations
226
9.1
Approximation by Orthonormal Vectors
. . . . . . . . . . . . . . . . . . .
227
9.2
Gram–Schmidt Orthonormalization Algorithm
. . . . . . . . . . . . . . .
228
9.3
Gram–Schmidt QR Factorization
. . . . . . . . . . . . . . . . . . . . . . .
230
9.4
Householder Reflections
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
9.5
Householder QR Factorization
. . . . . . . . . . . . . . . . . . . . . . . .
233
9.6
LU Factorization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
9.7
Example of the LU Factorization
. . . . . . . . . . . . . . . . . . . . . . .
237
9.8
LU Factorizations and Gaussian Elimination
. . . . . . . . . . . . . . . .
238
9.9
Permuted LU Factorizations . . . . . . . . . . . . . . . . . . . . . . . . . .
240
9.10
Cholesky Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
9.11
Least Squares Approximation
. . . . . . . . . . . . . . . . . . . . . . . . .
242
9.12
Singular Value Decomposition . . . . . . . . . . . . . . . . . . . . . . . . .
243
9.13
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
245
9.14
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
247
10 Iterative Algorithms in Numerical Linear Algebra
252
10.1
Richardson’s Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
252
10.2
Jacobi’s Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
253
10.3
Gauss–Seidel Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
254
10.4
Vector Norms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
255
10.5
Metric Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
257
10.6
Convergence of Sequences
. . . . . . . . . . . . . . . . . . . . . . . . . . .
257
10.7
Comparable Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258
10.8
Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
259
10.9
Formulas for Matrix Norms
. . . . . . . . . . . . . . . . . . . . . . . . . .
261
10.10 Matrix Inversion via Geometric Series
. . . . . . . . . . . . . . . . . . . .
262
10.11 Affine Iteration and Richardson’s Algorithm . . . . . . . . . . . . . . . . .
263
10.12 Splitting Matrices and Jacobi’s Algorithm
. . . . . . . . . . . . . . . . . .
264
10.13 Induced Matrix Norms and the Spectral Radius . . . . . . . . . . . . . . .
265
10.14 Analysis of the Gauss–Seidel Algorithm
. . . . . . . . . . . . . . . . . . .
266
10.15 Power Method for Finding Eigenvalues . . . . . . . . . . . . . . . . . . . .
266
10.16 Shifted and Inverse Power Method
. . . . . . . . . . . . . . . . . . . . . .
268
10.17 Deflation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
269
10.18 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
270
10.19 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
272
Contents
xi
Part IV
The Interplay of Geometry and Linear Algebra
279
11 Affine Geometry and Convexity
281
11.1
Linear Subspaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
11.2
Examples of Linear Subspaces . . . . . . . . . . . . . . . . . . . . . . . . .
282
11.3
Characterizations of Linear Subspaces
. . . . . . . . . . . . . . . . . . . .
283
11.4
Affine Combinations and Affine Sets
. . . . . . . . . . . . . . . . . . . . .
284
11.5
Affine Sets and Linear Subspaces
. . . . . . . . . . . . . . . . . . . . . . .
286
11.6
The Affine Span of a Set . . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
11.7
Affine Independence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
11.8
Affine Bases and Barycentric Coordinates
. . . . . . . . . . . . . . . . . .
288
11.9
Characterizations of Affine Sets . . . . . . . . . . . . . . . . . . . . . . . .
289
11.10 Affine Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
290
11.11 Convex Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
292
11.12 Convex Hulls
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
292
11.13 Carath´eodory’s Theorem on Convex Hulls
. . . . . . . . . . . . . . . . . .
293
11.14 Hyperplanes and Half-Spaces in Rn . . . . . . . . . . . . . . . . . . . . . .
295
11.15 Closed Convex Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
296
11.16 Cones and Convex Cones
. . . . . . . . . . . . . . . . . . . . . . . . . . .
298
11.17 Intersection Lemma for V-Cones
. . . . . . . . . . . . . . . . . . . . . . .
299
11.18 All H-Cones Are V-Cones
. . . . . . . . . . . . . . . . . . . . . . . . . . .
300
11.19 Projection Lemma for H-Cones
. . . . . . . . . . . . . . . . . . . . . . . .
301
11.20 All V-Cones Are H-Cones
. . . . . . . . . . . . . . . . . . . . . . . . . . .
302
11.21 Finite Intersections of Closed Half-Spaces
. . . . . . . . . . . . . . . . . .
303
11.22 Convex Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
305
11.23 Derivative Tests for Convex Functions
. . . . . . . . . . . . . . . . . . . .
306
11.24 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
307
11.25 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
310
12 Ruler and Compass Constructions
317
12.1
Geometric Constructibility
. . . . . . . . . . . . . . . . . . . . . . . . . .
318
12.2
Arithmetic Constructibility
. . . . . . . . . . . . . . . . . . . . . . . . . .
319
12.3
Preliminaries on Field Extensions . . . . . . . . . . . . . . . . . . . . . . .
320
12.4
Field-Theoretic Constructibility . . . . . . . . . . . . . . . . . . . . . . . .
322
12.5
Proof that GC ⊆AC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
322
12.6
Proof that AC ⊆GC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324
12.7
Algebraic Elements and Minimal Polynomials
. . . . . . . . . . . . . . . .
328
12.8
Proof that AC = SQC
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
330
12.9
Impossibility of Geometric Construction Problems
. . . . . . . . . . . . .
331
12.10 Constructibility of a Regular 17-Sided Polygon
. . . . . . . . . . . . . . .
332
12.11 Overview of Solvability by Radicals . . . . . . . . . . . . . . . . . . . . . .
334
12.12 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
335
12.13 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
336
13 Dual Vector Spaces
341
13.1
Vector Spaces of Linear Maps
. . . . . . . . . . . . . . . . . . . . . . . . .
341
13.2
Dual Bases
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
342
13.3
The Zero-Set Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
343
13.4
The Annihilator Operator
. . . . . . . . . . . . . . . . . . . . . . . . . . .
344
13.5
The Double Dual V ∗∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
13.6
Correspondence between Subspaces of V and V ∗
. . . . . . . . . . . . . .
347
xii
Contents
13.7
Dual Maps
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
348
13.8
Bilinear Pairings of Vector Spaces
. . . . . . . . . . . . . . . . . . . . . .
350
13.9
Theorems on Bilinear Pairings
. . . . . . . . . . . . . . . . . . . . . . . .
351
13.10 Real Inner Product Spaces
. . . . . . . . . . . . . . . . . . . . . . . . . .
353
13.11 Complex Inner Product Spaces
. . . . . . . . . . . . . . . . . . . . . . . .
354
13.12 Duality for Infinite-Dimensional Spaces
. . . . . . . . . . . . . . . . . . .
355
13.13 A Preview of Affine Algebraic Geometry . . . . . . . . . . . . . . . . . . .
357
13.14 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
358
13.15 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
360
14 Bilinear Forms
367
14.1
Definition of Bilinear Forms
. . . . . . . . . . . . . . . . . . . . . . . . . .
368
14.2
Examples of Bilinear Forms
. . . . . . . . . . . . . . . . . . . . . . . . . .
369
14.3
Matrix of a Bilinear Form
. . . . . . . . . . . . . . . . . . . . . . . . . . .
371
14.4
Congruence of Matrices
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
373
14.5
Orthogonality in Bilinear Spaces
. . . . . . . . . . . . . . . . . . . . . . .
374
14.6
Bilinear Forms and Dual Spaces
. . . . . . . . . . . . . . . . . . . . . . .
376
14.7
Theorem on Orthogonal Complements
. . . . . . . . . . . . . . . . . . . .
377
14.8
Radical of a Bilinear Form . . . . . . . . . . . . . . . . . . . . . . . . . . .
378
14.9
Diagonalization of Symmetric Bilinear Forms
. . . . . . . . . . . . . . . .
379
14.10 Structure of Alternate Bilinear Forms
. . . . . . . . . . . . . . . . . . . .
381
14.11 Totally Isotropic Subspaces
. . . . . . . . . . . . . . . . . . . . . . . . . .
382
14.12 Orthogonal Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
384
14.13 Reflections
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
385
14.14 Writing Orthogonal Maps as Compositions of Reflections
. . . . . . . . .
387
14.15 Witt’s Cancellation Theorem
. . . . . . . . . . . . . . . . . . . . . . . . .
389
14.16 Uniqueness Property of Witt Decompositions
. . . . . . . . . . . . . . . .
390
14.17 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
390
14.18 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
393
15 Metric Spaces and Hilbert Spaces
399
15.1
Metric Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
400
15.2
Convergent Sequences
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
401
15.3
Closed Sets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
402
15.4
Open Sets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
403
15.5
Continuous Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
404
15.6
Compact Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
405
15.7
Completeness
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
406
15.8
Definition of a Hilbert Space
. . . . . . . . . . . . . . . . . . . . . . . . .
408
15.9
Examples of Hilbert Spaces
. . . . . . . . . . . . . . . . . . . . . . . . . .
409
15.10 Proof of the Hilbert Space Axioms for ℓ2(X)
. . . . . . . . . . . . . . . .
411
15.11 Basic Properties of Hilbert Spaces
. . . . . . . . . . . . . . . . . . . . . .
413
15.12 Closed Convex Sets in Hilbert Spaces
. . . . . . . . . . . . . . . . . . . .
415
15.13 Orthogonal Complements
. . . . . . . . . . . . . . . . . . . . . . . . . . .
416
15.14 Orthonormal Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
417
15.15 Maximal Orthonormal Sets
. . . . . . . . . . . . . . . . . . . . . . . . . .
418
15.16 Isomorphism of H and ℓ2(X)
. . . . . . . . . . . . . . . . . . . . . . . . .
419
15.17 Continuous Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . .
421
15.18 Dual Space of a Hilbert Space . . . . . . . . . . . . . . . . . . . . . . . . .
423
15.19 Adjoints
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
424
Contents
xiii
15.20 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
425
15.21 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
427
Part V
Modules and Classification Theorems
435
16 Finitely Generated Commutative Groups
437
16.1
Commutative Groups
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
437
16.2
Generating Sets for Commutative Groups
. . . . . . . . . . . . . . . . . .
439
16.3
Z-Independence and Z-Bases
. . . . . . . . . . . . . . . . . . . . . . . . .
440
16.4
Elementary Operations on Z-Bases
. . . . . . . . . . . . . . . . . . . . . .
441
16.5
Coordinates and Z-Linear Maps . . . . . . . . . . . . . . . . . . . . . . . .
442
16.6
UMP for Free Commutative Groups
. . . . . . . . . . . . . . . . . . . . .
443
16.7
Quotient Groups of Free Commutative Groups
. . . . . . . . . . . . . . .
444
16.8
Subgroups of Free Commutative Groups
. . . . . . . . . . . . . . . . . . .
445
16.9
Z-Linear Maps and Integer Matrices
. . . . . . . . . . . . . . . . . . . . .
447
16.10 Elementary Operations and Change of Basis . . . . . . . . . . . . . . . . .
449
16.11 Reduction Theorem for Integer Matrices
. . . . . . . . . . . . . . . . . . .
451
16.12 Structure of Z-Linear Maps of Free Commutative Groups
. . . . . . . . .
454
16.13 Structure of Finitely Generated Commutative Groups
. . . . . . . . . . .
454
16.14 Example of the Reduction Algorithm . . . . . . . . . . . . . . . . . . . . .
456
16.15 Some Special Subgroups
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
458
16.16 Uniqueness Proof: Free Case
. . . . . . . . . . . . . . . . . . . . . . . . .
459
16.17 Uniqueness Proof: Prime Power Case . . . . . . . . . . . . . . . . . . . . .
460
16.18 Uniqueness of Elementary Divisors
. . . . . . . . . . . . . . . . . . . . . .
463
16.19 Uniqueness of Invariant Factors . . . . . . . . . . . . . . . . . . . . . . . .
463
16.20 Uniqueness Proof: General Case
. . . . . . . . . . . . . . . . . . . . . . .
464
16.21 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
465
16.22 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
467
17 Introduction to Modules
477
17.1
Module Axioms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
477
17.2
Examples of Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
479
17.3
Submodules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
480
17.4
Submodule Generated by a Subset
. . . . . . . . . . . . . . . . . . . . . .
481
17.5
Direct Products and Direct Sums . . . . . . . . . . . . . . . . . . . . . . .
482
17.6
Homomorphism Modules
. . . . . . . . . . . . . . . . . . . . . . . . . . .
484
17.7
Quotient Modules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
485
17.8
Changing the Ring of Scalars
. . . . . . . . . . . . . . . . . . . . . . . . .
486
17.9
Fundamental Homomorphism Theorem for Modules
. . . . . . . . . . . .
487
17.10 More Module Isomorphism Theorems
. . . . . . . . . . . . . . . . . . . .
488
17.11 Free Modules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
491
17.12 Finitely Generated Modules over a Division Ring
. . . . . . . . . . . . . .
493
17.13 Zorn’s Lemma
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
495
17.14 Existence of Bases for Modules over Division Rings . . . . . . . . . . . . .
496
17.15 Basis Invariance for Modules over Division Rings
. . . . . . . . . . . . . .
497
17.16 Basis Invariance for Free Modules over Commutative Rings
. . . . . . . .
498
17.17 Jordan–H¨older Theorem for Modules . . . . . . . . . . . . . . . . . . . . .
500
17.18 Modules of Finite Length
. . . . . . . . . . . . . . . . . . . . . . . . . . .
502
17.19 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
502
17.20 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
505
xiv
Contents
18 Principal Ideal Domains, Modules over PIDs, and Canonical Forms
514
18.1
Principal Ideal Domains
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
515
18.2
Divisibility in Commutative Rings
. . . . . . . . . . . . . . . . . . . . . .
515
18.3
Divisibility and Ideals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
516
18.4
Prime and Irreducible Elements . . . . . . . . . . . . . . . . . . . . . . . .
517
18.5
Irreducible Factorizations in PIDs
. . . . . . . . . . . . . . . . . . . . . .
518
18.6
Free Modules over a PID
. . . . . . . . . . . . . . . . . . . . . . . . . . .
519
18.7
Operations on Bases
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
520
18.8
Matrices of Linear Maps between Free Modules
. . . . . . . . . . . . . . .
521
18.9
Reduction Theorem for Matrices over a PID . . . . . . . . . . . . . . . . .
523
18.10 Structure Theorems for Linear Maps and Modules
. . . . . . . . . . . . .
525
18.11 Minors and Matrix Invariants
. . . . . . . . . . . . . . . . . . . . . . . . .
526
18.12 Uniqueness of Smith Normal Form
. . . . . . . . . . . . . . . . . . . . . .
527
18.13 Torsion Submodules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
528
18.14 Uniqueness of Invariant Factors . . . . . . . . . . . . . . . . . . . . . . . .
529
18.15 Uniqueness of Elementary Divisors
. . . . . . . . . . . . . . . . . . . . . .
531
18.16 F[x]-Module Defined by a Linear Operator
. . . . . . . . . . . . . . . . .
532
18.17 Rational Canonical Form of a Linear Map
. . . . . . . . . . . . . . . . . .
534
18.18 Jordan Canonical Form of a Linear Map . . . . . . . . . . . . . . . . . . .
535
18.19 Canonical Forms of Matrices
. . . . . . . . . . . . . . . . . . . . . . . . .
536
18.20 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
538
18.21 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
540
Part VI
Universal Mapping Properties and Multilinear
Algebra
549
19 Introduction to Universal Mapping Properties
551
19.1
Bases of Free R-Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . .
553
19.2
Homomorphisms out of Quotient Modules
. . . . . . . . . . . . . . . . . .
553
19.3
Direct Product of Two Modules . . . . . . . . . . . . . . . . . . . . . . . .
555
19.4
Direct Sum of Two Modules . . . . . . . . . . . . . . . . . . . . . . . . . .
556
19.5
Direct Products of Arbitrary Families of R-Modules
. . . . . . . . . . . .
557
19.6
Direct Sums of Arbitrary Families of R-Modules
. . . . . . . . . . . . . .
558
19.7
Solving Universal Mapping Problems . . . . . . . . . . . . . . . . . . . . .
561
19.8
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
563
19.9
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
565
20 Universal Mapping Problems in Multilinear Algebra
571
20.1
Multilinear Maps
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
571
20.2
Alternating Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
572
20.3
Symmetric Maps
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
573
20.4
Tensor Product of Modules
. . . . . . . . . . . . . . . . . . . . . . . . . .
574
20.5
Exterior Powers of a Module
. . . . . . . . . . . . . . . . . . . . . . . . .
577
20.6
Symmetric Powers of a Module
. . . . . . . . . . . . . . . . . . . . . . . .
579
20.7
Myths about Tensor Products . . . . . . . . . . . . . . . . . . . . . . . . .
581
20.8
Tensor Product Isomorphisms . . . . . . . . . . . . . . . . . . . . . . . . .
582
20.9
Associativity of Tensor Products
. . . . . . . . . . . . . . . . . . . . . . .
584
20.10 Tensor Product of Maps
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
585
20.11 Bases and Multilinear Maps . . . . . . . . . . . . . . . . . . . . . . . . . .
586
20.12 Bases for Tensor Products of Free Modules
. . . . . . . . . . . . . . . . .
588
20.13 Bases and Alternating Maps . . . . . . . . . . . . . . . . . . . . . . . . . .
589
Contents
xv
20.14 Bases for Exterior Powers of Free Modules . . . . . . . . . . . . . . . . . .
590
20.15 Bases for Symmetric Powers of Free Modules
. . . . . . . . . . . . . . . .
591
20.16 Tensor Product of Matrices
. . . . . . . . . . . . . . . . . . . . . . . . . .
592
20.17 Determinants and Exterior Powers
. . . . . . . . . . . . . . . . . . . . . .
593
20.18 From Modules to Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . .
594
20.19 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
596
20.20 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
600
Appendix: Basic Definitions
607
Sets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
607
Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
607
Relations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
608
Partially Ordered Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
609
Further Reading
611
Bibliography
615
Index
619
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
Preface
What is linear algebra, and how is it used? Upon examining almost any introductory text
on linear algebra, we find a standard list of topics that seems to define the subject. On
one hand, one part of linear algebra consists of computational techniques for solving linear
equations, multiplying and inverting matrices, calculating and interpreting determinants,
finding eigenvalues and eigenvectors, and so on. On the other hand, there is a theoretical side
to linear algebra involving abstract vector spaces, subspaces, linear independence, spanning
sets, bases, dimension, and linear transformations.
But there is much more to linear algebra than just vector spaces, matrices, and linear
equations! The goal of this book is to explore a variety of advanced topics in linear algebra
which highlight the rich interconnections linking this subject to geometry, algebra, analysis,
combinatorics, numerical computation, and many other areas of mathematics. The book
consists of twenty chapters, grouped into six main subject areas (algebraic structures,
matrices, structured matrices, geometric aspects of linear algebra, modules, and multilinear
algebra). Some chapters approach introductory material from a more sophisticated or
abstract viewpoint, other chapters provide elementary expositions of more theoretical
concepts, yet other chapters offer unusual perspectives or novel treatments of standard
results.
Unlike some advanced mathematical texts, this book has been carefully designed to
minimize the dependence of each chapter on material found in earlier chapters. Each chapter
has been conceived as a “mathematical vignette” devoted to the development of one specific
topic. If you need to learn about Jordan canonical forms, ruler and compass constructions,
the singular value decomposition, Hilbert spaces, QR factorizations, convexity, normal
matrices, or modules, you may turn immediately to the relevant chapter without first wading
through ten chapters of algebraic background or worrying that you will need a theorem
covered two hundred pages earlier. We do assume the reader has already encountered
the basic linear algebra concepts described earlier (solving linear systems, computing with
matrices and determinants, knowing elementary facts about vector spaces and linear maps).
These topics are all revisited in a more sophisticated setting at various points throughout
the book, but this is not the book you should read to learn the mechanics of Gaussian
elimination or matrix multiplication for the first time! Chapter 1 provides a condensed
review of some pertinent definitions from abstract algebra. But, the vast majority of the
book requires very little knowledge of abstract algebra beyond the definitions of fields, vector
spaces over a field, subspaces, linear transformations, linear independence, and bases. The
last three chapters build on the material on modules covered in Chapter 17, and in a few
places, we need some facts about permutations (covered in Chapter 2) and polynomials
(discussed in Chapter 3).
Although this book focuses on theoretical aspects of linear algebra, giving complete
proofs of all results, we supplement and explain the general theory with many specific
examples and concrete computations. The level of abstraction gradually increases as
deliberately avoided presenting the mathematical content as a dry skeleton of itemized
definitions, theorems, and proofs. Instead, the material is presented in a narrative format,
xvii
the book progresses, as we move from matrices to vector spaces to modules. We have
xviii
Preface
providing motivation, examples, and informal discussions to help the reader understand the
significance of the theorems and the intuition behind the proofs. Each chapter ends with a
summary containing a structured list of the principal definitions and results covered in the
chapter, followed by a set of exercises. Most exercises are not used in the main text and
consist of examples, computations, and proofs intended to aid the reader in assimilating the
material from the chapter. You are encouraged to use a computer algebra system to help
solve computationally intensive exercises.
This text is designed for use by advanced undergraduates or beginning graduate students,
or as a reference. A second course in linear algebra might cover portions of the chapters
in Parts II, III, and IV, reviewing Part I as needed. A second course in abstract algebra,
focusing on the role of linear-algebraic ideas, might cover Parts I, V, and VI, along with
selections from the rest of the book (such as Chapter 12 or Chapter 13). The next section
describes the contents of each chapter. For more details on what is covered in a given
chapter, consult the summary for that chapter.
Synopsis of Topics Covered
Part I: Background on Algebraic Structures
Chapter 1: Overview of Algebraic Systems. This chapter contains a condensed
reference for the abstract algebra background needed in some parts of the book. We review
the definitions of groups, commutative groups, rings, integral domains, fields, vector spaces,
algebras, and homomorphisms of these structures. We also quickly cover the constructions of
subgroups, product groups, quotient groups, and their analogs for other algebraic systems.
Finally, we recall some fundamental definitions and results from elementary linear algebra
involving linear independence, spanning sets, bases, and dimension. Most of the later
chapters in this book require only a small subset of the material covered here, such as
the definitions of a field, vector space, linear map, subspace, linearly independent list, and
ordered basis.
Chapter 2: Permutations. This chapter provides the basic definitions and facts about
permutations that are required for the study of determinants and multilinear algebra. After
describing the symmetric groups Sn, we show how to visualize functions on a finite set using
directed graphs. This idea leads to factorizations of permutations into products of disjoint
cycles. We can also write permutations as products of transpositions or basic transpositions.
To understand the role of basic transpositions, we introduce inversions and prove that a list
of numbers f = [f(1), f(2), . . . , f(n)] can be sorted into increasing order by interchanging
adjacent elements inv(f) times. For permutations f, we define sgn(f) = (−1)inv(f) and use
this formula to establish the fundamental properties of the sgn function.
Chapter 3: Polynomials. This chapter covers background on one-variable polynomials
over a field. This material is used in chapters on canonical forms, ruler and compass
constructions, primary decompositions, commuting matrices, etc. Topics include intuitive
and formal definitions of polynomials, evaluation homomorphisms, the universal mapping
property for polynomial rings, polynomial division with remainder, greatest common
divisors, roots of polynomials, irreducible polynomials, existence and uniqueness of prime
factorizations, minimal polynomials, and ways to test polynomials for irreducibility.
Preface
xix
Part II: Matrices
Chapter 4: Basic Matrix Operations. This chapter revisits some basic material about
matrices from a somewhat more sophisticated perspective. Topics studied include formal
definitions of matrices and matrix operations, vector spaces of functions and matrices,
matrix multiplication and its properties, invertible matrices, elementary row and column
operations and elementary matrices, Smith canonical form, factorization of invertible
matrices into elementary matrices, the equality of row rank and column rank, the theorem
giving equivalent conditions for a matrix to be invertible, block matrix multiplication, and
tensor products of matrices.
Chapter 5: Determinants via Calculations. The main properties of matrix determi-
nants are often stated without full proofs in a first linear algebra course. This chapter
establishes these properties starting from an explicit definition of det(A) as a sum, indexed
by permutations, of signed products of entries of A. Topics include the multilinearity of
the determinant as a function of the rows (or columns) of A, the alternating property, the
effect of elementary row (or column) operations, the product formula, Laplace expansions,
the classical adjoint of A, the explicit formula for A−1, Cramer’s Rule, the Cauchy–Binet
Formula, the Cayley–Hamilton Theorem, and an introduction to permanents.
Chapter 6: Comparing Concrete Linear Algebra to Abstract Linear Algebra. In
introductory linear algebra, we learn to execute computations involving concrete objects
such as column vectors and matrices. In more advanced linear algebra, we study abstract
vector spaces and linear transformations. Concrete concepts defined for matrices (such
as matrix multiplication, matrix transpose, diagonalizability, idempotence) have abstract
counterparts for linear transformations (such as composition of maps, dual maps, existence
of a basis of eigenvectors, and being a projection). This chapter gives a thorough account
of the relations between the concrete world of column vectors and matrices on the one
hand, and the abstract world of vector spaces and linear maps on the other hand. We
build a dictionary linking these two worlds, explaining the precise connection between each
abstract concept and its concrete manifestation. In particular, we carefully describe how
taking coordinates relative to an ordered basis yields a vector space isomorphism between
an abstract vector space V and a concrete space F n of column vectors. Similarly, computing
the matrix of a linear map relative to an ordered basis gives an algebra isomorphism between
an algebra of linear operators and an algebra of matrices. We also discuss how congruence,
orthogonal similarity, and unitary similarity of matrices are related to bilinear maps, real
inner product spaces, and complex inner product spaces.
Part III: Matrices with Special Structure
Chapter 7: Hermitian, Positive Definite, Unitary, and Normal Matrices. This
chapter develops the basic facts about the special types of matrices mentioned in the
title by building an analogy between properties of complex numbers (being real, being
positive, having modulus 1) and properties of matrices (being Hermitian, being positive
definite, being unitary). This analogy provides motivation for the central concept of a normal
matrix. The chapter also includes a discussion of unitary similarity, triangularization, and
diagonalization, the Spectral Theorem for Normal Matrices, simultaneous triangularization
and diagonalization of commuting families, the Polar Decomposition of a matrix, and the
Singular Value Decomposition.
Chapter 8: Jordan Canonical Forms. This chapter gives a novel and elementary
derivation of the existence and uniqueness of the Jordan canonical form of a complex matrix.
The first step in the proof is to classify nilpotent linear operators (with scalars in any field)
xx
Preface
by a visual analysis of partition diagrams. Once this classification is complete, we combine
it with a version of Fitting’s Lemma to obtain Jordan canonical forms with no explicit
mention of generalized eigenspaces. The chapter concludes with remarks on how to compute
the Jordan form, an application to systems of differential equations, and a discussion of the
Jordan–Chevalley decomposition of a linear operator into a sum of commuting nilpotent
and diagonalizable linear maps.
Chapter 9: Matrix Factorizations. There are many ways to factor a matrix into products
of other matrices that have a special structure (such as triangular or unitary matrices).
These factorizations often appear as building blocks in algorithms for the numerical solution
of linear algebra problems. This chapter proves the existence and uniqueness of several
matrix factorizations and explores the algebraic and geometric ideas leading to these
factorizations. Topics covered include the Gram–Schmidt orthonormalization algorithm,
QR-factorizations, Householder reflections, LU factorizations and their relation to Gaussian
elimination, Cholesky’s factorization of positive semidefinite matrices, the normal equations
for solving least-squares problems, and the Singular Value Decomposition.
Chapter 10: Iterative Algorithms in Numerical Linear Algebra. This chapter
studies methods for solving linear systems and computing eigenvalues that employ an
iterative process to generate successive approximations converging to the true solution.
Iterative algorithms for solving Ax = b include Richardson’s method, the Jacobi method,
and the Gauss–Seidel method. After developing some background on vector norms and
matrix norms, we establish some theoretical results ensuring the convergence of these
algorithms for certain classes of matrices. The chapter ends with an analysis of the power
method for iteratively computing the largest eigenvalue of a matrix. Techniques for finding
other eigenvalues (the shifted power method, inverse power method, and deflation) are also
discussed.
Part IV: The Interplay of Geometry and Linear Algebra
Chapter 11: Affine Geometry and Convexity. Most introductions to linear algebra
include an account of vector spaces, linear subspaces, the linear span of a subset of Rn, linear
independence, bases, dimension, and linear transformations. However, the parallel theory
of affine sets and affine transformations is seldom covered in detail. This chapter starts by
developing the basic concepts of affine geometry: affine sets, the affine span of a subset of
Rn, affine combinations, characterizations of affine sets (as translates of linear subspaces, as
solution sets of linear equations, and as intersections of hyperplanes), affine independence,
barycentric coordinates, and affine transformations. We continue with an introduction
to the subject of convexity, discussing convex sets, convex hulls, convex combinations,
Carath´eodory’s Theorem, simplexes, closed convex sets, convex cones, descriptions of convex
sets as intersections of half-spaces, convex functions, epigraphs, Jensen’s Inequality, and
derivative tests for convex functions.
Chapter 12: Ruler and Compass Constructions. Why can a regular 17-gon be
constructed with ruler and compass, while a regular 7-gon cannot? Which angles can be
trisected with a ruler and compass? These questions and others are answered here in a
linear-algebraic way by viewing field extensions as vector spaces and looking at dimensions.
This material is often presented at the end of an abstract algebra course or is skipped
altogether. Our development is quite elementary, requiring only basic facts about analytic
geometry, plane geometry, fields, polynomials, and dimensions of vector spaces.
Chapter 13: Dual Vector Spaces. A fruitful idea in mathematics is the interplay between
geometric spaces and structure-preserving functions defined on these spaces. This chapter
Preface
xxi
studies the fundamental case of a finite-dimensional vector space V and the dual space V ∗
of linear functions from V to the field of scalars. We show how the concepts of zero sets
and annihilators lead to inclusion-reversing bijections between the subspaces of V and the
subspaces of V ∗. Once this correspondence is understood, we explore how the choice of an
inner product on V sets up an isomorphism between V and V ∗. We also study the double
dual V ∗∗and its relation to V , along with dual maps and adjoint operators. The chapter
concludes with a discussion of bilinear pairings and real and complex inner product spaces.
We briefly comment on how the ideas in this chapter extend to Banach spaces and affine
algebraic geometry.
Chapter 14: Bilinear Forms. A bilinear form on a vector space V generalizes the familiar
dot product on Rn. Just as the dot product enhances Rn with extra geometric information
(such as the length of a vector, the distance between vectors, and the angle between vectors),
a bilinear form confers similar geometric structure on the abstract space V. After defining
bilinear forms and giving some examples, we study the matrix of a bilinear form, congruence
of matrices, orthogonality, dual spaces, orthogonal complements, the radical of a bilinear
form, totally isotropic subspaces, orthogonal maps, and reflections. We obtain structural
results such as the representation of symmetric bilinear forms by diagonal matrices, the
classification of alternating bilinear spaces as direct sums of hyperbolic spaces, Witt’s
Decomposition Theorem, Witt’s Cancellation Theorem, and the generation of orthogonal
groups by reflections.
Chapter 15: Metric Spaces and Hilbert Spaces. This chapter gives readers a glimpse of
some aspects of infinite-dimensional linear algebra that play a fundamental role in modern
analysis. We begin with a self-contained account of the material we need from analysis,
including metric spaces, convergent sequences, closed sets, open sets, continuous functions,
compactness, Cauchy sequences, and completeness. Next, we develop the basic theory of
Hilbert spaces, exploring topics such as the Schwarz Inequality, the Parallelogram Law,
orthogonal complements of closed subspaces, orthonormal sets, Bessel’s Inequality, maximal
orthonormal sets, Parseval’s Equation, abstract Fourier expansions, the role of the Hilbert
spaces ℓ2(X), Banach spaces of continuous linear maps, the dual of a Hilbert space, and
the adjoint of an operator. A major theme is the interaction between topological properties
(especially completeness), algebraic computations, and geometric ideas such as convexity
and orthogonality. Beyond a few brief allusions to L2 spaces, this chapter does not assume
any detailed knowledge of measure theory or Lebesgue integration.
Part V: Modules and Classification Theorems
Chapter 16: Finitely Generated Commutative Groups. A central theorem of group
theory asserts that every finitely generated commutative group is isomorphic to a product
of cyclic groups. This theorem is frequently stated, but not always proved, in first courses
on abstract algebra. This chapter proves this fundamental result using linear-algebraic
methods. We begin by reviewing basic concepts and definitions for commutative groups,
stressing the analogy to corresponding concepts for vector spaces. The key idea in the proof
of the classification theorem is to develop the analog of Gaussian elimination for integer-
valued matrices and to use this algorithm to reduce such matrices to a certain canonical
form. We also discuss elementary divisors and invariant factors and prove the uniqueness
results for these objects. The uniqueness proof is facilitated by using partition diagrams
to visualize finite groups whose size is a prime power. This chapter uses the very concrete
setting of integer matrices to prepare the reader for more abstract algebraic ideas (universal
mapping properties and the classification of finitely generated modules over principal ideal
domains) covered in later chapters.
xxii
Preface
Chapter 17: Introduction to Modules. This chapter introduces the reader to some
fundamental concepts in the theory of modules over arbitrary rings. We use the analogy
to vector spaces (which are modules over fields) to motivate fundamental constructions for
modules, while carefully pointing out that certain special facts about vector spaces fail to
generalize to modules. In particular, the fact that not every module has a basis leads to a
discussion of free modules and their properties. Specific topics covered include submodules,
quotient modules, direct sums, generating sets, direct products, Hom modules, change
of scalars, module homomorphisms, kernels, images, module isomorphism theorems, the
Jordan–H¨older Theorem, length of a module, free modules, bases, Zorn’s Lemma, existence
of bases for modules over division rings, invariance of dimension for such modules, and an
analogous result for modules using a commutative ring of scalars.
Chapter 18: Principal Ideal Domains, Modules over PIDs, and Canonical Forms.
This chapter gives a detailed proof of one of the cornerstones of abstract algebra: the
classification of all finitely generated modules over a PID. The chapter begins by proving
the necessary algebraic properties of principal ideal domains, including the fact that every
PID is a unique factorization domain. The classification proof is modeled upon the concrete
case of commutative groups (covered in Chapter 16); a central idea is to develop a matrix
reduction algorithm for matrices with entries in a PID. This algorithm changes any matrix
into a diagonal matrix called a Smith normal form, which leads to the main structural
results for modules. As special cases of the general theory, we deduce theorems on the
rational canonical form and Jordan canonical form of linear operators and matrices. We
also prove the uniqueness of the elementary divisors and invariant factors of a module, as
well as the uniqueness of the various canonical forms for matrices.
Part VI: Universal Mapping Properties and Multilinear Algebra
Chapter 19: Introduction to Universal Mapping Properties. The concept of a
universal mapping property (UMP) pervades linear and abstract algebra but is seldom
mentioned in introductory treatments of these subjects. This chapter gives a careful and
detailed introduction to this idea, starting with the UMP satisfied by a basis of a vector
space. The UMP is formulated in several equivalent ways (as a diagram completion property,
as a unique factorization property, and as a bijection between collections of functions).
The concept is further developed by describing the UMPs characterizing free R-modules,
quotient modules, direct products, and direct sums. This chapter serves as preparation for
the next chapter on multilinear algebra.
Chapter 20: Universal Mapping Properties in Multilinear Algebra. The final
chapter uses the idea of a universal mapping property (UMP) to organize the development
of basic constructions in multilinear algebra. Topics covered include multilinear maps,
alternating maps, symmetric maps, tensor products of modules, exterior powers, symmetric
powers, and the homomorphisms of these structures induced by linear maps. We also
discuss isomorphisms between tensor product modules, bases of tensor products and
related modules, tensor products of matrices, the connection between exterior powers and
determinants, and tensor algebras.
I wish you a rewarding journey through the boundless landscape of linear algebra!
Nicholas A. Loehr
Part I
Background on Algebraic
Structures
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
1
Overview of Algebraic Systems
This chapter gives a rapid overview of the algebraic systems (such as groups, rings, fields,
vector spaces, and algebras) that appear later in the book. After giving the axioms defining
each of these systems and some basic examples, we describe some constructions (such as
subspaces, product spaces, and quotient spaces) for building new systems from old ones.
Then we discuss homomorphisms, which are structure-preserving maps between algebraic
systems. The chapter concludes with a review of linear independence, spanning, basis, and
dimension in the context of vector spaces over a field.
Unlike the rest of the book, this chapter is intended to be used as a review (for readers
familiar with abstract algebra) or as a reference (for readers unfamiliar with the definitions),
not as a leisurely introduction to the subject. To read the majority of the book, it suffices
to know the meaning of the following terms defined in this chapter: commutative ring,
field, vector space over a field, linear transformation, subspace, linearly independent list,
basis, and dimension. Some further basic definitions regarding sets, functions, relations, and
partially ordered sets appear in the Appendix.
1.1
Groups
Given any set S, a binary operation on S is a function p : S × S →S. We say that S is
closed under this binary operation to emphasize that p(a, b) is required to belong to S for
all a, b ∈S. There are many operation symbols that are used instead of the notation p(a, b):
for example, any of the expressions a + b, a · b, a ◦b, a × b, ab, or [a, b] may be used to
abbreviate p(a, b) in different situations.
Next, we define some special properties that a binary operation p on S may or may not
have. First, p is commutative iff1 p(a, b) = p(b, a) for all a, b ∈S. Second, p is associative
iff p(a, p(b, c)) = p(p(a, b), c) for all a, b, c ∈S. Third, S has an identity element relative to
p iff there exists e ∈S (necessarily unique) such that p(a, e) = a = p(e, a) for all a ∈S. If
such an identity e exists, we say a ∈S is invertible relative to p iff there exists a′ ∈S with
p(a, a′) = e = p(a′, a). The element a′ is called an inverse of a relative to p.
A group is a pair (G, p), where G is a set and p is an associative binary operation on
G such that G has an identity element relative to p, and every element of G is invertible
relative to p. Writing p(a, b) = a ⋆b, the group axioms take the form shown in Table 1.1.
For example, the set of all nonzero real numbers is a group under multiplication with
identity e = 1. More generally, the set GLn(R) of all n × n real-valued matrices A having
nonzero determinant is a group under matrix multiplication, where the identity element is
the n×n identity matrix. (To check the inverse axiom, we need the theorem from elementary
linear algebra that says A is invertible iff det(A) ̸= 0. We prove a more general result in
1Throughout this text, the word iff is defined to mean “if and only if.”
DOI: 10.1201/9781003484561-1
3
4
Advanced Linear Algebra
TABLE 1.1
Group axioms for (G, ⋆).
1. For all a, b ∈G, a ⋆b is in G (closure).
2. For all a, b, c ∈G, a ⋆(b ⋆c) = (a ⋆b) ⋆c (associativity).
3. There exists e ∈G such that for all a ∈G, a ⋆e = a = e ⋆a (identity).
4. For all a ∈G, there exists a−1 ∈G with a ⋆a−1 = e = a−1 ⋆a (inverses).
Section 5.11.) For another example, let X be any set and S(X) be the set of all bijections
(one-to-one, onto functions) f : X →X. Taking the binary operation to be composition
of functions, we can verify that (S(X), ◦) is a group. This group is discussed further in
Chapter 2. The group operations in GLn(R) and S(X) are not commutative in general.
A commutative group (also called an Abelian group) is a group G in which the group
operation does satisfy commutativity. Writing p(a, b) = a + b for the group operation, the
axioms for a commutative group (in additive notation) take the form shown in Table 1.2. We
define subtraction in a commutative group (G, +) by setting a −b = a + (−b) for a, b ∈G.
TABLE 1.2
Axioms for a commutative group (G, +).
1. For all a, b ∈G, a + b is in G (closure).
2. For all a, b, c ∈G, a + (b + c) = (a + b) + c (associativity).
3. There exists 0G ∈G such that for all a ∈G, a + 0G = a = 0G + a
(additive identity).
4. For all a ∈G, there exists −a ∈G with a + (−a) = 0G = (−a) + a
(additive inverses).
5. For all a, b ∈G, a + b = b + a (commutativity).
The familiar number systems (the integers Z, the rational numbers Q, the real numbers
R, and the complex numbers C) are all commutative groups under addition. The set
Rk of k-dimensional real vectors v = (v1, . . . , vk) is a commutative group under vector
addition; here, (v1, . . . , vk) + (w1, . . . , wk) = (v1 + w1, . . . , vk + wk) for all vi, wi ∈R. The
identity element of this group is 0 = (0, . . . , 0), and the inverse of v = (v1, . . . , vk) is
−v = (−v1, . . . , −vk).
All of the commutative groups just mentioned are infinite. To give examples of finite
groups, fix a positive integer n. Let Zn = {0, 1, 2, . . . , n −1} be the set of integers modulo
n. Define addition modulo n as follows: given a, b ∈Zn, let a ⊕b = a + b if a + b < n and
a ⊕b = a + b −n if a + b ≥n. Equivalently, a ⊕b is the remainder when a + b is divided
by n, denoted (a + b) mod n. We may readily verify that (Zn, ⊕) is a commutative group
of size n.
Overview of Algebraic Systems
5
1.2
Rings and Fields
A ring is a triple (R, p, q), where R is a set and p and q are binary operations on R
(denoted by p(a, b) = a + b and q(a, b) = a · b) such that (R, +) is a commutative group; the
multiplication operation q is associative with an identity element 1R; and the two distributive
laws a · (b + c) = (a · b) + (a · c) and (a + b) · c = (a · c) + (b · c) hold for all a, b, c ∈R. The
ring axioms are written out in detail in Table 1.3. Note that some authors do not require,
as we do, that all rings have a multiplicative identity.
TABLE 1.3
Ring axioms for (R, +, ·).
1. (R, +) is a commutative group. (See Table 1.2.)
2. For all a, b ∈R, a · b is in R (closure under multiplication).
3. For all a, b, c ∈R, a · (b · c) = (a · b) · c (associativity of multiplication).
4. There is 1R ∈R so that for all a ∈R, a · 1R = a = 1R · a (multiplicative identity).
5. For all a, b, c ∈R, a · (b + c) = (a · b) + (a · c) (left distributive law).
6. For all a, b, c ∈R, (a + b) · c = (a · c) + (b · c) (right distributive law).
We say R is a commutative ring iff its multiplication operation is commutative (for all
a, b ∈R, a · b = b · a). We say R is an integral domain iff R is a commutative ring such that
0R ̸= 1R and for all a, b ∈R, if a ̸= 0R and b ̸= 0R, then a·b ̸= 0R. The last condition states
that R has no zero divisors, which are nonzero ring elements whose product is zero. We say
R is a field iff R is a commutative ring such that 0R ̸= 1R and every nonzero element of
R has a multiplicative inverse. When R is a field, the set of nonzero elements of R forms a
commutative group under multiplication with identity 1R. We may show that every field is
an integral domain.
The number systems Z, Q, R, and C are all commutative rings. Q, R, and C are fields,
but Z is not, since most integers do not have multiplicative inverses in the set Z of integers.
However, Z is an integral domain, since the product of two nonzero integers is never zero.
For n ≥1, we can make the set Zn of integers modulo n into a commutative ring of size
n. Addition here is addition mod n, and multiplication is the operation ⊗defined (for
a, b ∈Zn) by letting a ⊗b be the remainder when ab is divided by n, denoted ab mod n. We
can verify that the ring (Zn, ⊕, ⊗) is a field iff n is a prime number (an integer larger than
1 that is divisible only by itself and 1).
Another example of a commutative ring is the set R[x] of polynomials in one variable
with real coefficients, with the ordinary rules for adding and multiplying polynomials. This
ring is not a field, since the only polynomials whose inverses are also polynomials are the
nonzero constants. This example can be generalized in several ways. We can replace the
real coefficients with coefficients in an arbitrary field (or even an arbitrary ring). Or, we can
allow polynomials in more than one variable. For more on polynomials, read Chapter 3.
An example of a non-commutative ring is provided by the set of all n × n real matrices
under matrix addition and matrix multiplication, where n ≥2 is a fixed integer. More
generally, for any ring R, we can consider the set Mn(R) of all n × n matrices with entries
in R. Using the same rules for adding and multiplying matrices as in the real case, this
set of matrices becomes a ring that is almost never commutative. For more details, read
Chapter 4.
6
Advanced Linear Algebra
1.3
Vector Spaces
Most introductions to linear algebra study real vector spaces, where we can add two
vectors or multiply a vector by a real number (also called a scalar). For more advanced
investigations, it is helpful to replace the real scalars with elements of more general number
systems (such as rings or fields). To ensure that the nice properties of real vector spaces
carry over to the more general setting, it turns out that we need to have multiplicative
inverses for all nonzero scalars. This leads to the following definition of a vector space over
a field, which generalizes the concept of a real vector space.
Let (F, +, ·) be a field, whose elements are called scalars. A vector space over F (also
called an F-vector space) is a triple (V,+, s), where (V,+) is a commutative group, and
s : F × V →V is a scalar multiplication operation satisfying the axioms listed in Table 1.4.
Here, we use two different fonts for the addition + in F and the addition + in V , but
later, we use the same symbol + for both. Similarly, using boldface letters for vectors v in
V is not required. We almost always use juxtaposition to abbreviate various multiplication
operations, writing s(c, v) = cv and c · d = cd for c, d ∈F and v ∈V . If we replace the field
F by any ring R, we obtain the definition of an R-module. Modules are studied later in the
book, starting in Chapter 17.
TABLE 1.4
Axioms for a vector space (V,+, s) over a field (F, +, ·).
1. (V,+) is a commutative group. (See Table 1.2.)
2. For all c ∈F and v ∈V , s(c, v) = cv is in V (closure under scalar multiplication).
3. For all c, d ∈F and v ∈V , (c + d)v = cv + dv
(distributive law for scalar addition).
4. For all c ∈F and v, w ∈V , c(v + w) = cv + cw
(distributive law for vector addition).
5. For all c, d ∈F and v ∈V , c(dv) = (c · d)v (associativity of scalar multiplication).
6. For all v ∈V , 1F v = v (identity for scalar multiplication).
The most well-known example of a real vector space is the set Rk of vectors
v = (v1, . . . , vk) with each vi ∈R. As discussed earlier, Rk is a commutative group under
vector addition. The scalar multiplication operation is given by cv = (cv1, cv2, . . . , cvk) for
c ∈R and v ∈Rk. This example generalizes readily to the case where scalars come from
an arbitrary field F. We define F k to be the set of k-tuples v = (v1, v2, . . . , vk) with each
vi ∈F. Given such a v and w = (w1, w2, . . . , wk) ∈F k and c ∈F, define
v + w = (v1 + w1, v2 + w2, . . . , vk + wk);
cv = (cv1, cv2, . . . , cvk).
The operations on the right sides of these equations are the given addition and multiplication
operations in F. Using these definitions, it is tedious but straightforward to confirm that
F k is an F-vector space.
Some further examples of F-vector spaces, which we study in detail later in this book,
are the set Mm,n(F) of m×n matrices with entries in F (Chapter 4); the set of all functions
from an arbitrary set X into F (Chapter 4); the set of all linear maps from one F-vector
Overview of Algebraic Systems
7
space to another (Chapters 6 and 13); any field K containing F as a subfield (Chapter 12);
and the set of polynomials in one or more variables with coefficients in F (Chapter 3).
We conclude with the definition of an algebra over a field F, although this concept is
only used in a few places in the book. For a field F, an F-algebra is a structure (A,+, ⋆, s)
such that (A,+, ⋆) is a ring, (A,+, s) is an F-vector space, and the ring multiplication and
scalar multiplication are related by the identities
c(v ⋆w) = (cv) ⋆w = v ⋆(cw)
for all c ∈F and v, w ∈A.
(More precisely, A is an associative F-algebra with identity.) Some examples of F-algebras
are the set Mn(F) of n×n matrices with entries in F; the set EndF (V ) of linear maps from
a fixed F-vector space V to itself; and the set F[x1, . . . , xn] of polynomials in n variables
with coefficients in F. Chapter 6 explores the close relationship between the first two of
these algebras.
1.4
Subsystems
For each of the algebraic systems mentioned so far, we can construct new algebraic systems
of the same kind using a variety of algebraic constructions. Some recurring constructions
in abstract algebra are subsystems, direct products, and quotient systems. The next few
sections review how these constructions are defined for groups, rings, fields, vector spaces,
and algebras. For a more extensive discussion covering the case of modules, read Chapter 17.
Generally speaking, a subsystem of a given algebraic system is a subset that is closed
under the relevant operations. For instance, a subgroup of a group (G, ⋆) is a subset H of G
such that for all a, b ∈H, a⋆b ∈H; the identity e of G is in H; and for all a ∈H, a−1 ∈H.
The first condition says that the subset H of G is closed under the group operation ⋆; the
second condition says that H is closed with respect to the identity; and the third condition
says that H is closed under inverses. When these closure conditions hold, it follows that
the set H becomes a group if we restrict the binary operation ⋆: G×G →G to the domain
H ×H. A similar comment applies to the constructions of other types of subsystems below.
H is a normal subgroup of G iff H is a subgroup such that a ⋆h ⋆a−1 ∈H for all a ∈G
and all h ∈H. The quantity a ⋆h ⋆a−1 is called a conjugate of h in G. So the condition for
normality can be stated by saying that H is closed under conjugation. Every subgroup of a
commutative group G is automatically normal in G.
A subring of a ring (R, +, ·) is a subset S of R such that 0R ∈S, 1R ∈S, and for all
a, b ∈S, a + b ∈S, −a ∈S, and a · b ∈S. In other words, S is a subring of R iff S is an
additive subgroup of (R, +) that is closed under multiplication and under the multiplicative
identity. S is a left ideal of R iff S is an additive subgroup such that a · s ∈S for all a ∈R
and s ∈S; we say that S is closed under left multiplication by elements of R. Similarly, S
is a right ideal of R iff S is an additive subgroup such that s · a ∈S for all a ∈R and s ∈S
(i.e., S is closed under right multiplication by elements of R). S is an ideal of R (also called
a two-sided ideal for emphasis) iff S is both a left ideal and a right ideal of R. Under our
conventions, the different types of ideals in R need not be subrings, since the ideals are not
required to contain the multiplicative identity 1R. In fact, any left, right, or two-sided ideal
of R containing 1R must be the entire ring.
If F is a field, a subfield of F is a subring E of F such that for all nonzero a ∈E, a−1
is in E. So, a subfield must contain the 0 and 1 of the original field and be closed under
addition, additive inverses, multiplication, and multiplicative inverses for nonzero elements.
8
Advanced Linear Algebra
Let V be a vector space over a field F. A subset W of V is called a subspace of V iff:
0V ∈W; for all v, w ∈W, v + w ∈W; and for all c ∈F and w ∈W, cw ∈W. So, a
subset W of V is a vector subspace of V iff W is closed under zero, vector addition, and
scalar multiplication. The same definition applies if V is a module over a ring R, but now
subspaces are called submodules or R-submodules.
Let A be an algebra over a field F. A subset B of A is a subalgebra iff B is both a
subring of the ring A and a subspace of the vector space A. So, B contains 0A and 1A, and
B is closed under addition, additive inverses, ring multiplication, and scalar multiplication.
Here are some examples to illustrate the preceding definitions. For each fixed integer n,
the set nZ = {nk : k ∈Z} is a subgroup of the additive group (Z, +), which is a normal
subgroup of Z because Z is commutative. Each set nZ is also an ideal of the ring Z, but
not a subring except when n = ±1. We can check, using the division algorithm for integers,
that all subgroups of Z (and hence all ideals of Z) are of this form. Given any multiplicative
group (G, ⋆) and any fixed g ∈G, the set ⟨g⟩= {gn : n ∈Z} of powers of g is a subgroup of
G, called the cyclic subgroup generated by g. This subgroup need not be normal in G. Given
a commutative ring R and a fixed b ∈R, the set Rb = {r · b : r ∈R} is an ideal of R, called
the principal ideal generated by b. In general, this does not coincide with Zb = {nb : n ∈Z},
which is the additive subgroup of (R, +) generated by b. Z is a subring of R that is not a
subfield, since Z is not closed under multiplicative inverses. Q is a subfield of R, as is the
set Q(
√
2) of all real numbers of the form a + b
√
2, where a, b ∈Q. The set of polynomials
of degree at most 3 (together with zero) is a subspace of the vector space of all polynomials
with real coefficients. For any field F, the set {(t, t + u, −u) : t, u ∈F} is a subspace of
the F-vector space F 3. The set of upper-triangular n × n matrices is a subalgebra of the
R-algebra Mn(R) of real n × n matrices.
1.5
Product Systems
Next, we consider the direct product construction for vector spaces. Suppose V1, V2, . . . , Vn
are given vector spaces over the same field F. The product set V = V1 × V2 × · · · × Vn
consists of all ordered n-tuples v = (v1, v2, . . . , vn) with each vi ∈Vi. We can turn this
product set into an F-vector space by defining addition and scalar multiplication as follows.
Given v = (v1, . . . , vn) and w = (w1, . . . , wn) in V and given c ∈F, define v + w =
(v1 + w1, . . . , vn + wn) and cv = (cv1, . . . , cvn). In these definitions, the operations in
position i are the sum and scalar multiplication operations in the vector space Vi. It is
tedious but routine to check the vector space axioms for V . In particular, the additive
identity of V is 0V = (0V1, 0V2, . . . , 0Vn), and the additive inverse of v = (v1, . . . , vn) ∈V
is −v = (−v1, . . . , −vn). The set V with these operations is called the direct product of
the F-vector spaces V1, V2, . . . , Vn. Note that the F-vector space F n is a special case of this
construction obtained by taking every Vi to be F.
There are analogous constructions for the direct products of groups, rings, modules, and
algebras. In each case, all algebraic operations are defined one component at a time. For ex-
ample, given positive integers n1, n2, . . . , nk, the direct product G = Zn1 × Zn2 × · · · × Znk
is a commutative group of size n1n2 · · · nk, with operation
(a1, a2, . . . , ak)+(b1, b2, . . . , bk) = ((a1+b1) mod n1, (a2+b2) mod n2, . . . , (ak+bk) mod nk)
for ai, bi ∈Zni. In Chapter 16 we prove that every finite commutative group is isomorphic
to a direct product of this form. (Isomorphisms are defined in §1.7.)
Overview of Algebraic Systems
9
1.6
Quotient Systems
This section describes the notion of a quotient system of a given algebraic system. Quotient
constructions are more subtle and less intuitive than the constructions of subsystems and
product systems, but they play a prominent role in abstract algebra and other parts of
mathematics.
The simplest instance of the quotient construction occurs when (G, +) is a commutative
group with subgroup H. We build a new commutative group, denoted G/H, and called the
quotient group of G by H. Intuitively, the group G/H is a simplification of G obtained by
discarding information contained in H. For instance, if G is the additive group Z of integers
and H is the subgroup nZ of multiples of n, then Z/nZ is isomorphic to the group Zn of
integers mod n. Intuitively, the formation of the quotient group discards multiples of n and
focuses attention on the remainders when various integers are divided by n.
We now give the details of the construction of G/H, where (G, +) is a commutative
group with subgroup H. For each a ∈G, define the coset of H represented by a to be
a + H = {a + h : h ∈H}. Each coset a + H is a certain subset of G obtained by translating
the subset H by adding the fixed element a to each h in H. Define the quotient set G/H
to be {a + H : a ∈G}, so G/H is the set of all cosets of H in G.
A critical point is the fact that each coset (element of G/H) can have several different
names. In other words, there are often multiple ways of writing a particular coset in the
form a+H for some a ∈G. The next theorem gives a precise criterion for when two elements
of G give us different names for the same coset.
Coset Equality Theorem. Let (G, +) be a commutative group with subgroup H. For all
a, b ∈G, a + H = b + H if and only if a −b ∈H.
Proof. Fix a, b ∈G satisfying a + H = b + H. Since 0G ∈H, we have a = a + 0G ∈a + H.
Since the set a + H equals the set b + H by assumption, we have a ∈b + H, which means
a = b + h for some h ∈H. So a −b = h belongs to H, as needed.
For the converse, fix a, b ∈G satisfying a −b ∈H; we must prove a + H = b + H. First
we prove the set inclusion a + H ⊆b + H. Fix x ∈a + H. We can write x = a + h1 for
some h1 ∈H. Observe that x = a + h1 = b + (a −b) + h1 = b + k, where k = (a −b) + h1
is the sum of two elements of H, hence is in H. So x ∈b + H. To prove the opposite set
inclusion b + H ⊆a + H, fix y ∈b + H. Write y = b + h2 for some h2 ∈H. Then notice that
y = a + [−(a −b) + h2] ∈a + H, where a −b ∈H, −(a −b) ∈H, and −(a −b) + h2 ∈H
because H is a subgroup. So y ∈a + H as required.
Next, we prove that G is the disjoint union of the distinct cosets of H. On one hand,
every coset of H is a subset of G (by the closure axiom of G). On the other hand, for any
a ∈G, a is in the coset a + H since a = a + 0G with 0G ∈H. To see that any two distinct
cosets of H must be disjoint, suppose a + H and b + H are two cosets of H such that some
element c is in both cosets (here a, b, c ∈G). Write c = a + h = b + k for some h, k ∈H.
Then a −b = k −h ∈H, so the Coset Equality Theorem gives a + H = b + H.
Here is an example. In the group (Z12, ⊕), consider the subgroup H = {0, 3, 6, 9}. Three
of the cosets of H are 0+H = {0, 3, 6, 9}, 1+H = {1, 4, 7, 10}, and 2+H = {2, 5, 8, 11}. In
fact, these are all of the distinct cosets of H, since (for instance) 3+H = {3, 6, 9, 0} = 0+H,
7 + H = 1 + H because 7 −1 = 6 ∈H, and so on. Note that any element b in a coset a + H
can be used to give a new name b + H for that coset. This holds since b = a + h for some
h ∈H, so b −a = h ∈H, so a + H = b + H by the Coset Equality Theorem. For instance,
the four possible names of the coset {2, 5, 8, 11} are 2 + H, 5 + H, 8 + H, and 11 + H. The
10
Advanced Linear Algebra
quotient set Z12/H = {a + H : a ∈Z12} = {{0, 3, 6, 9}, {1, 4, 7, 10}, {2, 5, 8, 11}} consists of
three distinct cosets. The set Z12 is indeed the disjoint union of these cosets.
Returning to the general case, the next step is to define a binary addition operation p on
G/H by setting p(a+H, b+H) = (a+b)+H for all a, b ∈G. If we use a circled plus symbol
⊕to denote the new operation p, the definition reads: (a + H) ⊕(b + H) = (a + b) + H.
Now a new subtlety emerges: our definition of the sum of two elements of G/H depends on
the particular names a + H and b + H that we have chosen to represent these elements. If
we change from these names to other names of the same two cosets, how can we be sure
that the output is not affected? To answer this question, suppose a, a1, b, b1 ∈G satisfy
a + H = a1 + H and b + H = b1 + H. Using the original names a + H and b + H, the sum of
these cosets is defined to be (a+b)+H. Using the new names a1+H and b1+H for these same
cosets, the sum of the cosets is calculated as (a1 +b1)+H. These two answers might appear
to be different, but we can prove they are, in fact, different names for the same coset. To see
this, note from the Coset Equality Theorem that a−a1 ∈H and b−b1 ∈H. Therefore, since
the subgroup H is closed under addition, (a+b)−(a1 +b1) = (a−a1)+(b−b1) ∈H. (This
calculation also requires commutativity of G.) Another application of the Coset Equality
Theorem gives (a + b) + H = (a1 + b1) + H, as needed. We summarize this calculation
by saying that the binary operation ⊕is well-defined (or single-valued). We must perform
such a check every time a binary operation or function is defined by a formula involving the
name of an object that has several possible names.
With this technical issue out of the way, we verify the commutative group axioms for
(G/H, ⊕) as follows. Consider three arbitrary elements in G/H, which we can name as
a + H, b + H, and c + H for some a, b, c ∈G. To check the closure of G/H under ⊕, note
(a + H) ⊕(b + H) = (a + b) + H does belong to G/H, since a + b ∈G by the closure axiom
for G. To check the associativity of ⊕, compute
(a + H) ⊕((b + H) ⊕(c + H)) = (a + H) ⊕((b + c) + H) = (a + (b + c)) + H
= ((a + b) + c) + H = ((a + b) + H) ⊕(c + H) = ((a + H) ⊕(b + H)) ⊕(c + H).
The third equality holds by associativity of + in G, and the other equalities hold by definition
of ⊕. Similarly, ⊕is commutative because (a + H) ⊕(b + H) = (a + b) + H = (b + a) + H =
(b + H) ⊕(a + H). This calculation uses commutativity of + in G. The identity element of
G/H is the coset 0 + H = H, where 0 is the additive identity element in G, because
(a + H) ⊕(0 + H) = (a + 0) + H = a + H = (0 + a) + H = (0 + H) ⊕(a + H)
by the identity axiom for (G, +). Finally, the additive inverse of a + H relative to ⊕is
(−a) + H, since the inverse axiom in (G, +) gives (a + H) ⊕((−a) + H) = (a + (−a)) +
H = 0 + H = ((−a) + a) + H = ((−a) + H) ⊕(a + H). Note that −a ∈G, and therefore
the proposed inverse (−a) + H does belong to the set G/H.
Continuing our earlier example where G = Z12 and H = {0, 3, 6, 9}, recall that G/H =
{0+H, 1+H, 2+H}. We have (1+H)+(2+H) = (1+2)+H = 3+H, and another name
for this answer is 0 + H. Furthermore, (2 + H) + (2 + H) = 4 + H = 1 + H. Comparing the
addition table for G/H to the addition table for the group (Z3, ⊕), we can check that the
groups Z12/H and Z3 are isomorphic (as defined in §1.7). Similarly, it can be shown that
for all n ≥1, the quotient group Z/nZ is isomorphic to the group (Zn, ⊕) defined in §1.1.
The quotient construction can be generalized to other types of algebraic structures.
First, we can replace the commutative group (G, +) and its subgroup H by an arbitrary
group (G, ⋆) and a subgroup H that is normal in G. For each a ∈G, we define the left
coset a ⋆H = {a ⋆h : h ∈H}, and we let G/H = {a ⋆H : a ∈G}. In this setting, the
Left Coset Equality Theorem states that for all a, b ∈G, a ⋆H = b ⋆H iff a−1 ⋆b ∈H
Overview of Algebraic Systems
11
iff b−1 ⋆a ∈H (note the inverse occurs to the left of the star in each case). The binary
operation p on the set G/H of left cosets of H is defined by p(a ⋆H, b ⋆H) = (a ⋆b) ⋆H
for all a, b ∈G. The verification that p is well-defined (Exercise 49) depends critically on
H being a normal subgroup of G. Once this has been done, it is straightforward to prove
(as above) that (G/H, p) is a group. A similar construction could be executed using right
cosets H ⋆a = {h ⋆a : h ∈H}, which satisfy H ⋆a = H ⋆b iff a ⋆b−1 ∈H. However, the
normality of H implies that every left coset a⋆H equals the right coset H ⋆a (Exercise 50),
so that the resulting quotient group is the same as before.
Second, we can replace G and H by a ring (R, +, ·) and an ideal I of R. The set of
(additive) cosets R/I = {a + I : a ∈R} is already known to be a commutative group, since
(R, +) is a commutative group with subgroup I. We introduce a second binary operation
• on R/I by setting (a + I) • (b + I) = (a · b) + I for all a, b ∈R. We can check that • is
well-defined using the assumption that I is a (two-sided) ideal. It is then routine to verify
that (R/I, ⊕, •) is a ring with additive identity 0R + I and multiplicative identity 1R + I.
Also, R/I is commutative if R is commutative. The ring R/I is called the quotient ring of
R by the ideal I.
Third, we can replace G and H by an F-vector space (V, +, ·) and a subspace W, where
F is a field. We construct the quotient vector space (V/W, ⊕, •) as follows. Since (V, +) is
a commutative group with subgroup W, we already have an additive commutative group
(V/W, ⊕), where (x + W) ⊕(y + W) = (x + y) + W for all x, y ∈V . We introduce
a new scalar multiplication • (which is a function from F × V/W to V/W) by setting
c • (v + W) = (c · v) + W for all c ∈F and all v ∈V . We must check that • is well-defined.
In the formula defining •, elements of F do not have multiple names, but elements of V/W
do. So, we fix c ∈F and x, y ∈V , assume x+W = y+W, and prove c•(x+W) = c•(y+W).
In other words, we must prove (c · x) + W = (c · y) + W. By the Coset Equality Theorem,
x −y ∈W. Since W is a subspace of V , we get c · (x −y) ∈W. So (c · x) −(c · y) ∈W. We
deduce (c · x) + W = (c · y) + W by the Coset Equality Theorem.
We can now verify the vector space axioms for (V/W, ⊕, •). We already proved that
(V/W, ⊕) is a commutative group. To verify the distributive law for vector addition in this
space, fix c ∈F and v, x ∈V , and calculate
c • ((v + W) ⊕(x + W)) = c • ((v + x) + W) = (c · (v + x)) + W.
By the known distributive law for vector addition in the original space V , the calculation
continues:
(c · (v + x)) + W = (c · v + c · x) + W = (c · v + W) ⊕(c · x + W) = c • (v + W) ⊕c • (x + W),
and the axiom is verified. The remaining axioms are checked similarly. The construction
of the quotient vector space generalizes at once to the case where R is a ring and V is an
R-module with submodule W (see Chapter 17).
1.7
Homomorphisms
The concept of homomorphism allows us to compare algebraic structures. There are many
different kinds of homomorphisms, one for each kind of algebraic system. Intuitively, a
homomorphism of an algebraic system is a function from one system to another system of
the same kind that preserves all relevant operations and structure.
12
Advanced Linear Algebra
For instance, if (G, ⋆) and (K, ∗) are groups, a group homomorphism from G to K is a
function T : G →K such that T(x ⋆y) = T(x) ∗T(y) for all x, y ∈G. It follows from this
condition that T(eG) = eK, where eG is the identity element of G and eK is the identity
element of K. We can also show T(x−1) = T(x)−1 for all x ∈G, where x−1 is the inverse
of x in G and T(x)−1 is the inverse of T(x) in K. More generally, T(xn) = T(x)n for
all x ∈G and all n ∈Z. We say that the group homomorphism T preserves the group
operations, the identity, inverses, and powers of group elements. In the case where G and
K are commutative groups with operations written in additive notation, the definition of a
group homomorphism becomes T(x + y) = T(x) + T(y) for all x, y ∈G. Now T(0G) = 0K,
T(−x) = −T(x), and T(nx) = nT(x) for all x ∈G and all n ∈Z. (The notation nx denotes
the sum of n copies of x for n > 0, the sum of |n| copies of −x for n < 0, or 0G for n = 0.)
Analogously, given two rings R and S, a ring homomorphism is a function T : R →S
such that for all x, y ∈R, T(x + y) = T(x) + T(y), T(xy) = T(x)T(y), and T(1R) = 1S. It
follows from these conditions that T(xn) = T(x)n for all x ∈R and all integers n ≥0. This
formula also holds for negative integers n when x is an invertible element of R.
Next, suppose V and W are vector spaces over a field F. An F-linear map (also called
a vector space homomorphism or linear transformation) is a function T : V →W such that
T(v + z) = T(v) + T(z) and T(cv) = cT(v) for all v, z ∈V and all c ∈F. The same
definition applies if V and W are modules over a ring R; in this case, we call T an R-linear
map or R-module homomorphism. Finally, an algebra homomorphism is a map between two
F-algebras that is both a ring homomorphism and a vector space homomorphism.
Let T : X →Y be a homomorphism of any of the types defined above. We call T an
isomorphism iff T is a bijective (one-to-one and onto) function from X to Y . In this case, T
has a two-sided inverse function T −1 : Y →X. We may check that T −1 is always a homo-
morphism of the same type as T and is also an isomorphism. Furthermore, the composition
of homomorphisms (resp. isomorphisms) is a homomorphism (resp. isomorphism), and the
identity map on a given algebraic structure X is an isomorphism for that type of structure.
We write X ∼= Y if there exists an isomorphism between X and Y . The preceding remarks
show that ∼= is an equivalence relation on any set of algebraic structures of a given kind.
Specifically, for any X, Y, Z in the given set, X ∼= X (reflexivity); if X ∼= Y , then Y ∼= X
(symmetry); and if X ∼= Y and Y ∼= Z, then X ∼= Z (transitivity).
Here are some examples of homomorphisms. If H is a subgroup of G, the inclusion map
i : H →G given by i(h) = h for all h ∈H is an injective group homomorphism; i is an
isomorphism iff H = G. If H is normal in G, the projection map p : G →G/H, given
by p(x) = x ⋆H for all x ∈G, is a surjective group homomorphism; p is an isomorphism
iff H = {eG}. Similarly, if R is a ring with ideal I, the projection p : R →R/I given by
p(x) = x + I for x ∈R is a surjective ring homomorphism. If V is an F-vector space with
subspace W, we obtain an analogous projection p : V →V/W, which is F-linear. In the case
of groups, vector spaces, and modules, the injective maps ji : Vi →V1 × · · · × Vi × · · · × Vn
sending xi ∈Vi to (0, . . . , xi, . . . , 0) are homomorphisms. However, this statement does
not hold for products of rings, since ji(1Vi) is almost never the multiplicative identity of
the product ring. On the other hand, for all the types of algebraic systems discussed, the
projections qi : V1 × · · · × Vi × · · · × Vn →Vi sending (x1, . . . , xn) to xi are surjective
homomorphisms.
We now discuss kernels, images, and the Fundamental Homomorphism Theorem.
Suppose T : X →Y is a function mapping a set X into an additive group Y with identity
element 0Y . The kernel of T is ker(T) = {x ∈X : T(x) = 0Y } ⊆X, and the image of T is
img(T) = {T(x) : x ∈X} ⊆Y . When T is a linear map between vector spaces, ker(T) is
also called the null space of T, and img(T) is also called the range of T. The following facts
are readily checked. (i) If T is a group homomorphism, then ker(T) is a normal subgroup
of X, and img(T) is a subgroup of Y . (ii) If T is a ring homomorphism, then ker(T) is an
Overview of Algebraic Systems
13
ideal of X and img(T) is a subring of Y . (iii) If T is a linear transformation of F-vector
spaces, then ker(T) is a subspace of X, and img(T) is a subspace of Y . (iv) For all types
of homomorphisms considered here, T is injective iff ker(T) = {0X}. (v) T is surjective iff
img(T) = Y .
Fundamental Homomorphism Theorem for Commutative Groups. Let X and Y
be commutative groups with operations written in additive notation. Suppose T : X →Y
is a group homomorphism with kernel K and image I. There is a group isomorphism
T ′ : X/K →I given by T ′(x + K) = T(x) for all x ∈X.
Proof. Step 1. We check that T ′ is well-defined. Let a given element of X/K have two names
x + K = w + K, where x, w ∈X. We must check that T ′(x + K) = T ′(w + K), which (by
definition of T ′) is the same as proving T(x) = T(w). By the Coset Equality Theorem,
x + K = w + K implies x −w ∈K. Now 0Y = T(x −w) = T(x) −T(w) by definition of the
kernel and since the homomorphism T preserves subtraction. So T(x) = T(w), as needed.
Step 2. We check that T ′ is a group homomorphism. Fix two elements of X/K, say u+K
and x + K with u, x ∈X. Compute
T ′((u+K)+(x+K)) = T ′((u+x)+K) = T(u+x) = T(u)+T(x) = T ′(u+K)+T ′(x+K).
From left to right, the equalities are true by definition of + in X/K, by definition of T ′, by
T being a homomorphism, and by definition of T ′.
Step 3. We check that T ′ is surjective (onto). Note first that T ′ does map into the
codomain I. This holds since each input to T ′ has the form x + K for some x ∈X, and
T ′(x + K) = T(x) is in the image of T, which is the set I. Conversely, given any y ∈I, the
definition of image shows that y = T(u) for some u ∈X, hence y = T ′(u + K) for some
u + K ∈X/K.
Step 4. We check that T ′ is injective (one-to-one). Fix u, x ∈X satisfying T ′(u + K) =
T ′(x + K); we must prove u + K = x + K. By definition of T ′, we have T(u) = T(x). So
T(u) −T(x) = 0Y , and T(u −x) = 0Y (homomorphisms preserve subtraction). This means
u −x belongs to the kernel of T, namely u −x ∈K. By the Coset Equality Theorem,
u + K = x + K, as needed.
There are analogous Fundamental Homomorphism Theorems for groups, rings, vector
spaces, modules, and algebras. Here is the version of the theorem for rings.
Fundamental Homomorphism Theorem for Rings. Suppose X and Y are rings, and
T : X →Y is a ring homomorphism with kernel K and image I. There is a ring isomorphism
T ′ : X/K →I given by T ′(x + K) = T(x) for all x ∈X.
Proof. First, T is a group homomorphism from X to Y regarded as additive groups. By the
Fundamental Homomorphism Theorem for Commutative Groups, we already know that T ′
is a bijective, well-defined homomorphism of additive groups. We need only check that T ′
also preserves the ring multiplication and identity. To do so, fix two elements of X/K, say
u+ K and x+ K with u, x ∈X. Using the definition of the product of cosets, the definition
of T ′, and the fact that T preserves multiplication, we get
T ′((u + K)(x + K)) = T ′((ux) + K) = T(ux) = T(u)T(x) = T ′(u + K)T ′(x + K).
For similar reasons, we have T ′(1X/K) = T ′(1X + K) = T(1X) = 1Y = 1I.
The reader can formulate and prove the analogous theorems for other algebraic
structures. Chapter 17 discusses this theorem and other isomorphism theorems for modules,
which include commutative groups and vector spaces as special cases.
14
Advanced Linear Algebra
1.8
Spanning, Linear Independence, Basis, and Dimension
Introductions to linear algebra often discuss the concepts of linear independence, spanning
sets, bases, and dimension in the case of real vector spaces. The same ideas occur in the
more general setting of vector spaces over a field. In this section, after reviewing the basic
definitions, we state without proof some theorems concerning linear independence and bases
of vector spaces. We prove these theorems in a more general setting in Chapter 17.
Let V be a vector space over a field F. Let L = (v1, v2, . . . , vk) be a finite list of
vectors with each vi ∈V . Any vector of the form c1v1 + c2v2 + · · · + ckvk with each
ci ∈F is called an F-linear combination of the vectors in L. The list L spans V iff every
w ∈V can be written in at least one way as an F-linear combination of the vectors in
L. The list L is linearly dependent over F (or F-linearly dependent) iff there exist scalars
c1, c2, . . . , ck ∈F such that at least one ci is nonzero, and 0V = c1v1 + c2v2 + · · · + ckvk. In
other words, saying that L is linearly dependent means the zero vector can be written as a
linear combination of the vectors in L where at least one coefficient is not the zero scalar.
L is linearly independent over F iff L is not linearly dependent over F. Spelling this out,
saying that L is F-linearly independent means: for all c1, . . . , ck ∈F, if c1v1+· · ·+ckvk = 0,
then c1 = c2 = · · · = ck = 0.
The definition of linear independence can be rephrased to resemble more closely the
definition of spanning. We claim L is F-linearly independent iff every w ∈V can be written
in at most one way as an F-linear combination of the vectors in L. On one hand, if L
is linearly dependent, then the vector w = 0 can be written in at least two ways as a
linear combination of vectors in L: one way is 0 = 0v1 + 0v2 + · · · + 0vk, and the other
way is the linear combination appearing in the definition of linear dependence. On the
other hand, assume L is linearly independent. Suppose a given w ∈V can be written as
w = Pk
i=1 aivi and also as w = Pk
i=1 bivi for some ai, bi ∈F. Subtracting these equations
gives 0 = Pk
i=1(ai −bi)vi. The assumed linear independence of L then gives ai −bi = 0 for
all i, hence ai = bi for all i. So there is at most one way of writing w as a linear combination
of the vectors in L.
Continuing the definitions, we say that the list L = (v1, . . . , vk) is an ordered basis of V
iff L spans V and L is F-linearly independent. This means that every v ∈V can be written
in exactly one way as an F-linear combination of vectors in L. The vector space V is
called finite-dimensional iff V has a finite ordered basis. The Basis Cardinality Theorem
(proved in §17.15) states that any two ordered bases of a finite-dimensional vector space V
must have the same size, where the size of a list L is its length (number of entries). The
dimension of a finite-dimensional vector space V , denoted dim(V ) or dimF (V ), is the size
of any ordered basis of V .
For vector spaces that might be infinite-dimensional, we adapt the definitions of
spanning, linear independence, and bases to apply to sets of vectors as well as lists of
vectors. Note that a set of vectors is unordered and could be infinite, while a list of vectors
(in this setting) is ordered and finite. Let S be any subset of an F-vector space V . The
set S spans V iff every w ∈V can be written as an F-linear combination of a finite list of
vectors (v1, . . . , vn) with each vi ∈S (the list used depends on w). The set S is linearly
independent iff every finite list of distinct elements of S is linearly independent (as defined
earlier). The set S is a basis of V iff S spans V and S is linearly independent. By adapting
the proof for lists, we can show that S is a basis of V iff every w ∈V can be written in
exactly one way as a linear combination of elements of S. More precisely, this means that
Overview of Algebraic Systems
15
for each w ∈V , there is exactly one function c : S →F that is zero for all but finitely
many inputs and satisfies w =
X
v∈S:c(v)̸=0
c(v)v.
We state some facts about these concepts, which are proved in a more general setting
in Chapter 17. Let V be a vector space over the field F, and let S and T be any subsets of
V . Then:
(i) If S spans V and S ⊆T, then T spans V .
(ii) If T is linearly independent and S ⊆T, then S is linearly independent.
(iii) V spans V .
(iv) If T spans V , then there is a basis B of V with B ⊆T.
(v) The empty set ∅is linearly independent.
(vi) If S is linearly independent, then there is a basis C of V with S ⊆C.
(vii) V has a basis (by (iii) and (iv), or (v) and (vi)).
(viii) If S is linearly independent and T spans V , then |S| ≤|T|.
(ix) Basis Cardinality Theorem: If S and T are any two bases of V , then |S| = |T|.
Here, |S| denotes the cardinality of the set S, which is the number of elements in S if S
is finite. In general, |S| ≤|T| means there exists a one-to-one function g : S →T, while
|S| = |T| means there exists a bijective (one-to-one and onto) function h : S →T. Analogs
of facts (i) through (ix) hold for ordered lists in a finite-dimensional vector space V . For
instance, any linearly independent ordered list in V can be extended to an ordered basis of
V by appending zero or more (appropriately chosen) vectors.
Because of facts (vii) and (ix), we can define the dimension of V , denoted dim(V ) or
dimF (V ), to be the unique cardinality of any basis of V . Fact (viii) says that a linearly
independent set in V cannot have strictly larger cardinality than a spanning set in V . A
basis is both a spanning set and a linearly independent set. Therefore:
(x) Any S ⊆V with |S| > dim(V ) must be linearly dependent.
(xi) Any T ⊆V with |T| < dim(V ) cannot span V .
Note also that an (ordered) list L = (v1, . . . , vk) spans V iff the (unordered) set
{v1, . . . , vk} spans V . On the other hand, L is a linearly independent list iff L contains
no repeated vectors and the set {v1, . . . , vk} is linearly independent. So in the finite-
dimensional case, any ordered basis of V has the same size as any basis of V , and the
two definitions of dimension (one using lists, the other using sets) are consistent.
We illustrate these ideas by proving the Rank–Nullity Theorem.
Rank–Nullity Theorem. Let V and W be F-vector spaces with V finite-dimensional. For
any linear map T : V →W with null space N = ker(T) and image R = img(T),
dim(N) + dim(R) = dim(V ).
Here, dim(N) is called the nullity of T and dim(R) is called the rank of T. The theorem
says that the nullity plus the rank equals the dimension of the domain of T.
Proof. Let n = dim(V ) (which is finite) and k = dim(N). Note that k must be finite
by fact (viii) or fact (x), since a basis of N is also a linearly independent subset of V . Let
BN = (v1, . . . , vk) be an ordered basis of N. Since BN is a linearly independent list, it can be
extended (by fact (vi) for lists) to an ordered basis of V , say BV = (v1, . . . , vk, vk+1, . . . , vn).
Define yi = T(vk+i) for 1 ≤i ≤n−k. We will show BR = (y1, . . . , yn−k) is an ordered basis
of R. We can then conclude dim(R) = n−k, which proves the theorem since k+(n−k) = n.
First, we check that the list BR spans the image R of T. Given an arbitrary z ∈R, we
have z = T(x) for some x ∈V . Expressing x in terms of the basis BV , there are scalars
c1, . . . , cn ∈F with x = Pn
i=1 civi. Applying the linear map T to this expression gives
16
Advanced Linear Algebra
T(x) = Pn
i=1 ciT(vi). Now T(vi) = 0 for 1 ≤i ≤k since these vi are in the kernel of T,
while T(vi) = yi−k for k < i ≤n. We therefore get z = T(x) = Pn
i=k+1 ciyi−k, which
expresses z as a linear combination of vectors in the list BR.
Second, we check that BR is a linearly independent list. Assume d1, . . . , dn−k ∈F satisfy
Pn−k
j=1 djyj = 0W ; we must show every dj is zero. Since yj = T(vk+j), linearity of T gives
T(Pn−k
j=1 djvk+j) = 0W . So the vector u = d1vk+1 + · · · + dn−kvn is in the kernel N of T.
As BN is a basis of this kernel, u can be expressed as some linear combination of v1, . . . , vk,
say u = f1v1 + · · · + fkvk for some fi ∈F. Equating the two expressions for u, we get
−f1v1 + · · · + (−fk)vk + d1vk+1 + · · · + dn−kvn = 0V .
Since (v1, . . . , vn) is a linearly independent list, we conclude that −f1 = · · · = −fk = d1 =
· · · = dn−k = 0. In particular, every dj is zero, as needed.
The Rank–Nullity Theorem can also be deduced from the Fundamental Homomorphism
Theorem for Vector Spaces (see Exercise 66).
1.9
Summary
Table 1.5 summarizes some of the definitions of algebraic axioms, algebraic systems,
subsystems, homomorphisms, and linear algebra concepts discussed in this chapter. We
also recall the following points.
1.
Examples and Notation for Algebraic Structures. Z, Q, R, and C are commutative
rings and additive groups. Zn = {0, 1, . . . , n} is a finite commutative ring with
operations a ⊕b = (a + b) mod n and a ⊗b = (ab) mod n. Q, R, C, and Zp (with
p prime) are fields. The set Mn(F) of n × n matrices with entries in a field F is a
ring and an F-algebra. The set GLn(F) = {A ∈Mn(F) : det(A) ̸= 0} is a group
under matrix multiplication. The set S(X) of bijections f : X →X is a group
under function composition. The set F[x] of polynomials in one variable with
coefficients in a field F is a commutative ring, F-vector space, and F-algebra.
Examples of F-vector spaces include F k with componentwise operations, the set
of F-linear maps from one vector space to another, and the set Mm,n(F) of m×n
matrices with entries in F.
2.
Direct Products. Given F-vector spaces V1, . . . , Vk, the set V1 × · · · × Vk
=
{(v1, . . . , vk) : vi ∈Vi} becomes an F-vector space by defining addition and
scalar multiplication one component at a time. A similar construction works for
other algebraic systems.
3.
Cosets. If (G, +) is a commutative group with subgroup H, the quotient set G/H
is the set of cosets x + H = {x + h : h ∈H} for x ∈G. The Coset Equality
Theorem states that for all x, y ∈G, x + H = y + H iff x −y ∈H. G is the
disjoint union of the distinct cosets of H. When defining functions or operations
that act on cosets, we must check that the output is well-defined (independent of
the names used for the input cosets).
4.
Quotient Systems. Given a commutative group (G, +) with subgroup H, the set
G/H becomes a commutative group with operation (x+H)⊕(y+H) = (x+y)+H
for x, y ∈G. The identity of G/H is 0G + H = H, and the inverse of x + H
is (−x) + H. A similar construction is possible when G is a group with normal
Overview of Algebraic Systems
17
TABLE 1.5
Summary of definitions of algebraic concepts.
Axiom for Set S
Meaning
closure of S under ⋆
∀a, b ∈S, a ⋆b is in S
associativity
∀a, b, c ∈S, (a ⋆b) ⋆c = a ⋆(b ⋆c)
identity
∃e ∈S, ∀a ∈S, a ⋆e = a = e ⋆a
inverses
∀a ∈S, ∃b ∈S, a ⋆b = e = b ⋆a (where e is the identity)
commutativity
∀a, b ∈S, a ⋆b = b ⋆a
left distributive law
∀a, b, c ∈S, a · (b + c) = (a · b) + (a · c)
right distributive law
∀a, b, c ∈S, (a + b) · c = (a · c) + (b · c)
Algebraic System
Definition
group (G, ⋆)
operation ⋆has closure, associativity, identity, and
inverses
commutative group
group where operation is commutative
ring (R, +, ·)
(R, +) is comm. group; · has closure, associativity,
identity, left distributive law, and right
distributive law
commutative ring
ring where multiplication is commutative
integral domain
comm. ring with 1 ̸= 0 and no zero divisors
field F
comm. ring where 1 ̸= 0 and every x ̸= 0 has a mult.
inverse
F-vector space (V, +, ·)
(V, +) is comm. group; scalar mult. obeys closure, scalar
associativity, two distributive laws, and scalar
identity law
R-module
same as vector space, but scalars come from ring R
F-algebra
ring and F-vector space with compatible multiplications
Subsystem
Required Closure Properties
subgroup H of (G, ⋆)
H closed under ⋆, identity, and inverses
normal subgroup of G
subgroup closed under conjugation by elements of G
subring S of (R, +, ·)
S closed under +, −, ·, 0, and 1
ideal I of (R, +, ·)
I closed under +, −, 0, and left/right mult. by elements
of R
subfield of field F
subring closed under inverses of nonzero elements
subspace of vector space V
closed under 0, +, and scalar multiplication
submodule of module
same as subspace, but scalars come from a ring
subalgebra of A
subring and subspace of A
Homomorphism
What Must Be Preserved
group homomorphism
group operation (hence also identity, inverses, powers)
ring homomorphism
ring +, ring ·, and 1 (multiplicative identity)
F-linear map
vector addition, scalar multiplication by scalars in F
R-module homomorphism
vector addition, scalar multiplication by scalars in R
algebra homomorphism
ring +, ring ·, 1, scalar multiplication
Linear Algebra Term
Definition
linear combination of vi
a sum c1v1 + · · · + ckvk with all ci ∈F
list L spans V
every w ∈V is a linear combination of vectors in L
linearly independent list L
for all ci ∈F, if Pk
i=1 civi = 0, then c1 = · · · = ck = 0
ordered basis of V
linearly independent list that spans V
spanning set in V
every v ∈V is finite linear combination of vectors
from set
linearly independent set
all finite lists of distinct elements of set are lin.
independent
basis of V
linearly independent set that spans V
dimension of V
size of any basis (or ordered basis) of V
18
Advanced Linear Algebra
subgroup H; when R is a ring with ideal I; when V is a vector space with subspace
W; and when V is an R-module with submodule W. In a quotient vector space
V/W, scalar multiplication is given by c(x+W) = (cx)+W for c ∈F and x ∈V .
5.
Facts about Independence, Spanning, and Bases. Every F-vector space V has
a basis (possibly infinite). Any two bases of V have the same cardinality. Any
linearly independent subset of V can be extended to a basis of V . Any spanning
subset of V contains a basis of V . Subsets of linearly independent sets are
also linearly independent, and supersets of spanning sets still span. A linearly
independent subset of V can never be larger than a spanning set. For a vector
space homomorphism T : V →W where dim(V ) is finite, the Rank–Nullity
Theorem says dim(V ) = dim(ker(T)) + dim(img(T)).
1.10
Exercises
1.
Prove that a binary operation ⋆on a set S can have at most one identity element.
2.
Let ⋆be an associative binary operation on a set S with identity element e. Show
that each a ∈S can have at most one inverse relative to ⋆.
3.
Explain why the following sets and binary operations are not groups by pointing
out a group axiom that fails to hold.
(a) S is the set of nonnegative integers, p(x, y) = x + y for x, y ∈S.
(b) S = {−1, 0, 1}, p(x, y) = x + y for x, y ∈S.
(c) S = Z, p(x, y) = 4 for all x, y ∈S.
(d) S = Z, p(x, y) = (x + y) mod 10 for all x, y ∈S.
(e) S = {0, 1, 2, 3, 4}, p(0, x) = x = p(x, 0) for all x ∈S, and p(x, y) = 0 for all
nonzero x, y ∈S.
4.
Let X be a fixed subset of R and x ⋆y = min(x, y) for all x, y ∈X. For each of
the axioms for a commutative group, find conditions on X that are necessary and
sufficient for (X, ⋆) to satisfy that axiom. For which X is (X, ⋆) a commutative
group?
5.
(a) For fixed n ≥1, prove that the set of n × n matrices with real entries is a
commutative group under the operation of matrix addition.
(b) Is the set of matrices in (a) a group under matrix multiplication? Explain.
6.
Decide (with proof) which sets are groups under function composition.
(a) S is the set of injective (one-to-one) functions f : R →R.
(b) S is the set of surjective (onto) functions g : R →R.
(c) S is the set of one-to-one functions h : Z5 →Z5.
7.
Prove Zn is a commutative group under ⊕(addition mod n).
8.
Cancellation Laws for Groups. Let (G, ⋆) be a group. Prove: for all a, x, y ∈G,
if a ⋆x = a ⋆y, then x = y; and if x ⋆a = y ⋆a, then x = y. Point out where each
of the four group axioms is used in the proof.
9.
(a) Give a specific example of a ring (G, +, ⋆) and nonzero a, x, y ∈G for which
the cancellation laws in the previous exercise are not true.
(b) If (G, +, ⋆) is a field, are the cancellation laws for ⋆true? Explain.
(c) Find and prove a version of the multiplicative cancellation law that is valid
in an integral domain.
Overview of Algebraic Systems
19
10.
Sudoku Theorem for Groups. Let (G, ⋆) be a group consisting of the n
distinct elements x1, . . . , xn. Prove: for all a ∈G, the list a ⋆x1, . . . , a ⋆xn is a
rearrangement of the list x1, . . . , xn. (A similar result holds for x1 ⋆a, . . . , xn ⋆a.)
11.
Exponent Theorem for Groups. Let (G, ⋆) be an n-element commutative
group with identity e. Prove: for all a ∈G, an = e. Do this by letting G =
{x1, . . . , xn} and evaluating the product (a ⋆x1) ⋆(a ⋆x2) ⋆· · · ⋆(a ⋆xn) in
two ways. (The theorem holds for non-commutative groups too, but the proof is
harder.)
12.
Let A and B be normal subgroups of a group G.
(a) Show: if A ∩B = {eG}, then ∀a ∈A, ∀b ∈B, ab = ba. (Study aba−1b−1.)
(b) Prove AB = {ab : a ∈A, b ∈B} is a subgroup of G.
(c) Prove or disprove: AB must be a normal subgroup of G.
13.
Fix n ≥1. Let ⊕and ⊗denote addition mod n and multiplication mod n.
(a) Prove: for all a, b ∈Zn, a ⊕b ∈Zn and a ⊕b = a + b −qn for some q ∈Z.
(b) Prove: for all a, b ∈Zn, a ⊗b ∈Zn and a ⊗b = ab −sn for some s ∈Z.
(c) Prove: for all c, d ∈Zn, c = d iff c −d = tn for some t ∈Z.
(d) Prove (Zn, ⊕, ⊗) is a commutative ring. (Hint: Use (a) and (b) to eliminate
⊕and ⊗from each side of each ring axiom, then use (c) to see that the two sides
must be equal.)
14.
For any ring R, define R∗(the set of units of R) to be the set of invertible elements
in R. So x ∈R belongs to R∗iff ∃y ∈R, xy = 1R = yx.
(a) Prove that R∗is a group under the multiplication operation in R.
(b) Show: If R is a commutative ring, then R∗is a commutative group.
15.
Find R∗(see the previous exercise) for each ring R:
(a) Z
(b) R
(c) a field R
(d) Z12
(e) M3(R)
(f) R[x]
16.
Prove that every field is an integral domain.
17.
Prove that every finite integral domain R is a field. (For nonzero a ∈R, define
La : R →R by La(x) = a · x for x ∈R. Prove La is one-to-one, hence onto.)
18.
(a) Prove: if p is prime, then Zp is a field.
(b) Prove: if n is not prime, then Zn is not an integral domain.
19.
Let S be the set of all functions f : R →R. For f, g ∈S, define f + g : R →R
by (f + g)(x) = f(x) + g(x) for x ∈R. Prove (S,+) is a commutative group.
20.
Let S be the set of all functions f : R →R. For f, g ∈S, define f • g : R →R by
(f • g)(x) = f(x)g(x) for x ∈R. Is (S, •) a commutative group? Explain.
21.
Let S be the set of all functions f : R →R, with addition + and multiplication
• defined in the previous two exercises.
(a) Prove (S,+, •) is a commutative ring.
(b) Is S a field? Is S an integral domain? Explain.
(c) Define a scalar multiplication · on S and prove (S,+, •, ·) is an R-algebra.
22.
Let S be the set of all functions f : R →R, let + be the sum defined in
Exercise 19, and let ◦be function composition. Which axioms for a commutative
ring are true for (S,+, ◦)? Explain.
23.
Let F be a field. Verify the vector space axioms for the direct product V1 × V2 ×
· · · × Vk of F-vector spaces V1, V2, . . . , Vk. Deduce that F k is an F-vector space.
24.
For c ∈R and v ∈Z, let s(c, v) = cv be the ordinary product of the real number
c and the integer v. Which axioms for a real vector space hold for (Z, +, s)?
20
Advanced Linear Algebra
25.
Define scalar multiplication by s(c, v) = 0 for all c, v ∈R. Show that (R, +, s)
satisfies all the axioms for a real vector space except the identity axiom for scalar
multiplication.
26.
Define scalar multiplication by s(c, v) = v for all c, v ∈R. Show that (R, +, s)
satisfies all the axioms for a real vector space except for the distributive law for
scalar addition.
27.
Define scalar multiplication s : C×R →R by s(a + ib, v) = av for all a, b, v ∈R.
Show that (R, +, s) satisfies all the axioms for a complex vector space except for
associativity of scalar multiplication.
28.
Define scalar multiplication by s(c, v) = c2v for all c, v ∈R. Which axioms for a
real vector space hold for (R, +, s)?
29.
Define scalar multiplication s by s(c, v) = c for all c, v ∈R. Which axioms for a
real vector space hold for (R, +, s)?
30.
Let V = R>0 be the set of positive real numbers. Define the non-standard addition
p(v, w) = vw (the ordinary product of v and w) for v, w ∈V . Define scalar
multiplication by s(c, v) = vc for c ∈R and v ∈V . Which axioms for a real
vector space hold for (V, p, s)?
31.
Prove that for the field Zp (where p is prime), the distributive law for vector
addition follows from the other axioms for a Zp-vector space.
32.
Prove that for the field Q of rational numbers, the distributive law for vector
addition follows from the other axioms for a Q-vector space.
33.
†Can you find an example of a field F and a structure (V, +, s) satisfying all the
F-vector space axioms except the distributive law for vector addition?2
34.
Let W be the set of matrices of the form
 a
0
b
0

for some a, b ∈R. Is W an
additive subgroup of M2(R)? a subring? a left ideal? a right ideal? an ideal? a
subspace? a subalgebra? Explain.
35.
Give an example of a ring R and a right ideal I in R that is not an ideal of R.
36.
If possible, give an example of a subset of Z that is closed under addition and
inverses, yet is not a subgroup of (Z, +).
37.
For any field F and k ≥1, show that W = {(t, t, . . . , t) : t ∈F} is a subspace of
the F-vector space F k. Is an analogous result true for groups or for rings?
38.
Let G be the group GL2(R) of invertible 2 × 2 matrices with real entries.
(a) Give three different examples of normal subgroups of G.
(b) Give an example of a subgroup of G that is not normal in G.
39.
Prove that Z[i] = {a + bi : a, b ∈Z} is a subring of C. Is this a subfield of C?
40.
Quaternions. Let 1 =

1
0
0
1

, i =

i
0
0
−i

, j =

0
i
i
0

, k =

0
−1
1
0

.
(a) Prove Q = {±1, ±i, ±j, ±k} is a non-commutative subgroup of GL2(C).
(b) Prove H = {a1 + bi + cj + dk : a, b, c, d ∈R} is a real subalgebra of M2(C).
(c) Prove every nonzero element of H has a multiplicative inverse in H.
(So H satisfies all field axioms except commutativity of multiplication.)
41.
Given rings R1, . . . , Rn, carefully prove that the direct product R = R1 ×· · ·×Rn
is a ring (with componentwise operations). In particular, what are 0R and 1R?
2The mark † signals an exercise that may be especially challenging.
Overview of Algebraic Systems
21
42.
Prove that the direct product of two or more fields is never a field.
43.
Suppose G1 and G2 are groups, H1 is a subgroup of G1, and H2 is a subgroup of
G2. Prove H1 × H2 is a subgroup of G1 × G2. Prove if H1 is normal in G1 and
H2 is normal in G2, then H1 × H2 is normal in G1 × G2.
44.
State and prove results analogous to the previous exercise for subrings and ideals
in a product ring and for subspaces in a product vector space.
45.
Give an example of vector spaces V1 and V2 and a subspace W of V1 × V2 such
that W is not the direct product of a subspace of V1 and a subspace of V2.
46.
Let (G, ⋆) be a group. Assume H is a finite nonempty subset of G that is closed
under ⋆. Prove H is a subgroup of G. (Given a ∈H, show the list a1, a2, a3, . . .
has repeated entries. Deduce this list must contain eG and a−1.)
47.
Left Coset Equality Theorem. Let (G, ⋆) be a group with subgroup H. Prove:
for all a, b ∈G, the following conditions are equivalent: a ⋆H = b ⋆H; a ∈b ⋆H;
b ∈a ⋆H; a = b ⋆h for some h ∈H; b = a ⋆k for some k ∈H; b−1 ⋆a ∈H;
a−1 ⋆b ∈H.
48.
State and prove the Right Coset Equality Theorem for groups.
49.
Let H be a normal subgroup of a group (G, ⋆). Verify that the binary operation
p : G/H × G/H →G/H, given by p(a ⋆H, b ⋆H) = (a ⋆b) ⋆H for all a, b ∈G,
is well-defined. Indicate where your proof uses normality of H. Then prove the
group axioms for (G/H, p).
50.
Let (G, ⋆) be a group with subgroup H. Prove: H is a normal subgroup of G iff
for all a ∈G, a ⋆H = H ⋆a.
51.
Let I be an ideal in a ring (R, +, ·). For a, b ∈R, set (a + I) • (b + I) = (a · b) + I.
(a) Prove • is a well-defined binary operation on R/I.
(b) Prove the ring axioms for (R/I, +, •).
(c) Prove: if R is commutative, then R/I is commutative.
52.
Let W = {(t, −t) : t ∈R}, which is a subspace of the real vector space R2. Draw
a picture of the set R2/W. Calculate [(1, 2) + W] ⊕[(0, −1) + W], illustrate this
calculation in your picture, and give three different names for the answer.
53.
Suppose G and K are groups, and T : G →K is a group homomorphism.
Prove: ∀x ∈G, ∀n ∈Z, T(xn) = T(x)n. (Treat n = 0 and n = −1 separately.)
54.
Let T : X →Y and U : Y →Z be F-linear maps of F-vector spaces.
(a) Prove U ◦T : X →Z is F-linear.
(b) Prove if T is a vector space isomorphism, then so is T −1.
(c) Prove ker(T) is a subspace of X, and img(T) is a subspace of Y .
(d) Prove ker(T) ⊆ker(U ◦T).
(e) Prove img(U ◦T) ⊆img(U).
(f) Find and prove a characterization of when ker(T) = ker(U ◦T).
55.
State and prove the Fundamental Homomorphism Theorem for Vector Spaces.
56.
Given a field F and an integer k > 0, define vectors e1, . . . , ek ∈F k by letting ei
have 1F in position i and 0F elsewhere. Define fi = e1+e2+· · ·+ei for 1 ≤i ≤k.
(a) Prove that (e1, e2, . . . , ek) is an ordered basis of F k.
(b) Prove that (f1, f2, . . . , fk) is an ordered basis of F k.
57.
For each real vector space V , find an ordered basis for V and compute dim(V ).
(a) the set of all real n × n matrices
(b) the set of all complex n × n matrices
22
Advanced Linear Algebra
(c) the set of all real upper-triangular n × n matrices
(d) the set of all real symmetric n × n matrices
58.
Prove facts (i), (ii), (iii) and (v) stated in Section 1.8 (page 15).
59.
Assume V is a vector space over a field F, the list T = (w1, . . . , wn) spans V ,
and L = (v1, . . . , vk) is a linearly independent list in V . Show that if L does not
span V , then there exists j such that L′ = (v1, . . . , vk, wj) is linearly independent.
Deduce that L can be extended to a basis of V by appending zero or more vectors
from T.
60.
Assume V is an F-vector space, S is a linearly independent subset of V , and T
is a finite spanning set for V . Show that if S is not a subset of T, then there is a
linearly independent set S′ with |S′| = |S| and |S′ ∩T| = |S ∩T| + 1. Iterate this
result to prove |S| ≤|T|.
61.
Let T : V →W be a linear map between finite-dimensional vector spaces.
(a) Prove: if U is a subspace of V , then dim(U) ≤dim(V ).
(b) Prove: if U is a subspace of V and dim(U) = dim(V ), then U = V .
(c) Prove: dim(img(T)) ≤min(dim(V ), dim(W)).
(d) Find and prove inequalities relating dim(ker(T)) to dim(V ) and dim(W).
62.
Let V be an F-vector space and S be a list or set of vectors in V . Prove that
the set W of all finite F-linear combinations of vectors in S is a subspace of V .
Prove also that for every subspace Z of V that contains S, W ⊆Z. W is called
the subspace of V generated by S.
63.
Let V and W be F-vector spaces, let T : V →W be an F-linear map and U be
a subspace of V . Prove that T[U] = {T(u) : u ∈U} is a subspace of W. Show
that if a list (u1, . . . , uk) spans U, then the list (T(u1), . . . , T(uk)) spans T[U].
64.
Let T : V →W be a linear map of finite-dimensional vector spaces. For any list
L = (v1, . . . , vk) of vectors in V , let T[L] be the list (T(v1), . . . , T(vk)).
(a) Prove T is one-to-one iff for every linearly independent list L in V , T[L] is a
linearly independent list in W.
(b) Prove T is onto iff for every list L in V that spans V , T[L] spans W.
(c) Prove T is an isomorphism iff for every ordered basis L of V , T[L] is an
ordered basis of W.
65.
Let V be a finite-dimensional F-vector space with subspace W. Prove that
dim(V/W) = dim(V ) −dim(W). (Hint: Extend a basis of W to a basis of V ,
and prove that the cosets of the new basis vectors form a basis of V/W.)
66.
Use Exercises 55 and 65 to reprove the Rank–Nullity Theorem.
67.
Endomorphism Ring of a Commutative Group. Given a commutative
group (M, +), define End(M) to be the set of all group homomorphisms
f : M →M. Define the sum of f, g ∈End(M) to be the function f + g : M →M
given by (f + g)(x) = f(x) + g(x) for all x ∈M. Define the product of
f, g ∈End(M) to be the composition f ◦g, given by (f ◦g)(x) = f(g(x)) for
all x ∈M. Prove the ring axioms for (End(M),+, ◦). End(M) is called the
endomorphism ring of M.
2
Permutations
This chapter gives the basic definitions and facts about permutations we need to discuss
determinants and multilinear algebra. First we discuss how composition of functions confers
a group structure on the set Sn of permutations of {1, 2, . . . , n}. Next, we introduce a way of
visualizing a function using a directed graph, which leads to a description of permutations
in terms of disjoint directed cycles. We use this description to obtain some algebraic
factorizations of permutations in the group Sn. The chapter concludes by studying inversions
of functions and permutations, which give information about how many steps it takes to
sort a list into increasing order. We use inversions to define the sign of a permutation, which
plays a critical role in our subsequent treatment of determinants (Chapter 5).
2.1
Symmetric Groups
This section assumes familiarity with the definition of a group (§1.1). For each positive
integer n, let [n] denote the finite set {1, 2, . . . , n}. The symmetric group Sn is defined to
be the set of all bijective functions f : [n] →[n], with composition of functions as the
binary operation. Recall that a function f : X →Y is a bijection iff f is one-to-one (for
all x, z ∈X, f(x) = f(z) implies x = z) and f is onto (for each y ∈Y there is x ∈X with
f(x) = y). For a function f : X →X mapping a finite set X into itself, we may check that
f is one-to-one iff f is onto.
Recall that the composition of two functions f : X →Y and g : Y →Z is defined to
be the function g ◦f : X →Z given by (g ◦f)(x) = g(f(x)) for x ∈X. The composition
of bijections is a bijection, so Sn is closed under the composition operation. Composition
of functions is always associative, so associativity holds. The identity function on [n], given
by id(x) = x for x ∈[n], is a bijection and hence belongs to Sn. Since f ◦id = f = id ◦f
for all f ∈Sn, Sn has an identity element relative to ◦. Finally, each f ∈Sn has an inverse
function f −1 : [n] →[n], which is also a bijection and satisfies f ◦f −1 = id = f −1 ◦f.
Therefore, (Sn, ◦) is a group. For all n ≥3, Sn is not a commutative group.
Elements of Sn are called permutations of n objects. To explain this terminology, note
that any function f : [n] →[n] is completely determined by the list [f(1), f(2), . . . , f(n)].
We refer to this list as the one-line form for f. If f belongs to Sn, so that i ̸= j implies
f(i) ̸= f(j), then this list is a rearrangement (or “permutation”) of the list [1, 2, . . . , n].
For example, the function f : [4] →[4] given by f(1) = 3, f(2) = 2, f(3) = 4, and
f(4) = 1 has one-line form f = [3, 2, 4, 1]. Given the one-line form g = [4, 1, 3, 2] ∈S4,
we must have g(1) = 4, g(2) = 1, g(3) = 3, and g(4) = 2. Then f ◦g = [1, 3, 4, 2] since
f ◦g(1) = f(g(1)) = f(4) = 1, f(g(2)) = f(1) = 3, etc. On the other hand, g◦f = [3, 1, 2, 4].
Using one-line forms, we now show that the group Sn has exactly n! elements. We can
build each f ∈Sn by making the following choices. There are n ways to choose f(1). There
are n −1 ways to choose f(2), which must be something in [n] unequal to f(1). Since f(3)
must be different from f(1) and f(2), there are n −2 ways to choose f(3). We continue
DOI: 10.1201/9781003484561-2
23
24
Advanced Linear Algebra
similarly. At the end there is one choice for f(n). Combining all these choices by the Product
Rule for Counting, we see there are n × (n −1) × (n −2) × · · · × 1 = n! permutations in Sn.
2.2
Representing Functions as Directed Graphs
Let f : [n] →[n] be any function. We create a graphical representation of f by drawing n
dots labeled 1 through n, then drawing an arrow from i to f(i) for all i ∈[n]. If i = f(i),
this arrow is a loop pointing from i to itself. See Figure 2.1 for an example where n = 23
and
f = [10, 22, 7, 11, 15,
19, 19, 12, 22, 12,
11, 1, 6, 11, 5,
8, 22, 9, 21, 11,
3, 3, 2].
Note that we can recover the function f from its directed graph.
18
9
23
2
22
3
7
17
13
6
19
21
1
10
12
8
16
4
20
14
11
5
15
FIGURE 2.1
Directed graph for a function.
Each node in the graph of f has exactly one arrow leaving it, since f is a function.
Thus, starting from any given node x0, there is a well-defined path through the graph
obtained by repeatedly following the arrows for f. This path visits the nodes x0, x1 = f(x0),
x2 = f(f(x0)), x3 = f(f(f(x0))), and so on. Since there are only finitely many nodes in the
graph, there must exist integers i, j with 0 ≤i < j and xi = xj. Pick the least i and then
the least j (for this i) satisfying these conditions. Then the path starting at x0 ends in a
directed cycle (consisting of the nodes xi, xi+1, . . . , xj = xi) and begins in a tail x0, . . . , xi−1
that feeds into the cycle at position xi. If i = 0, then there is no tail, and the path we are
following lies completely on the cycle. If j = i + 1, the cycle consists of a single loop edge
based at the node xi.
For example, consider the f shown in Figure 2.1 and x0 = 2. The sequence (xi : i ≥0)
is (2, 22, 3, 7, 19, 21, 3, 7, 19, 21, . . .), so i = 2, j = 6, the tail part is 2, 22, and the cycle part
is 3, 7, 19, 21 with 21 leading back to 3. For x0 = 15, we get a directed cycle 5, 15 involving
two vertices. For x0 = 20, the tail is node 20 and the cycle is node 11 with its loop edge.
Permutations
25
2.3
Cycle Decompositions of Permutations
Suppose that f : [n] →[n] is a permutation, so f is one-to-one and onto. What does the
directed graph for f look like in this case? Since f is onto, there is at least one arrow
entering each node in the graph. Since f is one-to-one, there is at most one arrow entering
each node in the graph. Consider the path starting at x0 with tail part x0, . . . , xi−1 and
cycle part xi, xi+1, . . . , xj−1, xj = xi. In this path, the tail part must be absent (meaning
that i = 0), since otherwise the arrows starting at xi−1 and xj−1 both lead to xi. This
is impossible, since xi−1 ̸= xj−1 (by minimality of i) and f is one-to-one. We conclude
that every path obtained by following arrows through the graph of f is a directed cycle,
possibly involving a single node. Every node belongs to such a cycle, since every node has
an arrow leaving it. Different cycles visit disjoint sets of nodes, since otherwise there would
be two arrows entering the same node. So, the directed graph representing a permutation is
a disjoint union of directed cycles.
It follows that we can describe a permutation f of [n] by listing all the cycles in the
directed graph of f, writing the elements within each cycle in the order they appear along
the cycle. The cycles themselves can be listed in any order, and we can start at any position
on each cycle when listing the nodes in that cycle. Any such listing of the directed cycles of
f is called a cycle decomposition of f. The notation (i1, i2, . . . , ik) represents a cycle that
visits the k distinct nodes i1, i2, . . . , ik in this order and then returns to i1. Observe that
(i2, . . . , ik, i1) and (i3, . . . , ik, i1, i2) are alternate notations for the cycle just mentioned,
but (ik, . . . , i2, i1) is a different cycle (for k ≥3) since the direction of the edges matters. If
the graph of f has multiple cycles, we juxtapose the notation for each individual cycle to
get a cycle decomposition of f. For example, let n = 6 and f = [4, 2, 5, 6, 3, 1] in one-line
form. The directed graph for f is shown in Figure 2.2, and a cycle decomposition of f is
(1, 4, 6)(2)(3, 5). Two other cycle decompositions for the same f are (5, 3)(6, 1, 4)(2) and
(2)(3, 5)(4, 6, 1).
3
5
2
6
4
1
FIGURE 2.2
Directed graph of a permutation.
A cycle containing k nodes is called a k-cycle. When the domain [n] has been specified,
we are permitted to omit some or all 1-cycles from the cycle decomposition of a permutation.
For example, the f in Figure 2.2 could also be written (1, 4, 6)(3, 5). The identity function
in S6 has cycle decomposition (1)(2)(3)(4)(5)(6), which we may abbreviate as (1). Given
h = (1, 3)(2, 5)(4)(6) ∈S6, we can write h = (1, 3)(2, 5) = (5, 2)(4)(3, 1). If we did not know
n = 6, then the notation (1, 3)(2, 5) is ambiguous: it could represent a permutation in Sn
for any n ≥5. Given that n is known, individual cycles of any length define permutations
of [n] by restoring the omitted 1-cycles. For instance, we have permutations f1 = (1, 4, 6),
f2 = (2) = id, and f3 = (3, 5), which are given in one-line form by f1 = [4, 2, 3, 6, 5, 1],
f2 = [1, 2, 3, 4, 5, 6], and f3 = [1, 2, 5, 4, 3, 6].
The one-line form for a function encloses the function values in square brackets, while
the cycle notation for a k-cycle uses round parentheses around the nodes in the cycle. For
26
Advanced Linear Algebra
example, f = [4, 2, 5, 6, 3, 1] is different from the 6-cycle (4, 2, 5, 6, 3, 1) = (1, 4, 2, 5, 6, 3).
The one-line form of this 6-cycle is [4, 5, 1, 2, 6, 3] ̸= f.
We
can
compose
two
permutations
using
their
cycle
decompositions.
Given
f = (1, 4, 6)(2)(3, 5) and g = (2, 6, 4, 3)(1, 5), let us find f ◦g and g ◦f. For f ◦g, we
could compute f(g(1)) = f(5) = 3, f(g(2)) = f(6) = 1, f(g(3)) = f(2) = 2, etc., leading
to the one-line form f ◦g = [3, 1, 2, 5, 4, 6]. Converting this to a cycle decomposition, we
have f ◦g = (1, 3, 2)(4, 5)(6). Next, we see how to compute a cycle decomposition of g ◦f
without finding the one-line form first. Node 1 belongs to some cycle of g ◦f; indeed,
g(f(1)) = g(4) = 3, so 1 is followed on its cycle by 3. Next, g(f(3)) = g(5) = 1, so 3 is
followed on the cycle by 1, and we see that the 2-cycle (1, 3) is part of the directed graph
of g ◦f. Next, we consider node 2, and note g(f(2)) = g(2) = 6, then g(f(6)) = g(1) = 5,
then g(f(5)) = g(3) = 2, so we have the cycle (2, 6, 5). Finally, we find the cycle involving
node 4 by noting g(f(4)) = g(6) = 4. Thus, one possible cycle decomposition of g ◦f is
(1, 3)(2, 6, 5)(4). If needed, we can now find the one-line form g ◦f = [3, 6, 1, 4, 2, 5].
Let f = (1, 4, 6)(2)(3, 5), f1 = (1, 4, 6), f2 = (2), and f3 = (3, 5). By a computation
similar to the one given in the last paragraph, we see that
f = (1, 4, 6)(2)(3, 5) = (1, 4, 6) ◦(2) ◦(3, 5) = f1 ◦f2 ◦f3.
Note here that the single permutation f on the left side has been written as a product (in the
group S6) of three permutations f1, f2, f3 on the right side. Intuitively, the action of f on [n]
can be accomplished by first applying f3, which moves only the elements on the cycle (3, 5);
then applying f2, which happens to be the identity map; and then applying f1, which moves
only the elements on the cycle (1, 4, 6). Similarly, f = f1 ◦f3 ◦f2 = f2 ◦f1 ◦f3 = f3 ◦f1 ◦f2,
etc. We achieve the same net effect no matter what order we apply the functions f1, f2, and
f3, which translates into the algebraic statement that these three elements of Sn commute
with each other.
2.4
Composition of Cycles
We now generalize the example at the end of the last section. Fix n, and consider two cycles
g = (a1, . . . , ak) and h = (b1, . . . , bm) in the group Sn. We say g and h are disjoint cycles iff
{a1, . . . , ak} and {b1, . . . , bm} are disjoint sets. We prove that if g and h are disjoint cycles,
then g ◦h = h◦g. We say that disjoint cycles commute in Sn. For the proof, recall that two
functions p, q : [n] →[n] are equal iff p(x) = q(x) for all x ∈[n]. In the situation at hand,
we have p = g ◦h and q = h ◦g. Take an arbitrary x ∈[n], and consider three cases. First,
if x = ai for some i, then
g ◦h(x) = g(h(ai)) = g(ai) = h(g(ai)) = h ◦g(x)
since h(aj) = aj for all j, and g(ai) equals some aj. Second, if x = bi for some i, then
g ◦h(x) = g(h(bi)) = h(bi) = h(g(bi)) = h ◦g(x)
since g(bj) = bj for all j, and g(bi) equals some bj. Third, if x ̸∈{a1, . . . , ak, b1, . . . , bm},
then
g ◦h(x) = x = h ◦g(x).
This completes the proof that g ◦h = h ◦g. We remark that the 1-cycle id commutes with
every element of Sn, without needing any assumption of disjointness.
Permutations
27
We now verify a few more identities involving composition of cycles. Suppose
f = C1C2 · · · Ck is any cycle decomposition of f ∈Sn. Let us check carefully that
f = C1 ◦C2 ◦· · · ◦Ck. The notation Ci refers both to a cycle in the directed graph of
f, and the function in Sn corresponding to that cycle. We must show that the function
C1 ◦C2 ◦· · · ◦Ck applied to any x ∈[n] yields f(x). If x does not appear in any of the
cycles Ci, then all of the permutations Ck, Ck−1, . . . , C1 leave x fixed. (We say a function
g fixes an element x iff g(x) = x.) Also, f itself must fix x since we are only allowed to
omit 1-cycles in a cycle decomposition. Thus, f(x) = x = C1 ◦C2 ◦· · · ◦Ck(x). The other
possibility is that x appears in a cycle Ci for some (necessarily unique) i. If f(x) = y, then
Ci(x) = y because of the way we constructed the cycles from the directed graph of f. All
the other cycles Cj do not overlap with Ci, and therefore the functions associated with
these other cycles must fix x and y. Starting at x and applying the functions Ck through
C1 in this order, x is always fixed until Ci sends x to y, and then y is always fixed. Hence,
C1 ◦· · · ◦Ck(x) = y = f(x), as needed.
Next, consider a general k-cycle (a1, a2, . . . , ak), where k ≥3. We show that
(a1, a2, . . . , ak) = (a1, ak) ◦(a1, ak−1) ◦· · · ◦(a1, a3) ◦(a1, a2).
(2.1)
If x does not equal any aj, then both sides of (2.1) fix x. Both sides send a1 to a2 and
ak to a1. (Remember that the maps on the right side are applied from right to left.) For
1 < j < k, the left side sends aj to aj+1. Reading the other side from right to left, the
first j −2 factors leave aj fixed. The next 2-cycle, namely (a1, aj), sends aj to a1. The next
2-cycle, namely (a1, aj+1), sends a1 to aj+1. The remaining 2-cycles fix aj+1, so the net
effect is that aj goes to aj+1.
By similar arguments, we can verify the identity
(j, i) = (i, j) = (i, i + 1) ◦(i + 1, i + 2) ◦· · · ◦(j −2, j −1) ◦(j −1, j)
◦(j −2, j −1) ◦· · · ◦(i + 1, i + 2) ◦(i, i + 1),
(2.2)
which holds for all i, j ∈[n] with i < j. For example, (2, 5) = (2, 3)◦(3, 4)◦(4, 5)◦(3, 4)◦(2, 3).
2.5
Factorizations of Permutations
We now consider various ways of factoring permutations in the group Sn into products
of simpler permutations. Here, “product” refers to the group operation in Sn, which is
composition of functions. These results are analogous to the Fundamental Theorem of
Arithmetic, which states that any positive integer can be written as a product of prime
integers.
Factorization into Disjoint Cycles. Given any permutation f ∈Sn, our analysis of the
directed graph of f shows that f has a cycle decomposition f = C1C2 · · · Ck. We proved in
Section 2.4 that this cycle decomposition implies the factorization f = C1 ◦C2 ◦· · · ◦Ck.
So, every f ∈Sn can be written as a product of k pairwise disjoint cycles in Sn.
To what extent is this factorization unique? On one hand, the cycles C1, . . . , Ck are
pairwise disjoint, so they all commute with each other. On the other hand, we are free
to add or remove 1-cycles from this factorization, since all 1-cycles represent the identity
element of Sn. If we do not consider these minor variations (reordering disjoint cycles,
omitting 1-cycles) to be different from one another, then the factorization of f into disjoint
cycles is unique. We omit a formal proof of uniqueness. The idea of the proof is to show
28
Advanced Linear Algebra
that the non-identity cycles in any factorization of f into a product of disjoint cycles must
correspond (in some order) to the cycles in the directed graph of f involving more than one
node. These directed cycles are uniquely determined by f, so the cycles in the factorization
of f are uniquely determined as well.
Factorizations into Transpositions. A transposition is a 2-cycle (i, j). The formula (2.1)
shows that any k-cycle with k ≥3 can be factored in Sn as a product of transpositions.
Applying this formula to each cycle in a disjoint cycle factorization of f, we see that every
f ∈Sn can be written as a product of zero or more transpositions. Here, an empty product
represents the identity permutation, which can also be factored as id = (1, 2) ◦(1, 2) for
n ≥2. Note that the transpositions used in such a factorization often are not disjoint.
Furthermore, the factorization of a permutation into transpositions is not unique. For
example, given any factorization of any g ∈Sn into transpositions, we can always create
new factorizations of g by appending pairs of factors of the form (i, j)◦(i, j). There can also
be essentially different factorizations involving the same number of transposition factors.
For instance,
(1, 4, 3, 2) = [4, 1, 2, 3] = (1, 3) ◦(1, 4) ◦(2, 3) = (3, 4) ◦(2, 3) ◦(1, 2).
Factorizations into Basic Transpositions. A basic transposition in Sn is a transposition
of the form (i, i+1) for some i ∈{1, 2, . . . , n−1}. Formula (2.2) shows that any transposition
can be factored in Sn into a product of basic transpositions. Applying this formula to each
transposition in a factorization of f ∈Sn into transpositions, we see that every f ∈Sn can
be written as a product of zero or more basic transpositions. These factorizations are not
unique; for instance,
(1, 3, 4) = [3, 2, 4, 1] = (1, 2) ◦(2, 3) ◦(3, 4) ◦(1, 2) = (2, 3) ◦(1, 2) ◦(2, 3) ◦(3, 4).
The next section gives an algorithm (based on sorting the one-line form of f) for factoring
a given f ∈Sn into a product of basic transpositions.
2.6
Inversions and Sorting
Consider a function f : [n] →[m], where n and m are positive integers. An inversion of
f is a pair (i, j) such that 1 ≤i < j ≤n and f(i) > f(j). In terms of the one-line form
[f(1), f(2), . . . , f(n)], an inversion of f is a pair of positions (i, j), not necessarily adjacent,
such that f(i) and f(j) appear out of order. Let Inv(f) be the set of inversions of f and
inv(f) = | Inv(f)| be the number of inversions of f. These concepts apply, in particular, to
permutations f ∈Sn. For example, given f = [3, 1, 4, 2], one inversion of f is (1, 2), since
f(1) > f(2) (3 > 1). Listing all the inversions, we find that Inv(f) = {(1, 2), (1, 4), (3, 4)}
and inv(f) = 3. For another example, g = [1, 1, 2, 1, 1, 2] has Inv(g) = {(3, 4), (3, 5)} and
inv(g) = 2.
Given f : [n] →[m], we can sort the list [f(1), f(2), . . . , f(n)] into weakly increasing
order by a sequence of basic transposition moves. By definition, a basic transposition move
consists of switching two adjacent list elements that are strictly out of order; in other words,
we are permitted to switch f(i) and f(i + 1) iff f(i) > f(i + 1). How many such moves does
it take to sort the list? We claim that, regardless of which moves are made in what order,
the list is always sorted in exactly inv(f) moves. This claim follows from the following three
assertions. (1) The list is fully sorted iff inv(f) = 0. (2) If the list is not fully sorted, then
Permutations
29
there exists at least one permissible basic transposition move. (3) If g is obtained from f by
applying any one basic transposition move, then inv(g) = inv(f)−1. Assertions (1) and (2)
are readily verified. To check (3), suppose f(i) > f(i+1), and we get g from f by switching
these two values in the one-line form of f. By definition of inversions, we see that the set
Inv(f) can be transformed into Inv(g) by deleting the inversion (i, i+1), and then replacing
i by i + 1 and i + 1 by i in all other inversion pairs in which i or i + 1 appears. We see
from this that Inv(g) has one less element than Inv(f), so that (3) holds. For example, if
f = [3, 1, 4, 2] is transformed into g = [3, 1, 2, 4] by switching the last two elements, then
Inv(f) = {(1, 2), (1, 4), (3, 4)} is transformed into Inv(g) = {(1, 2), (1, 3)} by the process
just described.
This sorting method leads to an algorithm for factoring permutations f ∈Sn into
products of basic transpositions. Suppose g is obtained from f by switching f(i) and f(i+1)
in the one-line form of f. We may check that g = f ◦(i, i + 1), so that (in the case
f(i) > f(i + 1)) the basic transposition move switching the entries in positions i and i + 1
can be accomplished algebraically in Sn by multiplying f on the right by (i, i + 1). Sorting f
to id = [1, 2, . . . , n] by a sequence of such moves therefore gives a formula of the form
f ◦(i1, i1 + 1) ◦(i2, i2 + 1) ◦· · · ◦(ik, ik + 1) = id,
where k = inv(f). Solving for f, we find that f = (ik, ik + 1) ◦· · · ◦(i2, i2 + 1) ◦(i1, i1 + 1).
In our running example where f = [3, 1, 4, 2], we can write f ◦(3, 4) ◦(1, 2) ◦(2, 3) = id,
and hence f = (2, 3) ◦(1, 2) ◦(3, 4). To summarize, we have proved that any f ∈Sn can be
factored into the product of inv(f) basic transpositions.
We can also prove that each f ∈Sn cannot be factored into a product of fewer than inv(f)
basic transpositions. Suppose f = t1 ◦t2 ◦· · · ◦tk where each tj is a basic transpositioon.
Then f ◦tk ◦· · · ◦t2 ◦t1 = id. This equation says that the one-line form for f can be sorted
to the one-line form for id by k steps, where each step switches two adjacent entries. Each
such step either raises or lowers the inversion count by 1. Since the starting list has inv(f)
inversions and the final list has zero inversions, we must have k ≥inv(f), as needed.
2.7
Signs of Permutations
For any function f : [n] →[n], define the sign of f to be sgn(f) = (−1)inv(f) if f is a
bijection, and sgn(f) = 0 otherwise. For example, if f = id = [1, 2, . . . , n], then inv(f) = 0
and sgn(f) = 1. If h is the basic transposition (i, i + 1), then the one-line form for h is
[1, . . . , i −1, i + 1, i, i + 2, . . . , n],
so that Inv(h) = {(i, i + 1)}, inv(h) = 1, and sgn(h) = −1.
Sign Homomorphism Theorem. For all n ≥1, the function sgn : Sn →{−1, 1} is a
group homomorphism, meaning:
for all f, h ∈Sn, sgn(f ◦h) = sgn(f) · sgn(h).
(2.3)
Proof. First, (2.3) holds when h = id, since f ◦id = f and sgn(id) = 1. Next, consider the
special case where h is a basic transposition (i, i + 1). Fix f ∈Sn, and set g = f ◦h = f ◦
(i, i + 1). We know that the one-line form for g is obtained from the one-line form for f by
switching the entries f(i) and f(i + 1) in positions i and i + 1. Hence, if f(i) > f(i + 1),
30
Advanced Linear Algebra
then g is obtained from f by applying a basic transposition move. By assertion (3) in §2.6,
inv(g) = inv(f) −1, and so
sgn(g) = −sgn(f) = sgn(f) · sgn((i, i + 1)) = sgn(f) · sgn(h).
In the case where f(i) < f(i + 1), we see that f is obtained from g by applying a basic
transposition move. Applying assertion (3) with f and g interchanged, we see that inv(g) =
inv(f) + 1, and again
sgn(g) = −sgn(f) = sgn(f) · sgn((i, i + 1)) = sgn(f) · sgn(h).
Therefore, (2.3) holds when h is any basic transposition.
To prove (2.3) for arbitrary h ∈Sn, write h = h1 ◦· · · ◦hs where each hj is a basic
transposition. We now use induction on s. If s = 0 or s = 1, then we already know that (2.3)
holds for all f ∈Sn. For the induction step, assume s ≥2 and that (2.3) holds whenever h
is a product of fewer than s basic transpositions. Using associativity, we get
sgn(f ◦h)
=
sgn((f ◦h1 ◦· · · ◦hs−1) ◦hs)
=
sgn(f ◦h1 ◦· · · ◦hs−1) · sgn(hs)
=
sgn(f) · sgn(h1 ◦· · · ◦hs−1) · sgn(hs)
=
sgn(f) · sgn(h1 ◦· · · ◦hs−1 ◦hs)
=
sgn(f) · sgn(h).
We note five consequences of the Sign Homomorphism Theorem. First, since f◦f −1 = id,
we have 1 = sgn(id) = sgn(f)·sgn(f −1). So sgn(f −1) = 1/ sgn(f) = sgn(f)−1. Since 1−1 = 1
and (−1)−1 = −1, we conclude: sgn(f −1) = sgn(f) for all f ∈Sn.
Second, if h = h1 ◦· · · ◦hs has been factored as a product of s basic transpositions hi,
then sgn(h) = Qs
i=1 sgn(hi) = (−1)s. For h = (i, j), (2.2) shows that we can write h as
the product of an odd number of basic transpositions. Therefore, sgn((i, j)) = −1 for all
transpositions (not just basic transpositions).
Third, if h = h1 ◦· · · ◦hs has been factored as a product of s transpositions hi, then
sgn(h) = Qs
i=1 sgn(hi) = (−1)s. In particular, if h can be written in some other way as a
product of s′ transpositions, then we must have (−1)s = (−1)s′, so that s and s′ are both
even or both odd. Thus, while factorizations of a given permutation into transpositions are
not unique, the parity of the number of transpositions in such factorizations is unique.
Fourth, if h is a k-cycle, (2.1) shows that h can be written as a product of k −1
transpositions. Therefore, sgn(h) = (−1)k−1. So even-length cycles have negative sign,
whereas odd-length cycles have positive sign.
Fifth, suppose h ∈Sn is an arbitrary permutation whose disjoint cycle decomposition
(including all 1-cycles) consists of c cycles with respective lengths k1, . . . , kc. Then sgn(h) =
Qc
i=1(−1)ki−1. Since k1 + · · · + kc = n, the net power of −1 is n −c. So for h ∈Sn,
sgn(h) = (−1)n−c, where c is the number of cycles in any cycle decomposition of h that
includes all 1-cycles.
2.8
Summary
1.
Symmetric Groups. For each positive integer n, let [n] = {1, 2, . . . , n}. Sn consists
of all bijections (permutations) f : [n] →[n] with composition of functions as the
group operation. Sn is a group of size n!, which is non-commutative for n ≥3.
Permutations
31
2.
Directed Graphs of Functions. A function f : [n] →[n] can be represented by
a directed graph with vertex set [n] and a directed edge from i to f(i) for each
i ∈[n] (which is a loop if f(i) = i). For general f, the directed graph of f consists
of directed trees feeding into directed cycles. For bijections f ∈Sn, the directed
graph of f consists of one or more directed cycles involving pairwise disjoint sets of
vertices. We obtain cycle decompositions of f by traversing all these cycles (with
the possible omission of length-1 cycles) and listing the elements encountered.
3.
Factorizations of Permutations. Every f ∈Sn can be factored in Sn into a product
of disjoint cycles. This factorization is unique except for the order of the factors
and the possible omission of 1-cycles. Disjoint cycles commute. Every f ∈Sn
can be factored as a product of transpositions (i, j) and as a product of basic
transpositions (i, i + 1). These factorizations are not unique, but the parity (even
or odd) of the number of transpositions used is unique.
4.
Facts about Inversions. For any function f : [n] →[m], inv(f) is the number
of pairs i < j such that f(i) > f(j). If we sort the list [f(1), . . . , f(n)] by
interchanging adjacent elements that are out of order, then the sorting finishes
after exactly inv(f) interchanges. Any f ∈Sn can be written as the product of
inv(f) basic transpositions, but no fewer.
5.
Facts about Sign. For f ∈Sn, define sgn(f) = (−1)inv(f). For all f, g ∈Sn,
sgn(f ◦g) = sgn(f)·sgn(g) (Sign Homomorphism Theorem). We have sgn(id) = 1,
sgn((i, j)) = −1 for all i ̸= j, and sgn(g−1) = sgn(g) for all g ∈Sn. For f ∈Sn,
sgn(f) = (−1)n−c, where c is the number of cycles in any cycle decomposition of
f that includes all 1-cycles. If f can be factored into a product of s transpositions,
then sgn(f) = (−1)s. The sign of any k-cycle is (−1)k−1.
2.9
Exercises
1.
Let f = [2, 2, 5, 1, 3] and g = [3, 4, 2, 5, 1]. Find f ◦g, g ◦f, f 3 (defined to be
f ◦f ◦f), and g−1, giving all answers in one-line form.
2.
Let f = [3, 1, 2, 4] and g = [4, 3, 2, 1]. Find f ◦g, g ◦f, f ◦g ◦f −1, f 3, and f 1001,
giving all answers in one-line form. Explain how you found f 1001.
3.
Suppose f : X →Y and g : Y →Z are functions.
(a) Prove: if f and g are one-to-one, then g ◦f is one-to-one.
(b) Prove: if f and g are onto, then g ◦f is onto.
(c) Prove: if f and g are bijections, then g ◦f is a bijection.
4.
Suppose f : X →Y and g : Y →Z are functions.
(a) Prove: if g ◦f is one-to-one, then f is one-to-one.
(b) Prove: if g ◦f is onto, then g is onto.
(c) Give an example where g ◦f is one-to-one but g is not one-to-one.
(d) Give an example where g ◦f is onto but f is not onto.
5.
Prove that function composition is associative: for all functions h : W →X,
g : X →Y , and f : Y →Z, f ◦(g ◦h) = (f ◦g) ◦h.
6.
Show that the group Sn is not commutative for n ≥3.
7.
(a) Given f : [n] →[n], explain informally why f is one-to-one iff f is onto.
32
Advanced Linear Algebra
(b) Give an example of a function f : Z →Z that is one-to-one but not onto.
(c) Give an example of a function g : Z →Z that is onto but not one-to-one.
8.
For any set X, let S(X) be the set of all bijections f : X →X.
(a) Prove that (S(X), ◦) is a group.
(b) For which sets X is S(X) a commutative group?
(c) Suppose X = {x1, . . . , xn} is an n-element set. Prove that the groups S(X)
and Sn are isomorphic.
9.
Draw the directed graphs for each of these functions given in one-line form:
(a) [2, 3, 4, 5, 5]
(b) [5, 4, 3, 2, 1]
(c) [2, 2, 2, 2, 2]
(d) [1, 2, 3, 4, 5]
(e) [3, 4, 5, 1, 2]
(f) [3, 3, 7, 12, 12, 11, 12, 9, 8, 7, 11, 10]
10.
Let f be the function whose directed graph is drawn in Figure 2.1. Find the
one-line forms of f ◦f and f ◦f ◦f, and draw the associated directed graphs.
11.
Suppose X is an infinite set. Must the directed graph of a bijection f : X →X be
a disjoint union of directed cycles? If not, can you describe the general structure
of this graph?
12.
Let f = [4, 2, 1, 3] ∈S4. Find all possible cycle decompositions of f.
13.
Compute each of the following, giving a cycle decomposition for each answer.
(a) (1, 3, 6, 2) ◦(2, 4, 5)(1, 6)
(b) the inverse of (1, 3, 5, 7, 9)(2)(4, 8, 6)
(c) [3, 1, 2, 4] ◦(3, 1, 2, 4)
(d) (3, 1, 2, 4) ◦[3, 1, 2, 4]
(e) (3, 1, 2, 4) ◦(3, 1, 2, 4)
(f) (1, 3, 4) ◦(2, 3)(1, 5) ◦(2, 5, 3, 1)−1 ◦(2, 3, 4) ◦(1, 2)(3, 4)
(g) (2, 3) ◦(1, 2) ◦(2, 3) ◦(3, 4) ◦(2, 3) ◦(4, 5) ◦(3, 4) ◦(2, 3) ◦(1, 2)
14.
Let f = (3, 1, 6)(2, 4) and g = (5, 2, 1, 4, 3) in S6.
(a) Compute cycle decompositions of f ◦g and g ◦f.
(b) Compute cycle decompositions of f ◦g ◦f −1 and g ◦f ◦g−1.
(c) Compute f k for k = 2, 3, 4, 5, 6.
(d) Find (with explanation) f 670.
15.
List all f ∈S4 with f ◦f = id. Explain why these f, and no others, work.
16.
Given a cycle decomposition of f ∈Sn, how can one quickly obtain a cycle
decomposition of f −1? Prove your answer.
17.
Given a cycle decomposition of f ∈Sn and a large positive integer m, describe
how one can find a cycle decomposition of f m = f ◦f ◦· · · ◦f (m copies of f).
18.
Find six permutations h ∈S8 such that h ◦h = (1, 3)(2, 5)(4, 8)(6, 7).
19.
How many h ∈S4n satisfy h ◦h = (1, 2)(3, 4)(5, 6) · · · (4n −1, 4n)? Explain.
20.
For which f ∈Sn does there exist h ∈Sn with h ◦h = f? Find and prove a
necessary and sufficient condition involving the cycle decomposition of f.
21.
Prove (b1, b2, . . . , bm) = (b1, b2) ◦(b2, b3) ◦· · · ◦(bm−1, bm).
22.
Prove Formula (2.2) in §2.4.
23.
Prove that the map Cg : Sn →Sn, given by Cg(f) = g ◦f ◦g−1 for f ∈Sn, is a
group isomorphism.
24.
Conjugation Rule for Permutations. Suppose g ∈Sn and f = (i1, i2, . . . , ik)
is a k-cycle in Sn. Prove that g ◦f ◦g−1 = (g(i1), g(i2), . . . , g(ik)). Deduce that
for all f, g ∈Sn, a cycle decomposition for g◦f ◦g−1 can be obtained from a cycle
Permutations
33
decomposition for f by applying g to each number in the cycle decomposition,
leaving all parentheses unchanged.
25.
Find all f ∈S5 that commute with g = (1, 2, 4)(3, 5).
26.
Suppose the directed graph of f ∈Sn consists of k1 1-cycles, k2 2-cycles, and so
on. Count the cycle decompositions of f in which no 1-cycles are omitted, and
shorter cycles are always listed before longer cycles. Use the Conjugation Rule
(Exercise 24) to prove that this count is the number of g ∈Sn that commute
with f.
27.
Uniqueness of Disjoint Cycle Factorizations. Let f ∈Sn have a cycle
decomposition C1C2 · · · Ck in which all 1-cycles have been omitted. View each
Ci as an element of Sn. Prove: for any factorization f = D1 ◦D2 ◦· · · ◦Dj,
where the Di are pairwise disjoint non-identity cycles in Sn, we have j = k and
D1, . . . , Dk is a reordering of C1, . . . , Ck of Sn). (Study the cycles of the directed
graph of D1 ◦· · · ◦Dj.)
28.
Make a table listing all permutations f ∈S4. In column 1, write f in one-line
form. In column 2, draw the directed graph for f. In column 3, give a cycle
decomposition of f. In column 4, compute inv(f). In column 5, compute sgn(f).
29.
Let g = [2, 4, 7, 1, 5, 3, 6] ∈S7. Compute Inv(g), inv(g), sgn(g), Inv(g−1),
inv(g−1), and sgn(g−1).
30.
Let h = (2, 4, 7)(1, 5)(3, 6) ∈S7. Compute Inv(h), inv(h), sgn(h), Inv(h−1),
inv(h−1), and sgn(h−1).
31.
Suppose f ∈S12 has a cycle decomposition f = (a, b, c)(d, e, g, h)(j, k). What is
sgn(f)?
32.
Find and prove a simple relation between inv(f) and inv(f −1), for f ∈Sn. Deduce
that sgn(f) = sgn(f −1).
33.
Write each permutation as a product of basic transpositions.
(a) [4, 1, 3, 5, 2]
(b) (3, 6)
(c) (1, 2, 4)(3, 5)
(d) (1, 4, 2, 5, 6, 3)
34.
Find, with explanation:
(a) the number of basic transpositions in Sn;
(b) the number of transpositions in Sn;
(c) the number of n-cycles in Sn.
35.
For even n ≥2, how many f ∈Sn satisfy f(1) < f(3) < f(5) < · · · < f(n −1)
and f(i) < f(i + 1) for all odd i ∈[n]?
36.
Write f = [3, 7, 4, 1, 6, 8, 2, 5] as a product of transpositions in three different ways.
What is the least number of transpositions appearing in any such factorization
for f?
37.
Find all f ∈S5 with inv(f) = 8.
38.
Verify assertions (1) and (2) in §2.6.
39.
Prove: for all functions f, g : [n] →[n], sgn(f ◦g) = sgn(f) · sgn(g). (Recall that
sgn(h) = 0 when h is not a bijection.)
40.
Fix i < j in [n]. Use the definitions to find Inv((i, j)), inv((i, j)), and sgn((i, j)).
41.
Which permutation in Sn has the most inversions? Give the one-line form and a
cycle decomposition for this permutation.
42.
Suppose f : [n] →[n] has one-line form [f(1), . . . , f(n)], and g is obtained from
this one-line form by switching f(i) and f(j) (the entries in positions i and j).
Prove g = f ◦(i, j).
34
Advanced Linear Algebra
43.
Suppose f : [n] →[n] and h = (i, j) ◦f. Describe (with proof) how the one-line
form for h can be obtained from the one-line form for f.
44.
Use one of the previous two exercises and a sorting algorithm for one-line forms
to prove that any f ∈Sn can be written as a product of transpositions.
45.
Show that An = {f ∈Sn : sgn(f) = +1} is a normal subgroup of Sn. Find |An|.
46.
How many permutations in Sn have sign −1?
47.
Show that V = {(1), (1, 2)(3, 4), (1, 3)(2, 4), (1, 4)(2, 3)} is a normal subgroup of
S4. Show that H = {(1), (1, 2, 3, 4), (1, 3)(2, 4), (1, 4, 3, 2)} is a subgroup of S4 that
is not normal in S4.
48.
Prove: for all n ≥3 and all f ∈Sn with sgn(f) = +1, f can be factored into a
product of (not necessarily disjoint) 3-cycles.
49.
(a) Prove that any f ∈Sn can be written as a product of factors, each of which
is either (1, 2) or (1, 2, . . . , n). Illustrate your proof with f = [3, 5, 1, 4, 2].
(b) What is the relation between the one-line forms of f and f ◦(1, 2)? What is
the relation between the one-line forms of f and f ◦(1, 2, . . . , n)?
(c) Interpret parts (a) and (b) as a result about the ability to sort a list into
increasing order using two particular sorting moves.
50.
What can you say about the minimum number of steps needed to sort a list using
the two sorting moves in the previous exercise? What if right-multiplication by
(n, . . . , 2, 1) is also allowed as a move?
51.
True or false? Explain each answer.
(a) (3, 1, 2, 4) is a 3-cycle.
(b) [3, 1, 2, 4] is a 3-cycle.
(c) There exists f ∈S4 with [f(1), f(2), f(3), f(4)] = (f(1), f(2), f(3), f(4)).
(d) For all functions f : [n] →[n], if f ◦f ∈Sn then f ∈Sn.
(e) For all f ∈Sn, if f −1 = f then f = id or f is a 2-cycle.
(f) For all distinct cycles f, g ∈Sn, f ◦g = g ◦f iff f and g are disjoint cycles.
(g) Any product of c disjoint cycles in Sn has sign (−1)n−c.
(h) Every k-cycle in Sn (k > 1) is a product of k −1 basic transpositions.
(i) Every f ∈Sn (n > 1) can be written as a product of inv(f)+8 transpositions.
(j) For all f, g ∈Sn and 1 ≤i < n, if g = (i, i + 1) ◦f, then inv(g) = inv(f) ± 1.
(k) Every g ∈Sn can be written as a product of disjoint transpositions.
(l) For all f ∈Sn, if f = h ◦h for some h ∈Sn, then sgn(f) = +1.
(m) For all f ∈Sn, if sgn(f) = +1, then f = h ◦h for some h ∈Sn.
3
Polynomials
Polynomials appear everywhere in linear algebra and throughout mathematics. Most of us
learn about polynomial functions in calculus. In algebraic settings, we need a more formal
concept of polynomial as a symbolic expression a0 + a1x + a2x2 + · · · + anxn that is not
necessarily a function of x. We begin this chapter with an intuitive description of these
formal polynomials, which form a ring using the familiar algebraic rules for adding and
multiplying polynomials. Then we make this intuitive discussion more precise by giving
a rigorous definition of polynomials in terms of formal power series and connecting this
definition with the idea of a polynomial function.
The next part of the chapter studies the divisibility structure of the ring of one-variable
polynomials with coefficients in a field. There is a close analogy between factorization of
such polynomials and corresponding results on factorization of integers. Recall that for any
integers a, b with b ̸= 0, there is a division algorithm that produces a unique quotient q ∈Z
and remainder r ∈Z satisfying a = bq + r and 0 ≤r < |b|. We develop a similar division
algorithm for polynomials, where the remainder must now have smaller degree than b. This
algorithm gives useful information on divisors, greatest common divisors, and least common
multiples of polynomials. To understand how polynomials factor, we introduce irreducible
polynomials, which are non-constant polynomials that cannot be factored into products of
two other polynomials of smaller degree. Irreducible polynomials are the analogs of prime
integers. We show that every nonzero polynomial has a unique factorization into a product
of irreducible polynomials, just as every nonzero positive integer can be written uniquely
as a product of primes.
The chapter concludes with more facts on irreducible polynomials. We present some
theorems and algorithms for testing whether polynomials with coefficients in certain fields
are irreducible. In particular, Kronecker’s factoring algorithm gives a laborious but definitive
method for testing a polynomial with rational coefficients for irreducibility, or finding the
irreducible factorization of such a polynomial. We also discuss minimal polynomials, which
are polynomials that help us understand the structure of finite-dimensional F-algebras.
3.1
Intuitive Definition of Polynomials
We begin by presenting an informal definition of polynomials, which is made precise in §3.3.
Let R be any ring. Intuitively, a one-variable polynomial with coefficients in R is a symbolic
expression of the form
a0 + a1x + a2x2 + · · · + anxn,
where n ≥0 is an integer, each ai is an element of R, and the letter x is an indeterminate
or formal variable. We often display such a polynomial using the summation notation
Pn
i=0 aixi. It is convenient to define ai for all integers i ≥0 by setting ai = 0R for i > n.
By definition, two polynomials P
i≥0 aixi and P
i≥0 bixi are equal iff ai = bi for all i ≥0.
DOI: 10.1201/9781003484561-3
35
36
Advanced Linear Algebra
So, two polynomials are equal iff all corresponding coefficients are equal. A polynomial is
only permitted to have finitely many nonzero coefficients. The symbol R[x] denotes the set
of all polynomials with coefficients in R. For example, 1 + 3x −πx2 +
√
7x4 ∈R[x], but
P∞
n=0(1/n!)xn is not a polynomial.
The unique polynomial whose coefficients are all zero is called the zero polynomial. The
degree of a nonzero polynomial p = P
i≥0 aixi is the largest integer n such that an ̸= 0R.
The degree of the zero polynomial is undefined. Given a polynomial p = Pn
i=0 aixi of degree
n, we write deg(p) = n and introduce the following terminology: anxn is called the leading
term of p; xn is called the leading monomial of p; and an is called the leading coefficient
of p. We call p a monic polynomial iff an = 1. Polynomials of degree zero, one, two, three,
four, and five are respectively called constant polynomials, linear polynomials, quadratic
polynomials, cubic polynomials, quartic polynomials, and quintic polynomials. The zero
polynomial is also considered to be a constant polynomial.
3.2
Algebraic Operations on Polynomials
Given any ring R, we can give R[x] a ring structure by introducing the following addition
and multiplication operations on polynomials. Suppose p = P
i≥0 aixi and q = P
j≥0 bjxj
are two elements of R[x], where ai, bj ∈R. We define the sum of p and q by
p + q =
X
k≥0
(ak + bk)xk.
If the distributive law is to hold for polynomials, the product pq must be given by
pq =
X
i≥0
X
j≥0
aibjxi+j.
To present this answer in standard form, we need to collect like powers of x. In the preceding
expression, the coefficient of xk is the sum of all products aibj such that i + j = k. So we
define the product of p and q by
pq =
X
k≥0

X
i+j=k
aibj

xk =
X
k≥0
 k
X
i=0
aibk−i
!
xk.
(In summations like this, it is always understood that the summation variables only take
nonnegative integer values.) With these definitions, it is now tedious but straightforward to
check that the ring axioms do hold for R[x]. We must confirm that addition of polynomials
is closed, associative, commutative, has an identity element (the zero polynomial), and
additive inverses exist. Similarly, multiplication of polynomials is closed, associative, has an
identity element (the constant polynomial 1R[x] = 1R + 0Rx + 0Rx2 + · · · ), and distributes
over addition. When the coefficient ring R is commutative, R[x] is also commutative. We
verify a few of the ring axioms now, leaving the others as exercises.
First, consider the additive inverse axiom. Assume we already have checked that the
zero polynomial is the additive identity of R[x]. Given p = Pn
i=0 aixi ∈R[x], does p have
an additive inverse in R[x]? Since R is a ring, each coefficient ai ∈R has an additive inverse
−ai ∈R, so that q = Pn
i=0(−ai)xi is an element of R[x]. By definition of addition of
Polynomials
37
polynomials, we compute
p + q =
n
X
i=0
(ai + (−ai))xi =
n
X
i=0
0Rxi = 0R[x] =
n
X
i=0
((−ai) + ai)xi = q + p.
Therefore q is an additive inverse for p in R[x].
Second, consider associativity of multiplication. Given polynomials p, q, r ∈R[x], write
p = P
i≥0 aixi, q = P
j≥0 bjxj, and r = P
k≥0 ckxk, where all ai, bj, ck ∈R. We have
pq = P
t≥0 dtxt where dt = P
i+j=t aibj, and qr = P
u≥0 euxu where eu = P
j+k=u bjck.
Applying the definition of product again, we see that (pq)r = P
v≥0 fvxv where
fv =
X
t+k=v
dtck =
X
(t,k): t+k=v


X
(i,j): i+j=t
aibj

ck =
X
(i,j,k): (i+j)+k=v
(aibj)ck.
The last step uses the distributive law in the ring R, as well as commutativity and
associativity of addition in R. Likewise, p(qr) = P
v≥0 gvxv where
gv =
X
i+u=v
aieu =
X
(i,u): i+u=v
ai


X
(j,k): j+k=u
bjck

=
X
(i,j,k): i+(j+k)=v
ai(bjck).
Since (aibj)ck = ai(bjck) holds for all i, j, k by associativity in R, we see that fv = gv for
all v ≥0. Therefore, by definition of equality of polynomials, (pq)r = p(qr).
Now assume that F is a field. We define scalar multiplication on polynomials in F[x] by
setting c ·
 Pn
i=0 aixi
= Pn
i=0(cai)xi for c, ai ∈F. Routine verifications show that F[x],
using polynomial addition and this scalar multiplication, is a vector space over F. Moreover,
B = {1, x, x2, x3, . . . , xn, . . .} is a basis for F[x]. So F[x] is an infinite-dimensional F-vector
space. We can check that for all c ∈F and p, q ∈F[x], c · (pq) = (c · p)q = p(c · q). Thus,
F[x] is an F-algebra as defined in §1.3. In the case of an arbitrary ring R, we see similarly
that R[x] is an R-module.
3.3
Formal Power Series and Polynomials
In §3.1, we defined a polynomial p ∈R[x] to be a “symbolic expression” of the form
a0 + a1x + · · · + anxn, with each ai ∈R. However, we were rather vague about the
meaning of the letter x in this expression. In order to build a sound theory, we need to
have a more rigorous definition of what a polynomial is. The key to the formal definition
of polynomials is our earlier stipulation that two polynomials are equal iff they have the
same sequence of coefficients. This means we can identify a polynomial Pn
i=0 aixi with its
sequence of coefficients (a0, a1, . . . , an). For many purposes (such as adding two polynomials
of different degrees), it is technically more convenient to use an infinite coefficient sequence
(a0, a1, . . . , an, 0R, 0R, . . .) that ends with an infinite list of zero coefficients.
Using this idea of a coefficient sequence, we define a formal power series with coefficients
in the ring R to be an infinite sequence f = (f0, f1, f2, . . . , fi, . . .) = (fi : i ≥0) with every
fi ∈R. Two formal power series f = (fi : i ≥0) and g = (gi : i ≥0) are equal iff fi = gi
for all i ≥0; this is the ordinary meaning of equality of two sequences. A formal power
series f is called a formal polynomial iff there exists N ∈N with fn = 0R for all n > N
(which means f ends in an infinite string of zero terms). We write R[[x]] to denote the set
38
Advanced Linear Algebra
of all formal power series with coefficients in R, and (as above) we write R[x] to denote
the subset of R[[x]] consisting of the formal polynomials. By analogy with our intuitive
notation for polynomials, we often write f = P
i≥0 fixi = P∞
i=0 fixi to denote a formal
power series, and we call fi the coefficient of xi in f. However, note carefully that (for now)
the summation symbol here is only a notational device — no summation (finite or infinite)
is being performed! Similarly, the powers xi appearing here are (for now) only symbols that
form part of the notation for a formal power series or a formal polynomial.
Extending the earlier formulas for polynomial addition and multiplication to formal
power series, we arrive at the following definitions. Suppose R is a ring and f = (fi : i ≥0),
g = (gi : i ≥0) are two elements of R[[x]]. Define f + g = (fi + gi : i ≥0). Define
fg = (hk : k ≥0), where hk = P
i+j=k figj = Pk
i=0 figk−i for each k ≥0. Note that
any particular coefficient in f + g or fg can be found by computing finitely many sums
and products in the underlying ring R. So our definitions of f + g and fg do not rely on
limits or any other infinite process. As in the case of polynomials, we can verify that R[[x]]
with these operations is a ring, which is commutative if R is commutative. For example,
the proofs of the additive inverse axiom and associativity of multiplication, which we gave
earlier for polynomials, carry over almost verbatim to the present situation. As another
example, we check that R[[x]] is closed under multiplication. Suppose f, g ∈R[[x]], and
write f = (fi : i ≥0), g = (gi : i ≥0) for some fi, gi ∈R. By definition, fg = (hk : k ≥0)
where hk = Pk
i=0 figk−i. To see that fg ∈R[[x]], we must prove that hk ∈R for all k ≥0.
For fixed k ≥0 and 0 ≤i ≤k, figk−i is in R because fi ∈R, gk−i ∈R, and R is closed
under multiplication. Since hk is a sum of finitely many elements of this form, hk is also in
R because R is closed under addition. So fg ∈R[[x]].
Once we prove all the ring axioms for R[[x]], we can reprove that R[x] is a ring by
verifying that R[x] is a subring of R[[x]]. Since the formal series 0 = (0, 0, 0, . . .) and 1 =
(1, 0, 0, . . .) are polynomials, it suffices to check that for all f, g ∈R[x], f + g ∈R[x] and
−f ∈R[x] and fg ∈R[x]. In other words, if the sequences f and g each end in infinitely
many zeroes, the same holds for f +g and −f and fg. We let the reader check this assertion
in the case of f + g and −f. To check the assertion for fg, fix N, M ∈N such that fn = 0
for all n > N and gm = 0 for all m > M. For any fixed k > N + M, the coefficient of xk in
fg is P
n+m=k fngm. Since k > N + M, the condition n + m = k forces n > N or m > M,
so fn = 0 or gm = 0, hence fngm = 0. Thus, the coefficient of xk is a sum of zeroes, so is 0.
We can now give a rigorous meaning to our original notation for polynomials as “symbolic
expressions” involving x. Define the formal variable :w x to be the particular sequence
(0, 1, 0, 0, . . .), which is both a polynomial in R[x] and a formal series in R[[x]]. To motivate
this definition, recall that we designed this coefficient sequence as the formal model for
the informal polynomial expression 0 + 1x1 + 0x2 + 0x3 + · · · . Iterating the definition of
multiplication, we can check by induction that for all i ≥0, xi (the product of i copies of
the sequence x in R[[x]]) is the sequence (ej : j ≥0), where ei = 1 and ej = 0 for all j ̸= i.
Next, we can identify any ring element a ∈R with the constant polynomial (a, 0, 0, . . .)
in R[[x]]. More precisely, the map f(a) = (a, 0, 0, . . .) is an injective ring homomorphism
of R into R[[x]]. We use this injection to regard R as a subset of R[[x]] (or R[x]), writing
a = (a, 0, 0, . . .). Using this convention, we can check that
a0 + a1x + a2x2 + · · · + anxn = (a0, a1, a2, . . . , an, 0, 0, . . .),
where the left side is built up by performing sum and product operations in the ring R[x].
Similarly, we can prove that the notation P∞
i=0 fixi for a formal power series really is an
infinite sum of terms, each of which is a product of fi and i copies of x. But in this case,
we need to introduce a limit concept for formal power series to define what the infinite sum
means. We omit a detailed discussion of this point.
Polynomials
39
For a field F, F[[x]] is an F-vector space and F-algebra with scalar multiplication
defined by c · (fi : i ≥0) = (cfi : i ≥0) for all c, fi ∈F. F[x] is a subspace and subalgebra
of F[[x]]. Similarly, for a ring R, R[[x]] is an R-module with submodule R[x]. Using the
explicit description of the sequences xi given in the previous paragraph, we can check that
the set B = {1, x, x2, . . . , xn, . . .} is an F-linearly independent subset of F[[x]]. However,
B is not a basis of F[[x]]. The reason is that the span of B consists of all finite F-linear
combinations of the powers of x, which are precisely the elements of F[x]. The formal series
(1F , 1F , 1F , . . .) = P∞
n=0 xn is an example of an element of F[[x]] outside the span of B. It
is true that B can be extended to a basis of F[[x]], but it does not seem possible to give an
explicit description of any specific basis of F[[x]].
3.4
Properties of Degree
In this section, we study how the algebraic operations in R[x] affect degree. Let R be
any ring, and suppose p, q are nonzero polynomials in R[x]. Write p = PN
i=0 aixi and
q = PM
j=0 bjxj where N = deg(p), M = deg(q), and ai, bj ∈R. By definition of addition,
we see that deg(p + q) = N if N > M, while deg(p + q) = M if M > N. On the other hand,
when N = M, it is possible for the leading terms to cancel. More precisely, when N = M,
we have: deg(p + q) = N if bN ̸= −aN; deg(p + q) < N if bN = −aN and p + q ̸= 0; and
deg(p + q) is undefined if p + q = 0. To summarize, we always have
deg(p + q) ≤max(deg(p), deg(q))
assuming that p, q, and p + q are nonzero; and strict inequality occurs iff deg(p) = deg(q)
and the leading terms of p and q cancel.
Next, consider the degree of the product pq, where deg(p) = N and deg(q) = M as
above. In our earlier proof that R[x] is a subring of R[[x]], we showed that the coefficient
of xk in pq is zero for all k > N + M. We use a similar argument to find the coefficient
of xN+M in pq. By definition, this coefficient is P
i+j=N+M aibj. One term in this sum is
aNbM. For all the other terms, either i > N and ai = 0, or j > M and bj = 0. Hence,
pq = aNbMxN+M + (terms involving lower powers of x).
We can deduce several consequences from this computation. First, for all nonzero p, q ∈
R[x], if the leading coefficients of p and q do not multiply to zero, then
pq ̸= 0 and deg(pq) = deg(p) + deg(q).
Let us call the displayed conclusion the Degree Addition Formula. Note that the Degree
Addition Formula always holds if R is an integral domain (which has no zero divisors) or a
field (which is a special kind of integral domain). In fact, R is an integral domain iff R[x]
is an integral domain. The forward implication follows from the Degree Addition Formula
(p, q ̸= 0 implies pq ̸= 0), and the converse follows since R is isomorphic to a subring of
R[x], namely the subring of constant polynomials.
On the other hand, we can check that for any ring R, R[x] is never a field. We now prove:
for an integral domain R, p ∈R[x] has a multiplicative inverse in R[x] iff deg(p) = 0 and p
is invertible viewed as an element of R. For the forward implication, assume p ∈R[x] has an
inverse q ∈R[x]. Applying the Degree Addition Formula to pq = 1 shows deg(p)+deg(q) =
deg(1) = 0. Since degrees are nonnegative integers, this forces deg(p) = deg(q) = 0. We also
40
Advanced Linear Algebra
see that q (being a constant polynomial) can be regarded as an inverse of p in the ring R.
For the converse implication, assume deg(p) = 0 (so p ∈R) and pq = 1 for some q ∈R.
The relation pq = 1 still holds when we view p and q as constant polynomials in R[x]. So p
is invertible in R[x]. As a special case of this result, for any field F, the invertible elements
of F[x] are precisely the nonzero constant polynomials.
Given a field F, we can use degree to define some finite-dimensional subspaces of the
F-vector space F[x]. Specifically, for each n ≥0, let Vn be the set consisting of the zero
polynomial and all p ∈F[x] of degree at most n. Each Vn is an (n+1)-dimensional subspace
of F[x] with ordered basis (1, x, x2, . . . , xn).
3.5
Evaluating Polynomials
From the outset, we have stressed that a formal polynomial is a sequence of coefficients,
not a function of an input value x. Nevertheless, we can use a formal polynomial to create
a polynomial function, as follows. Given a ring R and a polynomial p = Pn
i=0 aixi ∈R[x],
define the polynomial function fp : R →R associated with p by setting fp(c) = Pn
i=0 aici
for each c ∈R. Note that the expression defining fp(c) is a formula involving the addition
and multiplication operations in the given ring R. We call fp(c) the value of the polynomial
p at the input x = c. We often write p(c) instead of fp(c), although the latter notation is
more precise.
We must take care to distinguish the (formal) polynomial p from its associated
polynomial function fp. We reiterate that p is the sequence (a0, a1, . . . , an, 0, 0, . . .), while
fp is a certain function from R to R. This distinction may at first seem pedantic or trivial,
but it is essential. Indeed, it is quite possible for two unequal polynomials to produce the
same function from R to R. For example, let R = Z2 = {0, 1} be the ring of integers
modulo 2. There are only four distinct functions g : Z2 →Z2, but there are infinitely many
polynomials p ∈Z2[x] (which are sequences of zeroes and ones terminating in an infinite
string of zeroes). Thus, one of these four functions must arise from infinitely many distinct
polynomials. For an explicit example, consider the function g : Z2 →Z2 such that g(0) = 0
and g(1) = 1. For any i ≥1, the polynomial xi ∈Z2[x] has g as its associated polynomial
function, since fxi(0) = 0i = 0 = g(0) and fxi(1) = 1i = 1 = g(1). However, the polynomials
x1 = (0, 1, 0, 0, . . .), x2 = (0, 0, 1, 0, 0, . . .), etc., are all distinct elements of Z2[x].
We obtained the function fp from a given formal polynomial p = P
i≥0 aixi by regarding
x as an actual variable that ranges over the elements of the coefficient ring R. For further
study of polynomial rings, it is helpful to reverse this viewpoint, holding the value of x fixed
and letting p range over all polynomials in R[x]. More precisely, suppose we are given a
fixed ring element c ∈R. We define a map Ec : R[x] →R by setting Ec(p) = fp(c) = p(c)
for p ∈R[x]. The map Ec sends each polynomial in R[x] to the value of that polynomial
at the fixed input x = c. We call Ec evaluation at x = c. If R is a commutative ring and
c ∈R, then the map Ec is a ring homomorphism, meaning that (p + q)(c) = p(c) + q(c) and
(pq)(c) = p(c)q(c) for all p, q ∈R[x], and 1R[x](c) = 1R. The verification that Ec preserves
multiplication requires commutativity of R.
The fact that Ec is a ring homomorphism is a special case of the following more general
fact, called a universal mapping property (UMP). Universal mapping properties appear
throughout algebra and provide a unifying framework for understanding various algebraic
constructions. Chapter 19 gives a systematic introduction to universal mapping properties.
Polynomials
41
Universal Mapping Property (UMP) for Polynomial Rings. Suppose R and S are
commutative rings and h : R →S is a ring homomorphism. For every c ∈S, there exists
a unique ring homomorphism H : R[x] →S such that H(x) = c and H(r) = h(r) for all
r ∈R (we say H extends h).
Proof. We first prove the uniqueness of H. Suppose H : R[x] →S is a ring homomorphism
extending h such that H(x) = c. Let p = Pn
i=0 aixi be an arbitrary polynomial in R[x]. As
explained in §3.3, the displayed expression for p can be regarded as a sum in R[x] of certain
products of polynomials in R[x] (namely, ai times several copies of x). Since H is a ring
homomorphism, we must have
H(p) =
n
X
i=0
H(aixi) =
n
X
i=0
H(ai)H(x)i =
n
X
i=0
h(ai)ci.
Thus the value of H(p) is completely determined by h and c, proving uniqueness.
To prove existence, use the formula just displayed to define a function H : R[x] →S.
Identifying each a ∈R with the constant polynomial (a, 0, 0, . . .), we see that H(a) =
h(a)c0 = h(a) for all a ∈R, so that H does extend h. In particular, H(1R[x]) = h(1R) = 1S.
Recalling that x = (0, 1, 0, 0, . . .), we see similarly that H(x) = h(1R)c1 = c. All that remains
is to check that H preserves sums and products. Let p = P
i≥0 aixi and q = P
i≥0 bixi be
two polynomials in R[x], where ai, bi ∈R. Since p + q = P
i≥0(ai + bi)xi, the definition of
H gives
H(p+q) =
X
i≥0
h(ai +bi)ci =
X
i≥0
(h(ai)+h(bi))ci =
X
i≥0
h(ai)ci +
X
i≥0
h(bi)ci = H(p)+H(q).
Next, since pq = P
i≥0(P
u+v=i aubv)xi, we find that
H(pq)
=
X
i≥0
h
 X
u+v=i
aubv
!
ci =
X
i≥0
 X
u+v=i
h(au)h(bv)
!
ci
=
X
i≥0
X
u+v=i
h(au)cuh(bv)cv
(since S is commutative and cucv = ci)
=

X
u≥0
h(au)cu



X
v≥0
h(bv)cv


(using the distributive law in S)
=
H(p)H(q).
The evaluation map Ec is the special case of the map H in the UMP obtained by taking
S = R and h = idR. We can also apply the UMP in the situation where S is a commutative
ring containing R as a subring, and h : R →S is the inclusion map. In this case, for every
c ∈S, we obtain a ring homomorphism Hc : R[x] →S such that, for each p = P
i≥0 aixi in
R[x], Hc(p) = p(c) = P
i≥0 aici. The image of this ring homomorphism is written R[{c}] or
sometimes R[c]. This image is a subring of S, and it is the smallest subring of S containing
both R and c. You can check that this construction still works if, instead of assuming S is
commutative, we only assume rs = sr for all r ∈R and s ∈S.
42
Advanced Linear Algebra
3.6
Polynomial Division with Remainder
Our next topic is the theory of divisibility in polynomial rings. The cornerstone of this theory
is the following result, which precisely describes the process of polynomial long division with
remainder.
Division Theorem for Polynomials. Suppose f, g ∈F[x], where F is a field and g is
nonzero. There exist unique polynomials q, r ∈F[x] such that f = gq + r and either r = 0
or deg(r) < deg(g).
We call q the quotient and r the remainder when f is divided by g.
Proof. First we prove the existence of q and r. If f = 0, then we take q = r = 0. For
nonzero f, we proceed by strong induction on n = deg(f). The base case occurs when
n < deg(g); here we take q = 0 and r = f. For the induction step, fix a polynomial f
with deg(f) = n ≥deg(g), and assume that the existence assertion is already known for all
polynomials of smaller degree than f. Write
f = axn + (lower terms)
and
g = bxm + (lower terms),
where a and b are the leading coefficients of f and g, and n ≥m. Since b is a nonzero element
of the field F, its inverse b−1 exists in F. So we can define h = f −(ab−1xn−m)g, which is in
F[x]. The polynomial h is the difference of two polynomials, whose leading terms both equal
axn. It follows that h = 0 or deg(h) < n. If h = 0, then we take q = ab−1xn−m and r = 0.
Otherwise, the induction hypothesis is applicable to h and yields polynomials q0, r0 ∈F[x]
such that h = q0g + r0 and either r0 = 0 or deg(r0) < deg(g). Since f = h + (ab−1xn−m)g,
f = qg + r holds if we take q = q0 + ab−1xn−m and r = r0. Since r = r0, we have r = 0 or
deg(r) < deg(g).
We now prove that the quotient and remainder associated with a given f and g are
unique. Suppose f = q1g + r1 and f = q2g + r2 where: q1, q2, r1, r2 ∈F[x]; r1 = 0 or
deg(r1) < deg(g); and r2 = 0 or deg(r2) < deg(g). We need to prove that q1 = q2 and r1 =
r2. Equating the given expressions for f and rearranging, we find that (q1 −q2)g = r2 −r1.
The polynomial r2 −r1 is either zero or nonzero. In the case where r2 −r1 = 0, we deduce
r1 = r2 and (q1 −q2)g = 0. Since F is a field, F[x] is an integral domain. Then, since g ̸= 0,
it follows that q1 −q2 = 0 and q1 = q2. Thus the uniqueness result holds. In the other case,
where r2 −r1 ̸= 0, it follows that q1 −q2 ̸= 0. So we may compute degrees, obtaining
deg(r2 −r1) = deg((q1 −q2)g) = deg(q1 −q2) + deg(g) ≥deg(g).
This calculation uses the Degree Addition Formula in F[x], which is valid because the field
F is an integral domain. But the assumptions on r1 and r2 ensure that r2 −r1 has degree
strictly less than deg(g). This contradiction shows that the second case never occurs.
We close this section with two further remarks, one computational and one theoretical.
First, observe that the induction step in the existence proof can be unravelled into the
iterative algorithm for long division of polynomials learned in elementary algebra. For
example, suppose we are dividing f = 6x3 −7x2 + 4 by g = 3x + 1 in the ring Q[x].
The first step in the long division is to divide the leading terms of f and g, obtaining
6x3/3x = 2x2. This monomial corresponds to the expression ab−1xn−m in the proof above.
As prescribed in the proof, we now multiply all of g by this monomial, subtract the result
from f, and continue the division process. Usually the computations are presented in a
tabular form as shown in Figure 3.1. The process ends when the leftover part is either zero
Polynomials
43
2x2 −3x + 1
3x + 1

6x3 −7x2
+ 4
−6x3 −2x2
−9x2
9x2 + 3x
3x + 4
−3x −1
3
FIGURE 3.1
Example of polynomial division with remainder.
or has degree less than the degree of g. In this example, once the division is complete, we
obtain the quotient q = 2x2 −3x + 1 and the remainder r = 3.
The second remark concerns the possibility of extending the Division Theorem to
polynomial rings R[x] where R may not be a field. We restrict attention to commutative
rings R. We have italicized the steps in the proof above that used the assumption that F was
a field. We see that the uniqueness proof remains valid if R[x] is an integral domain, which
holds iff R itself is an integral domain. On the other hand, to make the existence proof work,
we needed the leading coefficient of g (called b in the proof) to be an invertible element of the
coefficient ring. We therefore obtain the following generalization of the Division Theorem.
Generalized Division Theorem for Polynomials. Given a commutative ring R and
polynomials f, g ∈R[x] such that g ̸= 0 and the leading coefficient of g is invertible in R,
there exist q, r ∈R[x] with f = qg + r and r = 0 or deg(r) < deg(g). If R is an integral
domain, then q and r are unique.
This theorem applies, in particular, when we divide an arbitrary polynomial f in Z[x] by
a monic polynomial g in Z[x]. The resulting quotient and remainder are uniquely determined
polynomials in Z[x].
3.7
Divisibility and Associates
For the rest of this chapter, we focus attention on polynomial rings F[x] where the ring F
of coefficients is a field. Given f, g ∈F[x], we say that g divides f (written g|f) iff there
exists q ∈F[x] with f = gq. We also say that g is a divisor of f, and that f is a multiple
of g in F[x]. For f = 0, taking q = 0 shows that g|0 for every g ∈F[x]. If f is nonzero and
g|f, applying the Degree Addition Formula to the equation f = gq shows that g is nonzero
and deg(g) ≤deg(f). For nonzero g, g divides f iff the remainder when we divide f by g
is zero. This observation provides an algorithm for deciding whether or not one polynomial
divides another.
Observe that f|f for all f ∈F[x], since f = f · 1. For all f, g, h ∈F[x], if f|g and g|h,
then f|h. To prove this, fix f, g, h ∈F[x] with f|g and g|h. There exist q, s ∈F[x] with
g = fq and h = gs. So h = gs = (fq)s = f(qs) with qs ∈F[x], hence f|h. Thus, divisibility
is a reflexive and transitive relation on F[x]. Suppose f|g and g|f; does it follow that f = g?
44
Advanced Linear Algebra
In general, the answer is no. The hypotheses f|g and g|f ensure that g = fs and f = gt for
some s, t ∈F[x], and therefore f1 = f = gt = f(st). Assuming f is nonzero, we can cancel
f in the integral domain F[x] to conclude that st = 1. By calculating degrees, we see that s
and t are nonzero elements of the field F, and t = s−1. Thus, f = gt is a scalar multiple of
g in the vector space F[x], but f need not equal g. On the other hand, if f and g are both
known to be monic, then f|g and g|f do imply f = g, since comparison of leading terms in
the equation f = gt forces t = 1.
In general, we can always multiply a polynomial in F[x] by a nonzero constant from
F (a polynomial of degree zero) without affecting divisibility properties. More precisely,
suppose f, g ∈F[x] and c ∈F where c ̸= 0. We can check that f|g iff (cf)|g iff f|(cg).
For any f, h ∈F[x], let us write f ∼h iff there exists a nonzero c ∈F with h = cf. In
this situation, we say that f and h are associates in the ring F[x]. We can verify that the
relation ∼is reflexive, symmetric, and transitive (symmetry requires the hypothesis that F
is a field), so ∼is an equivalence relation. Furthermore, the equivalence class of any nonzero
f (namely, the set of associates of f in F[x]) contains exactly one monic polynomial, which
can be found by dividing f by its leading coefficient.
Suppose f, p, q, a, b ∈F[x] are polynomials such that f|p and f|q. We claim that f|(ap+
bq). To prove this, write p = fc and q = fd for some c, d ∈F[x]. We compute ap + bq =
a(fc) + b(fd) = f(ac + bd). Since ac + bd ∈F[x], f|(ap + bq). It follows by induction that
for all n ≥1 and all f, p1, . . . , pn, a1, . . . , an ∈F[x], if f divides every pi, then f also divides
a1p1 + · · · + anpn. We call a1p1 + · · · + anpn an F[x]-linear combination of p1, . . . , pn, since
the coefficients ai are polynomials in F[x].
3.8
Greatest Common Divisors of Polynomials
Suppose f and g are polynomials in F[x] that are not both zero. A common divisor of f
and g is a polynomial p ∈F[x] such that p|f and p|g. A greatest common divisor (gcd) of
f and g is a common divisor of f and g whose degree is as large as possible. We claim that
gcds of f and g always exist, although they may not be unique. To see this, let CDiv(f, g)
denote the set of all common divisors of f and g in F[x]. This set is nonempty, since 1|f
and 1|g, and the set does not contain zero, since f ̸= 0 or g ̸= 0. If f ̸= 0, then every
common divisor has degree at most deg(f), and similarly if g ̸= 0. Thus, the set of degrees
of polynomials in CDiv(f, g) is a finite nonempty subset of Z>0, so this set has a greatest
element. Any polynomial in CDiv(f, g) having this maximum degree is a gcd of f and g.
The next theorem gives a more precise result.
Theorem on GCDs of Two Polynomials. Suppose F is a field and f, g ∈F[x] are not
both zero. There exists exactly one monic gcd of f and g in F[x], which is written gcd(f, g).
The set of all gcds of f and g consists of the associates of gcd(f, g) in F[x]. The set of
common divisors of f and g in F[x] equals the set of divisors of gcd(f, g) in F[x]. There
exist polynomials s, t ∈F[x] with gcd(f, g) = sf + tg.
The proof we give, called Euclid’s Algorithm, gives a specific computational procedure
for finding gcd(f, g) and polynomials s, t such that gcd(f, g) = sf + tg.
Proof. We begin with the following observation: if a, b, q, r ∈F[x], b ̸= 0, and a = bq + r,
then CDiv(a, b) = CDiv(b, r). To prove this, first assume p ∈F[x] satisfies p|a and p|b;
then p divides 1a + (−q)b = r. So CDiv(a, b) ⊆CDiv(b, r). To prove the reverse inclusion,
assume p ∈F[x] satisfies p|b and p|r. Then p divides qb + 1r = a. We have now proved
Polynomials
45
CDiv(a, b) = CDiv(b, r). We may conclude that when a = bq + r with b ̸= 0, h ∈F[x] is a
gcd of a and b iff h is a gcd of b and r.
We now describe the algorithm for computing a gcd of f and g. Switching f and g
if needed, we may assume that g ̸= 0. We construct two finite sequences of polynomials
r0, r1, r2, . . . , rN+1 and q1, q2, . . . , qN by repeatedly performing polynomial long division.
To start, let r0 = f and r1 = g ̸= 0. Divide r0 by r1 to obtain
r0 = q1r1 + r2,
where r2 = 0 or deg(r2) < deg(r1).
If r2 = 0, the construction ends with N = 1. If r2 ̸= 0, we continue by dividing r1 by r2,
obtaining
r1 = q2r2 + r3,
where r3 = 0 or deg(r3) < deg(r2).
If r3 = 0, the construction ends with N = 2. If r3 ̸= 0, we continue similarly. At step i,
assuming that ri is nonzero, we divide ri−1 by ri to obtain
ri−1 = qiri + ri+1,
where ri+1 = 0 or deg(ri+1) < deg(ri).
If ri+1 = 0, we end the construction with N = i. Otherwise, we proceed to step i + 1 of the
computation.
We claim first that the computation must end after a finite number of steps. We prove
this claim by a contradiction argument. If the algorithm never terminates, then ri is never
zero for any i, so we obtain an infinite decreasing sequence of degrees
deg(r1) > deg(r2) > deg(r3) > · · · > deg(ri) > · · · .
(3.1)
Then the set S = {deg(ri) : i ≥1} has no least element. But S is a nonempty subset of
the well-ordered set Z>0, so S must have a least element. This contradiction shows that the
algorithm must terminate. In fact, we see from (3.1) that N ≤deg(r1) + 1.
We now analyze the common divisors of f and g. By the observation preceding the
description of the algorithm, the relation ri−1 = qiri + ri+1 implies that CDiv(ri−1, ri) =
CDiv(ri, ri+1) for 1 ≤i ≤N. Accordingly,
CDiv(f, g) = CDiv(r0, r1) = CDiv(r1, r2) = · · · = CDiv(rN, rN+1) = CDiv(rN, 0).
Thus, the common divisors of f and g are the same as the common divisors of rN and 0.
Since everything divides 0, the common divisors of f and g are the same as the divisors
of rN. Now, the divisors of rN of maximum degree are rN and its associates. Therefore, a
polynomial h is a gcd of f and g iff h is an associate of rN. As pointed out earlier, rN has
a unique monic associate, which is gcd(f, g). Our argument has shown that every common
divisor of f and g divides rN, hence divides every associate of rN. In particular, all common
divisors of f and g divide gcd(f, g). Conversely, any divisor of gcd(f, g) is a common divisor
of f and g.
To finish the proof, we must find polynomials s, t ∈F[x] such that gcd(f, g) = sf + tg.
It suffices to construct polynomials si, ti ∈F[x] such that ri = sif + tig for 0 ≤i ≤N; we
can then find s and t by dividing uN and vN by the leading coefficient of rN. We use strong
induction on i. If i = 0, choose si = 1 and ti = 0 (recalling that r0 = f). If i = 1, choose
si = 0 and ti = 1 (recalling that r1 = g). For the induction step, assume 1 ≤i ≤N and
the polynomials si−1, ti−1, si, ti with the required properties have already been computed.
Then
ri+1
=
ri−1 −qiri
=
(si−1f + ti−1g) −qi(sif + tig)
=
(si−1 −qisi)f + (ti−1 −qiti)g.
46
Advanced Linear Algebra
Therefore, ri+1 = si+1f + ti+1g holds if we choose
si+1 = si−1 −qisi ∈F[x]
and
ti+1 = ti−1 −qiti ∈F[x].
These equations give an explicit recursive algorithm for computing s and t from f and g.
The theorem does not say that the polynomials s and t such that gcd(f, g) = sf + tg
are unique. Indeed, for any polynomial z ∈F[x], sf + tg = (s + gz)f + (t −fz)g, so that
there are many possible choices for s and t.
3.9
GCDs of Lists of Polynomials
We now extend the discussion of gcds to the case of more than two polynomials. Suppose
n ≥1 and f1, . . . , fn are polynomials in F[x]. There is no loss of generality in assuming that
all fi are nonzero. We call g ∈F[x] a common divisor of f1, . . . , fn iff g divides every fi; let
CDiv(f1, . . . , fn) be the set of these polynomials. A greatest common divisor of f1, . . . , fn
is a polynomial of maximum degree in CDiv(f1, . . . , fn). As in the case n = 2, greatest
common divisors of f1, . . . , fn exist, although they need not be unique. In the case n = 1,
CDiv(f1) is the set of all divisors of f1, the greatest common divisors of the collection {f1}
are the associates of f1, and we let gcd(f1) be the unique monic associate of f1.
Theorem on GCDs of n Polynomials. For all n ≥1 and all nonzero f1, . . . , fn ∈F[x],
there exists a unique monic greatest common divisor of f1, . . . , fn, written gcd(f1, . . . , fn).
The set of common divisors of f1, . . . , fn equals the set of divisors of gcd(f1, . . . , fn). There
exist polynomials s1, . . . , sn ∈F[x] such that gcd(f1, . . . , fn) = s1f1 + · · · + snfn. If n ≥2,
then
gcd(f1, . . . , fn−1, fn) = gcd(gcd(f1, . . . , fn−1), fn).
(3.2)
Proof. All assertions in the theorem are already known when n = 1 or n = 2. Proceeding
by induction, assume n > 2 and the theorem holds for all lists of n −1 nonzero polynomials
in F[x]. We first prove (3.2) as follows. For fixed h ∈F[x], the following conditions are
logically equivalent:
(a) h divides each of f1, . . . , fn−1, fn;
(b) h divides each of f1, . . . , fn−1, and h divides fn;
(c) h divides gcd(f1, . . . , fn−1), and h divides fn;
(d) h divides gcd(gcd(f1, . . . , fn−1), fn).
The equivalence of (b) and (c) follows from part of the induction hypothesis for lists of n−1
polynomials, and the equivalence of (c) and (d) follows similarly using the Theorem on GCDs
of Two Polynomials. From the equivalence of (a) and (d), we see that CDiv(f1, . . . , fn) is
exactly the set of polynomials dividing g = gcd(gcd(f1, . . . , fn−1), fn). It now follows from
the definition that the greatest common divisors of f1, . . . , fn consist of g and its associates.
In particular, g is the unique monic gcd of f1, . . . , fn.
To prove the remaining statement in the theorem, use the induction hypothesis to write
gcd(f1, . . . , fn−1) = t1f1 + · · · + tn−1fn−1
for some polynomials t1, . . . , tn−1 ∈F[x]. Setting g = gcd(gcd(f1, . . . , fn−1), fn) as above,
we can also write
g = c gcd(f1, . . . , fn−1) + dfn
Polynomials
47
for some polynomials c, d ∈F[x]. Combining these expressions, we see that
gcd(f1, . . . , fn) = g = (ct1)f1 + · · · + (ctn−1)fn−1 + dfn.
Thus, gcd(f1, . . . , fn) is expressible as an F[x]-linear combination of f1, . . . , fn.
Using Euclid’s Algorithm for computing the gcd of two polynomials, we may convert
the preceding proof into an algorithm that computes gcd(f1, . . . , fn) and the polynomials
s1, . . . , sn satisfying gcd(f1, . . . , fn) = Pn
i=1 sifi. The next section presents an alternative
algorithm to compute this information.
3.10
Matrix Reduction Algorithm for GCDs
Given polynomials f, g ∈F[x], we can compute d, a, b ∈F[x] with d = gcd(f, g) = af + bg
using a matrix reduction technique similar to the Gaussian elimination algorithm learned in
elementary linear algebra. We begin with the 2×3 matrix
 1
0
f
0
1
g

. We then perform a
sequence of elementary row operations (described below) to convert this matrix into a new
matrix of the form
 a
b
d
r
s
0

, where all entries are elements of F[x] and one entry in the
rightmost column is 0. We will show that d is a gcd of f and g, and d = af + bg.
The allowable row operations are as follows. First, we can multiply a row by a nonzero
constant in F. Second, we can switch the two rows. Third, for any polynomial z ∈F[x], we
can add z times row 1 to row 2, or add z times row 2 to row 1.
We illustrate the algorithm by computing gcd(x2 + 2x −3, x3 −2x2 + 2x −1) in Q[x].
We begin by adding −x times row 1 to row 2 to eliminate the x3 term:
 1
0
x2 + 2x −3
0
1
x3 −2x2 + 2x −1

−→

1
0
x2 + 2x −3
−x
1
−4x2 + 5x −1

.
Next, we add 4 times row 1 to row 2:

1
0
x2 + 2x −3
−x
1
−4x2 + 5x −1

−→

1
0
x2 + 2x −3
−x + 4
1
13x −13

.
Now multiply row 2 by 1/13, then add −x times row 2 to row 1:
−→

1
0
x2 + 2x −3
(−x + 4)/13
1/13
x −1

−→
 (1/13)x2 −(4/13)x + 1
−x/13
3x −3
(−x + 4)/13
1/13
x −1

.
Finally, add −3 times row 2 to row 1 to obtain the matrix
 (1/13)x2 −(1/13)x + (1/13)
(−x −3)/13
0
(−x + 4)/13
1/13
x −1

.
Switching rows 1 and 2, we conclude that the gcd is x −1, and
((−x + 4)/13) · (x2 + 2x −3) + (1/13) · (x3 −2x2 + 2x −1) = x −1.
Why does the matrix reduction algorithm work correctly? First, we show that the
algorithm always produces a zero in the rightmost column in finitely many steps if we
48
Advanced Linear Algebra
use appropriate row operations. Suppose the current matrix looks like
 a
b
c
r
s
t

. If c
and t are both nonzero, we may proceed by induction on deg(c) + deg(t). As in the proof
of the Division Theorem for Polynomials, if deg(c) ≤deg(t), we can add an appropriate
polynomial multiple of row 1 to row 2 to lower the degree of t. Similarly, if deg(c) > deg(t),
we can add a multiple of row 2 to row 1 to lower the degree of c. In either case, the
nonnegative integer deg(c) + deg(t) strictly decreases as a result of the row operation (or
one of c or t becomes zero, causing termination). Since there is no infinite strictly decreasing
sequence of nonnegative integers, the algorithm does terminate in finitely many steps.
To see why the algorithm gives correct results, we prove the following statement
by induction on the number of row operations performed: if the current matrix is
A =
 a
b
c
r
s
t

, then af + bg = c, rf + sg = t, and there exist u, v, w, y ∈F[x] with
f = uc+vt and g = wc+yt. When the algorithm starts, we have a = s = 1, b = r = 0, c = f,
and t = g. So the given statement holds, taking u = y = 1 and v = w = 0. For the induction
step, assume the statement holds for the matrix A shown above. Suppose we multiply
row 1 by a nonzero constant k ∈F, producing the new matrix B =
 ka
kb
kc
r
s
t

. Then
rf+sg = t still holds; (ka)f+(kb)g = (kc) follows from af+bg = c; and f = (uk−1)(kc)+vt,
g = (wk−1)(kc)+yt. We argue similarly if row 2 is multiplied by k. Now suppose we modify
the matrix A by adding z times row 2 to row 1, for some z ∈F[x]. The new matrix is
C =
 a + zr
b + zs
c + zt
r
s
t

. Adding z times rf + sg = t to af + bg = c, we deduce
(a + zr)f + (b + zs)g = c + zt, and the equation rf + sg = t is still true. Furthermore, given
that f = uc+vt and g = wc+yt, we have f = u(c+zt)+(v−uz)t and g = w(c+zt)+(y−wz)t
where u, v −uz, w, y −wz ∈F[x]. Thus, the given statement holds for the new matrix C.
We argue similarly in the case where we add z times row 1 to row 2. It is routine to check
that the given statement remains true if we interchange the two rows of A.
Now consider what happens at the end of the algorithm, when one of the entries in the
rightmost column of A (say t) becomes zero. Since af + bg = c, we see that any common
divisor of f and g must also divide c. On the other hand, since f = uc + v0 = uc and
g = wc + y0 = wc, c is a common divisor of f and g. Thus, c must be a gcd of f and g. We
can use one more row operation to make c monic.
We can generalize the matrix reduction algorithm to take given nonzero inputs f1, . . . , fn
in F[x] (where n ≥1) and compute outputs d, s1, . . . , sn in F[x] with d = gcd(f1, . . . , fn) =
s1f1 + · · · + snfn. We start with an n × n identity matrix, augmented by an extra column
with entries f1, . . . , fn. Performing elementary row operations, we reduce this matrix to a
new matrix with only one nonzero entry d in the last column. Then d is a gcd of f1, . . . , fn,
and the remaining entries in the row containing d give coefficients si expressing d as a
F[x]-linear combination of the f1, . . . , fn. We ask the reader to provide a detailed proof of
this assertion (Exercise 50), which is similar to the proof given above for n = 2.
The matrix reduction algorithm can also be adapted to the computation of gcds of two
or more integers. In this case, we are only allowed to multiply a particular row by 1 or −1,
but we can add any integer multiple of a row to a different row. To prove termination, we
can use induction on the sum of the absolute values of the entries in the rightmost column.
Examples and proofs for the integer version of this algorithm appear in the exercises.
Polynomials
49
3.11
Roots of Polynomials
Let F be a field. Given p ∈F[x] and c ∈F, we call c a root or zero of p iff p(c) = fp(c) = 0.
We now show that c is a root of p iff (x −c) divides p in the ring F[x]. To see this, fix
p ∈F[x] and c ∈F. Dividing p by x −c gives p = (x −c)q + r for some q, r ∈F[x]
with r = 0 or deg(r) < deg(x −c) = 1. This means that the remainder r is a constant
polynomial (possibly zero). Evaluating both sides of p = (x −c)q + r at x = c shows that
p(c) = (c −c)q(c) + r = r. Now, c is a root of p iff p(c) = 0 iff the remainder r is 0 iff
(x −c)|p in F[x].
More generally, if K is a field with subfield F, p ∈F[x], and c ∈K, then c is called a
root of p iff p(c) = 0. If c ̸∈F, the previous theorem does not apply directly, since x −c
is not an element of F[x]. However, since F ⊆K, we can regard p as an element of the
polynomial ring K[x]. Then c is a root of p in K iff (x −c)|p in the ring K[x].
We can now establish a bound on the number of roots of a polynomial.
Theorem on Number of Roots of Polynomials. Suppose F is a field and p ∈F[x] has
degree n > 0. Then p has at most n roots in F.
Proof. We use induction on n. For n = 1, write p = ax + b for some a, b ∈F with a ̸= 0.
We can check that x = −a−1b is the unique root of this degree 1 polynomial. Now suppose
n > 1 and the result is known for polynomials of degree n−1. Given p of degree n, consider
two cases. If p has no root in F, then the result certainly holds. Otherwise, let c be a root
of p in F. Then we can write p = (x −c)q for some polynomial q of degree n −1. If d ∈F is
any root of p different from c, then 0 = p(d) = (d −c)q(d). Since d −c ̸= 0 and F is a field,
we see that q(d) = 0. This means that every root of p besides c is a root of q; conversely,
every root of q is also a root of p. Now, by the induction hypothesis, q has at most n −1
roots in F (possibly including c). Thus, p has at most n roots in F, namely all the roots of
q together with c.
The preceding theorem also holds for polynomials p in R[x], where R is an integral
domain. But the theorem can fail dramatically for general commutative rings R. For
instance, the polynomial p = x2 + 7 ∈Z8[x] has four roots (namely 1, 3, 5, and 7) in
the ring R = Z8. Since 2 · 4 = 0 in Z8, this ring is neither an integral domain nor a field.
Next, let F be any field, and consider the question of finding the roots of a given p ∈F[x]
in the field F. If F is a finite field, we can find the roots of p (if any) by evaluating p at
all the elements of F and seeing when the answer is zero. Now let F = Q be the field
of rational numbers. Given a non-constant p ∈Q[x], we can replace p by an associate
cp (for some nonzero c ∈Q) such that cp is in Z[x], and cp has the same rational roots
as p. To accomplish this, we could pick c to be the product (or least common multiple)
of all denominators appearing in coefficients of p. The next theorem gives a finite list of
possibilities for the rational roots of a polynomial in Z[x].
Rational Root Theorem. Let f = anxn + an−1xn−1 + · · · + a1x + a0 be in Z[x], where
a0, a1, . . . , an are integers and an ̸= 0. Every rational root of f must have the form r/s for
some integers r, s such that r divides a0 and s divides an.
Proof. Assume r, s are relatively prime integers such that r/s is a root of f. Evaluating f
at this root gives the equation
0 = an(rn/sn) + an−1(rn−1/sn−1) + · · · + a1(r1/s1) + a0.
50
Advanced Linear Algebra
Now multiply both sides by the integer sn to remove the fractions, giving
0 = anrn + an−1rn−1s1 + · · · + a1r1sn−1 + a0sn.
Isolating a0sn on one side, we obtain
a0sn = −(anrn + an−1rn−1s1 + · · · + a1r1sn−1).
The right side is a multiple of r in Z, so r divides a0sn in Z. As r and s are relatively
prime, we deduce that r divides a0. (This can be shown, for instance, by considering prime
factorizations; see Exercise 37.) Similarly, isolating anrn in the earlier equation shows that
s divides anrn, so s divides an because gcd(r, s) = 1.
To illustrate the Rational Root Theorem, consider the polynomial p = x4 + (13/3)x3 +
(22/3)x2+6x+(4/3) in Q[x]. Taking c = 3, we have cp = 3x4+13x3+22x2+18x+4 ∈Z[x].
We find a4 = 3 and a0 = 4, so the possible rational roots of p are ±1, ±1/3, ±2, ±2/3, ±4,
and ±4/3. Evaluating p at each of these points, we find that p has rational roots −1/3 and
−2. We can then factor p = (x + 1/3)(x + 2)(x2 + 2x + 2) in Q[x]. As another example, let
q = x2 −3 ∈Q[x]. By the Rational Root Theorem, the only possible roots of q in Q are ±1
and ±3. Since none of these four numbers is a root of q, we deduce that
√
3 is not rational.
For a generalization of this example, see Exercise 80.
3.12
Irreducible Polynomials
Prime numbers and factorizations of numbers into products of primes play a pivotal role
in the theory of integers. Our next goal is to develop analogous concepts for the theory of
polynomials.
Let F be a field and p ∈F[x] have positive degree. We call p an irreducible polynomial
over F iff whenever p = fg for some polynomials f and g in F[x], either f or g has degree
zero. In this case, if p has degree n > 0 and f (say) has degree zero, then g has degree n
and is an associate of p. Conversely, for any nonzero scalar c ∈F, we have the factorization
p = c(c−1p) where c−1p is an associate of p. It follows that the divisors of an irreducible
polynomial p are precisely the nonzero constant polynomials and the associates of p. We
can check that an associate of p is irreducible iff p is irreducible.
Negating the definition of irreducibility, we see that p is reducible in F[x] iff there exists
a proper factorization p = fg, meaning that neither f nor g is constant. Computing degrees,
we see that we must have 0 < deg(f) < deg(p) and 0 < deg(g) < deg(p) in this situation.
This leads to a criterion for detecting irreducibility of polynomials p of degree at most 3.
First, every polynomial of degree 1 is irreducible, since otherwise deg(f) would be an integer
strictly between 0 and 1. Second, if p ∈F[x] is such that deg(p) > 1 and p(c) = 0 for some
c ∈F, then (x −c)|p in F[x] and hence p is reducible. Conversely, if deg(p) is 2 or 3 and
p is reducible, then one of the factors in a proper factorization p = fg must have degree 1.
Moving a constant from f to g if needed, we can assume p has a monic factor x −c, and
then c ∈F is a root of p. Thus, a polynomial p of degree 2 or 3 is irreducible in F[x] if and
only if p has no root in F. An irreducible polynomial of degree at least 4 has no root in F,
but the converse does not hold in general. For example, p = (x2 −3)2 is reducible in Q[x],
yet p has no root in Q.
Note that the field of coefficients F must be considered when determining irreducibility,
since we only allow factorizations p = fg where the factors f and g belong to F[x]. For
Polynomials
51
example, consider the polynomial p = x2 −2. Applying the criterion in the last paragraph,
we see that p is irreducible in Q[x] (since there is no rational number c with c2 = 2). But
p is reducible in R[x], with factorization x2 −2 = (x −
√
2)(x +
√
2). Similarly, x2 + 1 is
irreducible in Q[x] and in R[x], but becomes reducible in the ring C[x], with factorization
x2 + 1 = (x −i)(x + i).
For some fields F, we can give specific algorithms for testing the irreducibility of
polynomials p ∈F[x]. For example, if F is any finite field, such as Zq for prime q, then a
given p ∈F[x] has only a finite number of possible divisors. We can use polynomial long
division to test each non-constant f ∈F[x] of degree less than n = deg(p) to see if f|p; p
is irreducible in F[x] iff no such divisor f exists. In fact, we can use the Degree Addition
Formula to verify that if p = fg is reducible, then one of its factors must have degree at most
⌊n/2⌋, so we need only test divisors up to this degree. The same process gives a factoring
algorithm to find all possible divisors of a reducible f ∈F[x].
Irreducibility over infinite fields is harder to check, with a few exceptions. For the field
C, the Fundamental Theorem of Algebra asserts that every non-constant p ∈C[x] has a
root in C. We omit the proof of this theorem. Combining this theorem with our earlier
observations relating roots to irreducibility, we see that p ∈C[x] is irreducible in C[x] iff
deg(p) = 1. With a little more work, we can also deduce from the Fundamental Theorem
of Algebra that p ∈R[x] is irreducible in R[x] iff deg(p) = 1 or p = ax2 + bx + c for some
a, b, c ∈R with a ̸= 0 and b2 −4ac < 0. Later in this chapter, we give an algorithm for
testing irreducibility of polynomials in Q[x].
Let us return to the case of a general field F. We now establish a crucial property of
irreducible polynomials, which is an analog of a corresponding property of prime integers: for
all p, r, s ∈F[x], if p is irreducible and p|rs, then p|r or p|s. To prove this, fix p, r, s ∈F[x]
with p irreducible, and assume p|rs but p does not divide r. Since p|rs, we may write
rs = pt for some t ∈F[x]. What is gcd(p, r)? This gcd must be a divisor of the irreducible
polynomial p, so it is either 1 or the unique monic associate of p. The latter possibility is
ruled out since p does not divide r, so gcd(p, r) = 1. We may therefore write 1 = ap + br for
some a, b ∈F[x]. Multiplying by s gives s = (ap+br)s = aps+brs = aps+bpt = p(as+bt).
We see from this that p|s. More generally, an induction argument using the result just
proved shows that if an irreducible polynomial p ∈F[x] divides a product f1f2 · · · fn, then
p divides fi for some i. Conversely, if p ∈F[x] is any non-constant polynomial such that
for all r, s ∈F[x], p|rs implies p|r or p|s, then p must be irreducible in F[x]. To see this,
suppose p is reducible, with proper factorization p = fg for some f, g ∈F[x]. Then p|fg (as
p divides itself), but p cannot divide f or g since these polynomials have degree less than p.
3.13
Unique Factorization of Polynomials
We can now prove the following fundamental result on factorizations of polynomials with
coefficients in a field.
Unique Factorization Theorem for Polynomials. Let F be a field. Every non-constant
polynomial g ∈F[x] can be written in the form g = cp1p2 · · · pr, where c ∈F is a nonzero
constant and each pi is a monic irreducible polynomial in F[x]. This factorization is unique
except for reordering p1, p2, . . . , pr.
In more detail, the uniqueness assertion means: if g = dq1q2 · · · qs, where d ∈F is
constant and each qj is a monic irreducible polynomial in F[x], then c = d, r = s, and
(after reordering factors if needed) pi = qi for 1 ≤i ≤r.
52
Advanced Linear Algebra
Proof. To prove the existence of the asserted factorization for g, we use strong induction
on deg(g). Suppose n = deg(g) ≥1 and the result is already known for all non-constant
polynomials of degree less than n. If g happens to be irreducible (which always occurs when
n = 1), then we have the factorization g = c(c−1g) where c ∈F is the leading coefficient of g.
If g is reducible, then we may write g = fh, where f, h ∈F[x] satisfy 0 < deg(f) < deg(g)
and 0 < deg(h) < deg(g). The induction hypothesis tells us that f and h are constants
times products of monic irreducible polynomials. Multiplying these expressions, we obtain
the needed factorization for g by combining the two constants.
To prove uniqueness, we also use strong induction on the degree of g. Suppose g =
cp1p2 · · · pr = dq1q2 · · · qs where c, d ∈F and each pi and qj is monic and irreducible in F[x].
Assume the uniqueness result is already known to hold for factorizations of polynomials of
degree less than deg(g). Since the pi and qj are all monic, comparison of leading coefficients
immediately gives c = d. Multiplying by c−1 gives p1p2 · · · pr = q1q2 · · · qs. We deduce from
this equality that pr divides the product q1q2 · · · qs. Since pr is irreducible, pr must divide
some qj. By reordering q1, . . . , qs, we may assume that pr divides qs. Since qs is irreducible
and pr is non-constant, pr must be an associate of qs. But pr and qs are both monic, so
pr = qs is forced. Since F[x] is an integral domain, we can cancel this nonzero factor from
p1p2 · · · pr = q1q2 · · · qs to get p1p2 · · · pr−1 = q1q2 · · · qs−1. If r = 1, then the empty product
on the left side is 1, which forces s = 1 = r. Similarly, s = 1 forces r = 1 = s. In the
remaining case, we have a nonconstant polynomial h = p1p2 · · · pr−1 = q1q2 · · · qs−1. Since
deg(h) < deg(g), the induction hypothesis is applicable to these two factorizations of h. We
conclude that r −1 = s −1 and (after reordering) pi = qi for 1 ≤i ≤r −1. Therefore,
r = s, pi = qi for 1 ≤i ≤r −1, and pr = qs = qr. This completes the induction.
We can rephrase the preceding result in the following way. Given a field F, let {pi : i ∈I}
be the collection of all monic irreducible polynomials in F[x], where I is some indexing set.
Every non-constant f ∈F[x] can be written uniquely in the form
f = c
Y
i∈I
pei
i ,
(3.3)
where c is a nonzero element of F, the ei are nonnegative integers, and all but a finite
number of ei are zero. This expression for f is obtained from the previous theorem by
writing f = cq1q2 · · · qs (where the qj are monic and irreducible in F[x]) and then letting
ei be the number of j with qj = pi. Uniqueness of the ei follows from the uniqueness of the
factorization of f, because reordering the qj does not affect the exponents ei. If we allow
all ei to be zero, we see that every nonzero polynomial (including constant polynomials)
can be written uniquely in the form (3.3). The zero polynomial can also be written in this
form if we allow c = 0, but the ei are not unique in this case (typically we would take all ei
to be zero). The expression (3.3) is often called the prime factorization of the polynomial
f. When needed, we may write c = cf and ei = ei(f) to indicate the dependence of these
quantities on f.
3.14
Prime Factorizations and Divisibility
Now that unique prime factorizations of polynomials are available, we can approach the
theory of divisibility and polynomial gcds from a new viewpoint. Let F be a field and f, g
be nonzero polynomials in F[x]. Assume first that g|f in F[x], so that f = gh for some
Polynomials
53
polynomial h ∈F[x]. Writing the prime factorizations (3.3) of f, g, and h, the relation
f = gh becomes
cf
Y
i∈I
pei(f)
i
=
 
cg
Y
i∈I
pei(g)
i
!
·
 
ch
Y
i∈I
pei(h)
i
!
= (cgch)
Y
i∈I
pei(g)+ei(h)
i
,
where cf, cg, ch ∈F and ei(f), ei(g), ei(h) ∈Z≥0. By uniqueness of the prime factorization
of f, it follows that cf = cgch and ei(f) = ei(g) + ei(h) for all i ∈I. Since ei(h) ≥0, we see
that ei(g) ≤ei(f) for all i ∈I. Conversely, if ei(g) ≤ei(f) for all i ∈I, then g|f since
f = g · (cf/cg)
Y
i∈I
pei(f)−ei(g)
i
.
So: for all nonzero f, g ∈F[x], g divides f in F[x] iff ei(g) ≤ei(f) for all i ∈I.
Next, consider nonzero polynomials f1, . . . , fn, g ∈F[x]. By the preceding paragraph,
we see that g is a common divisor of f1, . . . , fn iff ei(g) ≤ei(fj) for all i ∈I and all j ∈[n]
iff ei(g) ≤min(ei(f1), . . . , ei(fn)) for all i ∈I. To obtain a common divisor g of the largest
possible degree, we need to take ei(g) = min(ei(f1), . . . , ei(fn)) for all i ∈I. This means
that
gcd(f1, . . . , fn) =
Y
i∈I
pmin(ei(f1),...,ei(fn))
i
.
The previous discussion also gives a new proof that the set of common divisors of f1, . . . , fn
equals the set of divisors of gcd(f1, . . . , fn).
A common multiple of f1, . . . , fn ∈F[x] is a polynomial h ∈F[x] such that every fj
divides h. A least common multiple of f1, . . . , fn is a common multiple of f1, . . . , fn having
minimum degree. Note that h is a common multiple of f1, . . . , fn iff ei(fj) ≤ei(h) for all
j ∈[n] and all i ∈I iff max(ei(f1), . . . , ei(fn)) ≤ei(h) for all i ∈I. To keep the degree
of h as small as possible, we need to take ei(h) = max(ei(f1), . . . , ei(fn)) for all i ∈I. It
is now evident that the least common multiples of f1, . . . , fn are precisely the associates of
the monic polynomial
lcm(f1, . . . , fn) =
Y
i∈I
pmax(ei(f1),...,ei(fn))
i
.
Furthermore,
the
common
multiples
of
f1, . . . , fn
are
precisely
the
multiples
of
lcm(f1, . . . , fn). In the case n = 2, we have max(ei(f1), ei(f2)) + min(ei(f1), ei(f2)) =
ei(f1) + ei(f2) for all i ∈I. Comparing prime factorizations, we deduce the identity
lcm(f1, f2) gcd(f1, f2) = cf1f2,
where c ∈F is chosen to make the right side monic.
3.15
Irreducible Polynomials in Q[x]
Let us now return to the question of how to decide whether a specific polynomial f ∈Q[x]
is irreducible, where Q is the field of rational numbers. We remarked earlier that f is
irreducible iff any associate of f is irreducible. By replacing f by one of its associates, we
can assume that f belongs to Z[x], meaning that all the coefficients of f are integers. For
example, we can accomplish this by multiplying the original f by the product of all the
54
Advanced Linear Algebra
denominators of the coefficients appearing in f. For the rest of our discussion, assume f is
in Z[x]. We seek a method for deciding if f is irreducible in Q[x].
Recall that all degree 1 polynomials are irreducible. Also, for f of degree 2 or 3, we know
f is irreducible in Q[x] iff f has a rational root. For f ∈Z[x] of arbitrary degree, we can
find all rational roots of f (if any) by checking the finitely many rational numbers of the
form described in the Rational Root Theorem (§3.11).
To continue, we need the following technical lemma. Assume f ∈Z[x] has degree n > 0
and is reducible in Q[x]. Then there exist g, h in Z[x] with f = gh and deg(g), deg(h) < n.
Note that the conclusion would follow immediately from the definition of reducible
polynomials if we allowed g, h to come from Q[x], but we are demanding the stronger
conclusion that g and h belong to Z[x]. Obtaining this stronger result is surprisingly tricky,
so we proceed in several steps.
Step 1: We show that for all prime integers p and all g, h ∈Z[x], if p divides gh in the
ring Z[x], then p|g or p|h in the ring Z[x]. We prove the contrapositive. First, note that p|g
in Z[x] iff g = pq for some q ∈Z[x] iff every coefficient in g is divisible (in Z) by p. Now, fix
a prime p ∈Z and g, h ∈Z[x], and assume p does not divide g and p does not divide h in
Z[x]. We must prove that p does not divide gh in Z[x]. Write g = P
i≥0 gixi, h = P
j≥0 hjxj
for some gi, hj ∈Z. By our assumption on g and h, at least one gi and at least one hj are
not divisible (in Z) by p. Let r be maximal such that p does not divide gr, and let s be
maximal such that p does not divide hs. Now, observe that the coefficient of xr+s in gh is
grhs +
X
i,j: i+j=r+s, i>r
gihj +
X
i,j: i+j=r+s, j>s
gihj.
By maximality of r, each term in the first sum is an integer multiple of p. By maximality
of s, each term in the second sum is an integer multiple of p. But, since p is prime and
neither gr nor hs is a multiple of p, grhs is not an integer multiple of p. We conclude that
the coefficient of xr+s in gh is not a multiple of p, and hence p does not divide gh in Z[x].
Step 2: We show that if f ∈Z[x] has degree n > 0 and is reducible in Q[x], then there
exist g, h ∈Z[x] and d ∈Z>0 with df = gh and deg(g), deg(h) < n. By reducibility of f in
Q[x], there exist u, v ∈Q[x] with f = uv and deg(u), deg(v) < n. The idea is to remove the
denominators in u and v. Say u = Ps
i=0(ai/bi)xi and v = Pt
j=0(cj/dj)xj for some s, t ∈
Z≥0, ai, cj ∈Z, and bi, dj ∈Z>0. Choose d = b0b1 · · · bsd0d1 · · · dt ∈Z>0, g = (b0 · · · bs)u,
and h = (d0 · · · dt)v. We see that g, h ∈Z[x], df = gh, and deg(g), deg(h) < n.
Step 3: Among all possible choices of d, g, h satisfying the conclusion of Step 2, choose
one with d minimal (which can be done, by the Well-Ordering Axiom for Z>0). We finish
the proof of the lemma by showing d = 1. To get a contradiction, assume that d > 1. Then
some prime integer p divides d in Z, say d = pd1 for some d1 ∈Z>0. Now, p divides df in
Z[x], and df = gh, so p|gh in Z[x]. By Step 1, p|g or p|h in Z[x]. Say p|g (the other case
is similar). Then g = pg∗for some g∗∈Z[x]. Now df = gh becomes pd1f = pg∗h. As Z[x]
is an integral domain, we can cancel p to obtain d1f = g∗h. Now d1, g∗, and h satisfy the
conclusion of Step 2. But since d1 < d, this contradicts the minimality of d. The proof is
now complete.
3.16
Testing Irreducibility in Q[x] via Reduction Modulo a Prime
Using the lemma from the last section, we can prove two theorems for testing irreducibility
of certain polynomials in Q[x]. The first theorem involves reducing a polynomial modulo
p, where p is a prime integer. Given f = P
i≥0 fixi ∈Z[x] and a prime p ∈Z, define
Polynomials
55
νp(f) = P
i≥0(fi mod p)xi ∈Zp[x]; we call νp(f) the reduction of f modulo p. Using the
definitions of the algebraic operations on polynomials, you can check that νp : Z[x] →Zp[x]
is a ring homomorphism. In particular, νp(gh) = νp(g)νp(h) for all g, h ∈Z[x].
Testing Irreducibility in Q[x] via Reduction Modulo a Prime. Suppose f ∈Z[x]
has degree n > 0 and p ∈Z is a prime not dividing the leading coefficient of f. If νp(f) is
irreducible in Zp[x], then f is irreducible in Q[x].
Proof. We prove the contrapositive. Fix f ∈Z[x] of degree n > 0 and a prime p not dividing
the leading coefficient of f, and assume f is reducible in Q[x]. By the lemma of §3.15, we
can write f = gh for some g, h in Z[x] with deg(g), deg(h) < n. Reducing f = gh modulo
p gives νp(f) = νp(g)νp(h) in Zp[x]. By hypothesis, the leading coefficient of f does not
become zero when we pass to νp(f) by reducing coefficients mod p. So νp(f) still has degree
n in Zp[x], and νp(g), νp(h) have lower degree than n. So νp(f) is reducible in Zp[x].
For example, consider f = x4 + 6x2 + 3x + 7 ∈Z[x]. Reducing mod 2 gives ν2(f) =
x4 + x + 1 ∈Z2[x]. The irreducible polynomials of degree at most 2 in Z2[x] are x, x+1, and
x2 +x+1. None of these divide x4 +x+1 in Z2[x], so the latter polynomial is irreducible in
Z2[x]. As f is monic, our criterion applies to show f is irreducible in Q[x]. Similarly, we can
check that x5 −4x + 2 is irreducible in Q[x] by verifying that its reduction mod 3, namely
x5+2x+2, is irreducible in Z3[x]. The latter fact follows by exhaustively checking the finitely
many possible factors of degree 2 or less. On the other hand, note that ν2(x5 −4x+2) = x5
is reducible in Z2[x]. This shows that for a fixed choice of p, the converse of the irreducibility
criterion need not hold. In fact, there exist monic polynomials f ∈Z[x] that are irreducible
in Q[x], and yet νp(f) is reducible in Zp[x] for every prime integer p. It can be shown that
x4 −10x2 + 1 has this property (see [52, Example 26, p. 66]).
3.17
Eisenstein’s Irreducibility Criterion for Q[x]
Here is a second method for testing irreducibility in Q[x].
Eisenstein’s Irreducibility Criterion for Q[x]. Suppose f = a0+a1x+· · ·+anxn ∈Z[x]
with n > 0 and an ̸= 0. If some prime p ∈Z divides ai for 0 ≤i < n, but p does not divide
an and p2 does not divide a0, then f is irreducible in Q[x].
Proof. The proof is by contradiction. Assume f and p satisfy the given hypotheses, but
f is reducible in Q[x]. By the lemma of §3.15, there exist g, h in Z[x] with f = gh and
deg(g), deg(h) < n. Reducing f = gh modulo p, we get νp(f) = νp(g)νp(h). Let b =
an mod p. By hypothesis, b is nonzero, but all other coefficients of f reduce to 0 mod p. So
νp(f) = bxn ∈Zp[x] with b ̸= 0. Since p is prime, we know Zp is a field, and therefore we
can apply the Unique Factorization Theorem for Polynomials (§3.13) in the ring Zp[x]. As
x is evidently irreducible in Zp[x], the only way that νp(f) = bxn can factor into a product
νp(g)νp(h) is to have νp(g) = cxi and νp(h) = dxj for some c, d ∈Zp with cd = b and some
integers i, j with i+j = n and 0 < i, j < n. For the reductions of g and h to have this form,
we must have
g = c′xi + · · · + ps,
h = d′xj + · · · + pt
for some c′, d′, s, t ∈Z, where c′ mod p = c, d′ mod p = d, and all coefficients not shown
are integer multiples of p. Then the constant term of f = gh is a0 = p2st, contradicting the
assumption that p2 does not divide a0.
56
Advanced Linear Algebra
For example, Eisenstein’s Criterion applies (with p = 2) to prove once again that
x5 −4x + 2 is irreducible in Q[x]. We can see that x10 −2100 is irreducible in Q[x] by
applying Eisenstein’s Criterion with p = 3 or p = 7, but not with p = 2 or p = 5. More
generally, for all n ≥1, a ∈Z, and prime p ∈Z such that p divides a but p2 does not divide
a, Eisenstein’s Criterion shows that xn −a is irreducible in Q[x]. With some extra work
(see Exercise 83), Eisenstein’s Criterion can be used to show that for every prime p ∈Z,
the polynomial xp−1 + xp−2 + · · · + x2 + x + 1 is irreducible in Q[x].
3.18
Lagrange’s Interpolation Formula
In linear algebra and other settings, we often need to construct a polynomial that takes
prescribed output values at a given list of input values. Lagrange’s Interpolation Formula
gives an explicit expression for such a polynomial.
Lagrange’s Interpolation Formula. Given a field F, an integer n ≥0, distinct elements
a0, a1, . . . , an ∈F, and (not necessarily distinct) b0, b1, . . . , bn ∈F, there exists a unique
polynomial g ∈F[x] such that g(ai) = bi for 0 ≤i ≤n, and either g = 0 or deg(g) ≤n.
Proof. First we prove existence. For each i between 0 and n, let
pi =
n
Y
j=0
j̸=i
(x −aj)
, n
Y
j=0
j̸=i
(ai −aj)
Each pi is a polynomial of degree n in F[x] satisfying pi(ai) = 1F and pi(aj) = 0F for
all j ̸= i. Note that the denominator in pi is a nonzero element of F, hence is invertible,
because a0, . . . , an are distinct. Define g = Pn
i=0 bipi. Then g is in F[x], deg(g) ≤n or
g = 0, and g(ai) = bi for 0 ≤i ≤n.
To prove uniqueness of g, suppose h ∈F[x] also satisfies h(ai) = bi for 0 ≤i ≤n and
either h = 0 or deg(h) ≤n. Then f = g −h is a polynomial in F[x] with f(ai) = 0 for
0 ≤i ≤n. This f has degree at most n or is the zero polynomial. But f has at least n + 1
distinct roots a0, a1, . . . , an in F. By the Theorem on Number of Roots of Polynomials,
deg(f) ≤n is impossible. So f must be 0, which means g = h.
3.19
Kronecker’s Algorithm for Factoring in Q[x]
This section describes Kronecker’s Algorithm, which takes as input an arbitrary polynomial
f ∈Q[x] and returns as output the factorization of f into irreducible factors in Q[x]. In
particular, we can use this algorithm to test if a given f ∈Q[x] is irreducible in Q[x].
By clearing denominators, we can assume the input f is a polynomial of degree n > 1
with integer coefficients. If f is reducible in Q[x], then the lemma of §3.15 shows that f = gh
for some g, h ∈Z[x] of smaller degree than n. In fact, the Degree Addition Formula ensures
that one of g or h (say g) has degree at most m = ⌊n/2⌋. It suffices to find g (or, to prove
irreducibility of f, to show that g must be a constant). Then we can find h by polynomial
division, and apply the algorithm recursively to g and to h until the complete factorization
of f into irreducible polynomials is found.
Polynomials
57
To proceed, pick arbitrary distinct integers a0, a1, . . . , am. If f(ai) = 0 for some i, then
ai is a root of f in Z, and hence g = x−ai is a divisor of f in Q[x]. Assume henceforth that
f(ai) ̸= 0 for every i. The hypothesized factorization f = gh implies f(ai) = g(ai)h(ai) for
all i. In these equations, everything is an integer, and each f(ai) is a nonzero integer with
only finitely many divisors. The algorithm tries to discover g by looping over all possible
sequences of integers (b0, b1, . . . , bm) with bi a divisor of f(ai) in Z for all i; note that there
are only finitely many such sequences. By Lagrange’s Interpolation Formula, each sequence
of this type yields exactly one g ∈Q[x] such that g(ai) = bi for 0 ≤i ≤m, and we can
explicitly calculate g knowing all ai and bi. Construct this finite list of possibilities for g
and see if anything on the list divides f, belongs to Z[x], and has positive degree at most
m. If so, then we have found a proper factor of f. If no g on this list works, then we know
that no other g can possibly work, because of the equations f(ai) = g(ai)h(ai) that must
hold in Z if f = gh. In this latter case, the algorithm has proved irreducibility of f in Q[x].
For example, we apply Kronecker’s Algorithm to prove that f = x4 + x3 + x2 + x + 1
is irreducible in Q[x]. Here, n = deg(f) = 5 and m = 2. For convenience of calculation, we
choose a0 = 0, a1 = 1, and a2 = −1. Now f(a0) = 1 forces b0 ∈{1, −1}; f(a1) = 5 forces
b1 ∈{1, −1, 5, −5}; and f(a2) = 1 forces b2 ∈{1, −1}. So there are 2 · 4 · 2 = 16 sequences
(b0, b1, b2) to be tested. Now, if g is the unique polynomial of degree at most 2 such that
g(ai) = bi for i = 0, 1, 2, then −g is evidently the unique polynomial of degree at most 2
such that g(ai) = −bi for i = 0, 1, 2. Because of this sign symmetry, we really only have
8 sequences to test. We examine two of these sequences explicitly and let the reader (or
the reader’s computer) take care of the others. First, if b0 = b1 = b2 = 1, the associated
polynomial g is the constant polynomial 1. This g is, of course, a divisor of f, but we need to
examine the other sequences to see if one of them yields a non-constant divisor. Second, say
b0 = 1, b1 = 5, and b2 = −1. From this data, Lagrange’s Interpolation Formula produces
g = 1 · (x −1)(x + 1)
(0 −1)(0 + 1) + 5 · (x −0)(x + 1)
(1 −0)(1 + 1) + (−1) ·
(x −0)(x −1)
(−1 −0)(−1 −1) = x2 + 3x + 1.
Dividing f by g in Q[x] gives quotient x2 −2x + 6 and remainder −15x −5, so this g is not
a divisor of f. Checking all the other possible sequences (b0, b1, b2), we never find a divisor
of f other than ±1. This proves that f is, indeed, irreducible in Q[x].
It is nice that we can use Kronecker’s Algorithm to factor any specific f ∈Q[x] or prove
that f is irreducible. But, this algorithm is quite time-consuming if deg(f) is large or if the
integers f(ai) have many prime divisors. Another drawback is that the algorithm cannot
be used to prove irreducibility of collections of polynomials that depend on one or more
parameters (like xn −2 as n varies). This is one reason why it is also helpful to have general
theorems such as Eisenstein’s Criterion that can prove the irreducibility of polynomials in
Q[x] with special structure.
3.20
Algebraic Elements and Minimal Polynomials
Let F be a field and V be an F-algebra. So, V is an F-vector space and a ring (with
multiplication operation ⋆, say) satisfying c(x ⋆y) = (cx) ⋆y = x ⋆(cy) for all x, y ∈V
and all c ∈F. Fix an element z ∈V . We are interested in the F-algebra generated by z,
which is the intersection of all subalgebras of V containing z. We write F[{z}] to denote this
subalgebra (some authors write F[z] instead, but this could be confused with a polynomial
ring with formal indeterminate z). You may check that F[{z}] = {g(z) : g ∈F[x]}, which
is the set of all elements in the algebra V that can be written as polynomial expressions
58
Advanced Linear Algebra
in z using coefficients from F. It follows that the set S = {1V , z, z2, . . . , zn, . . .} spans the
subspace F[{z}] of the F-vector space V .
Consider the case where W = F[{z}] is finite-dimensional over F. Letting dim(W) = n,
the list (1, z, z2, . . . , zn) of n + 1 elements of W must be F-linearly dependent. So, there
exist scalars c0, c1, . . . , cn ∈F, not all zero, with Pn
i=0 cizi = 0V . This means that z is a
root in V of the nonzero polynomial f = Pn
i=0 cixi ∈F[x]. By definition, we say that v ∈V
is algebraic over F iff there exists a nonzero f ∈F[x] with f(v) = 0. We just proved that
for z ∈V , if F[{z}] is finite-dimensional, then z is algebraic over F. The next theorem
gives more specific information about algebraic elements of V .
Theorem on Minimal Polynomials. Suppose F is a field, V is any F-algebra, and z ∈V
is algebraic over F. Let I be the set of all nonzero polynomials in F[x] having z as a root.
There exists a unique monic polynomial m ∈I of minimum degree, and I equals the set of
all nonzero multiples of m in F[x].
The polynomial m is called the minimal polynomial of z over F. The theorem says
m(z) = 0, m is monic, and for any g ∈F[x] such that g(z) = 0, m divides g.
Proof. Note that I is nonempty because z is algebraic over F. Since the degrees of
polynomials in I constitute a nonempty subset of Z≥0, there exists a polynomial m ∈I of
minimum degree. Dividing by the leading coefficient if needed, we can assume that m is
monic. Now, let g ∈I be any nonzero polynomial having z as a root. Dividing g by m in
F[x] gives g = mq + r for some q, r ∈F[x] with r = 0 or deg(r) < deg(m). Evaluating at
z gives 0 = g(z) = m(z)q(z) + r(z) = 0q(z) + r(z) = r(z). If r were nonzero, then r ∈I
and deg(r) < deg(m), which contradicts the minimality of deg(m). So, in fact, r = 0 and
m divides g in F[x]. Conversely, any multiple of m in F[x] has z as a root. We have now
proved everything except uniqueness of m. If m1 is another monic polynomial of minimum
degree in I, then seen that m divides m1. Since the degrees are the same, m1 must be a
constant multiple of m. The constant must be 1, since m and m1 are both monic. This
proves that m is unique.
The minimal polynomial m of an algebraic z ∈V need not be irreducible in F[x]. But m
must be irreducible if the F-algebra V is an integral domain (in particular, if V is a field).
To prove this, suppose m = fg for some f, g ∈F[x] that both have lower degree than m.
Evaluating at z gives 0 = m(z) = f(z)g(z). If V is an integral domain, then f(z) = 0 or
g(z) = 0. Both possibilities contradict the minimality of the degree of m.
Let z be an arbitrary element of any F-algebra V . We proved that if W = F[{z}] is
finite-dimensional, then z is algebraic over F and has a minimal polynomial. Note that the
hypothesis on W automatically holds if V itself is finite-dimensional. The converse is also
true, as we now prove.
Theorem on a Basis of F[{z}]. Suppose F is a field, V is an F-algebra, z ∈V is
algebraic over F, and z has minimal polynomial m over F with deg(m) = d. Then F[{z}]
is a d-dimensional F-vector space with ordered basis B = (1V , z, z2, . . . , zd−1).
Proof. We prove F-linear independence of B. Assume a0, . . . , ad−1 ∈F satisfy a01 + a1z +
· · ·+ad−1zd−1 = 0V . Then z is a root of the polynomial Pd−1
i=0 aixi ∈F[x]. If this polynomial
were nonzero, the minimality of deg(m) would be contradicted. So this polynomial is zero,
which means a0, . . . , ad−1 are all zero.
Next, we show B spans W. It suffices to show that every element in the spanning set
{zk : k ≥0} for W is an F-linear combination of the vectors in B. This is certainly the
case for 0 ≤k < d, since these powers of z are in the list B. Now fix k ≥d, and assume
by induction that z0, z1, . . . , zk−1 are already known to belong to the subspace spanned
by B. Write m = xd + Pd−1
i=0 cixi for some ci ∈F. Evaluating at z, multiplying by zk−d,
Polynomials
59
and solving for zk gives zk = Pd−1
i=0 −cizi+k−d, so that zk is an F-linear combination of
lower powers of z. All these powers belong to the subspace spanned by B, by induction
hypothesis. So zk is also in this subspace, completing the induction step.
How can we find the minimal polynomial m of a specific algebraic z in a specific F-
algebra V ? The key observation is that if deg(m) = d, then zd is the lowest power of z that
is an F-linear combination of preceding powers of z, as we saw in the proof above. If m is not
known in advance, we look at d = 1, 2, 3, . . . in turn and check if the list (1, z, z2, . . . , zd) in
V is F-linearly dependent. If it is, the linear dependence relation c01+c1z+c2z2+· · ·+cdzd
with cd = 1F gives us the minimal polynomial m = Pd
i=0 cixi of z.
For example, consider z =
√
2+
√
3, which is an element of the Q-algebra R. We compute
z0 = 1, z1 =
√
2 +
√
3, z2 = 5 + 2
√
6, and z3 = 11
√
2 + 9
√
3. These powers of z are linearly
independent over Q (Exercise 92). But z4 = 49 + 20
√
6 = 10z2 −1, so x4 −10x2 + 1 must
be the minimal polynomial of z over Q. Because the Q-algebra R is a field, it follows that
x4 −10x2 + 1 (being a minimal polynomial) must be irreducible in Q[x].
Now consider the F-algebra V = Ms(F) of all s × s matrices with entries in F. This
algebra has finite dimension (namely s2) over F, so every matrix A ∈V is algebraic over
F and has a minimal polynomial mA ∈F[x]. The degree of mA is the dimension of the
subspace of matrices spanned by {Is, A, A2, . . . , An, . . .}, where Is denotes the s×s identity
matrix. This subspace has dimension at most s2, so deg(mA) ≤s2 for every s × s matrix
A. For a specific example, let s = 2, F = R, and A =

1
2
3
4

. The matrices I2 and A are
linearly independent, but A2 =

7
10
15
22

= 5A + 2I2, so the minimal polynomial of A
over R is mA = x2 −5x −2. Observe that this polynomial is reducible in R[x], since it has
real roots (5 ±
√
33)/2. A has the same minimal polynomial over Q, but mA is irreducible
in Q[x] since mA has no rational roots.
In the case of matrices A in Ms(F), we can say more about the minimal polynomial of
A. Define the characteristic polynomial of A to be χA = det(xIs −A), which is a monic
polynomial of degree s in F[x].
Cayley–Hamilton Theorem. For all A ∈Ms(F) with characteristic polynomial χA,
χA(A) = 0.
In other words, if χA = Ps
i=0 cixi with ci ∈F, then Ps
i=0 ciAi is the zero matrix. It
follows that the minimal polynomial mA of A must divide χA in F[x], giving the improved
bound deg(mA) ≤s.
For the 2 × 2 matrix A considered above, mA is equal to the characteristic polynomial
χA = det
 x −1
−2
−3
x −4

, but this equality does not hold for all matrices. In later
chapters, we prove the Cayley–Hamilton Theorem and use canonical forms to obtain a
deeper understanding of the relationship between the characteristic polynomial and the
minimal polynomial of a square matrix (see §5.15, §8.14, and §18.19).
3.21
Multivariable Polynomials
We conclude this chapter with an overview of formal power series and polynomials that
involve more than one variable. Intuitively, a polynomial in variables x1, . . . , xm with
coefficients in a given ring R is a finite sum of monomials cxe1
1 · · · xem
m , where c ∈R
60
Advanced Linear Algebra
and e1, . . . , em are nonnegative integers. To specify the polynomial, we must indicate the
coefficient c associated with each possible m-tuple of exponents (e1, . . . , em) ∈Zm
≥0. This
informal description motivates the following precise definitions.
Given a ring R and a positive integer m, let R[[x1, . . . , xm]] be the set of all functions
from Zm
≥0 to R. Such a function is called a formal power series in x1, . . . , xm with coefficients
in R. Given f ∈R[[x1, . . . , xm]], we introduce the formal summation notation
f =
X
e1≥0
· · ·
X
em≥0
f(e1, . . . , em)xe1
1 · · · xem
m
to represent f. We also define R[x1, . . . , xm] to be the subset of R[[x1, . . . , xm]] consisting of
all f where f(e1, . . . , em) = 0R for all but finitely many inputs (e1, . . . , em) ∈Zm
≥0. Elements
of R[x1, . . . , xm] are called formal polynomials in the variables x1, . . . , xm.
Generalizing the m = 1 case, we can define addition and multiplication operations
on R[[x1, . . . , xm]] that make this set a ring with subring R[x1, . . . , xm]. For all f, g ∈
R[[x1, . . . , xm]] and all v ∈Zm
≥0, define (f + g)(v) = f(v) + g(v) and
(f · g)(v) =
X
w,y∈Zm
≥0:
w+y=v
f(w)g(y).
We invite the reader to execute the tedious computations needed to verify the ring axioms.
As a hint for the proof of associativity of multiplication, you should show that (fg)h and
f(gh) are functions from Zm
≥0 to R that both send each v ∈Zn
≥0 to
X
w,y,z∈Zm
≥0:
w+y+z=v
f(w)g(y)h(z).
If R is commutative, then R[[x1, . . . , xm]] and its subring R[x1, . . . , xm] are commutative.
We can identify a ring element c ∈R with the constant polynomial (and formal series)
that sends (0, 0, . . . , 0) ∈Zm
≥0 to c and every other element of Zm
≥0 to zero. This identification
allows us to view R as a subring of R[x1, . . . , xm] and R[[x1, . . . , xm]]. Next, define the formal
variable xi to be the function from Zm
≥0 to R that sends (0, . . . , 0, 1, 0, . . . , 0) to 1R (where the
1 appears in position i) and sends every other element of Zm
≥0 to 0R. Using the definition of
multiplication, you can then check that the ring element cxe1
1 · · · xem
m (formed by multiplying
together c, then e1 copies of x1, etc.) is the function from Zm
≥0 to R that sends (e1, . . . , em)
to c and everything else to 0R. It follows that our formal notation
f =
X
(e1,...,em)∈Zm
≥0
f(e1, . . . , em)xe1
1 · · · xem
m
(3.4)
for the polynomial f can also be viewed as a valid algebraic identity in the ring R[x1, . . . , xm],
in which f is built up from various ring elements c and x1, . . . , xm by addition and
multiplication in the ring. (This comment can be generalized to formal power series, but
only after defining what an infinite sum of ring elements means in F[[x1, . . . , xm]].)
As in the case m = 1, the multivariable polynomial ring R[x1, . . . , xm] satisfies a
universal mapping property.
Universal Mapping Property (UMP) for R[x1, . . . , xm]. Suppose R and S are
commutative rings, h : R →S is a ring homomorphism, and c1, . . . , cm is any list of m
elements of S. There exists a unique ring homomorphism H : R[x1, . . . , xm] →S such that
H extends h and H(xi) = ci for 1 ≤i ≤m.
Polynomials
61
Proof. We outline the proof. Given f = P f(e1, . . . , em)xe1
1 · · · xem
m
in R[x1, . . . , xm], if
a ring homomorphism H exists with the stated properties, then H must send f to
P h(f(e1, . . . , em))ce1
1 · · · cem
m . This proves the uniqueness of H. To prove existence, take
the preceding formula as the definition of H(f), then check that H is a well-defined
ring homomorphism extending h and sending xi to ci for all i between 1 and m.
(Another approach to the existence proof uses induction on m and the UMP for one-
variable polynomial rings. This approach relies on the following recursive characterization
of multivariable polynomial rings: for m > 1, R[x1, . . . , xm] is isomorphic to the one-variable
polynomial ring T[xm], where T = R[x1, . . . , xm−1]. See Exercise 11.)
We now state without proof some of the basic facts about divisibility for multivariable
polynomials with coefficients in a field F. Given f, g ∈F[x1, . . . , xm], we say f divides
g (written f|g, as before) iff f = qg for some q ∈F[x1, . . . , xm]; and we say f and g
are associates iff f = cg for some nonzero c ∈F. A polynomial p ∈F[x1, . . . , xm] is
irreducible in F[x1, . . . , xm] iff the only divisors of p in F[x1, . . . , xm] are nonzero constants
and associates of p. It can be proved that every nonzero f ∈F[x1, . . . , xm] can be factored
uniquely into the form cf
Q
i∈I pei(f)
i
, where cf ∈F is nonzero, the pi range over the set of
all monic irreducible polynomials in F[x1, . . . , xm], and each ei(f) ∈Z≥0. Then divisibility,
gcds, and lcms of multivariable polynomials can be analyzed in terms of these irreducible
factorizations, as we did in §3.14 for m = 1. However, we warn the reader that some
techniques and results used in the one-variable case do not extend to polynomials in more
than one variable. For instance, given f, g ∈F[x1, . . . , xm] with m > 1, it need not be
true that d = gcd(f, g) is a polynomial combination of f and g. For a specific example,
you can check that gcd(x1, x2) = 1, but 1 cannot be written as ax1 + bx2 for any choice
of a, b ∈F[x1, . . . , xm]. There is a division algorithm for multivariable polynomials, but it
is much more subtle than the one-variable version. We do not give any details here, but
instead refer the reader to the outstanding book by Cox, Little, and O’Shea [12].
3.22
Summary
1.
Polynomials and Formal Power Series. Informally, a polynomial with coefficients
in a ring R is an expression Pd
n=0 fnxn with d ∈Z≥0 and each fn ∈R. A formal
power series is an expression P∞
n=0 fnxn with each fn ∈R. More precisely, the
set R[[x]] of formal power series consists of all sequences f = (fn : n ∈Z≥0) with
each fn ∈R. The set R[x] of formal polynomials consists of all (fn : n ∈Z≥0) in
R[[x]] such that fn = 0 for all but finitely many n. The set R[[x]] becomes a ring
using the operations
X
n≥0
fnxn +
X
n≥0
gnxn =
X
n≥0
(fn + gn)xn;

X
n≥0
fnxn

·

X
n≥0
gnxn

=
X
n≥0


X
j,k∈Z≥0: j+k=n
fjgk

xn.
R[x] is a subring of R[[x]] containing an isomorphic copy of R (the constant
polynomials). R is commutative iff R[x] is commutative, and R is an integral
domain iff R[x] is an integral domain; similarly for R[[x]]. R[x] is never a field.
If F is a field, then F[x] and F[[x]] are infinite-dimensional F-vector spaces and
62
Advanced Linear Algebra
F-algebras. There are analogous definitions and results for formal power series
and polynomials in m variables x1, . . . , xm.
2.
Degree. The degree of a nonzero polynomial f = P
n≥0 fnxn is the largest n with
fn ̸= 0. The degree of the zero polynomial is undefined. For nonzero polynomials
p, q such that p + q ̸= 0, deg(p + q) ≤max(deg(p), deg(q)), and strict inequality
occurs iff deg(p) = deg(q) and the leading terms of p and q sum to zero. Similarly,
if the leading terms of p and q do not multiply to zero, then the Degree Addition
Formula deg(pq) = deg(p) + deg(q) is valid.
3.
Polynomial Functions, Evaluation Homomorphisms, and the UMP for R[x].
Given a ring R and p = Pn
i=0 aixi ∈R[x], the associated polynomial function
fp : R →R is given by fp(c) = p(c) = Pn
i=0 aici for c ∈R. Polynomial rings
satisfy the following universal mapping property (UMP): for any commutative
rings R and S and any ring homomorphism h : R →S and any c ∈S, there exists
a unique ring homomorphism H : R[x] →S extending h such that H(x) = c. The
special case h = idR yields the evaluation homomorphisms Ec : R[x] →R given
by Ec(p) = fp(c) = p(c) for all p ∈R[x].
4.
Divisibility Definitions. For a field F and f, g ∈F[x], f divides g in F[x] (written
f|g) iff g = qf for some q ∈F[x]. Polynomials f, g ∈F[x] are associates
iff g = cf for some nonzero c ∈F; every nonzero polynomial has a unique
monic associate. We say f ∈F[x] is a greatest common divisor (gcd) of nonzero
g1, . . . , gk ∈F[x] iff f divides all gi and has maximum degree among all common
divisors of g1, . . . , gk. The notation gcd(g1, . . . , gk) refers to the unique monic gcd
of g1, . . . , gk. Similarly, h ∈F[x] is a least common multiple (lcm) of g1, . . . , gk
iff every gi divides h and h has minimum degree among all common multiples
of g1, . . . , gk. The notation lcm(g1, . . . , gk) refers to the unique monic lcm of
g1, . . . , gk. A polynomial p ∈F[x] is irreducible in F[x] iff its only divisors are
constants and associates of p. Whether p is irreducible depends on the field F of
coefficients.
5.
Polynomial Division with Remainder. For a field F and f, g ∈F[x] with g ̸= 0,
there exist a unique quotient q ∈F[x] and remainder r ∈F[x] with f = qg + r
and either r = 0 or deg(r) < deg(g). There is an algorithm to compute q and r
from f and g. Replacing F by a commutative ring R, q and r still exist if the
leading coefficient of g is invertible in R; q and r are unique if R is an integral
domain.
6.
Greatest Common Divisors. Given a field F and nonzero f1, . . . , fn ∈F[x], there
exists a unique monic gcd g of f1, . . . , fn, and the set of common divisors of
f1, . . . , fn equals the set of divisors of g. There exist s1, . . . , sn ∈F[x] with
g = s1f1 + · · · + snfn; this fact does not extend to polynomials in more than
one variable. When n = 2, we can find g by repeatedly dividing the previous
remainder by the current remainder (starting with f1 divided by f2), letting g be
the last nonzero remainder, then working backwards to find s1 and s2. For any
n ≥1, we can find g and s1, . . . , sn by starting with an n × n identity matrix
augmented with a column containing f1, . . . , fn, row-reducing this matrix until
a single nonzero entry g remains in the extra column, and looking at the other
entries in g’s row to find s1, . . . , sn.
7.
Roots of Polynomials. Given a field F, p ∈F[x], and c ∈F, c is a root of p iff
p(c) = 0 iff (x −c)|p in F[x]. If deg(p) = n, then p has at most n distinct roots in
F. For finite F, we can find the roots of p in F by exhaustive search. For F = Q,
Polynomials
63
we can replace p by an associate Pn
i=0 aixi in Z[x]. The possible rational roots
of p have the form r/s, where r divides a0 and s divides an in Z.
8.
Irreducible Polynomials. Let F be a field. Every p ∈F[x] of degree 1 is irreducible.
Given p ∈F[x] with deg(p) > 1, if p is irreducible then p has no root in F. If p
has no root in F and p has degree 2 or 3, then p is irreducible in F[x]. If F is
finite, then we can factor p ∈F[x] or prove p is irreducible by dividing by finitely
many potential divisors of degree at most deg(p)/2. For F = C, p is irreducible
iff deg(p) = 1. For F = R, p is irreducible iff deg(p) = 1 or p = ax2 + bx + c with
a, b, c ∈R, a ̸= 0, and b2 −4ac < 0. If an irreducible p ∈F[x] divides a product
f1f2 · · · fn with all fi ∈F[x], then p|fi for some i; this property characterizes
irreducible polynomials.
9.
Unique Factorization in F[x]. Let F be a field. Every non-constant g ∈F[x] can
be written g = cq1q2 · · · qr where c ∈F, c ̸= 0, and each qi is monic and irreducible
in F[x]; this factorization is unique except for reordering q1, . . . , qr. We can also
write the factorization as g = cg
Q
i∈I pei(g)
i
where cg ∈F is nonzero, {pi : i ∈I} is
an indexed set of all monic irreducible polynomials in F[x], and each ei(g) ∈Z≥0.
For all f, g ∈F[x], f|g iff ei(f) ≤ei(g) for all i ∈I. So we can compute
gcds and lcms by the formulas ei(gcd(f1, . . . , fn)) = min(ei(f1), . . . , ei(fn))
and ei(lcm(f1, . . . , fn)) = max(ei(f1), . . . , ei(fn)) for all i ∈I. In particular,
lcm(f, g) gcd(f, g) = fg.
10.
Criteria for Irreducibility in Q[x]. If u ∈Z[x] has degree n > 0 and is reducible
in Q[x], then u = gh for some g, h ∈Z[x] with deg(g), deg(h) < n. Given f =
Pn
i=0 fixi ∈Z[x] of degree n, if the reduction of f mod p is irreducible in Zp[x] for
some prime integer p not dividing fn, then f is irreducible in Q[x]. Eisenstein’s
Criterion says that if there is a prime p ∈Z such that p|fi for all i < n, p does
not divide fn, and p2 does not divide f0, then f is irreducible in Q[x].
11.
Lagrange’s Interpolation Formula. Given a field F, distinct a0, . . . , an ∈F, and
arbitrary b0, . . . , bn ∈F, there exists a unique g ∈F[x] with g = 0 or deg(g) ≤n
such that g(ai) = bi for all i. Explicitly,
g =
n
X
i=0
bipi, where pi =
Y
j̸=i
(x −aj)
, Y
j̸=i
(ai −aj) for 0 ≤i ≤n.
12.
Kronecker’s Factoring Algorithm in Q[x]. Given f ∈Z[x] of degree n, Kronecker’s
Algorithm produces a finite list of potential divisors of f of degree at most m =
⌊n/2⌋as follows. Pick distinct a0, . . . , am ∈Z. Loop over all (b0, . . . , bm) ∈Zm+1
such that bi divides f(ai) in Z for all i. Use Lagrange’s Interpolation Formula
to build g ∈Q[x] with g(ai) = bi for all i. If any such g is non-constant in Z[x]
and divides f, then we have partially factored f. If no g works, then f must be
irreducible in Q[x]. As a special case, if some f(ai) = 0, then (x −ai)|f in Q[x].
13.
Minimal Polynomials. Let F be a field, V an F-algebra, and z ∈V . We say
z is algebraic over F iff g(z) = 0 for some nonzero g ∈F[x]. For algebraic z,
there exists a unique monic polynomial m ∈F[x] with m(z) = 0 such that m
divides every other polynomial in F[x] having z as a root; m is called the minimal
polynomial of z over F. The subalgebra F[{z}] spanned by all powers of z is
finite-dimensional iff z is algebraic over F. In this case, a basis for this subalgebra
(viewed as an F-vector space) is (1, z, z2, . . . , zd−1) where d is the degree of the
minimal polynomial of z. The minimal polynomial of z must be irreducible for an
64
Advanced Linear Algebra
integral domain or field V , but can be reducible in other algebras. The Cayley–
Hamilton Theorem says that every s × s matrix A ∈Ms(F) has a minimal
polynomial that divides the characteristic polynomial of A.
3.23
Exercises
1.
Let R be a ring. Prove carefully that R[[x]] is a commutative group under addition,
indicating which ring axioms for R are needed at each point in the proof. Prove
that R[x] is an additive subgroup of R[[x]].
2.
Let R be a ring. Carefully prove the ring axioms for R[[x]] involving multiplication,
indicating which ring axioms for R are needed at each point in the proof. Prove
R[[x]] is commutative iff R is commutative. Prove R[x] is a subring of R[[x]].
3.
Repeat the previous two exercises for R[[x1, . . . , xm]] and R[x1, . . . , xm].
4.
Let F be a field. Prove F[[x]] is an F-vector space and F-algebra using the scalar
multiplication c · (fi : i ≥0) = (cfi : i ≥0) for c, fi ∈F. Prove F[x] is a subspace
and subalgebra of F[[x]]. Prove {1, x, x2, . . . , xn, . . .} is a basis of F[x].
5.
Repeat the previous exercise for formal series and polynomials in m variables,
replacing the set of powers of x with {xe1
1 xe2
2 · · · xem
m : (e1, . . . , em) ∈Zm
≥0}.
6.
Let R be a ring. (a) Show that j : R →R[x] given by j(a) = (a, 0, 0, . . .) for
a ∈R is an injective ring homomorphism. (b) For m ≥2, define a similar map
jm : R →R[x1, . . . , xm] and verify that it is an injective ring homomorphism.
7.
Let R be a ring with subring S. (a) Prove S[[x]] is a subring of R[[x]].
(b) Prove S[x] is a subring of R[x].
8.
Let R be a ring with ideal I. Write I[[x]] (resp. I[x]) for the subset of formal series
(resp. polynomials) having all coefficients in I. Prove there are ring isomorphisms
R[[x]]/I[[x]] ∼= (R/I)[[x]] and R[x]/I[x] ∼= (R/I)[x].
9.
Let R be a ring. Let x = (0R, 1R, 0R, 0R, . . .) ∈R[x].
(a) Prove carefully that for i ≥1, the product of i copies of x in R[x] is the
sequence (ej : j ≥0) with ei = 1R and ej = 0R for all j ̸= i.
(b) For m > 1 and 1 ≤k ≤m, define xk : Zm
≥0 →R by letting xk map
(0, . . . , 1, . . . , 0) to 1R (the 1 is in position k) and letting xk map everything else in
Zm
≥0 to 0R. Prove carefully that xe1
1 xe2
2 · · · xem
m (the product of e1+· · ·+em elements
of R[x1, . . . , xm]) is the function mapping (e1, . . . , em) to 1R and everything else
in Zm
≥0 to 0R. (This result justifies the monomial notation used for multivariable
formal series and polynomials.)
10.
Binomial Theorem. Let R be a ring. (a) Prove: for all r ∈R and n ∈Z>0,
(r + x)n =
n
X
k=0
n
k

rn−kxk
in R[x]. Here,
 n
k

is the integer
n!
k!(n−k)!, and the notation js (for j ∈Z≥0, s ∈R)
denotes the sum of j copies of s in R.
(b) Give a specific example showing that the identity in (a) can be false if the
formal variable x ∈R[x] is replaced by an element of R.
Polynomials
65
11.
Let R be a ring. Prove there are ring isomorphisms R[[x1, . . . , xm−1]][[xm]] ∼=
R[[x1, . . . , xm−1, xm]] and R[x1, . . . , xm−1][xm] ∼= R[x1, . . . , xm] for each m > 1.
(This gives a recursive description of multivariable formal series and polynomials.)
12.
Let R be a ring. Define the order of a nonzero formal series f ∈R[[x]], denoted
ord(f), to be the smallest n ≥0 with fn ̸= 0R. (a) Suppose f, g are nonzero
formal series with f + g ̸= 0. Find and prove an inequality relating ord(f + g)
to ord(f) and ord(g), and determine when strict inequality holds. (b) Find (with
proof) a condition on f, g ∈R[[x]] ensuring ord(fg) = ord(f) + ord(g).
13.
(a) Prove that a ring R is an integral domain iff R[[x]] is an integral domain.
(b) Fix m > 1. Prove R is an integral domain iff R[x1, . . . , xm] is an integral
domain iff R[[x1, . . . , xm]] is an integral domain.
14.
Give a specific example of a commutative ring R and polynomials of all degrees
in R[x] that have multiplicative inverses in R[x].
15.
Let F be a field. Prove that g ∈F[[x]] is invertible in F[[x]] iff the constant
coefficient in g is nonzero. Is F[[x]] ever a field?
16.
Let F be a field and n > 0. Is the set of p ∈F[x] with deg(p) = n or p = 0
a subspace of F[x]? What about the set of p with deg(p) > n or p = 0? What
about the set of p ∈F[[x]] with ord(p) > n or p = 0? (See Exercise 12.)
17.
Suppose F is a field and, for all n ∈Z≥0, pn ∈F[x] satisfies deg(pn) = n. Prove
{pn : n ≥0} is a basis of the F-vector space F[x].
18.
Let p = x4 + 3x2 + 4x + 1 ∈Z5[x]. Find the polynomial function fp : Z5 →Z5.
19.
(a) Explicitly describe all functions g : Z2 →Z2.
(b) For each g found in (a), find all polynomials p ∈Z2[x] such that fp = g.
20.
Let F be a finite field. Prove that for every function g : F →F, there exists at
least one p ∈F[x] such that g = fp. Is it true that for each g : F →F, there
exist infinitely many p ∈F[x] with g = fp?
21.
Give a specific example of an infinite field F and a function g : F →F such that
g ̸= fp for all polynomials p ∈F[x].
22.
Given a field F, let S be the set of all functions from F to F.
(a) Verify that S is a commutative ring under pointwise operations on functions:
(f + g)(c) = f(c) + g(c) and (f · g)(c) = f(c) · g(c) for all f, g ∈S and all c ∈F.
(b) Define ϕ : F[x] →S by ϕ(p) = fp, where fp is the polynomial function sending
c ∈F to p(c). Prove ϕ is a ring homomorphism.
(c) Prove: ϕ is surjective iff F is finite.
(d) Prove: ϕ is injective iff F is infinite. (So, we may only identify formal
polynomials with polynomial functions when the field of coefficients is infinite.)
23.
Let R be a commutative ring and c ∈R. Prove that the map ϕc : R[x] →R[x]
given by ϕc(p) = p(x −c) for p ∈R[x] is a ring isomorphism. (Use the UMP.)
24.
Fill in the details of the two proofs of the universal mapping property for
R[x1, . . . , xm] outlined in §3.21.
25.
Let R and S be commutative rings and f : R →S a ring homomorphism. Use
the UMP to show that F : R[x] →S[x] defined by F(P
i≥0 aixi) = P
i≥0 f(ai)xi
is a ring homomorphism. Prove if f is one-to-one, then F is one-to-one. Prove if
f is onto, then F is onto.
26.
Let R be a subring of a commutative ring S, and let c ∈S. Prove that R[{c}] =
{fp(c) : p ∈R[x]} is the smallest subring of S containing R and c.
66
Advanced Linear Algebra
27.
Let F be a field and V a (not necessarily commutative) F-algebra. For z ∈V ,
prove that F[{z}] = {fp(z) : p ∈F[x]} is an F-subalgebra of V . Prove also that
F[{z}] is spanned (as an F-vector space) by {zn : n ≥0} and is the smallest
subalgebra of V containing F and z.
28.
Let h : M2(R) →M2(R) be the identity map. Show there does not exist any
extension of h to a ring homomorphism H : M2(R)[x] →M2(R) such that H(x) =

1
1
0
1

. (So the UMP for R[x] can fail if R is not commutative.)
29.
For each f, g ∈Q[x], find the quotient q and remainder r when f is divided by g.
(a) f = x8 + x4 + 1, g = x4 −x2 + 1 (b) f = 3x3 −5x2 + 11x, g = x2 −2x + 3
(c) f = x4 −5x2 −x + 1, g = x −2 (d) f = x5 −1, g = 2x2 −1
30.
For each f, g ∈F[x], find the quotient q and remainder r when f is divided by g.
(a) f = x5 + x4 + x + 1, g = x3 + x in Z2[x]
(b) f = x5 + x4 + x + 1, g = x3 + x in Z3[x]
(c) f = 3x4 + x3 + 2x + 4, g = 2x2 + 3x + 1 in Z5[x]
(d) f = 3x4 + x3 + 2x + 4, g = 2x2 + 3x + 1 in Z7[x]
31.
(a) Give a specific example of an integral domain R and nonzero f, g ∈R[x] such
that there are no q, r ∈R[x] satisfying f = qg + r and r = 0 or deg(r) < deg(g).
(b) Give a specific example of a commutative ring R and f, g ∈R[x] such that
the quotient q when f is divided by g (with remainder) is not unique.
(c) Give a specific example of a commutative ring R and f, g ∈R[x] such that
the remainder r when f is divided by g is not unique.
32.
Division Theorem for Z. Prove that for all a, b ∈Z with b nonzero, there exist
unique q, r ∈Z with a = bq + r and 0 ≤r < |b|.
33.
Euclid’s Algorithm for Integer GCDs. Prove: for all nonzero integers
a1, . . . , an, there exists a unique positive gcd d of a1, . . . , an, and there exist
integers s1, . . . , sn with d = s1a1 + · · · + snsn. Moreover, the set of common
divisors of a1, . . . , an equals the set of divisors of d. For n = 2, describe an
algorithm (involving repeated integer division) for finding d and s1, s2 from a1, a2.
34.
Prove: for all integers p, r, s, if p is prime and p|rs, then p|r or p|s.
35.
Unique Factorization Theorem for Integers. Prove: every integer n ̸= 0 can
be factored as n = cp1p2 · · · pk, where c ∈{1, −1}, k ≥0, and each pi is a prime
positive integer. Prove the factorization is unique except for reordering p1, . . . , pk.
36.
Criterion for Invertibility Modulo n. Prove: for all n, t ∈Z>0, gcd(n, t) = 1
iff there exists s ∈Zn with st mod n = 1; and s is unique when it exists.
37.
Let F be a field and f, g, h ∈F[x].
(a) Prove: if f|gh and gcd(f, g) = 1, then f|h.
(b) Prove: if gcd(f, g) = 1 and f|h and g|h, then fg|h.
(c) Prove: if d ∈F[x] is a gcd of f and g, then hd is a gcd of hf and hg.
(d) Give examples to show that (a) and (b) can fail if gcd(f, g) ̸= 1.
38.
Repeat the previous exercise replacing F[x] by Z.
39.
Assume F is a field and f, f1 ∈F[x] are associates. Prove for all h ∈F[x], f|h iff
f1|h; and h|f iff h|f1. Prove f is irreducible iff f1 is irreducible.
40.
Let R be any commutative ring. For r, s ∈R, define r|s iff s = qr for some q ∈R.
Define r, s ∈R to be associates in R, denoted r ∼s, iff r|s and s|r.
(a) Prove that the divisibility relation | on R is reflexive and transitive.
(b) Prove that the association relation ∼is an equivalence relation on R.
Polynomials
67
(c) Prove that for all integral domains R and all r, s ∈R, r ∼s iff s = ur for
some invertible element u ∈R.
41.
For the equivalence relation ∼in Exercise 40, describe the equivalence classes of
∼for these rings: R = Z; R is a field; R = F[x], where F is a field; R = Z12.
42.
For each f, g ∈Q[x], compute d = gcd(f, g) and s, t ∈Q[x] with d = sf + tg.
(a) f = x6 −1, g = x4 −1 (b) f = x3 −5, g = x2 −2
(c) f = x3+x+1, g = 2x2−3x+1 (d) f = x5+x4−3x2−2x−2, g = x4−x3−x2+6
43.
Let f1 = x2−3x+2, f2 = x2−5x+6, and f3 = x2−4x+3 in Q[x]. Follow the proof
in §3.9 to find d = gcd(f1, f2, f3) and s1, s2, s3 ∈Q[x] with d = s1f1 +s2f2 +s3f3.
44.
Let K be a field with subfield F. Prove that for all nonzero f1, . . . , fn ∈F[x],
gcd(f1, . . . , fn) in the ring F[x] equals gcd(f1, . . . , fn) in the ring K[x] (even
though the sets of common divisors of f1, . . . , fn and the irreducible factorizations
of f1, . . . , fn may be different in these two rings).
45.
Let F be a field. Use the Division Theorem to prove that for every ideal I of F[x],
there exists g ∈F[x] such that I = F[x]g = {hg : h ∈F[x]}. (This says that F[x]
is a principal ideal domain, or PID. We study PIDs in Chapter 18.)
46.
Let F be a field. Given ideals I = F[x]g and J = F[x]h in F[x] (where g, h ∈F[x]
are generators of the ideals), show that I ⊆J iff h|g in F[x]. Deduce that I = J
iff h and g are associates in F[x].
47.
Let F be a field. Given nonzero f1, . . . , fn ∈F[x], reprove the theorem that these
polynomials have a unique monic gcd g, and that g has the form Pn
i=1 sifi for
some si ∈F[x], by studying the ideal J = {s1f1 + · · · + snfn : si ∈F[x]}.
48.
Let F be a field. Given nonzero f1, . . . , fn ∈F[x], show the set K = Tn
i=1 F[x]fi
is an ideal of F[x], so K is generated by some h ∈F[x]. Find (with proof) the
specific relation between h and f1, . . . , fn.
49.
For each field F and f, g ∈F[x], use the matrix reduction algorithm in §3.10 to
find d = gcd(f, g) and s, t ∈F[x] with d = sf + tg.
(a) F = Q, f = x12 −1, g = x8 −1
(b) F = Z5, f = x3 + x2 + 3x + 2, g = 2x4 + 2x2 + x + 3
(c) F = Z3, f = x3 + 2x2 + 2, g = x3 + x2 + 2x + 2
(d) F = Z7, f = x3 + 2x2 + 4x + 1, g = x2 + 5x + 6
(e) F = Z2, f = x8 + x4 + x3 + x + 1, g = x5 + x2 + x
50.
Give a careful proof that the matrix reduction algorithm in §3.10 for finding the
gcd of a list of n polynomials terminates and gives correct results.
51.
Let f1 = x6+x4+x+1, f2 = x8+x6+x5+x4+x3+x2+1, and f3 = x6+x5+x2+1
in Z2[x]. Use matrix reduction to find d = gcd(f1, f2, f3) and s1, s2, s3 ∈Z2[x]
with d = s1f1 + s2f2 + s3f3.
52.
Matrix Reduction Algorithm for Integer GCDs. Describe a version of the
matrix reduction algorithm in §3.10 that takes as input nonzero integers a1, . . . , an
and returns as output d, s1, . . . , sn ∈Z with d = gcd(a1, . . . , an) = Pn
i=1 siai.
Prove that your algorithm terminates and returns a correct answer.
53.
For each a, b, use the algorithm in the previous problem to compute d, s, t ∈Z
with d = gcd(a, b) = sa + tb. (a) a = 101, b = 57
(b) a = 516, b = 215
(c) a = 1300, b = 967
(d) a = 1702, b = 483
54.
Let F be a field. Reprove the theorem that a polynomial p ∈F[x] of degree n has
at most n roots in F using the uniqueness of the irreducible factorization of p.
68
Advanced Linear Algebra
55.
Prove: for all n ∈Z>0, there exists a commutative ring R such that x2−1R ∈R[x]
has more than n roots in R.
56.
Given f = x5 + 6x4 + 9x3 + 3x2 + 10x + 11 ∈Z13[x], find all roots of f in Z13.
57.
Prove: for any field F and any finite subset S of F, there exist infinitely many
p ∈F[x] such that S is the set of roots of p in F.
58.
Prove: for any finite field F of size n, Q
c∈F (x −c) = xn −x in F[x]. (Use
Exercise 11 in Chapter 1.)
59.
Given a polynomial f = 7x5 +10x4 +· · ·−9 ∈Z[x] of degree 5 (where the middle
coefficients are unknown integers), list all possible roots of f in Q.
60.
Find all rational roots of f = 10x5 + 11x4 −41x3 + 29x2 −51x + 18, and then
factor f into irreducible polynomials in Q[x] and in C[x].
61.
Find all x ∈Q solving 0 = 6x6 + 18.6x5 + 12.6x4 −5.4x3 −25.2x2 + 24.6x −6.
62.
Use the Fundamental Theorem of Algebra to prove that f ∈R[x] is irreducible iff
deg(f) = 1 or f = ax2 + bx + c for some a, b, c ∈R with a ̸= 0 and b2 −4ac < 0.
63.
Find all irreducible polynomials of degree 5 or less in Z2[x]. Explain how you know
your answers are irreducible, and how you know you have found all of them.
64.
Find all monic irreducible polynomials of degree 2 in Z3[x]. Explain how you know
your answers are irreducible, and how you know you have found all of them.
65.
For prime p, how many f ∈Zp[x] are monic irreducible polynomials of degree 2?
66.
(a) Prove that x4 + x2 + 2x + 1 is irreducible in Z3[x].
(b) Describe all polynomials in Q[x] whose irreducibility in Q[x] can be deduced
directly from (a) by reduction mod 3.
(c) Use (a) and Exercise 23 to show x4 + 8x3 + 25x2 + 38x + 25 is irreducible in
Q[x].
67.
For each field F, decide (with proof) whether x4 + x3 + x2 + x + 1 is irreducible
in F[x]. (a) C (b) R (c) Q (d) Z2 (e) Z3 (f) Z5
68.
For each field F, decide (with proof) whether (1F , 0F , −1F , 0F , 1F , 0F , 0F , . . .) is
irreducible in F[x]. (a) Z2 (b) Z3 (c) Z5 (d) Z7 (e) Z13 (f) Q (g) C
69.
Prove that f = x4 + 3x2 + 1 is irreducible in Q[x] as follows. Show f has no
rational root. Explain why the reducibility of f implies the existence of monic
quadratics g, h ∈Z[x] with f = gh. Write g = x2 + bx + c, h = x2 + dx + e for
b, c, d, e ∈Z. By comparing the coefficients of gh to the coefficients of f, show
that g, h cannot exist.
70.
Prove x5 −4x2 + 2 is irreducible in Q[x] by reducing modulo a prime.
71.
Prove x7 + 20x5 −8x4 + 11x3 + 14x −9 is irreducible in Q[x] (reduce mod 2).
72.
Prove x7 + 30x5 −6x2 + 12 is irreducible in Q[x] using Eisenstein’s Criterion.
73.
Prove x4 −5x2 + 1 is irreducible in Q[x].
74.
Prove x3 + ax + 1 is irreducible in Q[x], where a ∈Z, a ̸= −2, and a ̸= 0.
75.
Prove x5 + 30x4 + 210x3 + 300 is irreducible in Q[x].
76.
Prove x3 + 2x + 1 is irreducible in Z5[x].
77.
Prove x4 −8 is irreducible in Q[x].
78.
Find the factorization of x8 −1 into a product of irreducible polynomials in C[x],
R[x], Q[x], and Z2[x]. Explain how you know that the factors are irreducible.
Polynomials
69
79.
Find the factorization of x12 −1 into a product of irreducible polynomials in C[x],
R[x], Q[x], Z2[x], Z3[x], and Z5[x]. Explain why the factors are irreducible.
80.
Use the Rational Root Theorem to prove that for all a, n ∈Z>0,
n√a is rational
iff a = bn for some integer b.
81.
Use unique prime factorizations in Z to prove: for all a, n ∈Z>0,
n√a ∈Q iff a
has prime factorization Q
i pei
i with n dividing every ei iff
n√a ∈Z.
82.
Let F be a field, and assume p = a0 + a1x + a2x2 + · · · + an−1xn−1 + anxn is
irreducible in F[x], where ai ∈F and an ̸= 0. Let q = an + an−1x + an−2x2 +
· · · + a1xn−1 + a0xn. Prove q is irreducible in F[x].
83.
Let p be a prime integer and g = xp−1+xp−2+· · ·+x2+x+1. This problem proves
that g is irreducible in Q[x]. Recall (Exercise 23) that the map ϕ : Q[x] →Q[x],
given by ϕ(f) = f(x + 1) for f ∈Q[x], is a ring isomorphism. Compute the
coefficients of the polynomial g∗= g(x + 1). (Start by applying ϕ to the equation
xp −1 = (x−1)g.) Use a theorem from the text to prove g∗is irreducible in Q[x].
Explain why irreducibility of g follows.
84.
Does Eisenstein’s Criterion hold if p is a positive integer (not necessarily prime)?
Prove or give a justified counterexample. What happens if p is a product of two
or more distinct primes?
85.
Find a, b, c, d ∈Q such that f = a + bx + cx2 + dx3 ∈Q[x] satisfies f(0) = −4,
f(1) = −2, f(2) = 6, and f(1/2) = −3.
86.
Find a, b, c ∈Z7 such that g = a + bx + cx2 ∈Z7[x] satisfies g(1) = 5, g(2) = 3,
and g(3) = 0.
87.
Find a, b, c ∈Z13 such that h = a+bx+cx2 ∈Z13[x] satisfies h(2) = 12, h(4) = 10,
and h(6) = 9.
88.
Secret Sharing. A set of n people wish to share information about a master
secret in such a way that any k of the n people can pool their knowledge to recover
the secret, but no subset of fewer than k people can gain any knowledge about
the secret by working together. This goal can be achieved using polynomials, as
follows. Pick a prime p > n, and encode the master secret as a value a0 ∈Zp.
Choose random a1, . . . , ak−1 ∈Zp, and let g = Pk−1
i=0 aixi ∈Zp[x]. Number
the people 1 to n arbitrarily, and give person j the pair (j, g(j)). (a) Show how
any subset of k or more people can use their collective knowledge to recover a0.
(b) Prove that the collective knowledge of any subset of k −1 or fewer people
gives no information about the secret.
89.
Chinese Remainder Theorem. There is an analog of Lagrange’s Interpolation
Formula that lets us solve systems of congruences with pairwise relatively prime
moduli. Assume n1, . . . , nk are positive integers with gcd(ni, nj) = 1 for all i ̸= j.
Set N = n1n2 · · · nk. Prove: for every list (b1, . . . , bk) with each bi ∈Zni, there
exists a unique x ∈ZN such that x mod ni = bi for 1 ≤i ≤k. (To prove existence,
define qi = Q
j̸=i nj for 1 ≤i ≤k. Note gcd(qi, ni) = 1, so Exercise 36 gives an
integer ri ∈Zni with qiri mod ni = 1. Define pi = qiri ∈ZN; what is pi mod ni
and pi mod nj for j ̸= i? Define x in terms of p1, . . . , pn, b1, . . . , bn.)
90.
Find x ∈Z1001 solving x mod 7 = 5, x mod 11 = 4, and x mod 13 = 1.
91.
Use Kronecker’s Algorithm to find the irreducible factorization of each polynomial
in Q[x]. (a) x4 −x2 +2x−1 (b) x5 −4x+2 (c) x5 −3x4 +10x3 −10x2 +24x−7
70
Advanced Linear Algebra
92.
(a) Prove that (1,
√
2,
√
3) is a Q-linearly independent list.
(b) Deduce that (1,
√
2,
√
3,
√
6) is a Q-linearly independent list.
(c) Let z =
√
2 +
√
3. Prove that (1, z, z2, z3) is Q-linearly independent.
93.
Viewing C as a Q-algebra, find (with proof) the minimal polynomials over Q of
each of these complex numbers: −i,
3√
7, eπi/4,
√
5 + 3i,
√
2 +
3√
5.
94.
Find the minimal polynomial of each matrix over R (where a, b, c, d ∈R):

0
0
0
0

,


2
0
0
0
2
0
0
0
2

,


1
1
1
1
1
1
1
1
1

,


0
1
0
0
0
1
−d
−c
−b

,

a
b
0
d

.
95.
Let F be a field. Show g ∈F[x] is algebraic over F iff g is a constant.
96.
Formal Derivatives. Given a commutative ring R and f = P
n≥0 fnxn ∈R[[x]],
define the formal derivative of f to be D(f) = P
n≥1 nfnxn−1, where nfn denotes
the sum of n copies of fn in R. Prove the following formal derivative rules.
(a) For all f, g ∈R[[x]] and c ∈R, D(f + g) = D(f) + D(g) and D(cf) = cD(f).
(b) For all f, g ∈R[[x]], D(fg) = D(f)g + fD(g).
(c) For all f ∈R[x] and g ∈R[[x]], D(f(g)) = (D(f))(g) · D(g). Here, f(g) is the
evaluation of the polynomial f at x = g, and similarly for (D(f))(g).
97.
Repeated Roots. Let F be a field, p ∈F[x], and c ∈F. We say c is a repeated
root of p iff (x−c)2|p in F[x]. Prove c is a repeated root of p iff p(c) = 0 = D(p)(c)
(see Exercise 96). Prove if p has a repeated root in some field K containing F as
a subfield, then gcd(p, D(p)) (computed in F[x]) is not 1. (The converse is also
true, but you may need Exercise 44 in Chapter 12 to prove it.)
98.
True or false? Explain each answer.
(a) For all rings R and all g ∈R[x] of degree n > 0, g has at most n roots in R.
(b) For any field F, an irreducible p ∈F[x] has no root in F.
(c) For any field F, any p ∈F[x] with no root in F is irreducible in F[x].
(d) For all fields F and all p ∈F[x], if p(c) = 0 for all c ∈F, then p = 0.
(e) For all irreducible f ∈Q[x], the only divisors of f in Q[x] are 1 and f.
(f) The polynomial x5 + x4 + x3 + x2 + x + 1 is irreducible in Q[x].
(g) For all b, c ∈Z, every rational root of 2x3 + bx2 + cx + 3 must be in the set
{±1, ±2, ±1/3, ±2/3}.
(h) For all fields F and f, g ∈F[x], if 1 = sf + tg for some s, t ∈F[x], then
gcd(f, g) = 1.
(i) For all rings R and all nonzero f, g ∈R[x], deg(fg) = deg(f) + deg(g).
(j) For all f, g ∈Z2[x], if f|g and g|f then f = g.
(k) For all even a ∈Z and odd b, c ∈Z, x3 + ax2 + bx + c is irreducible in Q[x].
(l) Every monic f ∈Q[x] with deg(f) ≥1 can be factored as f = p1p2 · · · pk,
where each pi is irreducible in Q[x], and the only other factorizations of f into
irreducible factors are obtained by reordering p1, . . . , pk.
(m) For all p, q ∈Q[x], if p(m) = q(m) for all m ∈Z, then p = q.
(n) The minimal polynomial of an algebraic element in a commutative F-algebra
must be irreducible in F[x].
(o) For all fields F and all monic d, f, g ∈F[x], gcd(f, g) = d iff d = sf + tg for
some s, t ∈F[x].
Part II
Matrices
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
4
Basic Matrix Operations
Our goal in this chapter is to study some basic material about matrices from a somewhat
more advanced viewpoint. We begin by giving formal definitions of matrices, ordered n-
tuples, row vectors, and column vectors. We then give several equivalent formulations of the
basic matrix operations (addition, scalar multiplication, matrix multiplication, transpose,
etc.) and derive their main properties. We also show how to execute elementary row and
column operations by multiplying a matrix on the left or right by certain elementary
matrices.
Throughout this chapter, the letter F denotes an arbitrary field, and elements of F are
called scalars. However, most of our results on matrices extend to the case where F is a ring.
In this more general setting, vector spaces are replaced by modules (see Chapter 17), and
nonzero scalars in the field F are replaced by units (invertible elements) in the ring F. For
those readers who need this level of generality, we include annotations in square brackets
at any places in the chapter that explicitly require the assumption that F is a field, or that
multiplication in F is commutative.
4.1
Formal Definition of Matrices and Vectors
For each positive integer n, write [n] for the set {1, 2, . . . , n}. Informally, an n-tuple is an
ordered list x = (x1, x2, . . . , xn) of n scalars xi ∈F. Formally, we define an n-tuple with
entries in F to be a function x : [n] →F given by x(i) = xi for i ∈[n]. We frequently use
either notation (lists or functions) to describe n-tuples. Let F n be the set of all n-tuples
with entries in F.
Informally, an m×n matrix A is an array of scalars consisting of m rows and n columns.
The element of F in row i and column j of A is called the i, j-entry of A. This entry is often
written Ai,j or Aij or aij. Formally, let [m] × [n] be the set of ordered pairs {(i, j) : i ∈
[m], j ∈[n]}. We define an m×n matrix with entries in F to be a function A : [m]×[n] →F,
where A(i, j) is the i, j-entry of A. Let Mm,n(F) be the set of all m × n matrices over F
and Mn(F) be the set of all n × n (square) matrices over F.
A column vector of length n is an n × 1 matrix. A row vector of length n is a 1 × n
matrix. Formally, a column vector is a function xc : [n] × [1] →F, while a row vector is
a function xr : [1] × [n] →F. There is a bijection (one-to-one correspondence) between
column vectors and n-tuples that sends xc to the n-tuple (xc(1, 1), xc(2, 1), . . . , xc(n, 1)).
Similarly, there is a bijection between row vectors and n-tuples that sends xr to the n-tuple
(xr(1, 1), xr(1, 2), . . . , xr(1, n)). Using these bijections, we can regard column vectors and
row vectors as n-tuples.
Given a matrix A ∈Mm,n(F) and j in [n], define A[j] to be the jth column of A, which
can be viewed as the m-tuple (A(1, j), A(2, j), . . . , A(m, j)). For i in [m], define A[i] to be
the ith row of A, which can be viewed as the n-tuple (A(i, 1), A(i, 2), . . . , A(i, n)). Any
matrix A is completely determined by the ordered list of its columns (A[1], . . . , A[n]). A is
DOI: 10.1201/9781003484561-4
73
74
Advanced Linear Algebra
also determined by the ordered list of its rows (A[1], . . . , A[m]). More formally, we have a
bijection from Mm,n(F) to (F m)n and another bijection from Mm,n(F) to (F n)m.
Next, we define some special matrices. The rectangular zero matrix 0m,n ∈Mm,n(F) is
defined by 0m,n(i, j) = 0 for 1 ≤i ≤m and 1 ≤j ≤n. We write 0n for 0n,n. The square
identity matrix In ∈Mn(F) is defined by In(i, i) = 1 and In(i, j) = 0 for i ̸= j (where
1 ≤i, j ≤n). The unit matrix Jm,n ∈Mm,n(F) is defined by Jm,n(i, j) = 1 for 1 ≤i ≤m
and 1 ≤j ≤n. We omit the subscripts from the notation whenever they are understood
from context.
Recall that two functions f and g are equal iff they have the same domain, the same
codomain, and f(x) = g(x) for all x in the common domain. Applying this remark to the
formal definition of matrices, we see that an m × n matrix A equals an m′ × n′ matrix A′
iff m = m′ and n = n′ and A(i, j) = A′(i, j) for all i, j with 1 ≤i ≤m and 1 ≤j ≤n.
Similarly, v ∈F n equals v′ ∈F n′ iff n = n′ and v(i) = v′(i) for all i between 1 and n.
In most cases, we label the rows and columns of a matrix by positive integers. Specifically,
for an m×n matrix A, we use the set [m] = {1, 2, . . . , m} to index the rows of A, and we use
the set [n] = {1, 2, . . . , n} to index the columns of A. Sometimes, it is more convenient to use
other indexing sets to name the rows and columns (see §4.15 for one prominent example).
Let I and J be any finite, nonempty sets. Define an I × J matrix with entries in F to be
a function A : I × J →F. For i ∈I and j ∈J, A(i, j) is the entry of A in row i, column
j. We call I the row index set of A and J the column index set of A. To display an I × J
matrix as a rectangular array, we order the sets I and J, say I = {i1 < i2 < . . . < im} and
J = {j1 < j2 < · · · < jn}. We then place A(is, jt) in row s, column t of the array.
4.2
Vector Spaces of Functions
We are about to give entry-by-entry definitions of algebraic operations on matrices and
vectors. Before doing so, it is helpful to consider a more general situation. Let F be the
field of scalars, let S be an arbitrary set, and V be the set of all functions g : S →F.
By introducing pointwise operations on functions, we can turn the set V into an F-vector
space. [When F is a ring, V is a left F-module.] First, we define an addition operation
+ : V × V →V . Let g, h : S →F be two elements of V . Define g + h : S →F by setting
(g + h)(x) = g(x) + h(x) for all x ∈S. Note that the plus symbol on the right side is
addition in the given field F, while the bold plus symbol on the left side is the new addition
of functions that is being defined. Since F is closed under addition, g(x) + h(x) always
belongs to F, so that g + h is a function from S to F. In other words, we have the closure
condition: for all g, h ∈V , g + h ∈V . Similarly, the other additive axioms in the definition
of a vector space [or module] follow from the corresponding axioms for the field [or ring] F.
For instance, to confirm that (g + h) + k = g + (h + k) for all g, h, k ∈V , we check that
both sides (which are functions from S to S) agree at every x in the domain S:
[(g + h) + k](x) = (g + h)(x) + k(x) = (g(x) + h(x)) + k(x)
= g(x) + (h(x) + k(x)) = g(x) + (h + k)(x) = [g + (h + k)](x).
(4.1)
This calculation uses associativity of addition in F. A similar computation shows that
g + h = h + g for all g, h ∈V . You can check that the additive identity of V is the zero
function 0V : S →F given by 0V (x) = 0F for all x ∈S. The additive inverse of f ∈V
is the function (−f) : S →F defined by (−f)(x) = −(f(x)) for x ∈X. This equation
Basic Matrix Operations
75
specifies that the value of the function −f at input x is the additive inverse (in F) of the
scalar f(x).
Next, we define a scalar multiplication operation · : F ×V →V . Given any g : S →F in
V and any scalar c ∈F, define the function c·g : S →F by the equation (c·g)(x) = c·(g(x))
for x ∈S. The · on the right side is multiplication in the given field [or ring] F. You can
check the remaining axioms for V to be a vector space [or module], using the corresponding
properties of the field [or ring] F. We can now consider finite linear combinations of elements
of V , as in any vector space [or module]. Given fi ∈V and ci ∈F for i ∈[n], Pn
i=1 cifi ∈V
is the function from S to F sending input x ∈S to output Pn
i=1 cifi(x) ∈F.
Now assume S is a finite set. For each x ∈S, define a function ex : S →F by letting
ex(x) = 1F and ex(y) = 0F for all y ∈S with y ̸= x. Each ex is an element of V , so the set
X = {ex : x ∈S} is a subset of V . The map from S to X sending x ∈S to ex ∈X is a
bijection, because of the field axiom 1F ̸= 0F [which also holds in any nonzero ring].
Theorem on Bases of Function Spaces. Suppose S is a finite set and V is the F-vector
space of functions g : S →V . The set X = {ex : x ∈S} is a basis for V , so dimF (V ) = |S|.
Proof. First we prove X is a linearly independent set. Assume P
x∈S cxex = 0V , where each
cx ∈F. Both sides of this equation are functions with domain S. Evaluate each side at a
fixed y ∈S. Using the fact that ex(y) = 0 for x ̸= y and ey(y) = 1F , the left side evaluates
to cy1F = cy. The right side evaluates to 0V (y) = 0F . So cy = 0F for all y ∈S, proving the
linear independence of X.
Next, let f : S →F be any element of V . To prove X spans V , we prove P
x∈X f(x)ex =
f, which exhibits f as an F-linear combination of elements of X. To prove this equality
between two functions, we must show that both sides agree at any y ∈S. We compute
 X
x∈X
f(x)ex
!
(y) =
X
x∈X
f(x)ex(y) = f(y),
as needed. So X is a basis of V . By definition of dimension, dim(V ) = |X| = |S|.
4.3
Matrix Operations via Entries
We define vector space operations on matrices and n-tuples as special cases of the
constructions in the previous section. First consider the set F n of n-tuples of scalars from
F. Formally, F n is the set of all functions from the set [n] to F. Taking the set S in the
previous section to be [n], we see that F n is an F-vector space under pointwise operations on
functions. Rewriting the definitions of these operations using the list notation for n-tuples,
we obtain:
(x1, x2, . . . , xn) + (y1, y2, . . . , yn) = (x1 + y1, x2 + y2, . . . , xn + yn)
for all xi, yi ∈F;
c(x1, x2, . . . , xn) = (cx1, cx2, . . . , cxn)
for all c, xi ∈F.
The basis vector ei (where 1 ≤i ≤n) is the function such that ei(i) = 1F and ei(j) = 0F
for all j ̸= i. Using list notation, ei is a list that has 1F in position i and 0F elsewhere:
ei = (0, . . . , 1, . . . , 0). By the Theorem on Bases of Function Spaces, X = (e1, . . . , en) is an
ordered basis for F n. We call X the standard ordered basis for F n. In list notation, we have
(x1, . . . , xn) = x1e1 + · · · + xnen for all xi ∈F.
76
Advanced Linear Algebra
Note that the vector ei depends on n (the domain of ei is [n]), and this dependence is not
indicated in the notation ei. In most cases, we can infer n from context, so no ambiguity
occurs. We have dim(F n) = n.
Next, consider the set Mm,n(F) of m × n matrices over F. Letting S = [m] × [n], the
formal definition of matrices shows that Mm,n(F) is the set of all functions from S to F.
We know this set is a vector space using pointwise operations on functions. Writing the
operations using matrix entries, we get:
(A + B)(i, j) = A(i, j) + B(i, j),
(cA)(i, j) = c(A(i, j)),
where A, B ∈Mm,n(F), c ∈F, i ∈[m], and j ∈[n] are arbitrary. The basis vector e(i,j),
which we abbreviate eij, is the function from [m] × [n] to F such that eij(i, j) = 1F and
eij(i′, j′) = 0F if i′ ̸= i or j′ ̸= j. Using the array notation for matrices, eij is the array
that has 1F in row i and column j and has 0F in all other positions. By the Theorem on
Bases of Function Spaces, X = {eij : 1 ≤i ≤m, 1 ≤j ≤n} is a basis for Mm,n(F). X
is called the standard basis for Mm,n(F). If A is a matrix with i, j-entry aij, we have the
matrix identity A = Pm
i=1
Pn
j=1 aijeij. For example,
 2
−1
0
6

= 2
 1
0
0
0

+ (−1)
 0
1
0
0

+ 0
 0
0
1
0

+ 6
 0
0
0
1

.
As before, the meaning of eij does depend on m and n. We have dim(Mm,n(F)) = |X| = mn.
Similar comments apply to I ×J matrices for any index sets I and J. The set of all such
matrices is a vector space with basis X = {eij : i ∈I, j ∈J} and dimension |X| = |I| · |J|.
For each positive integer n, we have n-dimensional vector spaces Mn,1(F), M1,n(F), and
F n, consisting (respectively) of column vectors of length n, row vectors of length n, and
n-tuples. You can check that the bijections between these sets (described earlier) are vector
space isomorphisms. For i ∈[n], the standard basis vectors ei1 ∈Mn,1(F), e1i ∈M1,n(F),
and ei ∈F n correspond to each other under these isomorphisms.
We now define the operations of matrix multiplication, matrix-vector multiplication,
matrix transpose, and conjugate-transpose one entry at a time. Let A ∈Mm,n(F) and
B ∈Mn,p(F). The matrix product AB is the m × p matrix defined by
(AB)(i, j) =
n
X
k=1
A(i, k)B(k, j),
where 1 ≤i ≤m, 1 ≤j ≤p.
By letting m = 1 or p = 1, we get the formulas for multiplying a matrix on the left or right
by a row vector or column vector. Specifically, if A ∈Mm,n(F) and v ∈F n, then Av ∈F m
is defined by
(Av)(i) =
n
X
k=1
A(i, k)v(k)
for i ∈[m].
Here, we identify lists in F n and F m with column vectors, writing (Av)(i) = (Av)(i, 1) and
v(k) = v(k, 1). Alternatively, if w ∈F m and A ∈Mm,n(F), then wA ∈F n is given by
(wA)(j) =
m
X
k=1
w(k)A(k, j)
for j ∈[n],
where we now identify lists in F m and F n with row vectors.
Basic Matrix Operations
77
For example, if A =
 2
−1
4
0
1
3

∈M2,3(R), B =


3
2
1
1
−2
5

∈M3,2(R), and v =
(4, 0, 1) ∈R3, then
AB =
 −3
23
−5
16

, BA =


6
−1
18
2
0
7
−4
7
7

, vB = [10 13],
and Av =
 12
3

.
Note that we view the 3-tuple v as a row vector when computing vB, but we view v as a
column vector when computing Av.
For A ∈Mm,n(F), the transpose of A, written AT, is the n × m matrix with i, j-entry
AT(i, j) = A(j, i)
for 1 ≤i ≤n, 1 ≤j ≤m.
When F = C, the conjugate-transpose of A, written A∗, is the n × m complex matrix with
i, j-entry
A∗(i, j) = A(j, i)
for 1 ≤i ≤n, 1 ≤j ≤m.
The bar denotes complex conjugation: x + iy = x −iy for all x, y ∈R. For example, if
A =

2 + i
−3i
4
1 −2i
0
eπi/4

∈M2,3(C), then
AT =


2 + i
1 −2i
−3i
0
4
eπi/4

and A∗=


2 −i
1 + 2i
3i
0
4
e−πi/4

.
We make analogous definitions for matrices with rows and columns indexed by arbitrary
finite sets. The product of an I × K matrix A and a K × J matrix B is the I × J matrix
C = AB with entries C(i, j) = P
k∈K A(i, k)B(k, j) for all i ∈I and j ∈J. The transpose
of an I × J matrix D is the J × I matrix DT with entries DT(j, i) = D(i, j) for all j ∈J
and i ∈I. All properties proved below generalize at once to this setting.
4.4
Properties of Matrix Multiplication
We use the definition of the product of two matrices to prove some fundamental algebraic
properties of matrix multiplication. Let A, A′ ∈Mm,n(F) and B, B′ ∈Mn,p(F). First,
consider the two distributive laws A(B + B′) = AB + AB′ and (A + A′)B = AB + A′B. To
prove the first of these laws, compute the i, j-entry of each side, for i ∈[m] and j ∈[p]:
[A(B + B′)](i, j)
=
n
X
k=1
A(i, k)(B + B′)(k, j) =
n
X
k=1
A(i, k)[B(k, j) + B′(k, j)]
=
n
X
k=1
[A(i, k)B(k, j) + A(i, k)B′(k, j)]
=
n
X
k=1
A(i, k)B(k, j) +
n
X
k=1
A(i, k)B′(k, j)
=
(AB)(i, j) + (AB′)(i, j) = (AB + AB′)(i, j).
78
Advanced Linear Algebra
The second distributive law is proved similarly. Next, for A ∈Mm,n(F) and B ∈Mn,p(F)
and c ∈F [and F commutative], we have c(AB) = (cA)B = A(cB). This is proved by
showing that the i, j-entry of all three expressions is Pn
k=1 cA(i, k)B(k, j). We also have
AIn = A = ImA. For instance, we prove ImA = A by computing
(ImA)(i, j) =
m
X
k=1
Im(i, k)A(k, j) = A(i, j)
for all i ∈[m] and j ∈[n],
since Im(i, k) is 1 for k = i and 0 otherwise.
Given A ∈Mm,n(F), B ∈Mn,p(F), and C ∈Mp,q(F), we prove the associative law
(AB)C = A(BC). Note first that AB ∈Mm,p(F) and BC ∈Mn,q(F). So both products
(AB)C and A(BC) are defined and give outputs in Mm,q(F). To prove (AB)C = A(BC),
fix arbitrary i ∈[m] and j ∈[q]. On one hand,
[(AB)C](i, j)
=
p
X
k′=1
(AB)(i, k′)C(k′, j) =
p
X
k′=1
 n
X
k=1
A(i, k)B(k, k′)
!
C(k′, j)
=
p
X
k′=1
n
X
k=1
(A(i, k)B(k, k′))C(k′, j).
On the other hand,
[A(BC)](i, j)
=
n
X
k=1
A(i, k)(BC)(k, j) =
n
X
k=1
A(i, k)
 
p
X
k′=1
B(k, k′)C(k′, j)
!
=
n
X
k=1
p
X
k′=1
A(i, k)(B(k, k′)C(k′, j)).
The two final expressions are equal, since the finite sums (indexed by k and k′) can be
interchanged and since multiplication is associative in the field F. [The proof works for any
ring F, possibly non-commutative.]
Letting m = n = p = q, the results proved above show that the F-vector space Mn(F)
is an associative F-algebra with identity (see §1.3). This means that Mn(F) is an F-vector
space and a ring (with identity In) such that c(AB) = (cA)B = A(cB) for all c ∈F and all
A, B ∈Mn(F).
We conclude this section with some properties of the transpose and conjugate-transpose
operations. Let A, B ∈Mm,n(F) and C ∈Mn,p(F). We have (A + B)T = AT + BT since
the i, j-entry of both sides is A(j, i) + B(j, i) for all i ∈[n] and j ∈[m]. For c ∈F, we have
(cA)T = c(AT) since the i, j-entry of both sides is cA(j, i). We have (AC)T = CTAT since,
for all i ∈[p] and j ∈[m],
(AC)T(i, j) = (AC)(j, i) =
n
X
k=1
A(j, k)C(k, i) =
n
X
k=1
CT(i, k)AT(k, j) = (CTAT)(i, j).
[Commutativity of multiplication in F is needed in this proof.] When F = C, the conjugate-
transpose operation has similar properties: (A + B)∗= A∗+ B∗; (cA)∗= c(A∗); and
(AC)∗= C∗A∗. The proofs are similar to those just given. For instance, the second property
is true because, for all i ∈[n] and j ∈[m],
(cA)∗(i, j) = (cA)(j, i) = c(A(j, i)) = c · A(j, i) = c(A∗(i, j)) = (cA∗)(i, j).
We also have (AT)T = A and, for F = C, (A∗)∗= A.
Basic Matrix Operations
79
4.5
Generalized Associativity
We now prove a general associativity result that applies to products of three or more
matrices whose dimensions match. Informally, the result says that no parentheses are
required to evaluate such a product unambiguously (although the order of the factors
certainly matters in general). Formally, suppose we are given s matrices A1, . . . , As and
integers n0, . . . , ns such that Ai ∈Mni−1,ni(F) for each i. Suppose also that we are given
any complete parenthesization of the sequence A1A2 · · · As, which specifies exactly how
these matrices are to be combined via the binary operation of matrix multiplication. For
example, if s = 4, there are five possible complete parenthesizations:
A1(A2(A3A4)), ((A1A2)A3)A4, (A1A2)(A3A4), A1((A2A3)A4), (A1(A2A3))A4.
We define the standard complete parenthesization to be the n0 × ns matrix
s
Y
i=1
Ai = (· · · (((A1A2)A3)A4) · · · )As.
Formally, the symbol Qs
i=1 Ai is defined recursively by Q1
i=1 Ai = A1 and Qs
i=1 Ai =
(Qs−1
i=1 Ai)As for all s > 1.
Generalized Associative Law for Matrix Multiplication. Suppose A1, . . . , As are
matrices with Ai ∈Mni−1,ni(F) for each i. Any complete parenthesization of A1A2 · · · As
equals the standard parenthesization Qs
i=1 Ai.
Proof. We prove the result by induction on s. The result holds for s = 1 and s = 2
since there is only one possible parenthesization. The result holds for the case s = 3 by
the associative law proved in the previous section. Fix s > 3, and assume the result is
known to hold for smaller values of s. Given any complete parenthesization of A1 · · · As,
there exists a unique index t < s such that the last binary product operation involved in
the computation involves multiplying some complete parenthesization of A1 · · · At by some
complete parenthesization of At+1 · · · As. For example, for the five parenthesizations listed
above in the case s = 4, we have t = 1, t = 3, t = 2, t = 1, and t = 3, respectively. By
induction, the complete parenthesization of the first t factors evaluates to the n0×nt matrix
B = Qt
i=1 Ai, while the complete parenthesization of the last s −t factors evaluates to the
nt ×ns matrix C = Qs
j=t+1 Aj. If t = s−1, this second product is As, so the given complete
parenthesization of A1 · · · As evaluates to BAs = Qs
i=1 Ai by definition. Otherwise, in the
case t < s −1, set D = Qs−1
j=t+1 Aj. We have C = DAs by definition, so that the given
complete parenthesization of A1 · · · As evaluates to BC = B(DAs). By associativity for
three factors and the induction hypothesis, B(DAs) = (BD)As = (Qs−1
i=1 Ai)As = Qs
i=1 Ai.
There is a formula for the entries of Qs
u=1 Au that extends the formula for a product of
two matrices. Specifically, the i, j-entry of Qs
u=1 Au is
n1
X
k1=1
n2
X
k2=1
· · ·
ns−1
X
ks−1=1
A1(i, k1)A2(k1, k2)A3(k2, k3) · · · As−1(ks−2, ks−1)As(ks−1, j).
80
Advanced Linear Algebra
We prove this formula by induction on s. The base case s = 2 holds by definition of matrix
multiplication. Fix s > 2, and assume the formula holds for products of s −1 matrices. By
definition of matrix multiplication, the i, j-entry of Qs
u=1 Au = (Qs−1
u=1 Au)As is
ns−1
X
k=1
"s−1
Y
u=1
Au
#
(i, k)As(k, j).
By induction hypothesis, this entry equals
ns−1
X
k=1


n1
X
k1=1
· · ·
ns−2
X
ks−2=1
A1(i, k1)A2(k1, k2) · · · As−1(ks−2, k)

As(k, j).
Renaming the summation variable k to be ks−1, using the distributive law in F, and
reordering the finite summations, we obtain the required formula for the i, j-entry of a
product of s factors.
Now that we know the products are unambiguously defined, we can prove that
(A1A2 · · · As)T = AT
s · · · AT
2 AT
1 and, when F = C, (A1A2 · · · As)∗= A∗
s · · · A∗
2A∗
1. We
already proved these identities for s = 2, and the general case follows by induction using
generalized associativity. With generalized associativity in hand, we can also define the
positive powers of a square matrix A ∈Mn(F). For each integer s ≥1, we let As = Qs
i=1 Ai
where every Ai = A. Informally, As is the product of s copies of A. We also define A0 = In.
4.6
Invertible Matrices
A matrix A with entries in F is invertible iff A is square (say n×n) and there exists a matrix
B ∈Mn(F) with AB = In = BA. B is called an inverse of A. If such a matrix B exists,
then B is unique. To prove this, suppose B1 ∈Mn(F) also satisfies AB1 = In = B1A. Then
associativity of matrix multiplication gives
B1 = B1In = B1(AB) = (B1A)B = InB = B.
Given an invertible matrix A, define A−1 to be the unique inverse of A. For particular
matrices C, D ∈Mn(F), we can prove that C−1 exists and that C−1 = D by verifying
the defining condition CD = In = DC. The next theorem illustrates how to use this proof
technique.
Theorem on Closure Properties of Matrix Inverses.
(a) For all A ∈Mn(F), if A is invertible, then A−1 is invertible and (A−1)−1 = A.
(b) The identity matrix In is invertible, with I−1
n
= In.
(c) For all invertible U, V ∈Mn(F), UV is invertible, and (UV )−1 = V −1U −1.
Proof. To prove (a), suppose A ∈Mn(F) is invertible. Replace C by A−1 and D by A in the
remark preceding the theorem. Since A−1A = In = AA−1 by definition of A−1, the remark
tells us that (A−1)−1 exists and (A−1)−1 = A. To prove (b), replace C by In and D by In
and observe that InIn = In = InIn. To prove (c), fix invertible matrices U, V ∈Mn(F).
Compute
(UV )(V −1U −1) = U(V V −1)U −1 = UInU −1 = UU −1 = In.
A similar computation shows that (V −1U −1)(UV ) = In. The required conclusion follows
by taking C = UV and D = V −1U −1 in the remark.
Basic Matrix Operations
81
Given k invertible matrices U1, U2, . . . , Uk ∈Mn(F), the product U1U2 · · · Uk is also
invertible, and (U1U2 · · · Uk)−1 = U −1
k
· · · U −1
2 U −1
1 . You can prove this by induction on k,
using the case k = 2 that we just proved. (Note that generalized associativity is tacitly
needed here and whenever we write unparenthesized products of matrices.) If we take all
Ui equal to a given invertible matrix A, then we get (Ak)−1 = (A−1)k for all k ≥1. For
any invertible A ∈Mn(F) and positive integer k, define the negative power A−k to be
(A−1)k = (Ak)−1.
For all fields F and all positive integers n, let GLn(F) be the set of all invertible matrices
in Mn(F). The set GLn(F) is a group under matrix multiplication, which is almost always
non-commutative. The group axioms (closure, associativity, identity, and inverses) follow
from the preceding theorem. GLn(F) is called the general linear group of degree n.
For any matrix A ∈Mn(F), A is invertible iff the transpose AT is invertible, in which
case (AT)−1 = (A−1)T. To see this, first assume A−1 exists. Then
(AT)(A−1)T = (A−1A)T = (In)T = In = (In)T = (AA−1)T = (A−1)T(AT),
so the conclusion follows by taking C = AT and D = (A−1)T in the remark preceding the
theorem. So the forward implication holds for all matrices. Applying the forward implication
with A replaced by AT, we conclude that the invertibility of AT implies the invertibility of
(AT)T = A, which gives the reverse implication for the original A. An entirely analogous
proof, using the fact that (In)∗= In, shows that for all A ∈Mn(C), A is invertible iff A∗
is invertible, in which case (A∗)−1 = (A−1)∗.
[The rest of this section applies to fields and commutative rings F.] The next few
paragraphs assume familiarity with determinants, which we study in detail in Chapter 5.
Recall that for every square matrix A ∈Mn(F), there is an associated scalar det(A) ∈F
called the determinant of A. A famous theorem of linear algebra states that for any field F
and any A ∈Mn(F), A−1 exists iff det(A) ̸= 0F . In §5.11, we prove the more general fact
that for any commutative ring R and any A ∈Mn(R), A−1 exists in Mn(R) iff det(A)
is an invertible element of the ring R. The proof provides explicit formulas (involving
determinants) for every entry of A−1, when the inverse exists. We also prove in §5.13 that
for any commutative ring R and all A, B ∈Mn(R), det(AB) = det(A) det(B).
In our definition of an invertible matrix A, we require that A be square and that the
inverse matrix B satisfy both AB = In and BA = In. In fact, given that A and B are square,
either of the conditions AB = In and BA = In implies the other one. For example, assume
A, B ∈Mn(F) and AB = In. Taking determinants gives det(A) det(B) = det(In) = 1F .
Since F is commutative, we also have 1F = det(B) det(A), so that det(A) is an invertible
element of F. By one of the theorems quoted in the last paragraph, A−1 (the unique two-
sided inverse of A) exists. Multiplying both sides of AB = In on the left by A−1, we get
B = A−1, so that BA = A−1A = In. By a similar proof, you can show that BA = In
implies AB = In. However, our proof (which uses determinants) relies crucially on the fact
that the matrices in question are square. For A ∈Mm,n(F), we say B ∈Mn,m(F) is a left
inverse of A iff BA = In. We say B ∈Mn,m(F) is a right inverse of A iff AB = Im. When
m ̸= n, you can give examples where B is a left inverse but not a right inverse of A, and
where C is a right inverse but not a left inverse of A.
We now prove that for all U, V ∈Mn(F), U and V are invertible iff UV is invertible. We
proved the forward implication already by checking that (UV )−1 = V −1U −1. Conversely,
assume UV is invertible with inverse W ∈Mn(F). Then U(V W) = (UV )W = In, so
the result proved in the last paragraph shows that V W is the inverse of U. Similarly,
(WU)V = W(UV ) = In shows that WU is the inverse of V . More generally, induction on
k shows that for U1, . . . , Uk ∈Mn(F), every Ui is invertible iff the product U1U2 · · · Uk is
invertible. These results also follow quickly from the determinant criterion for invertibility.
82
Advanced Linear Algebra
4.7
Matrix Operations via Columns
We have seen that any m × n matrix A can be identified with the list of its columns in
(F m)n: A = (A[1], A[2], . . . , A[n]), where A[j] = (A(1, j), A(2, j), . . . , A(m, j)) is column j of
A. In symbols, A[j](i) = A(i, j) for 1 ≤i ≤m and 1 ≤j ≤n. It is fruitful to recast the
definitions of the matrix operations in terms of the columns of the matrices involved.
First, assume A, B ∈Mm,n(F) and c ∈F. For each column j, (A + B)[j] = A[j] + B[j]
since, for 1 ≤i ≤m, (A + B)[j](i) = (A + B)(i, j) = A(i, j) + B(i, j) = A[j](i) + B[j](i) =
(A[j] + B[j])(i). Similarly, you can check that (cA)[j] = c · A[j]. This means that we can add
two matrices one column at a time, and multiplying a matrix by a scalar multiplies each
column by that scalar.
Next, assume A ∈Mm,n(F) and B ∈Mn,p(F), and let C = AB ∈Mm,p(F). How are
the columns of A and B related to the columns of C? To answer this, fix i, j with 1 ≤i ≤m
and 1 ≤j ≤p. Compute
C[j](i) = (AB)(i, j) =
n
X
k=1
A(i, k)B(k, j) =
n
X
k=1
A(i, k)B[j](k) = (A(B[j]))(i).
The last equality is the definition of multiplying the matrix A on the right by the column
vector B[j]. So (AB)[j] = A(B[j]) for all j, which means that column j of AB is found by
multiplying matrix A on the right by column j of B. For example, the second column of the
product


2
0
−1
3
0
4



π
2
e
3i
√
37
9!
3
3√−11
109
(2 + 3i)7

is


2
0
−1
3
0
4



2
3

=


4
7
12

.
Now consider the product Av, where A is an m × n matrix and v is an n × 1 column
vector. For all i ∈[m],
(Av)(i) =
n
X
j=1
A(i, j)v(j) =
n
X
j=1
A[j](i)v(j) =


n
X
j=1
A[j]v(j)

(i).
So Av = Pn
j=1 A[j]v(j) in F m [or Av = Pn
j=1 v(j)A[j] for commutative rings F], which
means that the matrix-vector product Av is a linear combination of the columns of A, and
the coefficients in this linear combination are the entries in v. For example,


1
2
0
−1
3
i
−2
4
1/2
0
0
π




2
−1
5
0

=


0
−4 −i
1

= 2


1
3
1/2

−1


2
i
0

+5


0
−2
0

+0


−1
4
π

.
[The rest of this section assumes F is a field.] Let us deduce some consequences of the
fact that Av is a linear combination of the columns of A. Given A ∈Mm,n(F), define the
range or image of A to be the set R(A) = {Av : v ∈F n} ⊆F m, which is readily seen to be
a subspace of F m. Our formula for Av shows that
R(A) = {v(1)A[1] + · · · + v(n)A[n] : v(i) ∈F}.
Basic Matrix Operations
83
So the range of A coincides with the subspace of all linear combinations of the columns of
A in F m. This subspace is also called the column space of A and may be written Col(A).
The dimension of this subspace is called the column rank of A, written colrk(A).
For another application, consider a general matrix product AB, where A ∈Mm,n(F)
and B ∈Mn,p(F). We prove that colrk(AB) ≤min(colrk(A), colrk(B)), or (equivalently)
colrk(AB) ≤colrk(A) and colrk(AB) ≤colrk(B). First, since (AB)[j] = A(B[j]), where
B[j] is a column vector, we see that column j of AB is a linear combination of the columns
of A, where the coefficients come from column j of B. In particular, each column (AB)[j]
belongs to the column space of A. Knowing that (AB)[j] ∈Col(A) for all j, it follows that
Col(AB) ⊆Col(A) since the columns of AB generate the column space of AB. Taking
dimensions, we see that colrk(AB) ≤colrk(A). Next, we show that colrk(AB) ≤colrk(B).
Define a map LA : F n →F m by sending each column vector v ∈F n to the column vector
LA(v) = Av ∈F m. You can check that LA is a linear map. Let W = Col(B) ⊆F n, and
recall that W is spanned by the columns B[1], . . . , B[p] of B. Applying the linear map LA,
we see (using Exercise 63 of Chapter 1) that LA[W] is spanned by the vectors LA(B[j]) =
A(B[j]) = (AB)[j], for j = 1, . . . , p. The latter vectors are exactly the columns of AB. We
conclude that Col(AB) = LA[W]. Since the column space of AB is the image of the column
space of B under a linear map, we must have dim(Col(AB)) ≤dim(Col(B)) (see Exercise 61
in Chapter 1). In other words, colrk(AB) ≤colrk(B), as needed.
Suppose A ∈Mm,n(F). If P is any invertible m × m matrix, then we have inequalities
colrk(A) ≥colrk(PA) ≥colrk(P −1(PA)) = colrk(A),
so that colrk(PA) = colrk(A). If Q is any invertible n × n matrix, then the inequalities
colrk(A) ≥colrk(AQ) ≥colrk((AQ)Q−1) = colrk(A)
show that colrk(AQ) = colrk(A). Thus, multiplication on the left or right by an invertible
matrix does not change the column rank. In particular, let A be the identity matrix In,
which has column rank n since its columns form the basis (e1, . . . , en) of F n. We deduce
that every invertible n × n matrix has column rank n.
4.8
Matrix Operations via Rows
We now rewrite the definitions of matrix operations in terms of the rows of the matrices
and vectors involved. This time, we start with the fact that any m × n matrix A can
be identified with the list of its rows in (F n)m: A = (A[1], A[2], . . . , A[m]), where A[i] =
(A(i, 1), A(i, 2), . . . , A(i, n)) is row i of A. In symbols, A[i](j) = A(i, j) for 1 ≤i ≤m and
1 ≤j ≤n.
As in the case of columns, a short calculation shows that (A + B)[i] = A[i] + B[i] and
(cA)[i] = c(A[i]) for all A, B ∈Mm,n(F), all c ∈F, and all i ∈[m]. This means that we can
add two matrices of the same size one row at a time, and multiplying a matrix by a scalar
multiplies each row by that scalar.
Next, assume A ∈Mm,n(F) and B ∈Mn,p(F), and let C = AB ∈Mm,p(F). How are
the rows of A and B related to the rows of C? To answer this, fix i, j with 1 ≤i ≤m and
1 ≤j ≤p. Compute
C[i](j) = (AB)(i, j) =
n
X
k=1
A(i, k)B(k, j) =
n
X
k=1
A[i](k)B(k, j) = (A[i]B)(j).
84
Advanced Linear Algebra
So (AB)[i] = (A[i])B for all i ∈[m], which means that row i of AB is found by multiplying
matrix B on the left by row i of A. For example, the third row of the product


2
0
−1
3
0
4


 5
7
9
11
13
1
1
1
1
10

is
 0
4   5
7
9
11
13
1
1
1
1
10

=
 4
4
4
4
40 
.
Now consider the product wA, where A is an m×n matrix and w is a 1×m row vector.
For all j ∈[n],
(wA)(j) =
m
X
i=1
w(i)A(i, j) =
m
X
i=1
w(i)A[i](j) =
 m
X
i=1
w(i)A[i]
!
(j).
So wA = Pm
i=1 w(i)A[i] in F n [and the scalars w(i) must appear on the left for non-
commutative rings F]. This says that the vector-matrix product wA is a linear combination
of the rows of A, and the coefficients in this linear combination are the entries in w. For
example,
 3
0
−2 


1
2
0
3
i
−2
1/2
0
1

= 3
 1
2
0 
+ 0
 3
i
−2 
−2
 1/2
0
1 
.
[The rest of this section assumes F is a field.] Given A ∈Mm,n(F), define the row space
of A to be the subspace Row(A) ⊆F n spanned by the m rows of A, which consists of all
F-linear combinations of the rows of A. Define the row rank of A, written rowrk(A), to be
the dimension of the row space of A. Reasoning as we did earlier for columns, our formula for
wA shows that Row(A) = {wA : w ∈F m} ⊆F n. Furthermore, for all B ∈Mn,p(F), each
row i of the matrix product AB has the form (AB)[i] = (A[i])B. So row i of AB is a linear
combination of the rows of B with coefficients coming from row i of A. Since the rows of AB
generate Row(AB), we conclude that Row(AB) ⊆Row(B) and rowrk(AB) ≤rowrk(B).
On the other hand, consider the linear map RB : F n →F p that sends a row vector
w ∈F n to the row vector RB(w) = wB ∈F p. Let V = RB[Row(A)] be the image of
Row(A) under this linear map. Since Row(A) is spanned by A[1], . . . , A[m], V is spanned
by RB(A[1]), . . . , RB(A[m]). In other words, V is spanned by A[1]B = (AB)[1], . . . , A[m]B =
(AB)[m]. But these are exactly the rows of AB, and hence V = Row(AB). Since V =
RB[Row(A)], we must have dim(V ) ≤dim(Row(A)), and hence rowrk(AB) ≤rowrk(A). In
summary,
rowrk(AB) ≤min(rowrk(A), rowrk(B)).
Let A ∈Mm,n(F) be any matrix. If P is any invertible m × m matrix, then
rowrk(A) ≥rowrk(PA) ≥rowrk(P −1(PA)) = rowrk(A),
so rowrk(PA) = rowrk(A). Similarly, if Q is any invertible n × n matrix, then
rowrk(A) ≥rowrk(AQ) ≥rowrk((AQ)Q−1) = rowrk(A),
so rowrk(AQ) = rowrk(A). Thus, multiplication on the left or right by an invertible matrix
does not change the row rank. In particular, let A be the identity matrix In, which has row
rank n since its rows form the basis (e1, . . . , en) of F n. We deduce that every invertible
Basic Matrix Operations
85
n × n matrix has row rank n. In §4.12, we prove that rowrk(A) = colrk(A) for every matrix
A ∈Mm,n(F).
Before leaving this section, we mention interpretations for the transpose and conjugate-
transpose operations in terms of rows and columns. Suppose A is an m × n matrix. From
the definition AT(i, j) = A(j, i), we see immediately that (AT)[i](j) = AT(i, j) = A(j, i) =
A[i](j) for 1 ≤j ≤m, so that (AT)[i] = A[i] for all i ∈[n]. Similarly, (AT)[j](i) = AT(i, j) =
A(j, i) = A[j](i) for all i ∈[n] implies that (AT)[j] = A[j] for all j ∈[m]. Translating these
equations into words, we see that: row i of AT is column i of A (for all i ∈[n]), while column
j of AT is row j of A (for all j ∈[m]). Thus, the matrix transpose operation interchanges
the rows and columns of A. An analogous calculation when F = C shows that (A∗)[i] = A[i]
and (A∗)[j] = A[j], where the complex conjugate of a row or column vector is found by
conjugating each entry. So, we obtain A∗from A by interchanging rows and columns and
taking the complex conjugate of every entry.
4.9
Elementary Operations and Elementary Matrices
This section describes the link between elementary row and column operations on matrices
and multiplication by certain matrices called elementary matrices. There are three types of
elementary row operations that we can apply to an m × n matrix A. First, for any i ∈[m],
we can multiply row i of A by a nonzero scalar c in the field F. [In the case of rings, we
require c to be an invertible element (unit) in the ring F. For non-commutative rings, each
entry of the row is multiplied on the left by c.] Second, for any i, j in [m] with i ̸= j, we can
interchange row i and row j of A. Third, for any i, j in [m] with i ̸= j and any scalar c ∈F,
we can add c times row j to row i of A [multiplying by c on the left for non-commutative
rings]. There are three analogous elementary column operations that act on the columns
of A. [For non-commutative rings, we multiply column entries by c on the right.] These
elementary operations are helpful for solving systems of linear equations, for computing
normal forms and other invariants of matrices, and for other applications.
We now define three kinds of elementary matrices. First, for i ∈[p] and nonzero c ∈F
[or units c in the case of rings], let E1
p[c; i] be the p × p matrix with i, i-entry equal to c,
j, j-entries equal to 1F for j ̸= i, and all other entries equal to 0F . Second, for i ̸= j in [p],
let E2
p[i, j] be the p × p matrix with 1F in positions (i, j), (j, i), and (k, k) for all k ̸= i, j,
and 0F elsewhere. Third, for c ∈F and i ̸= j in [p], let E3
p[c; i, j] be the p × p matrix
having 1F in each diagonal position (k, k), c in position (i, j), and 0F elsewhere. We prove
below that the three types of elementary matrices encode the three types of elementary
row or column operations defined above. As a mnemonic aid, note that we can obtain the
elementary matrix for a given elementary operation by performing that operation on the
p × p identity matrix Ip. For example,
E1
3[4; 2] =


1
0
0
0
4
0
0
0
1

;
E2
3[2, 3] =


1
0
0
0
0
1
0
1
0

;
E3
3[3; 2, 1] =


1
0
0
3
1
0
0
0
1

.
Fix an m × n matrix A, and take p = m. We claim that multiplying A on the left by
an elementary matrix has the same effect as applying the corresponding elementary row
operation to A. This is not hard to check by writing formulas for all the entries, but it is
more instructive to give a conceptual proof using the ideas from §4.8. For example, consider
the matrix B = E3
m[c; i, j]A. Row k of B is E3
m[c; i, j][k]A. This row is the linear combination
86
Advanced Linear Algebra
of the m rows of A with coefficients given by the entries in the row vector E3
m[c; i, j][k]. If
k ̸= i, then the relevant row of the elementary matrix is ek, so row B[k] is 1 times row A[k]
plus 0 times every other row of A. This means row k row of B equals row k of A for all
k ̸= i. If k = i, then E3
m[c; i, j][i] = cej + ei by definition, so row B[i] is c times A[j] plus 1
times A[i]. This means row i of B is c times row j of A plus row i of A. This proves that
multiplication on the left by E3
m[c; i, j] has the same effect as the elementary row operation
of the third type. The assertions for E1
m[c; i] and E2
m[i, j] are checked in the same way.
On the other hand, multiplying A ∈Mm,n(F) on the right by an n×n elementary matrix
has the same effect as applying the corresponding elementary column operation to A. For
example, consider the matrix C = AE2
n[i, j]. For k ∈[n], column k of C is A(E2
n[i, j][k]),
which is the linear combination of the columns of A with coefficients given by the entries
in the column vector E2
n[i, j][k]. For k ̸= i, j, this column vector is ek, so C[k] = A[k]. For
k = i, this column vector is ej, so C[i] = A[j]. For k = j, this column vector is ei, so
C[j] = A[i]. So the columns of C are precisely the columns of A with column i and column
j interchanged. The assertions for the other elementary matrices are checked in the same
way. Note carefully that to add c times row j to row i, we left-multiply by E3
m[c; i, j]; but
to add c times column j to column i, we right-multiply by E3
n[c; j, i].
The inverse of an elementary matrix exists and is an elementary matrix of the same type.
Specifically, E1
p[c; i]−1 = E1
p[c−1; i], E2
p[i, j]−1 = E2
p[i, j], and E3
p[c; i, j]−1 = E3
p[−c; i, j].
You can verify these formulas by computing with entries, but they can also be checked
by applying the remarks in the previous two paragraphs. For example, the product
E3
p[c; i, j]E3
p[−c; i, j] is the matrix obtained from E3
p[−c; i, j] by adding c times row j to
row i. Row j of E3
p[−c; i, j] is ej, and row i is ei + (−c)ej, so the new row i after the row
operation is ei. For all k ̸= i, row k of E3
p[−c; i, j] is ek, and this is unchanged by the row
operation. So E3
p[c; i, j]E3
p[−c; i, j] = Ip, and the product in the other order is the identity for
analogous reasons. The formulas for the inverses of the other types of elementary matrices
can be verified similarly.
4.10
Elementary Matrices and Gaussian Elimination
[This section assumes F is a field.] Elementary matrices can help us prove many facts
about general matrices, such as the determinant product formula det(AB) = det(A) det(B)
(see §5.9). We begin here by reviewing some basic facts concerning the Gaussian elimination
algorithm and relating this algorithm to elementary matrices. We assume readers have
already seen some version of this algorithm in the context of solving systems of linear
equations. For a more detailed study of Gaussian elimination and its connection to matrix
factorizations, see Chapter 9.
Starting with any m×n matrix A, the Gaussian elimination algorithm applies a sequence
of elementary row operations to transform A to its reduced row-echelon form, denoted Aech.
To describe Aech precisely, let us call the leftmost nonzero entry in a given nonzero row of
Aech the leading entry of that row. Then Aech is characterized by the following properties:
any rows of Aech with all entries zero (if any such rows exist) occur at the bottom of the
matrix; the leading entry of each nonzero row is 1F ; for all i ∈[m −1], the leading entry
in row i is strictly left of the leading entry in row i + 1 (if any); and for each leading entry,
the other elements in the column containing that entry are all zero. It can be shown that
for every A ∈Mm,n(F), there exists a unique reduced row-echelon form Aech that can be
reached from A by applying finitely many elementary row operations. We can prove existence
using a version of Gaussian elimination called Gauss–Jordan elimination, which produces
Basic Matrix Operations
87
the zeroes in the columns of the leading entries. The uniqueness of Aech is more subtle, but
we do not need to invoke uniqueness in what follows.
We have seen that each elementary row operation used in the passage from A to Aech
can be accomplished by multiplying on the left by an appropriate elementary matrix.
Thus, Aech = EkEk−1 · · · E1A, where Es is the elementary matrix corresponding to the
row operation applied at step s of the Gaussian elimination algorithm.
For example, let us compute the reduced row-echelon form for the matrix
A =


0
3
−4
1
2
4
−2
6
4
−1
8
9


and find a specific factorization Aech = Ek · · · E1A. To obtain a leading 1 in the 1, 1-
position, we switch row 1 and row 2 of A and then multiply the new row 1 by 1/2. This can
be accomplished by left-multiplying A first by E1 = E2
3[1, 2], and then by E2 = E1
3[1/2; 1]:
E1A =


2
4
−2
6
0
3
−4
1
4
−1
8
9

;
E2E1A =


1
2
−1
3
0
3
−4
1
4
−1
8
9

.
Next, we add −4 times row 1 to row 3, by left-multiplying by the elementary matrix
E3 = E3
3[−4; 3, 1]. We continue by adding 3 times row 2 to row 3 (left-multiply by
E4 = E3
3[3; 3, 2]):
E3E2E1A =


1
2
−1
3
0
3
−4
1
0
−9
12
−3

;
E4E3E2E1A =


1
2
−1
3
0
3
−4
1
0
0
0
0

.
To finish, we multiply row 2 by 1/3 (left-multiply by E5 = E1
3[1/3; 2]) and then add −2
times the new row 2 to row 1 (left-multiply by E6 = E3
3[−2; 1, 2]):
E5E4E3E2E1A =


1
2
−1
3
0
1
−4/3
1/3
0
0
0
0

;
Aech = E6E5E4E3E2E1A =


1
0
5/3
7/3
0
1
−4/3
1/3
0
0
0
0

.
4.11
Elementary Matrices and Invertibility
[This section assumes F is a field.] Our first theorem involving elementary matrices states
that a square matrix A is invertible iff A can be written as a product of elementary matrices.
To prove this, assume first that A is a product of elementary matrices. We know every
elementary matrix is invertible, so any product of elementary matrices is also invertible
(§4.6). Conversely, assume that A ∈Mn(F) is invertible. Write Aech = EkEk−1 · · · E1A for
certain elementary matrices Ej. Since A and each elementary matrix Ej is invertible, so
is their product Aech. It readily follows that Aech cannot have any zero rows. Since Aech
is square, the definition of reduced row-echelon form forces Aech to be the identity matrix
In. Consequently, EkEk−1 · · · E1A = In. Solving for A, we find that A = E−1
1 E−1
2
· · · E−1
k .
Since the inverse of an elementary matrix is elementary, A has been expressed as a product
of elementary matrices.
88
Advanced Linear Algebra
For example, let us express the invertible matrix A =
 2
1
4
3

as a product of
elementary matrices. We reduce A to Aech = I2 by the following sequence of elementary
row operations: add −2 times row 1 to row 2, producing
 2
1
0
1

; multiply row 1 by 1/2,
producing
 1
1/2
0
1

; add −1/2 times row 2 to row 1, producing I2. In terms of matrices,
we have
I2 = E3
2[−1
2; 1, 2]E1
2[1/2; 1]E3
2[−2; 2, 1]A =
 1
−1/2
0
1
 1/2
0
0
1

1
0
−2
1
 2
1
4
3

.
Solving for A gives
 2
1
4
3

= A = E3
2[2; 2, 1]E1
2[2; 1]E3
2[1/2; 1, 2] =
 1
0
2
1
  2
0
0
1
  1
1/2
0
1

.
4.12
Row Rank and Column Rank
[This section assumes F is a field.] As our second application of elementary matrices, we
prove that for any m × n matrix A, rowrk(A) = colrk(A). We can therefore define the rank
of A, written rank(A), as the common value of the row rank of A and the column rank of
A. Recall that we have already shown (§4.8) that multiplying a matrix on the left or right
by an invertible matrix does not change the row rank. These operations do not affect the
column rank either (§4.7). Moreover, we have seen (§4.10) that Aech = PA for some matrix
P that must be invertible (as P is a product of elementary matrices).
Having transformed A to Aech by left-multiplying by P, let us continue to simplify the
matrix Aech by applying elementary column operations. These operations are achieved by
multiplying on the right by certain elementary matrices. The leading 1 in each nonzero
row of Aech is already the only nonzero entry in its column. Adding appropriately chosen
constant multiples of this column to the columns to its right, we can arrange that the
leading 1 in each nonzero row is the only nonzero entry in its row. The matrix now consists
of some number r of nonzero rows, each of which has a single 1 in it, and these 1s appear
in distinct columns. Using column interchanges, we can arrange the matrix so that the 1s
appear in the first r positions along the main diagonal. (The main diagonal of a rectangular
m×n matrix consists of the i, i-entries for 1 ≤i ≤min(m, n).) This final matrix is obtained
from Aech = PA by right-multiplying by some matrix Q, which is a product of elementary
matrices and is therefore invertible.
In summary, given A ∈Mm,n(F), we can find invertible matrices P ∈Mm(F) and
Q ∈Mn(F) and an integer r ∈{0, 1, . . . , min(m, n)} such that (PAQ)i,i = 1 for all i
between 1 and r, and all other entries of PAQ are 0. P is the product of the elementary
matrices implementing the elementary row operations used to transform A, while Q is the
product of the elementary matrices implementing the elementary column operations. The
matrix PAQ is called the Smith canonical form or Smith normal form for A. [See Chapter 18
for a discussion of the more general case in which F is a principal ideal domain (PID).] Now
we can prove our main result. We know that rowrk(A) = rowrk(PAQ) and colrk(A) =
colrk(PAQ). It is evident from the form of PAQ that rowrk(PAQ) = r = colrk(PAQ).
Thus, rowrk(A) = colrk(A). This proof provides an algorithm for computing the rank of A.
You can also show that the row rank (and hence the column rank) of A equals the number
Basic Matrix Operations
89
of nonzero rows in the reduced row-echelon form Aech. This observation gives a faster way
to compute the rank of A.
For example, the matrix A =


0
3
−4
1
2
4
−2
6
4
−1
8
9

has reduced row-echelon form
Aech = PA =


1
0
5/3
7/3
0
1
−4/3
1/3
0
0
0
0

,
where P =


−2/3
1/2
0
1/3
0
0
3
−2
1

is the product of the six elementary matrices used to reduce
A to Aech in §4.10. We see at once that rowrk(A) = rowrk(Aech) = 2, and you can also check
quickly that colrk(A) = colrk(Aech) = 2. Let us continue transforming Aech to reach the
Smith canonical form of A. We use the following elementary column operations to remove
the nonzero entries to the right of the leading 1s in Aech: add −5/3 times column 1 to
column 3; add −7/3 times column 1 to column 4; add 4/3 times column 2 to column 3; add
−1/3 times column 2 to column 4. In terms of matrices, let
Q = E3
4[−5/3; 1, 3]E3
4[−7/3; 1, 4]E3
4[4/3; 2, 3]E3
4[−1/3; 2, 4] =


1
0
−5/3
−7/3
0
1
4/3
−1/3
0
0
1
0
0
0
0
1

.
Then PAQ =


1
0
0
0
0
1
0
0
0
0
0
0

is the Smith normal form of A. The matrix PAQ has r = 2
nonzero entries on the main diagonal, confirming that rank(A) = rowrk(A) = colrk(A) = 2.
4.13
Conditions for Invertibility of a Matrix
[This section assumes F is a field.] Table 4.1 displays one of the central theorems of matrix
algebra, which gives a long list of conditions, each of which is logically equivalent to the
invertibility of a square matrix A. Condition (8b) says that the system of linear equations
Ax = b always has a solution x for any right side b. Condition (10b) says the system Ax = b
always has a unique solution x for any right side b. Condition (9b) says the homogeneous
system Ax = 0 has no solutions other than the zero solution x = 0. The proof shows that
these conditions can be reformulated by saying a certain linear map LA (left multiplication
by A) is surjective, or bijective, or injective. To detect when all these conditions hold, it is
sufficient to check that A has nonzero determinant, or to test the rows (or columns) of A
for linear independence, or to see if the reduced row-echelon form of A is In. We also see
from conditions (2a) and (2b) that A is invertible (meaning A has a two-sided inverse) if A
has a left inverse or if A has a right inverse.
We now prove the theorem in Table 4.1. We must verify many conditional and
biconditional statements.
(1)⇒(2a): If A is invertible, then (2a) follows by choosing B = A−1.
(1)⇒(2b): If A is invertible, then (2b) follows by choosing C = A−1.
90
Advanced Linear Algebra
TABLE 4.1
Theorem on Equivalent Conditions for Invertibility of a Matrix.
Assume F is a field, A ∈Mn(F), and LA : F n →F n is the linear map given by LA(x) = Ax
for all column vectors x ∈F n. The following conditions are all equivalent:
(1)
A is invertible.
(2a)
BA = In for some B ∈Mn(F).
(2b)
AC = In for some C ∈Mn(F).
(3)
det(A) ̸= 0F .
(4)
A is a product of finitely many elementary matrices.
(5)
The reduced row-echelon form Aech is In.
(6a)
rowrk(A) = n (the row space of A has dimension n).
(6b)
The n rows of A form a linearly independent list in F n.
(7a)
colrk(A) = n (the column space of A has dimension n).
(7b)
The n columns of A form a linearly independent list in F n.
(7c)
The range of A has dimension n.
(8a)
The map LA is surjective (onto).
(8b)
For all b ∈F n, there exists x ∈F n with Ax = b.
(9a)
The map LA is injective (one-to-one).
(9b)
For all x ∈F n, if Ax = 0 then x = 0.
(9c)
The null space of A (the kernel of LA) is {0}.
(10a)
The map LA is bijective (and hence an isomorphism).
(10b)
For all b ∈F n, there exists a unique x ∈F n with Ax = b.
(11)
AT is invertible.
(12)
[when F = C] A∗is invertible.
(2a)⇒(3): Given BA = In, take determinants and use the product formula to get
det(B) det(A) = det(In) = 1F . Since 1F ̸= 0F in a field F, this forces det(A) ̸= 0F .
(2b)⇒(3): The proof is analogous to (2a)⇒(3). (Similarly, (1) implies (3).)
(3)⇒(1): If det(A) ̸= 0F , then we can use the explicit formula for A−1 (see §5.11) to
confirm that A has a two-sided inverse. This formula involves a division by det(A), which
is why we need A to have nonzero determinant.
(1)⇔(4): We proved this in §4.11.
(1)⇔(5): For any matrix A, we saw in §4.10 that Aech = PA, where P ∈Mn(F) is a
product of elementary matrices, hence is invertible. So, A is invertible iff Aech is invertible
(§4.6). Let B ∈Mn(F) be any square matrix in reduced row-echelon form. Using the
definition of reduced row-echelon form, we see that B has no zero rows iff every row of
B contains a leading 1 iff every column of B contains a leading 1 iff B = In. Since In is
invertible and no matrix with a row of zeroes is invertible, we see that B is invertible iff
B = In. Applying this remark to B = Aech, we see that A is invertible iff Aech = In.
(1)⇔(6a): As above, Aech = PA for some invertible matrix P. Multiplying by an
invertible matrix does not change the row rank, so rowrk(Aech) = rowrk(A). If A is
invertible, then Aech = In (since (1) implies (5)), so rowrk(Aech) = n, so (6a) is true.
Conversely, if (6a) holds, then rowrk(Aech) = n, so Aech cannot have any zero rows. As
shown in the last paragraph, this forces Aech = In. Since (5) implies (1), A is invertible.
(6a)⇔(6b): By definition, the row space of A is spanned by the n rows of A. If (6b)
holds, then the list of n rows of A is a basis for the row space of A, so that rowrk(A) = n.
Conversely, if (6b) fails, then we can delete one or more vectors from the list of rows to get
a basis for the row space, forcing rowrk(A) < n.
Basic Matrix Operations
91
(6a)⇔(7a): This follows from the identity rowrk(A) = colrk(A), valid for all matrices A.
We proved this identity in §4.12.
(7a)⇔(7b)⇔(7c): We prove the equivalence of (7a) and (7b) in the same way we proved
the equivalence of (6a) and (6b). The range of A is the same as the column space of A
(§4.7), so (7a) is equivalent to (7c) by definition.
(7c)⇔(8a): The range of the matrix A is the subspace {Ax : x ∈F n} = {LA(x) : x ∈
F n}, which is precisely the image of the map LA. Note LA : F n →F n is surjective iff the
image of LA is all of F n iff the dimension of the image is n (since dim(F n) = n, but all
proper subspaces of F n have dimension smaller than n) iff the range of A has dimension n.
(8a)⇔(8b): Once we recall that Ax = LA(x), we see that (8b) is the very definition of
LA being surjective.
(8a)⇔(9c): Let the linear map LA : F n →F n have image R and kernel N. We know R
is the range of A. Moreover, N is the null space of A, since x ∈N iff LA(x) = 0 iff Ax = 0.
By the Rank–Nullity Theorem (see §1.8), dim(R) + dim(N) = dim(F n) = n. Now (8a) is
equivalent to (7c), which says dim(R) = n. In turn, this is equivalent to dim(N) = 0, which
is equivalent to N = {0}.
(9a)⇔(9c): The linear map LA is injective iff its kernel is {0}. As observed above, the
kernel of LA is the same as the null space of A.
(9b)⇔(9c): The null space of A equals {0} iff the null space of A is a subset of {0}.
Expanding the definition of the latter condition, we obtain (9b).
(9a)⇔(10a): Assume (9a) holds. Then (8a) holds also, so LA is injective and surjective,
so (10a) holds. The converse is immediate, since all bijections are one-to-one.
(10a)⇔(10b): Recalling that Ax = LA(x), we see that (10b) is none other than the
definition of LA being a bijection.
(1)⇔(11),(1)⇔(12): We proved these equivalences in §4.6.
4.14
Block Matrix Multiplication
Block matrix multiplication is a useful method of computing the product of matrices that
are built from smaller submatrices. As an example of this method, suppose X and Y are
matrices built from smaller matrices (with subscripts indicating their sizes) as follows:
X =
 P3×2
Q3×3
R5×2
S5×3

,
Y =
 A2×3
B2×4
C2×1
D3×3
E3×4
F3×1

.
(4.2)
Block matrix multiplication lets us compute XY (in block form) by treating each block as
if it were an individual matrix entry. In this example, the answer is
XY =
 PA + QD
PB + QE
PC + QF
RA + SD
RB + SE
RC + SF

,
where the upper-left block is 3 × 3, the block below it is 5 × 3, and so on.
To state and prove the general rule for block matrix multplication, we need to use general
index sets for the rows and columns of matrices. Suppose X is an I × J matrix and Y is a
J × K matrix. Recall that a set partition of a set S is a collection of nonempty, pairwise
disjoint sets Si whose union is S. Suppose we are given a set partition {I1, . . . , Im} of I, a
set partition {J1, . . . , Jn} of J, and a set partition {K1, . . . , Kp} of K. You can check that
{Ii × Jj : i ∈[m], j ∈[n]} is a set partition of I × J, and {Jj × Kk : j ∈[n], k ∈[p]} is a set
partition of J × K. Recall that the matrix X is a function from I × J to the field F. The
92
Advanced Linear Algebra
restriction of the function X to each subset Ii × Jj gives an Ii × Jj submatrix of X, written
Xi,j : Ii × Jj →F. Similarly, Y decomposes into submatrices Yj,k : Jj × Kk →F.
This formal description captures the visual decomposition of matrices into smaller pieces
as shown in (4.2). In that example, we have I = [8], J = [5], K = [8], m = n = 2, p = 3,
I1 = {1, 2, 3}, I2 = {4, 5, 6, 7, 8}, J1 = {1, 2}, J2 = {3, 4, 5}, K1 = {1, 2, 3}, K2 = {4, 5, 6, 7},
K3 = {8}, X1,1 = P, X1,2 = Q, X2,1 = R, X2,2 = S, Y1,1 = A, and so on. In these set
partitions, each subset in the partition consists of a block of consecutive integers, but this
is not required in general.
Block Matrix Multiplication Rule. With the above notation, the matrix product Z =
XY : I × K →F consists of submatrices Zi,k : Ii × Kk →F given by
Zi,k =
X
j∈[n]
Xi,jYj,k
for all i ∈[m], k ∈[p].
(4.3)
Proof. Given r ∈I and c ∈K, the entry Z(r, c) in row r and column c of Z = XY is
part of exactly one submatrix Zi,k, namely the one where r ∈Ii and c ∈Kk. We need only
check that the two sides of (4.3) have the same r, c-entry. The left side Zi,k is a submatrix
of Z = XY , so the r, c-entry is
Zi,k(r, c) = Z(r, c) = (XY )(r, c) =
X
s∈J
X(r, s)Y (s, c).
For the right side, we first compute the r, c-entry of Xi,jYj,k for a fixed j ∈[n]. Since Xi,j
is an Ii × Jj submatrix of X and Yj,k is a Jj × Kk submatrix of Y , this entry is
(Xi,jYj,k)(r, c) =
X
s∈Jj
Xi,j(r, s)Yj,k(s, c) =
X
s∈Jj
X(r, s)Y (s, c).
Summing over j ∈[n], the r, c-entry of the right side of (4.3) is
X
j∈[n]
X
s∈Jj
X(r, s)Y (s, c) =
X
s∈J
X(r, s)Y (s, c),
since J is the disjoint union of its subsets J1, . . . , Jn. Thus, the left and right sides of (4.3)
have the same r, c-entry.
4.15
Tensor Product of Matrices
This section introduces the tensor product operation on matrices, which has applications
in quantum computing, multilinear algebra, and other settings. Given an m × n matrix A
and an s × t matrix B, the tensor product A ⊗B is a certain ms × nt matrix. To define this
matrix, it is helpful to use general indexing sets for the rows and columns of the matrices.
Let I, J, K, L be finite, nonempty index sets. Let A : I × J →F be an I × J matrix and
B : K × L →F be a K × L matrix, where A and B have entries in a field [or ring] F. The
tensor product matrix A ⊗B is the (I × K) × (J × L) matrix defined by setting
(A ⊗B)((i, k), (j, ℓ)) = A(i, j)B(k, ℓ)
for all i ∈I, k ∈K, j ∈J, ℓ∈L.
(4.4)
Basic Matrix Operations
93
Note that the rows of A⊗B are indexed by ordered pairs (i, k) with i ∈I and k ∈K, while
the columns of A⊗B are indexed by ordered pairs (j, ℓ) with j ∈J and ℓ∈L. The defining
equation (4.4) says that the entry of A ⊗B in row (i, k), column (j, ℓ) is the product of the
i, j-entry of A and the k, ℓ-entry of B.
We can visualize the tensor product A⊗B as a block matrix, where each block is a copy
of B scaled by some entry of A. For example, let I = J = K = {0, 1}, L = {1, 2, 3},
A =
 0
1
0
2
5
1
−1
3

,
and
B =
 1
2
3
0
4
1
−3
1
2
0
7

.
Writing bc as an abbreviation for the ordered pair (b, c), the rows of A ⊗B are labeled by
the 4 bit strings 00, 01, 10, 11, while the columns of A⊗B are labeled by 01, 02, 03, 11, 12, 13.
Using this ordering for the row and column labels, we draw A ⊗B as follows:
A ⊗B =


01
02
03
11
12
13
00
8
2
−6
20
5
−15
01
4
0
14
10
0
35
10
−4
−1
3
12
3
−9
11
−2
0
−7
6
0
21

=

2B
5B
−1B
3B

.
We see the same pattern in the general case. Suppose each index set I, J, K, L is totally
ordered, say I = {i1 < i2 < · · · < im}, J = {j1 < j2 < · · · < jn}, etc. We introduce
lexicographic order on the set I ×K by letting (i, k) < (i′, k′) mean that i < i′, or i = i′ and
k < k′, for all i, i′ ∈I and k, k′ ∈K. We use the analogous lexicographic order on J × L.
Displaying A ⊗B by listing rows and columns in lexicographic order, we find that A ⊗B
has the block form
A ⊗B =


A(i1, j1)B
A(i1, j2)B
· · ·
A(i1, jn)B
A(i2, j1)B
A(i2, j2)B
· · ·
A(i2, jn)B
· · ·
· · ·
· · ·
· · ·
A(im, j1)B
A(im, j2)B
· · ·
A(im, jn)B

.
We could have instead defined (i, k) < (i′, k′) to mean k < k′, or k = k′ and i < i′, with a
similar ordering on J × L. Using these orderings, A ⊗B has a block form where a typical
block is B(k, ℓ)A.
Theorem on Bilinearity of Matrix Tensor Products. Suppose A and A′ are I × J
matrices, B and B′ are K × L matrices, and c ∈F. Then:
(a) (A + A′) ⊗B = A ⊗B + A′ ⊗B
(left distributive law).
(b) A ⊗(B + B′) = A ⊗B + A ⊗B′
(right distributive law).
(c) c(A ⊗B) = (cA) ⊗B = A ⊗(cB)
(scalar property).
Proof. Note that both sides of (a) are (I × K) × (J × L) matrices. Fix i ∈I, j ∈J, k ∈K,
and ℓ∈L. The entry in row (i, k) and column (j, ℓ) of (A + A′) ⊗B is
(A + A′)(i, j)B(k, ℓ) = (A(i, j) + A′(i, j))B(k, ℓ) = A(i, j)B(k, ℓ) + A′(i, j)B(k, ℓ).
The entry in row (i, k) and column (j, ℓ) of A ⊗B + A′ ⊗B is
(A ⊗B)((i, k), (j, ℓ)) + (A′ ⊗B)((i, k), (j, ℓ)) = A(i, j)B(k, ℓ) + A′(i, j)B(k, ℓ).
The entries are equal, so (a) is true. Parts (b) and (c) are proved similarly (Exercise 96).
94
Advanced Linear Algebra
Theorem on Matrix Multiplication and Tensor Products. Suppose A, B, C, D are
matrices where A is I × J, B is K × L, C is J × M, and D is L × P for index sets
I, J, K, L, M, P. If multiplication in F is commutative, then
(A ⊗B)(C ⊗D) = (AC) ⊗(BD).
(4.5)
Proof. Note that A ⊗B is a (I × K) × (J × L) matrix, C ⊗D is a (J × L) × (M × P)
matrix, AC is I × M, and BD is K × P. So both sides of (4.5) are (I × K) × (M × P)
matrices. Fix i ∈I, k ∈K, m ∈M, and p ∈P. By definition of matrix multiplication, the
((i, k), (m, p))-entry of (A ⊗B)(C ⊗D) is
X
(j,ℓ)∈J×L
(A ⊗B)((i, k), (j, ℓ))(C ⊗D)((j, ℓ), (m, p)) =
X
j∈J
X
ℓ∈L
A(i, j)B(k, ℓ)C(j, m)D(ℓ, p).
The ((i, k), (m, p))-entry of (AC) ⊗(BD), namely (AC)(i, m)(BD)(k, p), expands to

X
j∈J
A(i, j)C(j, m)

·
"X
ℓ∈L
B(k, ℓ)D(ℓ, p)
#
=
X
j∈J
X
ℓ∈L
A(i, j)C(j, m)B(k, ℓ)D(ℓ, p).
Because multiplication in F is commutative, this entry agrees with the previously computed
entry. So (A ⊗B)(C ⊗D) = (AC) ⊗(BD).
4.16
Summary
1.
Formal Definitions for Matrices. An element of F n is a function x : [n] →F,
which is often presented as a list (x(1), . . . , x(n)). An m × n matrix is a function
A : [m] × [n] →F. Special cases are column vectors (n = 1) and row vectors
(m = 1), which can be identified with elements in F m and F n, respectively. A
matrix can be identified with the list of its columns (A[1], . . . , A[n]) or with the
list of its rows (A[1], . . . , A[m]); by definition, A[j](i) = A(i, j) = A[i](j) for i ∈[m]
and j ∈[n]. An I × J matrix is a function A : I × J →F, where I and J are
finite index sets for the rows and columns.
2.
Vector Spaces of Functions. If S is any set and F is a field, the set V of all
functions from S to F becomes an F-vector space under pointwise addition and
scalar multiplication of functions. For x ∈S, let ex ∈V be the function that sends
x to 1F and everything else in S to 0F . When S is finite, the set X = {ex : x ∈S}
is a basis for V such that |X| = |S|. In particular, dim(V ) = |S| in this case.
3.
Spaces of Vectors and Matrices. The sets F n and Mm,n(F) are F-vector spaces
under componentwise operations. Viewing vectors and matrices as functions,
these operations are the same as pointwise addition and scalar multiplication
of functions from [n] or [m] × [n] into F. A basis for F n consists of the vectors ei
defined by ei(i) = 1 and ei(j) = 0 for j ̸= i. A basis for Mm,n(F) consists of the
matrices eij defined by eij(i, j) = 1 and eij(i′, j′) = 0 whenever i ̸= i′ or j ̸= j′.
We have dim(F n) = n and dim(Mm,n(F)) = mn.
4.
Matrix Multiplication. For A ∈Mm,n(F) and B ∈Mn,p(F), AB ∈Mm,p(F) is
the matrix whose i, j-entry is Pn
k=1 A(i, k)B(k, j). We have AIn = A = ImA and
c(AB) = (cA)B = A(cB) for c ∈F [and F commutative]. Matrix multiplication
Basic Matrix Operations
95
is distributive and associative. When m = n [and F is commutative], the product
operation turns the vector space Mn(F) into an F-algebra. By generalized
associativity, we can omit parentheses when writing a product of s matrices.
If Au is an nu−1 × nu matrix for 1 ≤u ≤s, then the i, j-entry of Qs
u=1 Au is
n1
X
k1=1
n2
X
k2=1
· · ·
ns−1
X
ks−1=1
A1(i, k1)A2(k1, k2) · · · As−1(ks−2, ks−1)As(ks−1, j).
Block matrix multiplication lets us compute AB by a formula that treats blocks
(submatrices of A and B) as if they were individual matrix entries.
5.
Transpose and Conjugate-Transpose. For A ∈Mm,n(F), we define AT ∈Mn,m(F)
and [when F = C] A∗∈Mn,m(C) by AT(i, j) = A(j, i) and A∗(i, j) = A(j, i) for
i ∈[n] and j ∈[m]. These operations satisfy the following identities:
(A + B)T = AT + BT, (AT)T = A, and (cA)T = c(AT) for c ∈F;
(A1 · · · As)T = AT
s · · · AT
1 [for F commutative];
(A + B)∗= A∗+ B∗, (A∗)∗= A, and (cA)∗= c(A∗) for c ∈C;
(A1 · · · As)∗= A∗
s · · · A∗
1.
6.
Computing with Rows and Columns. On one hand, (AB)[j] = A(B[j]), which says
that column j of AB is A times column j of B. On the other hand, (AB)[i] =
(A[i])B, which says that row i of AB is row i of A times B. For a matrix-vector
product Av, Av = P
j A[j]v(j), so that Av is a linear combination of the columns
of A. For a vector-matrix product wA, wA = P
i w(i)A[i], so that wA is a linear
combination of the rows of A. Some consequences [for fields F] are:
colrk(AB) ≤min(colrk(A), colrk(B));
rowrk(AB) ≤min(rowrk(A), rowrk(B));
left or right multiplication by an invertible matrix does not change the row rank or
the column rank; and the rank of an invertible n×n matrix is equal to n. Matrix
transposition interchanges the rows and columns of A. The conjugate-transpose
operation interchanges rows and columns and conjugates every entry.
7.
Elementary Operations and Elementary Matrices. The three elementary row
operations are interchange two rows; multiply a row by an invertible scalar; add
any scalar multiple of one row to a different row. To apply an elementary row
operation to A, we multiply A on the left by the corresponding elementary matrix
(formed by applying the same row operation to the identity matrix). There are
analogous elementary column operations, which can be achieved by multiplying
A on the right by the corresponding elementary matrix. Elementary matrices are
invertible, and their inverses are also elementary.
8.
Applications of Elementary Matrices. [This item assumes F is a field.] A square
matrix is invertible iff it is a product of elementary matrices. For any A (possibly
rectangular), there exists an invertible matrix P such that PA is in reduced row-
echelon form. Furthermore, there exists another invertible matrix Q such that
PAQ is a matrix in Smith canonical form, with rank(A) copies of 1 on the main
diagonal and all other entries 0. In particular, the column rank of A is always
equal to the row rank of A. Elementary matrices can be used to prove the product
formula det(AB) = det(A) det(B) for determinants.
9.
Invertible Matrices. [This item assumes F is a field.] A matrix A ∈Mn(F)
is invertible iff there exists a (necessarily unique) matrix A−1 ∈Mn(F) with
AA−1 = In = A−1A. The following conditions are equivalent to the existence of
96
Advanced Linear Algebra
A−1: A has a left inverse; A has a right inverse; A has nonzero determinant; A
is a product of elementary matrices; the reduced row-echelon form of A is In; A
has row rank n; A has column rank n; the n rows of A are linearly independent;
the n columns of A are linearly independent; the range of A has dimension n;
the null space of A is {0}; the map LA : F n →F n given by LA(x) = Ax for
x ∈F n is injective; LA is surjective; LA is bijective; the system Ax = b has a
solution x ∈F n for every b ∈F n; Ax = b has a unique solution x for every b;
Ax = 0 has only the zero solution x = 0; AT is invertible; [when F = C] A∗
is invertible. If A−1 exists, then (A−1)−1 = A, (AT)−1 = (A−1)T, and [when
F = C] (A∗)−1 = (A−1)∗. A product A = A1A2 · · · As is invertible iff every Ai
is invertible, in which case A−1 = A−1
s
· · · A−1
2 A−1
1 . The set GLn(F) of invertible
matrices in Mn(F) is a group under matrix multiplication.
10.
Tensor Products of Matrices. Given an I × J matrix A and a K × L matrix B,
A ⊗B is the (I × K) × (J × L) matrix where the entry in row (i, k), column
(j, ℓ) is A(i, j)B(k, ℓ). Using lexicographic order for the index sets, A ⊗B is a
block matrix with blocks A(i, j)B. The matrix tensor product is F-bilinear. For
commutative F, (A⊗B)(C⊗D) = (AC)⊗(BD) when these products are defined.
4.17
Exercises
Unless otherwise stated, assume F is a field in these exercises.
1.
State a precise formula for a map I : F n →Mn,1(F) that identifies n-tuples with
column vectors, and check that I is a vector space isomorphism. (Note that the
inputs and outputs for I are functions.)
2.
Define a map J : F n →M1,n(F) that identifies n-tuples with row vectors, and
prove J is a vector space isomorphism.
3.
(a) Define a vector space isomorphism from F to F 1.
(b) Define a vector space isomorphism from F to M1,1(F).
(This problem lets us identify 1-tuples and 1 × 1 matrices with scalars in F.)
4.
Consider the map C : Mm,n(F) →(F m)n sending A ∈Mm,n(F) to its list of
columns (A[1], . . . , A[n]). Give a precise formula for C−1, and check that C is F-
linear. Do the same for the map R : Mm,n(F) →(F n)m sending A ∈Mm,n(F)
to its list of rows (A[1], . . . , A[m]).
5.
Given finite index sets I and J, choose bijections f : I →[m] and g : J →[n].
Use f and g to define an isomorphism from the vector space of I × J matrices
with entries in F to the vector space Mm,n(F).
6.
Let S be a set and V be the set of all functions g : S →F with pointwise addition
and scalar multiplication.
(a) Prove: addition on V is commutative.
(b) Prove: the zero function from S to F is the additive identity element of V .
(c) Prove: the additive inverse of f ∈V is the function sending x ∈S to −f(x).
7.
Let V be the set of functions g : S →F. Prove that scalar multiplication on V
satisfies the five axioms in the definition of a vector space.
Basic Matrix Operations
97
8.
Let S be any set and E be any F-vector space. Let V be the set of all functions
g : S →E. Define pointwise addition and scalar multiplication operations on V ,
and prove that V with these operations is an F-vector space.
9.
Define S, E, and V as in the previous exercise. Suppose S = {x1, . . . , xk} is finite,
and E is finite-dimensional with ordered basis (v1, . . . , vm). Describe an explicit
basis for V , and compute dimF (V ).
10.
(a) Cite a theorem from the text to explain why the set V of all functions
f : R →R (under pointwise operations) is a real vector space.
(b) For x ∈R, let ex : R →R send x to 1 and all other inputs to 0. Which
functions in V are in the span of the set {ex : x ∈R}? Is this set a basis for V ?
(c) Explain why the set of continuous f : R →R is a subspace of V . Find the
intersection of this subspace and the span of {ex : x ∈R}.
11.
Let A =


5
2
1
1/2
−3
0
−1
−4
−3/2

, B =


1
−1
0
0
2
−2
3
−3
0

, C =
 4
2
−7
1
1/3
3

,
w = [2 4 1], and v = [1 −3 −1]T. Compute A + B, AB, BA, CA, CB, Av, wB,
wv, and vw.
12.
For the matrices A, B, C in the previous exercise, compute B ⊗C, C ⊗B, and
A ⊗A.
13.
Let A =
 5 −i
2i + 3
0
−1 −3i

, B =
 3 + 4i
−2i
e5πi/3
√
2 + 2i

, and v =
 i
1

.
Compute A + B, AB, AT, BT, A∗, B∗, Bv, vTv, v∗v, v∗Av, and v∗A∗Av.
14.
Prove: for all A, B ∈Mn(F), A and B commute iff AT and BT commute.
15.
Prove: for all invertible A, B ∈Mn(F), A and B commute iff A−1 and B−1
commute.
16.
Let A =
 1
4
2
3

∈M2(F). For each choice of F, find A2, A3, AAT, and (if
possible) A−1. (a) F = R
(b) F = Z5
(c) F = Z7
17.
Let A =


1
0
0
0
0
1
0
1
0

, B =


0
0
1
1
0
1
0
1
0

, and C =


1
0
0
0
0
1
0
1
1

in M2(Z2).
Compute Ak, Bk, and Ck for all k ∈Z.
18.
Let A, A′ ∈Mm,n(F), B ∈Mn,p(F), and c ∈F. Prove in detail:
(A + A′)B = AB + A′B, c(AB) = (cA)B, c(AB) = A(cB), and AIn = A.
Which properties must be true if F is a non-commutative ring?
19.
Let A, B ∈Mm,n(C) and C ∈Mn,p(C). Prove in detail:
(A + B)∗= A∗+ B∗, (AC)∗= C∗A∗, (A∗)∗= A,
A is invertible iff A∗is invertible, in which case (A∗)−1 = (A−1)∗.
20.
Hadamard Product of Matrices. For matrices A, B ∈Mm,n(F), define the
Hadamard product of A and B, written A⊙B, to be the matrix in Mm,n(F) with
entries (A ⊙B)(i, j) = A(i, j)B(i, j) for i ∈[m] and j ∈[n]. We obtain A ⊙B by
multiplying corresponding entries of A and B. Prove that ⊙is commutative,
associative, satisfies the left and right distributive laws, and has an identity
element. Which matrices in Mm,n(F) have inverses with respect to ⊙?
21.
Give a specific example of matrices A, B ∈Mn(F) with AB = 0 and BA ̸= 0.
22.
Give an example of matrices A, B ∈Mm,n(F) with m ̸= n, BA = 0, and AB ̸= 0.
98
Advanced Linear Algebra
23.
Give an example of A, B ∈M4(R) such that every entry of A and B is nonzero,
yet AB = BA = 0.
24.
A matrix A ∈Mn(F) is called diagonal iff A(i, j) = 0F for all i ̸= j in [n]. This
means that the only nonzero entries of A occur on the main diagonal. Prove that
the set of diagonal matrices is a commutative subalgebra of the F-algebra Mn(F).
Find a basis for this subalgebra and compute its dimension.
25.
A matrix A ∈Mn(F) is called upper-triangular iff A(i, j) = 0F for all i, j ∈[n]
with i > j. This means that all entries of A below the main diagonal are zero.
Prove the set U of upper-triangular matrices is a subalgebra of Mn(F). Find a
basis for U and compute dim(U).
26.
A matrix A ∈Mn(F) is called lower-triangular iff A(i, j) = 0F for all i, j ∈[n]
with i < j. This means that all entries of A above the main diagonal are zero.
Prove A is lower-triangular iff AT is upper-triangular. Use this and the previous
exercise to deduce that the set L of lower-triangular matrices is a subalgebra of
Mn(F). Find a basis for L and compute dim(L).
27.
A matrix A ∈Mn(F) is called strictly upper-triangular iff A(i, j) = 0F for all
i, j ∈[n] with i ≥j. This means that the only nonzero entries of A occur strictly
above the main diagonal. Prove: if A is strictly upper-triangular, then An = 0n.
(Use the formula for [Qs
u=1 Au] (i, j) in §4.5.) Deduce that A is not invertible.
28.
Prove: For all A, B ∈Mn(F), if A is strictly upper-triangular and B is upper-
triangular, then AB and BA are strictly upper-triangular.
29.
Show that the set S of strictly upper-triangular matrices is a subalgebra of Mn(F).
Exhibit a basis for S and compute dim(S).
30.
A matrix A in Mn(F) is called unitriangular iff A is upper-triangular and
A(i, i) = 1F for all i ∈[n]. Let T be the set of unitriangular matrices in Mn(F).
Is T closed under addition? additive inverses? additive identity? multiplication?
multiplicative identity? scalar multiplication? multiplicative inverses? (For the
last question, see Exercise 65.)
31.
A matrix A ∈Mn(F) is called nilpotent iff Ak = 0 for some integer k > 0.
Exercise 27 shows that any strictly upper-triangular matrix is nilpotent. Give
examples of the following:
(a) a nilpotent matrix that is neither upper-triangular nor lower-triangular
(b) two nilpotent matrices whose sum is not nilpotent
(c) two nilpotent matrices whose product is not nilpotent
32.
Prove: for all A, B ∈Mn(F), if A, B ∈Mn(F) are nilpotent and AB = BA, then
A + B and AB are nilpotent.
33.
A matrix A ∈Mn(F) is called skew-symmetric iff AT = −A. Prove that the set of
skew-symmetric matrices is a subspace of Mn(F). Find a basis for this subspace
and compute its dimension.
34.
Give an example of a nonzero matrix that is both symmetric and skew-symmetric,
or explain why no such matrix exists.
35.
Prove: for all A, B ∈Mn(F), if A and B are skew-symmetric and commute, then
AB is symmetric. Does the result hold without assuming AB = BA?
36.
Trace of a Matrix. The trace of a square matrix A ∈Mn(F) is defined by
tr(A) = Pn
i=1 A(i, i), which is the sum of the main diagonal entries of A.
(a) Compute tr(0n), tr(In), and tr(Jn).
(b) Prove that tr : Mn(F) →F is an F-linear map.
Basic Matrix Operations
99
(c) Prove: for all A, B ∈Mn(F), tr(AB) = tr(BA).
(d) Does part (c) hold for all A ∈Mm,n(F) and B ∈Mn,m(F) with n ̸= m?
37.
(a) Prove: for all A ∈Mn(F), tr(AT) = tr(A).
(b) Find and prove an identity relating tr(A∗) and tr(A) for A ∈Mn(C).
38.
Prove or disprove: for all invertible A ∈Mn(F), tr(A−1) = tr(A)−1.
39.
Give an example of a field F and a matrix A ∈Mn(F) with all entries of A
nonzero and tr(A) = 0F .
40.
Given A ∈Mm(F) and B ∈Mn(F), prove that tr(A ⊗B) = tr(A) tr(B).
41.
Commutator of Matrices. Given A, B ∈Mn(F), define the commutator of A
and B to be [A, B] = AB −BA. Prove the following hold for all A, B, C ∈Mn(F)
and all d ∈F:
(a) [A, B] = 0 iff A and B commute.
(b) [A, B] = −[B, A] (anticommutativity).
(c) [A + B, C] = [A, C] + [B, C], [A, B + C] = [A, B] + [A, C],
and [dA, B] = d[A, B] = [A, dB] (bilinearity).
(d) [A, [B, C]] + [B, [C, A]] + [C, [A, B]] = 0 (Jacobi Identity).
(e) The trace of [A, B] is zero.
42.
Is [A, [B, C]] = [[A, B], C] true for all A, B, C ∈Mn(F)? Either prove it, or find
sufficient conditions on A, B, C for this identity to hold.
43.
Write all 14 complete parenthesizations of a product ABCDE of five matrices.
44.
For n ≥1, let pn be the number of complete parenthesizations of a product of n
matrices. Show p1 = p2 = 1, p3 = 2, p4 = 5, and pn = Pn−1
k=1 pkpn−k for n > 1.
(It follows from this recursion that pn is the Catalan number
1
n
 2n−2
n−1

.)
45.
Laws of Exponents for Matrix Powers. Let A ∈Mn(F). Prove:
(a) For all integers r, s ≥0, Ar+s = ArAs.
(b) For all integers r, s ≥0, (Ar)s = Ars.
(c) If A−1 exists, then (a) and (b) hold for all r, s ∈Z.
46.
Let (G, ⋆) be any group, s ≥1, and a1, . . . , as ∈G. State and prove a Generalized
Associativity Law that justifies writing the product a1 ⋆a2 ⋆· · · ⋆as with no
parentheses. (Imitate the proof given in the text for matrix products.)
47.
(a) Given U ∈Mm,n(F) and V ∈Mn,p(F), how many additions and multiplica-
tions in F are needed to compute UV using the definition of matrix product?
(b) Suppose A is 2 × 10, B is 10 × 15, C is 15 × 4, and D is 4 × 8. For each
complete parenthesization of ABCD, count the number of multiplications needed
to evaluate this matrix product by multiplying matrices in the order determined
by the parenthesization.
48.
Prove: for all a, b, c, d ∈F, if ad −bc ̸= 0F , then A =
 a
b
c
d

is invertible,
and A−1 = (ad −bc)−1

d
−b
−c
a

. Assuming ad −bc ̸= 0F , find an explicit
factorization of A into a product of elementary matrices.
49.
Find the inverse of A =


0
1
2
1
1
0
1
1
0
2
2
1
1
2
2
1

in M4(Z3).
50.
Find necessary and sufficient conditions on the integer n and the field F for the
group GLn(F) to be commutative.
100
Advanced Linear Algebra
51.
(a) Find the size of the group GL3(Z2). (Build A ∈GL3(Z2) by choosing nonzero
rows, one at a time, that are not in the span of the previous rows.)
(b) Find the size of the group GL2(Z7).
(c) For any n ≥1 and any prime p, find the size of GLn(Zp).
52.
Give an example of functions f, g : R →R with f ◦g = idR but g ◦f ̸= idR.
53.
Prove: for all functions f, g : R →R, if f ◦g = idR and g ◦f ̸= idR, then f is onto
and not one-to-one, while g is one-to-one and not onto.
54.
Show that A =
 1
0
0
1
0
1

∈M2,3(R) has a right inverse but no left inverse.
Find all right inverses of A.
55.
Give a specific example of a matrix A ∈M4,3(R) that has a left inverse but no
right inverse. Find all left inverses of A.
56.
Prove: for all A ∈Mm,n(F) with m > n, if A has a left inverse, then the left
inverse is not unique.
57.
Prove: for all A ∈Mm,n(F), rank(A) ≤min(m, n).
58.
(a) Prove: for all A, B ∈Mm,n(F), rank(A + B) ≤rank(A) + rank(B).
(b) Give an example where strict inequality holds in (a).
59.
Let m, n be positive integers with m < n. By studying rank, show that no A in
Mm,n(F) has a left inverse, and no B in Mn,m(F) has a right inverse.
60.
Suppose A ∈Mm,n(F) and B ∈Mn,p(F). Prove [(AB)(i, j)] = A[i]B[j] for all
i ∈[m] and j ∈[p]. Give a verbal description of what this equation means.
61.
Define A ∈M3,5(R) and B ∈M5,3(R) by setting A(i, j) = i2j and B(j, i) = j +2i
for i ∈[3] and j ∈[5]. Compute row 3 of AB. Compute column 2 of AB. Express
column 4 of BA as a linear combination of the columns of B. Express row 1 of
BA as a linear combination of the rows of A.
62.
Let A, B ∈Mm,n(F) and c ∈F. Prove (A+B)[i] = A[i]+B[i] and (cA)[i] = c(A[i])
for all i ∈[m].
63.
Let A ∈Mn(F) be upper-triangular with A(i, i) ̸= 0 for all i ∈[n] and B ∈
Mm,n(F) be arbitrary. Prove: for all i ∈[n], the subspace of F n spanned by the
first i columns of B equals the subspace of F n spanned by the first i columns of
BA. Formulate and prove a similar result involving left multiplication by A.
64.
Formulate and prove results similar to the previous exercise for multiplication on
the left or right by lower-triangular matrices with no zeroes on the diagonal.
65.
Let A ∈Mn(F) be unitriangular (see Exercise 30). Prove A is invertible, and find
a recursive formula for computing the entries of A−1. (Solve BA = In for the
columns of B from left to right.)
66.
Illustrate the formula in Exercise 65 by finding the inverses of
A =


1
1
1
1
1
0
1
1
2
3
0
0
1
1
2
0
0
0
1
3
0
0
0
0
1


and
C =


1
1
1
1
1
1
1
1
0
1
0
1
0
1
0
1
0
0
1
1
0
0
1
1
0
0
0
1
0
0
0
1
0
0
0
0
1
1
1
1
0
0
0
0
0
1
0
1
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
1


.
Basic Matrix Operations
101
67.
Which lower-triangular A ∈Mn(F) are invertible? Prove that A−1, when it exists,
is also lower-triangular.
68.
Given A ∈Mn(F) and positive integers r < s, prove with minimal calculation
that Col(As) ⊆Col(Ar). Similarly, prove Row(As) ⊆Row(Ar).
69.
For each matrix below, determine the rank of the matrix by finding a subset of
the columns that forms a basis for the column space:
(a)


1
−1
0
0
2
−2
3
−3
0


(b)
 4
2
−7
1
1/3
3

(c)


0
1
1
1
1
1
1
0
0
0
0
0
1
0
0
1

in M4(Z2)
(d) Jm,n, which has every entry equal to 1
(e) the matrix A ∈Mm,n(R) with A(i, j) = ij for i ∈[m] and j ∈[n]
(f) the matrix A ∈Mn(R) with A(i, j) = (i −1)n + j for i, j ∈[n]
70.
Repeat Exercise 69, but compute the rank by finding a subset of the rows that
forms a basis for the row space.
71.
Repeat Exercise 69, but compute the rank by finding the reduced row-echelon
form of the matrix and counting the nonzero rows.
72.
Let A ∈Mm,n(F), i, ℓ∈[m], j, k ∈[n], and c, d ∈F with c ̸= 0. Prove:
(a) Left-multiplication by E1
m[c; i] multiplies row i of A by c.
(b) Right-multiplication by E1
n[c; j] multiplies column j of A by c.
(c) E1
m[c; i] is invertible with inverse E1
m[c−1; i].
(d) For i ̸= ℓ, left-multiplication by E2
m[i, ℓ] interchanges row i and row ℓof A.
(e) For i ̸= ℓ, E2
m[i, ℓ] is its own inverse.
(f) For j ̸= k, right-multiplication by E3
n[d; j, k] adds d times column j of A to
column k of A.
73.
Prove that every elementary matrix E2
p[i, j] can be expressed as a product of
several elementary matrices of the form E3
p[±1; r, s]. Conclude that the elementary
operation of interchanging two rows of a matrix can be simulated by other
elementary row operations.
74.
List all elementary matrices in M3(Z2). Express each matrix as a product
D1D2 · · · Dk, where each Di is one of the matrices A, B, C in Exercise 17.
75.
Draw pictures of all possible matrices A ∈M3,5(F) in reduced row-echelon form.
Each matrix entry should be a 0, a 1, or a ∗to indicate an arbitrary scalar.
76.
For each matrix A, compute the reduced row-echelon form Aech and an invertible
matrix P (a product of elementary matrices) with Aech = PA.
(a) A ∈M3,4(R) given by A(i, j) = i for i ∈[3] and j ∈[4]
(b) A ∈M4(R) given by A(i, j) = (−1)i+j for i, j ∈[4]
(c) the matrix A in Exercise 13
(d) A =


2
4
−3
0
−14
1
2
4
−2
13
0
0
1
0
4
2
4
2
0
6


(e) A =


0
1
0
1
0
0
1
1
1
1
1
0
0
0
1
0
0
0
1
0
1
0
0
0
0
0
0
0
0
0
1
0
1
0
0
0
0
0
0
0
1
1
1
0
1
0
0
0
1
1
1
1
1
0
0
0


∈M8,7(Z2)
77.
For each matrix A in Exercise 76, find an invertible matrix Q (a product of
elementary matrices) such that PAQ is the Smith canonical form of A.
102
Advanced Linear Algebra
78.
If possible, write each matrix and its inverse as a product of elementary matrices.
(a) A in Exercise 11 (b) B in Exercise 11 (c) A in Exercise 49
(d) A in Exercise 66
(e) C in Exercise 66
79.
Prove: for all A, B ∈Mm,n(F), rank(A) = rank(B) iff there exist invertible
matrices U ∈Mm(F) and V ∈Mn(F) with A = UBV .
80.
Use the definitions to give a direct proof of (7b)⇔(9b) in Table 4.1.
81.
Assume conditions (1), (6b), and (11) in Table 4.1 are known to be equivalent.
Using this, prove (6b)⇔(7b).
82.
Prove (8b)⇒(2b) in Table 4.1 by solving for the columns of C one at a time.
83.
Let A ∈Mn(F), and define RA : F n →F n by RA(x) = xA for each row vector
x ∈F n. Prove the following conditions are equivalent:
A is invertible; RA is injective; RA is surjective; RA is bijective.
84.
Let T : V →W be an F-linear map between two n-dimensional F-vector spaces
V and W, where n is finite. Prove the following conditions are equivalent:
T is injective; T is surjective; T is bijective.
85.
Give an example of an F-vector space V and a linear map T : V →V that is
one-to-one but not onto.
86.
Give an example of an F-vector space V and a linear map S : V →V that is
onto but not one-to-one.
87.
Give an example of an F-vector space V and linear maps T, S : V →V with
S ◦T = idV but T ◦S ̸= idV .
88.
Theorem on Right Invertibility. Fix integers m, n with 0 < m < n and
A ∈Mm,n(F). Prove that the following conditions on A are equivalent:
(a) A has a right inverse.
(b) A has at least two right inverses.
(c) The reduced row-echelon form of A has no zero rows.
(d) rowrk(A) = m.
(e) The list of m rows of A is linearly independent in F n.
(f) colrk(A) = m.
(g) The range of A has dimension m.
(h) The map LA : F n →F m sending each x ∈F n to Ax is surjective.
(i) For all b ∈F m, there exists x ∈F n with Ax = b.
(j) AT has a left inverse.
89.
Theorem on Left Invertibility. Fix integers m, n with 0 < n < m and A ∈
Mm,n(F). Formulate and prove a list of conditions on A (analogous to those in
Exercise 88), each of which is equivalent to A having a left inverse.
90.
Consider the theorem: for all A, B ∈Mn(F), if BA = In then AB = In.
(a) Show that the theorem follows from this statement:
(∗) for all A, B ∈Mn(F), if BA = In then A is invertible.
(b) We proved (∗) in the text using determinants (see the proof of (2a)⇒(3)⇒(1)
in §4.13). Give several new proofs of (∗) by using the equivalences proved in §4.13
(excluding conditions (2a) and (2b)) and showing that (2a)⇒(6a), (2a)⇒(7a),
and (2a)⇒(9b).
(c) Similarly, give proofs that AC = In implies A is invertible by showing
(2b)⇒(6a), (2b)⇒(7a), (2b)⇒(8b), and by using (11) and (∗).
(d) Prove, without using determinants, that for all U, V ∈Mn(F), if UV is
invertible, then U and V are invertible.
Basic Matrix Operations
103
91.
Show that the formulas (AB)[j] = A(B[j]) and (AB)[i] = (A[i])B (see §4.7
and §4.8) are special cases of the Block Matrix Multiplication Rule in §4.14.
92.
Show that the formulas Av = Pn
j=1 A[j]v(j) and wA = Pm
i=1 w(i)A[i] (see §4.7
and §4.8) are special cases of the Block Matrix Multiplication Rule in §4.14.
93.
Block Matrix Addition Rule. Suppose A and B are I × J matrices,
{I1, . . . , Im} is a set partition of I, and {J1, . . . , Jn} is a set partition of J. For
i ∈[m] and j ∈[n], let Ai,j and Bi,j be the submatrices of A and B with
domain Ii × Jj. Prove C = A + B consists of Ii × Jj submatrices Ci,j where
Ci,j = Ai,j + Bi,j for i ∈[m] and j ∈[n].
94.
Formally define the concept of a block diagonal matrix (cf. Exercise 24). State
and prove a rule for multiplying two block diagonal matrices with compatible
block sizes.
95.
Formally define the concept of a block upper-triangular matrix (cf. Exercise 25).
Prove that if A and B are block upper-triangular matrices with compatible block
sizes, then AB is also block upper-triangular with the same block sizes.
96.
Prove parts (b) and (c) of the Theorem on Bilinearity of Matrix Tensor Products.
97.
For any index set K, define the K ×K identity matrix IK by setting IK(k, k) = 1
and IK(k, k′) = 0 for all k, k′ ∈K with k ̸= k′. Prove: for all finite index sets K
and L, IK ⊗IL = IK×L.
98.
Suppose A ∈Mm(F) and B ∈Mn(F) are invertible matrices. Prove A ⊗B is
invertible, and (A ⊗B)−1 = A−1 ⊗B−1.
99.
Suppose λ is an eigenvalue of A ∈Mm(F) with eigenvector v, and µ is an
eigenvalue of B ∈Mn(F) with eigenvector w. Prove λµ is an eigenvalue of
A ⊗B ∈Mmn(F) with eigenvector v ⊗w.
100.
Let A be an I × I matrix, B be a J × J matrix, and C be a K × K matrix. Show
that (A ⊗B) ⊗C = A ⊗(B ⊗C) if we identify the index sets (I × J) × K and
I × (J × K) using the bijection sending ((i, j), k) to (i, (j, k)).
101.
Prove: for all A ∈Mm(F) and B ∈Mn(F), A ⊗B can be transformed to B ⊗A
by permuting rows and columns appropriately.
102.
This exercise outlines another proof [39] that rowrk(A) = colrk(A) for all A ∈
Mm,n(F). Call a row of A extraneous iff the row is an F-linear combination of
the other rows of A. Define extraneous columns analogously.
(a) Prove: if A has no extraneous rows, then rowrk(A) = m and m ≤n.
(b) Prove: if A has no extraneous columns, then colrk(A) = n and n ≤m.
(c) Prove: if A has no extraneous rows or columns, then rowrk(A) = colrk(A).
(d) Suppose A has an extraneous row. Show that deletion of this row produces
a smaller matrix with the same row rank and column rank as A. (Say row i is
extraneous. Consider the map T : F m →F m−1 that deletes the ith coordinate.
Show that the restriction of T to the column space of A is injective.)
(e) Prove a result similar to (d) if A has an extraneous column.
(f) Show that repeated use of (d) and (e) always leads to a smaller matrix B
having no extraneous rows or columns, such that rowrk(B) = rowrk(A) and
colrk(B) = colrk(A). Explain why rowrk(A) = colrk(A) follows.
103.
This exercise outlines a proof of Hans Liebeck [37] that colrk(A) = rowrk(A) for
all A ∈Mn(C). Prove:
(a) For all y ∈Mn,1(C), y = 0 iff y∗y = 0.
(b) For all A ∈Mn(C) and x ∈Mn,1(C), Ax = 0 iff A∗Ax = 0.
104
Advanced Linear Algebra
(c) For all A ∈Mn(C), colrk(A) = colrk(A∗A).
(d) For all A ∈Mn(C), colrk(A∗) = rowrk(A).
(e) For all A ∈Mn(C), colrk(A) ≤colrk(A∗).
(f) colrk(A) = rowrk(A) follows from (d) and (e).
5
Determinants via Calculations
Most introductions to linear algebra list the basic properties of determinants, including
formulas for computing and manipulating determinants. However, complete justifications
of these properties and formulas are not always provided. For example, the determinant of
a general n × n matrix A is often defined recursively by Laplace expansion along the first
row. We are told that using Laplace expansion along any row or column gives the same
answer, but this fact is not always proved.
In this chapter, we rigorously define determinants by an explicit, non-recursive formula
that involves a sum indexed by permutations. (See Chapter 2 for the background on
permutations needed here.) We use the explicit formula to prove some familiar properties
of determinants, such as the Laplace expansions. We give computational proofs whenever
possible. These proofs work for matrices with entries in any commutative ring (as defined
in §1.2), so we operate at that level of generality. Chapter 20 introduces a more abstract
approach to determinants based on multilinear algebra. While that approach is conceptually
harder, it yields many properties of determinants with a minimum of calculation.
5.1
Matrices with Entries in a Ring
Before defining determinants, we review the definitions of matrices with entries in a ring
and operations on such matrices. (This material is developed more fully in Chapter 4.) Let
R be a ring. For each positive integer m, let [m] be the set {1, 2, . . . , m}. An m × n matrix
A over R is a function A : [m] × [n] →R. For i ∈[m] and j ∈[n], the ring element A(i, j)
is also written Ai,j or Aij or aij, and we often display the matrix A as an array of m rows
and n columns such that A(i, j) appears in row i and column j. Let Mm,n(R) be the set of
all m × n matrices with entries in R and Mn(R) be the set of all n × n (square) matrices
with entries in R. For each n ≥1, we have a zero matrix 0n ∈Mn(R) defined by setting
0n(i, j) = 0R for all i, j ∈[n]. For each n ≥1, we also define the identity matrix In by
setting In(i, i) = 1R for all i ∈[n] and In(i, j) = 0R for all i ̸= j in [n]. We write 0 for 0n
and I for In if n is understood from context.
A row vector is an element of M1,n(R). A column vector is an element of Mn,1(R). We
often identify row vectors and column vectors with n-tuples in Rn, which are defined to be
functions x : [n] →R. We often represent such a function by the ordered list (x(1), . . . , x(n)).
If x is a row vector, we may write x(i) or xi instead of x(1, i); similarly if x is a column
vector. We prefer using parentheses instead of subscripts here, since we often work with
lists of row vectors. For instance, if (x1, . . . , xm) is a list of row vectors, then x3(5) is the
fifth entry in the row vector x3.
Table 5.1 defines some algebraic operations on matrices. The basic properties of these
operations are discussed in Chapter 4. The operations A and A∗are defined only for matrices
with complex entries.
DOI: 10.1201/9781003484561-5
105
106
Advanced Linear Algebra
TABLE 5.1
Definitions of matrix operations. Here, R is a ring, A, B ∈Mm,n(R), C ∈Mn,p(R), d ∈R,
and bars denote complex conjugation (x + iy = x −iy for x, y ∈R).
Matrix Operation
Defining Formula
1. matrix sum A + B ∈Mm,n(R)
(A + B)(i, j) = A(i, j) + B(i, j)
2. scalar multiplication dA ∈Mm,n(R)
(dA)(i, j) = d · A(i, j)
3. matrix product BC ∈Mm,p(R)
(BC)(i, j) = Pn
k=1 B(i, k) · C(k, j)
4. transpose AT ∈Mn,m(R)
AT(i, j) = A(j, i)
5. matrix conjugate A ∈Mm,n(C)
A(i, j) = A(i, j)
6. conjugate-transpose A∗∈Mn,m(C)
A∗(i, j) = A(j, i)
5.2
Explicit Definition of the Determinant
From now on, R is a fixed commutative ring. The determinant is a function that associates
with each n × n square matrix A an element of R (called the determinant of A). We define
the determinant function det : Mn(R) →R by the formula
det(A) =
X
f∈Sn
sgn(f)A(f(1), 1) · A(f(2), 2) · . . . · A(f(n), n)
for A ∈Mn(R).
(5.1)
(See Chapter 2 for the definitions of Sn and sgn(f).) The sums and products on the right
side of formula (5.1) are carried out in the ring R. Moreover, sgn(f) is interpreted as +1R
or −1R, where 1R is the multiplicative identity of R.
One motivation for this mysterious formula comes from the theory of exterior powers in
multilinear algebra, as we discuss in §20.17. For now, we describe a way of visualizing the
formula in terms of the geometric layout of the matrix A. A typical term in the determinant
formula arises by choosing one entry in each column of the matrix A and multiplying these n
entries together. If f(i) is the row of the entry chosen in column i, for each i ∈[n], we obtain
the product Qn
i=1 A(f(i), i). In order for this product to contribute to the determinant, f
is required to be a permutation. This means that that the entries f picks for two different
columns cannot come from the same row. We attach the sign sgn(f) to each term in the
formula, where sgn(f) = (−1)inv(f) is the number of transpositions of adjacent elements
needed to sort the list [f(1), . . . , f(n)] into increasing order (see §2.6). The determinant
of A is the sum of the signed terms coming from all n! choices of the permutation f. We
remark that we could also sum over all functions f : [n] →[n], if we use the convention that
sgn(f) = 0 when f is not a permutation.
For example, when n = 3,
det


a
b
c
d
e
f
g
h
i

= aei + gbf + dhc −gec −dbi −ahf.
The term −dbi comes from the permutation f = [f(1), f(2), f(3)] = [2, 1, 3]. The sign is
−1 = (−1)1, since it takes 1 transposition to sort the list [2, 1, 3]. The term gbf comes from
the permutation f = [3, 1, 2]. The sign is +1 = (−1)2, since it takes two transpositions of
adjacent elements to sort the list [3, 1, 2].
Determinants via Calculations
107
When n = 4, det(A) is a sum of 4! = 24 terms. When n = 5, det(A) is a sum of 5! = 120
terms. When n = 8, det(A) is a sum of 8! = 40320 terms. We see that the explicit formula
for the determinant quickly becomes unwieldy for doing computations. However, it is very
useful for theoretical purposes.
5.3
Diagonal and Triangular Matrices
If a matrix A has many entries equal to 0, then many of the n! terms in the formula for
det(A) are 0. This observation allows us to compute the determinants of certain special
matrices quickly. For instance, consider the n × n identity matrix In = I. By definition,
det(I) =
X
f∈Sn
sgn(f)I(f(1), 1)I(f(2), 2) · · · I(f(n), n).
Since I(i, j) = 0R for all i ̸= j in [n], the only way to get a nonzero summand is to
choose f so that f(1) = 1, f(2) = 2, . . ., and f(n) = n. In other words, f must be the
identity permutation of Sn. Since sgn(id) = +1 and I(i, i) = 1R for all i ∈[n], we see that
det(I) = 1R.
More generally, suppose D is a diagonal n × n matrix, which means that D(i, j) = 0R
for all i ̸= j in [n]. The argument given for I applies to D as well. So
det(D) = D(1, 1)D(2, 2) · · · D(n, n) =
n
Y
i=1
D(i, i).
Thus, the determinant of a diagonal matrix is the product of the entries on the diagonal.
A slight adjustment of this technique allows us to compute the determinant of a
triangular matrix. A matrix A ∈Mn(R) is upper-triangular iff A(i, j) = 0 for all i > j
in [n]. A is lower-triangular iff A(i, j) = 0 for all i < j in [n].
Determinant Rule for Triangular Matrices. The determinant of an upper-triangular
or lower-triangular matrix A is the product of the diagonal entries of A.
Proof. We give the proof for a lower-triangular matrix A. We show that if f ∈Sn generates
a nonzero summand A(f(1), 1)A(f(2), 2) · · · A(f(n), n) in (5.1) , then f must be the identity
permutation. First, note that A(f(n), n) ̸= 0 forces f(n) ≥n and hence f(n) = n. Second,
note that A(f(n −1), n −1) ̸= 0 forces f(n −1) ≥n −1. Since f is a permutation and
f(n) = n, we must have f(n −1) = n −1. Continuing by backwards induction, fix i ∈[n]
and assume that f(j) = j for all j > i in [n]. Then A(f(i), i) ̸= 0 forces f(i) ≥i, and
hence f(i) = i since the values larger than i have already been used. After n steps, we see
that f = id is the only permutation that gives a nonzero contribution to det(A). Hence,
det(A) = Qn
i=1 A(i, i) as claimed. The proof for upper-triangular matrices is analogous.
5.4
Changing Variables
To prove many properties of determinants, we often use a change of variable to simplify sums
of the form P
y∈S H(y). Here, S is a finite set (often, S is Sn when studying determinants),
108
Advanced Linear Algebra
and H : S →R is a function that maps each summation index y ∈S to a summand H(y).
Suppose T is a set and C : T →S is a bijection (a function that is one-to-one and onto).
Each y ∈S has the form y = C(x) for a unique x ∈T. Therefore, if a new summation
variable x ranges over all elements of T exactly once, then the values y = C(x) range over all
elements of S exactly once. Using generalized associativity and commutativity of addition
in the target ring R, it follows that
X
y∈S
H(y) =
X
x∈T
H(C(x)).
(5.2)
In particular, if T and S are the same set, we have
X
x∈S
H(x) =
X
y∈S
H(y) =
X
x∈S
H(C(x)).
(5.3)
For example, let S = Sn and C : S →S be the inversion map given by C(f) = f −1 ∈Sn
for every f ∈Sn. We have C(C(f)) = (f −1)−1 = f = idSn(f) for all f ∈Sn. So C◦C = idSn,
which proves that C is a bijection on Sn and C = C−1. Therefore,
X
f∈Sn
H(f) =
X
f∈Sn
H(C(f)) =
X
f∈Sn
H(f −1).
For another example, let g ∈Sn be fixed and Lg : Sn →Sn be the map given by Lg(f) = g◦f
for all f ∈Sn. The map Lg is called left multiplication by g. You can check that Lg−1 (left
multiplication by g−1) is the two-sided inverse for Lg, so Lg is a bijection. The change-of-
variable formula (5.3) therefore gives
X
f∈Sn
H(f) =
X
f∈Sn
H(Lg(f)) =
X
f∈Sn
H(g ◦f).
The same technique applies to the simplification of finite products Q
x∈S H(x), assuming
that the values H(x) belong to a given commutative ring R. Specifically, if S is finite and
C : S →S is any bijection, then
Y
x∈S
H(x) =
Y
x∈S
H(C(x)).
(5.4)
5.5
Transposes and Determinants
In this section, we give an alternate formula for det(A) by applying a change of variable
to the defining formula for the determinant. As a consequence, we obtain the identity
det(AT) = det(A).
Consider the definition (5.1) of the determinant of A ∈Mn(R). Writing the term
A(f(1), 1) · · · A(f(n), n) as a product over i ∈[n], we see that
det(A) =
X
f∈Sn
sgn(f)
Y
i∈[n]
A(f(i), i).
(5.5)
Determinants via Calculations
109
Since f ∈Sn, the map f −1 : [n] →[n] is a bijection of the index set of the product
operation in this formula. Therefore, applying (5.4) with x replaced by i, S replaced by [n],
H(x) replaced by A(f(i), i), and C replaced by f −1, we see that
Y
i∈[n]
A(f(i), i) =
Y
i∈[n]
A(f(f −1(i)), f −1(i)) =
Y
i∈[n]
A(i, f −1(i))
for each f ∈Sn. Putting this into the determinant formula gives
det(A) =
X
f∈Sn
sgn(f)
Y
i∈[n]
A(i, f −1(i)).
We now perform a change of variable on the outer summation. Let C : Sn →Sn be the
bijection C(f) = f −1 for f ∈Sn. Applying (5.3), we obtain
X
f∈Sn
sgn(f)
Y
i∈[n]
A(i, f −1(i)) =
X
f∈Sn
sgn(f −1)
Y
i∈[n]
A(i, (f −1)−1(i))
=
X
f∈Sn
sgn(f)
Y
i∈[n]
A(i, f(i)).
The last step uses the facts sgn(f) = sgn(f −1) and (f −1)−1 = f. We conclude that
det(A) =
X
f∈Sn
sgn(f)
Y
i∈[n]
A(i, f(i)).
(5.6)
Intuitively, this result means that we can compute the determinant as follows. Select one
element from each row of A, say the element in column f(i) of row i for each i ∈[n].
Multiply these chosen elements together, and multiply by sgn(f) (which is zero if f is not
a permutation). Then sum all terms obtained by making such choices. In contrast, our
previous description chose one element from each column of A. The symmetric roles of
the rows and columns in Formulas (5.5) and (5.6) let us deduce many column-oriented
properties of determinants from the corresponding row-oriented properties. Examples of
this symmetry between rows and columns appear in the coming sections.
Determinant Rules for AT, A, and A∗. For all A ∈Mn(R), det(AT) = det(A).
For all A ∈Mn(C), det(A∗) = det( A ) = det(A).
Proof. Fix A ∈Mn(R). Using the original formula (5.1) for det(AT), then the definition of
AT, then the new formula (5.6) for det(A), we compute:
det(AT)
=
X
f∈Sn
sgn(f)AT(f(1), 1)AT(f(2), 2) · · · AT(f(n), n)
=
X
f∈Sn
sgn(f)A(1, f(1))A(2, f(2)) · · · A(n, f(n))
=
X
f∈Sn
sgn(f)
Y
i∈[n]
A(i, f(i)) = det(A).
For the rest of the proof, assume A has complex entries. Given z ∈C, we temporarily write
c(z) = z for complex conjugation, so c(x + iy) = x −iy for x, y ∈R. You can check that
c(z + z′) = c(z) + c(z′), c(zz′) = c(z)c(z′), and c(az) = ac(z) for all z, z′ ∈C and a ∈R.
110
Advanced Linear Algebra
The first two formulas extend by induction to finite sums and products. Hence,
det( A )
=
X
f∈Sn
sgn(f)
Y
i∈[n]
A(f(i), i) =
X
f∈Sn
sgn(f)
Y
i∈[n]
c(A(f(i), i))
=
X
f∈Sn
sgn(f)c

Y
i∈[n]
A(f(i), i)


=
c

X
f∈Sn
sgn(f)
Y
i∈[n]
A(f(i), i)


=
c(det(A)) = det(A).
So the determinant of the matrix conjugate of A is the complex conjugate of the determinant
of A. Since A∗= AT, we get det(A∗) = det(AT) = det(A).
5.6
Multilinearity and the Alternating Property
We now investigate the relationship between the determinant of a square matrix A and the
rows of A. Given A ∈Mn(R) and i ∈[n], we let A[i] ∈Rn be row i of A, so that A[i](j) =
A(i, j) for all j ∈[n]. In this section, we often identify the matrix A with the ordered list
of rows (A[1], A[2], . . . , A[n]) ∈(Rn)n. Thus, the determinant map can be regarded as a
function det : (Rn)n →R of n input variables, where each variable A[i] = Ai is an element
of Rn that corresponds to row i of A. Using the row notation, formulas (5.1) and (5.6) for
the determinant of A become
det(A1, . . . , An) =
X
f∈Sn
sgn(f)
n
Y
k=1
Af(k)(k) =
X
f∈Sn
sgn(f)
n
Y
k=1
Ak(f(k)).
(5.7)
Multilinearity Property of Determinants.
The determinant, viewed as a function
det : (Rn)n →R of the n rows of A, is R-multilinear.
Multilinearity means that, if any index i ∈[n] and the rows Aj for all j ̸= i are held
constant, then the determinant is an R-linear function of the remaining input argument Ai,
which is an n-tuple in Rn. In more detail, fix i ∈[n] and n-tuples Aj ∈Rn for all j ̸= i
in [n], and define D : Rn →R by D(v) = det(A1, . . . , Ai−1, v, Ai+1, . . . , An) for v ∈Rn.
The multilinearity property says that D(v + w) = D(v) + D(w) and D(rv) = rD(v) for all
v, w ∈Rn and r ∈R.
Proof. With the above notation, replace Ai by v = (v(1), . . . , v(n)) in (5.7). We obtain
D(v) =
X
f∈Sn
sgn(f)
i−1
Y
k=1
Ak(f(k))v(f(i))
n
Y
k=i+1
Ak(f(k)) =
X
f∈Sn
v(f(i)) sgn(f)
Y
k∈[n]:
k̸=i
Ak(f(k)).
Each summand here is one of the components of v multiplied by a scalar in R. Grouping
together summands that involve the same component of v, we see that we can write D(v) =
Pn
j=1 cjv(j), where each cj is a scalar in R that depends on the fixed Ak with k ̸= i but
Determinants via Calculations
111
does not depend on v. Explicitly, for each j ∈[n], we have
cj =
X
f∈Sn:
f(i)=j
sgn(f)
Y
k∈[n]:
k̸=i
Ak(f(k)).
The R-linearity of the function D follows by computing
D(v + w) =
n
X
j=1
cj(v + w)(j) =
n
X
j=1
cjv(j) +
n
X
j=1
cjw(j) = D(v) + D(w);
D(av) =
n
X
j=1
cj(av)(j) = a
n
X
j=1
cjv(j) = aD(v)
for all v, w ∈Rn and a ∈R.
Since D is linear, D(0Rn) = 0R, which means that a matrix containing a row of zeroes
has determinant zero.
Here is an example of the formula D(v) = Pn
j=1 cjv(j) in the case where A ∈M3(R)
and i = 3. Fix rows A1 = (4, 1, −3) and A2 = (0, 2, 5). For v = (v1, v2, v3) ∈R3,
D(v) = det


4
1
−3
0
2
5
v1
v2
v3

= 11v1 −20v2 + 8v3,
and this is an R-linear function of the row vector v.
Alternating Property of Determinants. The determinant of a matrix with two equal
rows is 0.
Proof. Let A ∈Mn(R) have rows (A1, A2, . . . , An). We assume Ai = Aj for some i ̸= j and
prove det(A1, . . . , An) = 0. Using the assumption Ai = Aj, formula (5.7) becomes
det(A) =
X
f∈Sn
sgn(f)
Y
k∈[n]
Ak(f(k)) =
X
f∈Sn
sgn(f)Ai(f(i))Ai(f(j))
Y
k∈[n]:
k̸=i,j
Ak(f(k)).
Let us compare the summand coming from some f ∈Sn with the summand coming from
f ′ = f ◦(i, j). We have f ′(i) = f(j), f ′(j) = f(i), f ′(k) = f(k) for all k ̸= i, j, and
sgn(f ′) = −sgn(f) by §2.7. It follows that
sgn(f ′)Ai(f ′(i))Ai(f ′(j))
Y
k∈[n]:
k̸=i,j
Ak(f ′(k)) = −sgn(f)Ai(f(j))Ai(f(i))
Y
k∈[n]:
k̸=i,j
Ak(f(k)).
Since R is commutative, the summands for f and f ′ cancel each other. Pairing the n! terms
in the sum for det(A) in this way, we see that all terms cancel, hence det(A) = 0.
5.7
Elementary Row Operations and Determinants
Using the fact that the determinant is alternating and multilinear in the rows of A, we
can deduce some results on how elementary row operations affect determinants. Write
112
Advanced Linear Algebra
A = (A1, . . . , An) (a list of n rows). The first type of elementary row operation multiplies
row i of A by an invertible scalar c ∈R. From the multilinearity property, we know
det(A1, . . . , cAi, . . . , An) = c det(A1, . . . , Ai, . . . , An)
for any scalar c ∈R (invertible or not). So, multiplying one row of a matrix by a scalar
multiplies the determinant of the matrix by that scalar. Multiplying the whole matrix by a
given scalar c is the same as multiplying each of the n rows by c. Each row multiplication
introduces a factor of c, so
det(cA) = cn det(A)
for all A ∈Mn(R).
The second type of elementary row operation interchanges two distinct rows of A, say
row i and row j. We claim this operation changes the sign of det(A):
det(A1, . . . , Ai, . . . , Aj, . . . , An) = −det(A1, . . . , Aj, . . . , Ai, . . . , An).
(5.8)
To prove this, fix i, j in [n] with i ̸= j, fix the other rows Ak for all k ̸= i, j in [n]. Define
D : Rn × Rn →R by D(v, w) = det(A1, . . . , v, . . . , w, . . . , An) for v, w ∈Rn, where v is in
position i and w is in position j. By the alternating property already proved, D(v, v) = 0
for all v ∈Rn. Furthermore, D is linear in each of its two inputs, since the determinant is
multilinear. We therefore have
0
=
D(Ai + Aj, Ai + Aj) = D(Ai + Aj, Ai) + D(Ai + Aj, Aj)
=
D(Ai, Ai) + D(Aj, Ai) + D(Ai, Aj) + D(Aj, Aj)
=
0 + D(Aj, Ai) + D(Ai, Aj) + 0.
Rearranging, we get D(Ai, Aj) = −D(Aj, Ai), which proves (5.8).
The third type of elementary row operation adds c times row j to row i of a matrix,
where i, j in [n], i ̸= j, and c ∈R. We claim this operation does not change det(A):
det(A1, . . . , Ai + cAj, . . . , Aj, . . . , An) = det(A1, . . . , Ai, . . . , Aj, . . . , An).
We prove this using multilinearity and the alternating property. Defining D as in the last
paragraph, we have
D(Ai + cAj, Aj) = D(Ai, Aj) + cD(Aj, Aj) = D(Ai, Aj) + c · 0 = D(Ai, Aj).
We can interpret these results as facts about elementary matrices (see §4.9). Suppose A ∈
Mn(R) and A′ results from A by performing some elementary row operation. Then A′ = EA,
where E is the elementary matrix obtained from In by applying the same row operation that
took A to A′. We now show that det(A′) = det(E) det(A) for all three types of elementary
operations. First, suppose A′ is obtained by multiplying row i of A by an invertible c ∈R.
Here, E is a diagonal matrix with a c in the i, i-position and other diagonal entries equal
to 1. We know det(A′) = c det(A) and det(E) = c det(I) = c, so det(A′) = det(E) det(A)
in this case. Second, suppose A′ is obtained from A by interchanging rows i and j, so E
is obtained from I by interchanging rows i and j. Then det(A′) = −det(A) and det(E) =
−det(I) = −1R, so det(A′) = det(E) det(A) in this case. Third, suppose A′ is obtained from
A by adding c times row j to row i. Then det(A′) = det(A) and det(E) = det(I) = 1R, so
det(A′) = det(E) det(A) holds. To summarize, for all square matrices A and all elementary
matrices E of the same size as A, det(EA) = det(E) det(A). When R is a field, we can
use this fact to prove the Determinant Product Formula: det(BA) = det(B) det(A) for all
A, B ∈Mn(R) (see §5.9).
Determinants via Calculations
113
5.8
Determinant Properties Involving Columns
The discussion in the last two sections, relating determinants to the rows of a matrix, also
applies to the columns of a matrix. Given A ∈Mn(R) and j ∈[n], let A[j] ∈Rn be column
j of A, so that A[j](i) = A(i, j) for all i ∈[n]. We identify the matrix A with the ordered
list of its columns, writing A = (A[1], . . . , A[n]) ∈(Rn)n. Using this column notation, our
formulas (5.5) and (5.6) for the determinant become
det(A) = det(A[1], . . . , A[n]) =
X
f∈Sn
sgn(f)
n
Y
k=1
A[k](f(k)) =
X
f∈Sn
sgn(f)
n
Y
k=1
A[f(k)](k).
Starting from these formulas, we can repeat all the proofs in the last two sections verbatim,
changing all subscripts (row indices) to superscripts (column indices). We thereby obtain
the following facts.
(a) The determinant is an R-multilinear function of the n columns of A.
(b) If any two columns of A are equal or if A has a column of zeroes, then det(A) = 0.
(c) Multiplying a column of A by c ∈R multiplies the determinant by c.
(d) Interchanging two columns of A flips the sign of the determinant.
(e) Adding c times column i to column j ̸= i does not change the determinant.
Since elementary column operations correspond to right multiplication by elementary
matrices (§4.9), facts (c), (d), and (e) imply that det(AE) = det(A) det(E) for all matrices
A ∈Mn(R) and all elementary matrices E ∈Mn(R).
All of these results can also be derived quickly from the corresponding results for rows
by invoking the identity det(A) = det(AT). For instance, if A has two equal columns, then
AT has two equal rows, and so det(A) = det(AT) = 0. For another example of this proof
method, assume A is arbitrary and E is elementary. Then ET is also elementary (as is
readily checked), so that
det(AE) = det((AE)T) = det(ETAT) = det(ET) det(AT) = det(A) det(E).
This computation uses commutativity of the ring R (cf. Exercise 73).
5.9
Product Formula via Elementary Matrices
In this section only, we consider matrices with entries in a field F. We use facts about elemen-
tary matrices and row reduction to prove the Product Formula (det(AB) = det(A) det(B)
for A, B ∈Mn(F)). We also prove that A is invertible iff det(A) ̸= 0F . Proofs of the
corresponding results for matrices with entries in any commutative ring are given later
(see §5.11 and §5.13).
First, we indicate a method for computing determinants of matrices in Mn(F) that
is much more efficient than computing all n! terms in the defining formula. Given a
square matrix A, perform the Gaussian Elimination Algorithm, using a finite sequence
of elementary row operations to bring A into echelon form. Since A is square, the echelon
form must be an upper-triangular matrix. We can use the results in §5.7 to keep track of
how the determinant of the matrix changes as each row operation is applied. As we have
seen, for every elementary row operation, the determinant is multiplied by some nonzero
scalar in the field F (possibly the scalar 1). When a triangular matrix is reached, we can
find the determinant by multiplying the diagonal entries.
114
Advanced Linear Algebra
For example, let the matrix A =


0
2
1
3
6
−3
2
5
1

have unknown determinant det(A) = d.
We perform Gaussian elimination steps on A as shown below, keeping track of the effect of
each elementary row operation on the determinant:


0
2
1
3
6
−3
2
5
1


R1↔R2
−→


3
6
−3
0
2
1
2
5
1


R1×1/3
−→


1
2
−1
0
2
1
2
5
1


det = d
det = −d
det = −d/3
R3−2R1
−→


1
2
−1
0
2
1
0
1
3


R2↔R3
−→


1
2
−1
0
1
3
0
2
1


R3−2R2
−→


1
2
−1
0
1
3
0
0
−5


det = −d/3
det = d/3
det = d/3
The final matrix is triangular, with determinant 1·1·(−5) = −5. Since this also equals d/3,
we see that det(A) = −15.
Determinant Criterion for Matrix Invertibility. Let F be a field and A ∈Mn(F).
The matrix A is invertible if and only if det(A) ̸= 0F .
Proof. Given any A ∈Mn(F), perform row-reduction steps to convert A to a reduced row-
echelon form matrix Aech, as defined in §4.10. We know Aech = PA for some invertible
matrix P ∈Mn(F), where P is the product of the elementary matrices associated with the
elementary row operations used to reduce A. So, A is invertible iff Aech is invertible. First
suppose A is invertible. Then Aech is invertible, so Aech cannot have any zero rows. Since
Aech is square, we must have Aech = In, which has determinant 1F . The determinant of
A differs from this by some factor that is a product of nonzero elements of F. Hence, A
invertible implies det(A) ̸= 0F . Conversely, suppose A is not invertible. Then Aech is not
invertible, hence Aech must be an upper-triangular matrix with at least one zero on the
main diagonal. So det(Aech) = 0F . The determinant of A is a nonzero scalar multiple of
this determinant, so A non-invertible implies det(A) = 0F .
Determinant Product Formula. Let F be a field. For all A, B ∈Mn(F),
det(AB) = det(A) det(B).
Proof. We already proved the Product Formula when A is an elementary matrix and B is
any matrix (§5.7). Next, we show that the formula holds when A = E1 · · · Ek is a finite
product of elementary matrices, and B is arbitrary. We prove this by induction on k, the
case k = 1 already being known. Fix k > 1, and assume the formula holds when A is a
product of k −1 elementary factors. For A = E1 · · · Ek, we compute:
det(AB)
=
det(E1(E2 · · · EkB)) = det(E1) det((E2 · · · Ek)B)
=
det(E1) det(E2 · · · Ek) det(B) = det(E1(E2 · · · Ek)) det(B)
=
det(A) det(B).
The first and fifth equalities hold by associativity of matrix multiplication; the second and
fourth equalities hold by the product formula where the left factor is elementary; and the
third equality holds by induction hypothesis.
Determinants via Calculations
115
Using Gaussian elimination, you can show that A ∈Mn(R) is invertible iff A is a finite
product of elementary matrices (see §4.11). Therefore, we have proved that the Product
Formula det(AB) = det(A) det(B) holds when A is invertible and B is arbitrary. Similarly,
the formula holds when B is invertible and A is arbitrary. Finally, consider the case where
neither A nor B is invertible. As B is not invertible, there exists some nonzero v ∈F n with
Bv = 0 (see §4.13). Hence (AB)v = A(Bv) = A0 = 0, so AB is not invertible. It follows
that det(AB) = 0F = 0F · 0F = det(A) det(B).
5.10
Laplace Expansions
We now prove the Laplace expansions for determinants. Given A ∈Mn(R), let A[i|j] be the
matrix in Mn−1(R) obtained by deleting row i and column j of A. Formally, the s, t-entry
of A[i|j] is A(s, t) if s < i and t < j; A(s + 1, t) if i ≤s ≤n −1 and t < j; A(s, t + 1) if
s < i and j ≤t ≤n −1; and A(s + 1, t + 1) if i ≤s ≤n −1 and j ≤t ≤n −1. For any
logical statement P, write χ(P) = 1 if P is true and χ(P) = 0 if P is false. Then we can
define A[i|j] more succinctly by the formula
A[i|j](s, t) = A(s + χ(s ≥i), t + χ(t ≥j)) for all s, t ∈[n −1].
Laplace Expansions for Determinants. For all A ∈Mn(R), i ∈[n], and j ∈[n],
det(A) =
n
X
j=1
A(i, j)(−1)i+j det(A[i|j])
and
det(A) =
n
X
i=1
A(i, j)(−1)i+j det(A[i|j]).
The first formula, called Laplace expansion along row i, arises by grouping the n! terms in
the definition of det(A) based on the entry chosen from row i. The second formula, called
Laplace expansion along column j, arises by grouping terms based on the entry chosen from
column j.
Proof. Step 1: We prove the row expansion formula for i = n. Starting with the defining
formula
det(A) =
X
f∈Sn
sgn(f)
Y
k∈[n]
A(f(k), k),
we group together the terms in this sum that involve A(n, j) for each j between 1 and n.
The entry A(n, j) appears in the term indexed by f ∈Sn iff f(j) = n. Therefore,
det(A) =
n
X
j=1
A(n, j)Uj, where Uj =
X
f∈Sn:
f(j)=n
sgn(f)
Y
k∈[n]:
k̸=j
A(f(k), k).
We complete Step 1 by showing that Uj = (−1)n+j det(A[n|j]) for j ∈[n].
From now on, keep j fixed. Introduce the notation S = {f ∈Sn : f(j) = n} and
T = Sn−1. Define a bijection C : T →S as follows. View an element g ∈Sn−1 as a list
[g(1), . . . , g(n −1)] that uses the numbers 1 through n −1 once each. Define C(g) to be the
list
g′ = [g(1), . . . , g(j −1), n, g(j), . . . , g(n −1)],
which is an element of S. C is a bijection; the inverse of C takes an input list f ∈S and
deletes the entry in position j, which must be the integer n.
116
Advanced Linear Algebra
Applying the change-of-variable formula (5.2) and writing g′ for C(g), we see that
Uj =
X
f∈S
sgn(f)
Y
k∈[n]:
k̸=j
A(f(k), k) =
X
g∈T
sgn(g′)
Y
k∈[n]:
k̸=j
A(g′(k), k).
(5.9)
To continue simplifying, let us first relate sgn(g′) to sgn(g). Recall that sgn(g) = (−1)inv(g),
where inv(g) is the minimum number of transpositions of adjacent elements needed to sort
the list g = [g(1), . . . , g(n −1)] (see §2.6). Similarly, sgn(g′) = (−1)inv(g′), where inv(g′) is
the minimum number of transpositions of adjacent elements needed to sort the list
g′ = [g(1), . . . , g(j −1), n, g(j), . . . , g(n −1)].
The latter list may be sorted by first moving the largest entry n from position j to position
n (which requires n−j transposition moves) and then sorting the first n−1 elements of the
resulting list. Since the resulting list is [g(1), . . . , g(n−1), n], the second stage of the sorting
requires inv(g) moves. Therefore, inv(g′) = inv(g) + n −j. Raising −1R to this power and
noting that (−1)n−j = (−1)n+j in any ring, we see that
sgn(g′) = (−1)n+j sgn(g).
(5.10)
Next, we find A(g′(k), k) in terms of g and A[n|j]. If 1 ≤k < j, then g′(k) = g(k) by
definition. Also g(k) < n, so A[n|j](g(k), k) = A(g(k), k) = A(g′(k), k) in this case. On
the other hand, if j < k ≤n, then g′(k) = g(k −1) < n and A[n|j](g(k −1), k −1) =
A(g(k −1), k) = A(g′(k), k) by definition. We can therefore write
Y
k∈[n]:
k̸=j
A(g′(k), k)
=
Y
1≤k<j
A[n|j](g(k), k)
Y
j<k≤n
A[n|j](g(k −1), k −1)
=
Y
1≤k<j
A[n|j](g(k), k)
Y
j≤k≤n−1
A[n|j](g(k), k)
=
Y
k∈[n−1]
A[n|j](g(k), k).
Using this and (5.10) in (5.9), we see that
Uj = (−1)n+j
X
g∈Sn−1
sgn(g)
Y
k∈[n−1]
A[n|j](g(k), k) = (−1)n+j det(A[n|j]).
Step 2: We prove that if the Laplace expansion formula holds for a given row i > 1,
then it holds for row i −1. Combining this with Step 1, we can conclude that the Laplace
expansion is valid for any row of A. To prove Step 2, assume that 1 < i ≤n is fixed and
det(B) =
n
X
j=1
B(i, j)(−1)i+j det(B[i|j])
is known to be true for any B ∈Mn(R). Given A, let B be the matrix obtained from A
by interchanging row i −1 and row i. We know that det(B) = −det(A). We see from the
definitions that B[i|j] = A[i −1|j] and B(i, j) = A(i −1, j) for all j ∈[n]. So,
det(A)
=
−det(B) =
n
X
j=1
B(i, j)(−1)i+j+1 det(B[i|j])
=
n
X
j=1
A(i −1, j)(−1)(i−1)+j det(A[i −1|j]).
This is the Laplace expansion of det(A) along row i −1.
Determinants via Calculations
117
Step 3: We prove the Laplace expansion along column k, for each k ∈[n]. From Steps 1
and 2, we already know that
det(B) =
n
X
s=1
B(k, s)(−1)k+s det(B[k|s])
holds for any B and any row index k. Given A, take B = AT. The definitions show that
B(k, s) = A(s, k) and B[k|s] = A[s|k]T (since deleting row k and column s of B = AT
has the same effect as deleting row s and column k of A, and then transposing). We now
compute
det(A)
=
det(B) =
n
X
s=1
B(k, s)(−1)k+s det(B[k|s])
=
n
X
s=1
A(s, k)(−1)s+k det(A[s|k]T) =
n
X
i=1
A(i, k)(−1)i+k det(A[i|k]).
This is the Laplace expansion of det(A) along column k.
5.11
Classical Adjoints and Inverses
Given a matrix A ∈Mn(R), we define an n×n matrix called the classical adjoint or adjunct
of A, written adj(A) or A′. For i, j ∈[n], we set A′(i, j) = (−1)i+j det(A[j|i]). So the i, j-
entry of A′ is the determinant of the matrix obtained by deleting row j and column i of A,
times a certain sign.
Classical Adjoint Formula. For all A ∈Mn(R), AA′ = A′A = det(A)In.
Proof. We show AA′ = det(A)In in two steps. First, consider the i, j-entry of AA′, where
i, j ∈[n] and i ̸= j. The definition of matrix multiplication gives
(AA′)(i, j) =
n
X
k=1
A(i, k)A′(k, j) =
n
X
k=1
A(i, k)(−1)j+k det(A[j|k]).
Let C be the matrix obtained from A by replacing row j of A by row i. We see that
C[j|k] = A[j|k] and C(j, k) = A(i, k) for all k ∈[n]. Since C has two equal rows, det(C) = 0.
On the other hand, the Laplace expansion along row j of C gives
0 = det(C) =
n
X
k=1
C(j, k)(−1)j+k det(C[j|k]) =
n
X
k=1
A(i, k)(−1)j+k det(A[j|k]).
Comparing to the previous formula, we see that (AA′)(i, j) = 0 = (det(A)In)(i, j).
The second stage is to compute (AA′)(i, i), where i ∈[n]. Here, we find that
(AA′)(i, i) =
n
X
k=1
A(i, k)A′(k, i) =
n
X
k=1
A(i, k)(−1)i+k det(A[i|k]) = det(A),
where the last equality is Laplace expansion along row i. Thus, (AA′)(i, i) = det(A) =
(det(A)In)(i, i), as needed.
118
Advanced Linear Algebra
The companion formula A′A = In can be proved similarly, using Laplace expansions
along columns. Alternatively, it follows from the definitions that (A′)T = (AT)′. So the
result already proved (applied to AT) gives
(A′A)T = AT(A′)T = AT(AT)′ = det(AT)In = det(A)In.
Transposing both sides gives A′A = det(A)IT
n = det(A)In.
One consequence of the Classical Adjoint Formula is the following criterion for invert-
ibility of a matrix.
Determinant Criterion for Matrix Invertibility. Suppose R is a commutative ring and
A ∈Mn(R). The matrix A has an inverse in Mn(R) if and only if det(A) has a multiplicative
inverse in R.
Proof. First, assume det(A) is an invertible element of R. Multiplying the Classical Adjoint
Formula by the inverse of det(A) we see that A has an inverse in Mn(R) given by A−1 =
det(A)−1A′. For i, j ∈[n], an explicit formula for the i, j-entry of A−1 is
A−1(i, j) = (−1)i+j det(A[j|i]) det(A)−1.
(5.11)
For the converse, suppose A has an inverse B in Mn(R), so AB = In = BA. Taking deter-
minants and using the Product Formula, we get det(A) det(B) = det(In) = det(B) det(A).
Since det(In) = 1R, we see that det(B) is a two-sided inverse for det(A) in the ring R.
For example, consider the matrix A =


0
2
1
3
6
−3
2
5
1

∈M3(Z). The classical adjoint
of A is A′ =


21
3
−12
−9
−2
3
3
4
−6

. For instance, A′(2, 3) = (−1)2+3 det
 0
1
3
−3

= 3. We
found earlier that det(A) = −15, and you can check that AA′ = A′A = −15I3. Since −15
is not invertible in the ring Z, A has no inverse in the ring M3(Z). On the other hand, A is
invertible in the ring M3(R), with inverse A−1 = (−1/15)A′.
5.12
Cramer’s Rule
One application of the formula for A−1 is Cramer’s Rule for solving a nonsingular system of n
linear equations in n unknowns (nonsingular means the system has a unique solution). Write
this system in matrix notation as Ax = b, where A ∈Mn(R) has invertible determinant,
x ∈Rn is a column vector of unknowns, and b ∈Rn is a given column vector. To solve for
a particular unknown xi, multiply both sides by A−1 on the left, obtaining x = A−1b, and
take the ith component:
xi = (A−1b)i =
n
X
k=1
A−1(i, k)bk.
Using the formula (5.11) for A−1, we get
xi = det(A)−1
n
X
k=1
bk(−1)i+k det(A[k|i]).
Determinants via Calculations
119
To obtain Cramer’s Rule, let Ci be the matrix whose columns are
(A[1], . . . , A[i−1], b, A[i+1], . . . , A[n]).
Evidently, Ci[k|i] = A[k|i] and Ci(k, i) = bk for all k ∈[n]. Laplace expansion along column
i of Ci therefore gives
det(Ci) =
n
X
k=1
Ci(k, i)(−1)k+i det(Ci[k|i]) =
n
X
k=1
bk(−1)i+k det(A[k|i]).
Comparing this to the formula for xi, we see that xi = det(A)−1 det(Ci) for all i ∈[n]. To
summarize, we have proved:
Cramer’s Rule. Suppose A ∈Mn(R), det(A) is invertible in R, b is a given column vector,
and x is the unique solution to Ax = b. For all i ∈[n], we can compute xi by replacing
column i of A by b, taking the determinant of this matrix, and dividing by det(A).
We stress that Cramer’s Rule only applies when the coefficient matrix A is square with
invertible determinant. Furthermore, when R is a field, Gaussian elimination is often a more
efficient way to solve the linear system compared to the multiple determinant evaluations
required by Cramer’s Rule.
As an example of Cramer’s Rule, let us solve the linear system of equations



2x2
+x3
=
9
3x1
+6x2
−3x3
=
−2
2x1
+5x2
+x3
=
4.
This system has the form Ax = b, where A =


0
2
1
3
6
−3
2
5
1

and b =


9
−2
4

. Earlier,
we calculated det(A) = −15. Hence, Cramer’s Rule gives
x1
=
1
−15 det


9
2
1
−2
6
−3
4
5
1

= 135
−15 = −9;
x2
=
1
−15 det


0
9
1
3
−2
−3
2
4
1

= −65
−15 = 13
3 ;
x3
=
1
−15 det


0
2
9
3
6
−2
2
5
4

= −5
−15 = 1
3.
5.13
Product Formula via Computations
Determinant Product Formula. Let R be a commutative ring. For all A, B ∈Mn(R),
det(AB) = det(A) det(B).
Proof. Fix A, B ∈Mn(R). Replacing A by AB in the definition (5.1) gives:
det(AB) =
X
f∈Sn
sgn(f)(AB)(f(1), 1)(AB)(f(2), 2) · · · (AB)(f(n), n).
120
Advanced Linear Algebra
Next, use the definition of matrix multiplication to write each factor as
(AB)(f(j), j) =
n
X
kj=1
A(f(j), kj)B(kj, j).
Putting these expressions into the previous formula and using the distributive law repeatedly
(see Exercise 64), we get:
det(AB) =
X
f∈Sn
n
X
k1=1
· · ·
n
X
kn=1
sgn(f)A(f(1), k1)B(k1, 1) · · · A(f(n), kn)B(kn, n).
We reorder the sums and products, using commutativity of addition and multiplication in
R:
det(AB) =
n
X
k1=1
· · ·
n
X
kn=1
X
f∈Sn
sgn(f)A(f(1), k1) · · · A(f(n), kn)B(k1, 1) · · · B(kn, n).
We can regard the n outer summations over integers kj ∈[n] as a single summation over
lists k = [k1, k2, . . . , kn] ∈[n]n. Also, we can factor out the values B(kj, j) that do not
depend on the inner summation index f. We get:
det(AB) =
X
k∈[n]n
B(k1, 1) · · · B(kn, n)

X
f∈Sn
sgn(f)A(f(1), k1) · · · A(f(n), kn)

.
Given a fixed list k ∈[n]n, note that the term in brackets is the determinant of the matrix
C whose columns are (A[k1], . . . , A[kn]). To see why, note that C(i, j) = A(i, kj) for i, j ∈[n],
and so
det(C) =
X
f∈Sn
sgn(f)
Y
j∈[n]
C(f(j), j) =
X
f∈Sn
sgn(f)
Y
j∈[n]
A(f(j), kj).
If ki = kj for any i ̸= j, then det(C) = 0 because C has two equal columns. Therefore, we
can discard all summands of the outer summation indexed by lists k = [k1, . . . , kn] with a
repeated entry. The surviving summands are indexed by permutations k = [k1, . . . , kn] ∈
Sn. Suppose we sort the list [k1, . . . , kn] into increasing order by interchanging adjacent
elements, and at the same time interchanging the corresponding adjacent columns in C.
Each such interchange multiplies det(C) by −1. We need inv(k) interchanges to reach the
list [1, 2, . . . , n] (see §2.6), and at that point we have transformed C to the matrix A whose
columns are in their original order. Therefore, det(C) = (−1)inv(k) det(A) = sgn(k) det(A).
Replacing the term in square brackets by this formula, and factoring det(A) out of the main
sum, we now see that
det(AB) = det(A)
X
k∈Sn
sgn(k)B(k1, 1) · · · B(kn, n).
The remaining sum is precisely the definition of det(B), so the proof is finished.
A short but abstract proof of the Determinant Product Formula is given in §20.17.
Determinants via Calculations
121
5.14
Cauchy–Binet Formula
The Cauchy–Binet formula is a generalization of the Determinant Product Formula to
products of rectangular matrices.
Cauchy–Binet Formula. Suppose R is a commutative ring, A ∈Mm,n(R), B ∈Mn,m(R),
and m ≤n. Let J be the set of all strictly increasing lists j = [j1, j2, . . . , jm] ∈[n]m. Then
det(AB) =
X
j∈J
det(A[j1], A[j2], . . . , A[jm]) det(B[j1], B[j2], . . . , B[jm]).
Here, A[jk] is the jkth column of A, and B[jk] is the jkth row of B. So every determinant
appearing in the formula is the determinant of an m × m matrix. If m = n, then J consists
of the single list [1, 2, . . . , n], and the Cauchy–Binet Formula reduces to the Determinant
Product Formula for square matrices.
Proof. We generalize the proof given in the last section. First, the definitions of determinants
and matrix products give
det(AB) =
X
f∈Sm
sgn(f)
Y
i∈[m]
(AB)(f(i), i) =
X
f∈Sm
sgn(f)
Y
i∈[m]
X
ki∈[n]
A(f(i), ki)B(ki, i).
Using the distributive law repeatedly (see Exercise 64), we get
det(AB) =
X
f∈Sm
X
k1∈[n]
· · ·
X
km∈[n]
sgn(f)
Y
i∈[m]
A(f(i), ki)
Y
i∈[m]
B(ki, i).
Now we move the summation over f inside the other sums, replace the multiple summations
over indices k1, . . . , km by a single summation over lists k = [k1, . . . , km] ∈[n]m, and
rearrange factors in the resulting summation:
det(AB) =
X
k∈[n]m
Y
i∈[m]
B(ki, i)

X
f∈Sm
sgn(f)
Y
i∈[m]
A(f(i), ki)

.
We recognize the term in brackets as the definition of det(A[k1], . . . , A[km]). If ki = kj for
some i ̸= j, then the matrix (A[k1], . . . , A[km]) has two equal columns, so its determinant is
zero. Discarding these summands, we now have
det(AB) =
X
k∈[n]m:
ki distinct
det(A[k1], . . . , A[km])
Y
i∈[m]
B(ki, i).
Given a list k = [k1, . . . , km] of distinct elements of [n], let sort(k) be the list obtained
by sorting k into increasing order. Note that j = sort(k) belongs to J. Define inv(k)
and sgn(k) as in the case m = n (see §2.6). It takes inv(k) interchanges of adjacent
columns to turn the matrix (A[k1], . . . , A[km]) into the matrix (A[j1], . . . , A[jm]). Therefore,
det(A[k1], . . . , A[km]) = sgn(k) det(A[j1], . . . , A[jm]). Grouping summands in the formula for
122
Advanced Linear Algebra
det(AB) based on the value of j = sort(k), we obtain
det(AB)
=
X
j∈J
X
k∈[n]m:
sort(k)=j
det(A[k1], . . . , A[km])
Y
i∈[m]
B(ki, i)
=
X
j∈J
det(A[j1], . . . , A[jm])


X
k∈[n]m:
sort(k)=j
sgn(k)
Y
i∈[m]
B(ki, i)

.
To finish, compare the term in square brackets to the definition of det(B[j1], . . . , B[jm]).
The formula in brackets looks like the original definition of the determinant of a matrix,
except now we are indexing the rows of the matrix by the increasing sequence j1 < · · · < jm
instead of the standard indexing sequence 1 < 2 < · · · < m. This renaming of indices makes
no difference when we calculate inv(k) and sgn(k). Therefore, the term in brackets is none
other than det(B[j1], . . . , B[jm]). (To prove this formally, change the summation variable
from a sum over the set of bijections k : [m] →{j1, . . . , jm} to a sum over the set Sm of
bijections from [m] to [m].) To summarize,
det(AB) =
X
j∈J
det(A[j1], A[j2], . . . , A[jm]) det(B[j1], B[j2], . . . , B[jm]).
For another proof, see Exercise 56 in Chapter 20.
5.15
Cayley–Hamilton Theorem
Given A in Mn(R), the characteristic polynomial of A is defined as χA = det(xIn −A),
which is a polynomial in R[x].
Cayley–Hamilton Theorem. Suppose R is a commutative ring. For all A ∈Mn(R),
χA(A) = 0. More specifically, if χA = P
i≥0 cixi ∈R[x] with ci ∈R, then P
i≥0 ciAi is the
zero matrix.
We give a tricky, technical proof of this theorem here. For other proofs, see §8.14
and §18.19. We begin with a short, but incorrect, proof attempt. Fix A ∈Mn(R) and
let p(x) = χA(x) = det(xIn −A) be the characteristic polynomial of A. Suppose we try to
compute p(A) by simply replacing the variable x (inside the determinant) by the matrix
A. We apparently obtain p(A) = det(AIn −A) = det(0n) = 0, giving the conclusion of the
Cayley–Hamilton Theorem. However, something must be wrong with this calculation, since
p(A) is an n × n matrix, but det(0n) = 0R is a scalar in R!
The correct proof needs to be much more careful about the evaluation of the formal
variable x appearing in the characteristic polynomial. First recall the universal mapping
property (UMP) for polynomials proved in §3.5: given two commutative rings R and S,
a ring homomorphism h : R →S, and c ∈S, there exists a unique ring homomorphism
H : R[x] →S such that H(r) = h(r) for all r ∈R, and H(x) = c.
We would like to apply this result taking c to be the given matrix A. Another technical
problem immediately occurs, since A belongs to the non-commutative ring Mn(R), but S
(in the UMP) is required to be commutative. To address this issue, define S to be the set
of all matrices in Mn(R) of the form r0In + r1A + r2A2 + · · · + rkAk for some k ≥0 and
r0, r1, . . . , rk ∈R. By Exercise 61, S is a commutative subring of Mn(R) containing In and
Determinants via Calculations
123
A. We define h : R →S by setting h(r) = rIn for r ∈R; Exercise 61 shows that h is a ring
homomorphism. The UMP says there is a ring homomorphism H : R[x] →S extending h
and sending x to A. More specifically, for any r0, r1, . . . , rk ∈R,
H(r0 + r1x + · · · + rkxk) = r0In + r1A + · · · + rkAk ∈S.
We now have a precise way of talking about p(A): namely, p(A) = H(p) is the image of p
under the ring homomorphism H that extends h and sends x to A.
Since p = det(xIn −A), we need to find H(p) = H(det(xIn −A)). For this, we
need another technical digression. Suppose F : T →U is a ring homomorphism between
commutative rings T and U. We claim that there is an associated ring homomorphism
F ∗: Mn(T) →Mn(U) such that F ∗(B)i,j = F(Bi,j) for all B ∈Mn(T) and all i, j ∈[n].
This definition says that F ∗acts on a matrix B by applying F to each entry Bi,j. We claim
also that F(det(B)) = det(F ∗(B)) for all B ∈Mn(T). See Exercise 62 for the proofs of
these claims.
Applying this discussion to the ring homomorphism H : R[x] →S, we obtain a ring
homomorphism H∗: Mn(R[x]) →Mn(S) that acts by applying H to every entry of a
matrix in Mn(R[x]). Let C = H∗(xIn −A). Then
det(C) = det(H∗(xIn −A)) = H(det(xIn −A)) = H(p) = p(A).
(5.12)
Note that C ∈Mn(S) is an n × n matrix with entries in S, so each entry of C is itself an
n × n matrix built up from powers of A. This is what makes this proof tricky!
For example, suppose A =
 r
s
t
u

∈M2(R). Then xI2 −A =
 x −r
−s
−t
x −u

is in
M2(R[x]) and p = χA = det(xI2 −A) = (x −r)(x −u) −st = x2 −(r + u)x + (ru −st). So
p(A) = H(p) = A2 −(r + u)A + (ru −st)I2.
On the other hand, we get C = H∗(xI2 −A) by applying H to every entry of xI2 −A. This
produces the matrix of matrices
C =

A −rI2
−sI2
−tI2
A −uI2

=


 0
s
t
u −r

 −s
0
0
−s


−t
0
0
−t


r −u
s
t
0


.
Working in the commutative ring S, we verify (5.12) by computing
det(C) = (A −rI2)(A −uI2) −(−sI2)(−tI2) = A2 −(r + u)A + (ru −st)I2 = p(A).
Return to the case of general n and A. By definition of C, for all i, j ∈[n] with j ̸= i,
C(i, i) = A −A(i, i)In and C(i, j) = −A(i, j)In. We know that p(A) = H(p) = det(C)
by (5.12), so our task is to prove that det(C) is the zero matrix in S. For reasons that
emerge shortly, we instead prove the equivalent statement det(CT) = 0.
For a matrix A ∈Mn(R) with entries in R, we compute the vector-matrix product
wA for any row vector w ∈Rn via the formula (wA)j = Pn
i=1 wiA(i, j) for j ∈[n].
Since CT ∈Mn(S) is a matrix of matrices, we instead compute an expression of the form
zCT, where z is a row vector of row vectors. More precisely, suppose U ∈Mn(S) and
z = [z1 z2 · · · zn] ∈(Rn)n, where each zi is a row vector in Rn. Then zU ∈(Rn)n
is the row vector whose jth component is the row vector Pn
i=1 ziU(i, j). You can verify
the associativity property (zU)V = z(UV ), valid for all z ∈(Rn)n and U, V ∈Mn(S)
(Exercise 61).
124
Advanced Linear Algebra
Continuing the proof, let us take z = [e1 e2 · · · en], where each ei is the row vector of
length n with 1R in position i and 0R in all other positions. We claim zCT = 0, where 0
denotes the row vector consisting of n row vectors all equal to zero. To see this, note that
the jth row vector appearing in zCT is
n
X
i=1
eiCT(i, j)
=
ejC(j, j) +
X
i: i̸=j
eiC(j, i)
=
ej(A −A(j, j)In) +
X
i: i̸=j
ei(−A(j, i)In) = ejA −
n
X
i=1
A(j, i)ei.
The row vectors ejA and Pn
i=1 A(j, i)ei are both equal to the jth row of A, so their difference
is indeed zero. This holds for all j, so zCT = 0 as claimed.
To finish, multiply the equation zCT = 0 on the right by the classical adjoint adj(CT).
The right side is still 0 in (Rn)n. The left side becomes
(zCT) adj(CT) = z(CT adj(CT)) = z(det(CT)In).
Using the definition of zU one more time, we see that the n row vectors appearing in
0 = z(det(CT)In) are e1 det(CT), . . . , en det(CT), which are precisely the n rows of det(CT).
Since all these row vectors are zero, the matrix det(CT) = det(C) must be zero, completing
the proof of the Cayley–Hamilton Theorem.
Continuing the 2 × 2 example, we have
zCT = [[1 0] [0 1]]
 A −rI2
−tI2
−sI2
A −uI2

.
The first component of zCT is
[1 0](A −rI2) + [0 1](−sI2) = [r s] −[r 0] −[0 s] = [0 0].
Similarly, the second component of zCT is
[1 0](−tI2) + [0 1](A −uI2) = −[t 0] + [t u] −[0 u] = [0 0].
Multiplying on the right by adj(CT) gives
[[0 0] [0 0]] = (zCT) adj(CT) = z(det(CT)I2) = [[1 0] [0 1]]

det(CT)
0
0
det(CT)

.
Expanding the far right side shows that row 1 and row 2 of det(CT) are [0 0], so det(CT) =
det(C) is the 2 × 2 zero matrix.
5.16
Permanents
The permanent of a square matrix A ∈Mn(R) is defined by the formula
per(A) =
X
f∈Sn
Y
j∈[n]
A(f(j), j).
Determinants via Calculations
125
This formula comes from the defining formula (5.1) for det(A) by omitting the sign factor
sgn(f). The permanent of A is the sum of n! terms, each of which is a product of n entries
of A, which are chosen from distinct rows and columns in all possible ways.
Many of the results in this chapter extend without difficulty to permanents, by deleting
all occurrences of sgn(f) in the relevant proofs. For example, the changes of variable g = f −1
followed by j = g(i) show that
per(A) =
X
g∈Sn
Y
i∈[n]
A(i, g(i)) = per(AT).
Similarly, you can prove the following facts about permanents.
(a) For A ∈Mn(C), per( A ) = per(A∗) = per(A).
(b) per : (Rn)n →R is an R-multilinear function of the n rows of A.
(c) per : (Rn)n →R is an R-multilinear function of the n columns of A.
(d) The permanent of a matrix with a row (or column) of zeroes is zero.
(e) The permanent of a triangular or diagonal matrix is the product of the diagonal entries.
(f) Multiplying one row (or column) of A by c ∈R multiplies the permanent by c.
(g) Multiplying the entire n × n matrix by c multiplies the permanent by cn.
Instead of the alternating property of the determinant, the permanent has the following
symmetry property:
(h) Permuting the rows (or columns) of A does not change the permanent.
To prove (h), it suffices to show per(B) = per(A) when B is obtained from A by
interchanging row i and row j, since any permutation of rows can be achieved by a
sequence of such interchanges. Change the summation variable from f to g = f ◦(i, j).
Since g(i) = f(j) and g(j) = f(i), we find that
per(A) =
X
f∈Sn
Y
k∈[n]
A(k, f(k)) =
X
f∈Sn
A(i, f(i))A(j, f(j))
Y
k̸=i,j
A(k, f(k))
=
X
g∈Sn
A(i, g(j))A(j, g(i))
Y
k̸=i,j
A(k, g(k)) =
X
g∈Sn
B(j, g(j))B(i, g(i))
Y
k̸=i,j
B(k, g(k))
=
X
g∈Sn
Y
k∈[n]
B(k, g(k)) = per(B).
Unlike the case of determinants, it is not true that the permanent of a matrix with two
equal rows (or columns) must be zero. For example, the permanent of
 1
1
1
1

is 1+1 = 2.
In other words, the multilinear permanent function is not alternating. Consequently, those
properties of determinants whose proofs invoked the alternating property are no longer valid
for permanents. For instance, if B is obtained from A by adding c times row j of A to row
i of A, we cannot conclude that per(B) = per(A). Unfortunately, this means that we can
no longer use Gaussian elimination as an efficient method for computing permanents as we
did for determinants. The Laplace expansions for determinants work for permanents after
omitting the sign factors. For instance, for any i ∈[n],
per(A) =
n
X
k=1
A(i, k) per(A[i|k]).
(The general case can be deduced from the special case i = n by using the symmetry of
the permanent to reorder rows.) However, the recursive Laplace expansion still requires the
evaluation of n! terms to compute the permanent of A. The formula AA′ = det(A)In = A′A
does not generalize to permanents, and the product formula per(AB) = per(A) per(B) does
not hold in general.
126
Advanced Linear Algebra
5.17
Summary
Here are the main facts we proved about the determinant of a square n × n matrix A with
entries in a commutative ring R.
1.
Defining Formulas for the Determinant:
det(A) = P
f∈Sn sgn(f) Qn
i=1 A(f(i), i) = P
f∈Sn sgn(f) Qn
i=1 A(i, f(i)).
2.
Transpose and Conjugation: det(AT) = det(A).
For complex matrices, det( A ) = det(A) = det(A∗).
3.
Diagonal and Triangular Matrices: If A is upper-triangular, lower-triangular, or
diagonal, then det(A) = Qn
i=1 A(i, i). So det(In) = 1R and det(0n) = 0R.
4.
Multilinearity and the Alternating Property: We can view det : (Rn)n →R as a
function of the n rows of A or the n columns of A: det(A) = det(A[1], . . . , A[n]) =
det(A[1], . . . , A[n]). Either way, det is an R-multilinear function of its n inputs.
In other words, if all rows other than row i are fixed, then the function D(v) =
det(A1, . . . , Ai−1, v, Ai+1, . . . , An) (for v ∈Rn) is R-linear; similarly for columns.
If a matrix A has two equal rows, two equal columns, a zero row, or a zero column,
then det(A) = 0.
5.
Elementary Operations and Determinants: There are three elementary row and
column operations on matrices, which may be executed by multiplying on the
left or right by appropriate elementary matrices. First, multiplying one row of a
matrix by c ∈R multiplies the determinant by c; the corresponding elementary
matrix has determinant c (this matrix is elementary only for c invertible). Second,
interchanging any two distinct rows of a matrix multiplies the determinant by
−1R; the corresponding elementary matrix has determinant −1R. Third, adding
a scalar multiple of one row of a matrix to another row leaves the determinant
unchanged; the corresponding elementary matrix has determinant 1R. Combining
these facts with row-reduction via Gaussian elimination, we obtain an efficient
method for computing determinants when R is a field.
6.
Product Formula: For all A, B ∈Mn(R), det(AB) = det(A) det(B).
7.
Laplace Expansions: For all i, j ∈[n],
det(A) =
n
X
j=1
A(i, j)(−1)i+j det(A[i|j]) =
n
X
i=1
A(i, j)(−1)i+j det(A[i|j]),
where A[i|j] is the matrix obtained by deleting row i and column j of A.
8.
Classical Adjoint: The classical adjoint of A ∈Mn(R) is the matrix A′ = adj(A)
whose i, j-entry is (−1)i+j det(A[j|i]). We have A′A = det(A)In = AA′.
9.
Explicit Formula for Inverses: A matrix A is invertible in Mn(R) iff det(A) is
invertible in the commutative ring R. In this case, A−1 = det(A)−1 adj(A), so
A−1(i, j) = det(A)−1(−1)i+j det(A[j|i]).
10.
Cramer’s Rule: If Ax = b is a system of n linear equations in n unknowns whose
coefficient matrix A is invertible, then the unique solution x has components
xi = det(A)−1 det(Ci) for each i ∈[n], where Ci is the matrix obtained from A
by substituting b for the ith column of A.
Determinants via Calculations
127
11.
Cauchy–Binet Formula: If A ∈Mm,n(R), B ∈Mn,m(R), and m ≤n, then
det(AB) =
X
1≤j1<j2<···<jm≤n
det(A[j1], A[j2], . . . , A[jm]) det(B[j1], B[j2], . . . , B[jm]).
12.
Cayley–Hamilton Theorem: The characteristic polynomial of A is χA(x) =
det(xIn −A). If χA = P
i≥0 cixi with ci ∈R, then χA(A) = P
i≥0 ciAi = 0.
13.
Permanents: For A ∈Mn(R), the permanent of A is
per(A) =
X
f∈Sn
Y
j∈[n]
A(f(j), j) =
X
f∈Sn
Y
i∈[n]
A(i, f(i)) = per(AT).
The permanent is a symmetric, R-multilinear function per : (Rn)n →R whose n
inputs are the rows (or columns) of A. We have per(A) = Q
i∈[n] A(i, i) when A
is triangular or diagonal. For each k ∈[n], the Laplace expansions
per(A) =
n
X
j=1
A(k, j) per(A[k|j]) =
n
X
i=1
A(i, k) per(A[i|k])
are valid. However, the permanent is not alternating; we cannot use elementary
row operations to evaluate permanents; and there is no product formula for
permanents.
5.18
Exercises
Unless otherwise specified, assume R is a commutative ring in these exercises.
1.
Use (5.1) to write a formula for det(A) when A ∈M4(R).
2.
Suppose A ∈M5(R) satisfies A(i, j) = 0R for all i, j ∈[5] with i+j odd. Use (5.1)
to compute det(A).
3.
Describe how to find the determinant of a matrix that has exactly one nonzero
entry in every row and column. Illustrate your method by computing
det


0
4
0
0
0
2
0
0
0
0
0
0
0
0
1
0
0
5
0
0
0
0
0
3
0

.
4.
Prove: for any upper-triangular matrix A ∈Mn(R), det(A) = Qn
i=1 A(i, i).
5.
Suppose A ∈Mn(R) satisfies A(i, j) = 0R for all i, j ∈[n] with i + j > n + 1.
Find and prove a simple formula for det(A).
6.
Suppose A ∈Mn(R) satisfies A(i, j) = 0 for all i, j ∈[n] with i > j + 1. In
general, how many of the n! terms in (5.1) might be nonzero for such an A?
7.
Consider a matrix U ∈Mn+m(R) written in block form U =
 An×n
Bn×m
0m×n
Dm×m

,
where A, B, C, D have the indicated sizes. Prove: det(U) = det(A) det(D).
128
Advanced Linear Algebra
8.
Suppose U ∈M2k(R) has the block form
 A
B
C
D

where A, B, C, D ∈Mk(R)
and k ≥2. Is det(U) = det(A) det(D) −det(C) det(B) always true?
9.
Call U ∈Mn(R) block upper-triangular iff there exist positive integers s, k1, . . . , ks
such that k1+· · ·+ks = n and for all integers i, j, t with k1+· · ·+kt−1 < i ≤k1+
· · · + kt and all j ≤k1 + · · · + kt−1, U(i, j) = 0. Draw a picture showing the block
structure of U, and find a formula for det(U). (Use Exercise 7 and induction on
s.)
10.
Let (G, ⋆) be any group. For g ∈G, define maps Lg, Rg, and Cg from G to G by
setting Lg(x) = g ⋆x, Rg(x) = x ⋆g, and Cg(x) = g ⋆x ⋆g−1 for all x ∈G.
(a) Prove Lg is a bijection by showing Lg is one-to-one and onto.
(b) Prove Rg is a bijection by showing Rg−1 is a two-sided inverse of Rg.
(c) Prove Cg is a group isomorphism.
(d) Prove I : G →G, given by I(g) = g−1 for g ∈G, is a bijection.
(e) If T is any ring, G is finite, and f : G →T is any function, use Lg to explain
why P
x∈G f(x) = P
x∈G f(g ⋆x) for each fixed g ∈G. Write similar formulas
based on the maps Rg, Cg, and I.
11.
Assume A ∈Mn(R) satisfies AAT = In. Prove det(A) ∈{+1, −1}.
12.
Assume n is odd and A ∈Mn(R) satisfies AT = −A. Prove det(A) = 0.
13.
Assume A ∈Mn(C) satisfies AA∗= In. Prove | det(A)| = 1 and A∗A = In.
14.
Assume A ∈Mn(R) satisfies Ak = 0 for some k > 0. Prove det(A) = 0.
15.
Give an example of a commutative ring R and A ∈M2(R) where AAT = I2 but
det(A) ̸∈{+1, −1}.
16.
Give an example of a commutative ring R and A ∈M3(R) where AT = −A but
det(A) ̸= 0.
17.
Give an example of a commutative ring R and A ∈M2(R) where A ̸= 0 but
Ak = 0 for some positive integer k.
18.
Use Laplace expansions and induction on n to reprove that det(A) = det(AT) for
all A ∈Mn(R).
19.
Fix row vectors A1 = (2, 6, −1, −1), A2 = (3, 4, 0, −2), and A4 = (2, −3, 1, 5) in
R4. Define D : R4 →R by D(v) = det(A1, A2, v, A4) for v = (v1, v2, v3, v4) ∈R4.
Write D(v) explicitly in the form c1v1 + · · · + c4v4 for some ci ∈R, and confirm
that D is an R-linear function.
20.
Repeat Exercise 19 using column vectors A[2] = [3 5 4]T, A[3] = [2 7 −1]T,
and D : R3 →R defined on column vectors v = [v1 v2 v3]T ∈R3 by D(v) =
det(v, A[2], A[3]).
21.
In the formula D(v) = Pn
j=1 cjv(j) from §5.6, find an expression for cj involving
a sign times a determinant of a submatrix of A.
22.
Use elementary row operations to prove that if some row of A ∈Mn(R) is an R-
linear combination of other rows of A, then det(A) = 0R. Deduce a corresponding
result for columns.
23.
Suppose the rows of a matrix A ∈Mn(R) are R-linearly dependent, so there exist
r1, . . . , rn ∈R (not all 0) with r1A[1] + · · · + rnA[n] = 0 in Rn. Give a specific
example to show that det(A) need not be 0. (Compare to the previous exercise.)
24.
Use Laplace expansions to reprove the fact that multiplying row i of A ∈Mn(R)
by c ∈R replaces det(A) by c det(A).
Determinants via Calculations
129
25.
Use Laplace expansions to reprove the fact that interchanging two rows of a
matrix changes the sign of the determinant.
26.
Use Laplace expansions and induction to reprove the fact that adding c times
row i to row j ̸= i of A ∈Mn(R) does not change det(A).
27.
Let A1, . . . , An be row vectors in Rn and cij ∈R for each i < j in [n]. Prove that
det(A1, . . . , An) equals
det

A1 +
X
j>1
c1jAj, A2 +
X
j>2
c2jAj, . . . , Ai +
X
j>i
cijAj, . . . , An

.
Give two proofs, one using using elementary row operations, and one using the
Determinant Product Formula.
28.
Prove: for all elementary matrices E, ET is also an elementary matrix.
29.
Prove or disprove: for all elementary matrices E ∈Mn(R) and all k ∈Z, Ek is
also an elementary matrix.
30.
Use the definition (5.1) to prove that det(A) is a multilinear function of the n
columns of A ∈Mn(R). Prove that each elementary column operation has the
expected effect on determinants, without invoking the corresponding facts about
elementary row operations. Use this to prove that det(AE) = det(A) det(E) for
all A ∈Mn(R) and all elementary matrices E ∈Mn(R).
31.
Given a, b ∈R, evaluate det(aIn +bJn), where Jn ∈Mn(R) has every entry equal
to 1R.
32.
Define A ∈Mn(R) by A(i, j) = min(i, j) for i, j ∈[n]. Compute det(A).
33.
Given distinct r1, r2, . . . , rn in a field F, define A ∈Mn+1(F[x]) by
A(i, i) = x
for i ∈[n];
A(i, n + 1) = 1
for i ∈[n + 1];
A(i, j) = rj
for i > j in [n + 1];
A(i, j) = rj−1
for i < j in [n].
Prove det(A) = (x −r1)(x −r2) · · · (x −rn) by showing that the determinant is
a monic polynomial of degree n having every rj as a root. (Apply Exercise 62 to
certain evaluation homomorphisms.)
34.
In the previous exercise, suppose r1, r2, . . . , rn are not necessarily distinct ele-
ments of a commutative ring R. Does the formula for det(A) still hold?
35.
Let A ∈Mn(F) satisfy A(i, j) = 0 for all i ̸= j in [n −1]. Compute det(A).
36.
Compute the determinant of each matrix by using row operations to reduce the
matrix to an upper-triangular matrix in the given ring.
(a)


1
4
6
3
2
1
−2
4
5

∈M3(R)
(b)


4
0
1
2
3
3
1
4
2
1
1
3
3
1
3
4

∈M4(Z5)
130
Advanced Linear Algebra
(c)


1
1
1
0
1
0
1
0
0
0
1
1
0
0
1
1
1
0
0
1
0
0
0
1
0
1
1
0
0
1
1
1
0
1
1
1
0
1
1
0
1
1
1
0
0
0
0
0
1


∈M7(Z2)
37.
Compute the determinants of each matrix in Exercise 36 by repeated use of
Laplace expansions along convenient rows and columns.
38.
Find the characteristic polynomial of each matrix in Exercise 36, and verify by a
direct computation that the Cayley–Hamilton Theorem holds for these matrices.
39.
Compute the permanent of each matrix in Exercise 36.
40.
Vandermonde Determinant. Given x1, . . . , xn ∈R, define V ∈Mn(R) by
V (i, j) = xn−j
i
for i, j ∈[n]. Prove det(V ) = Q
1≤i<j≤n(xi −xj). (Use elementary
row and column operations and induction on n.)
41.
Given a field F, distinct elements a0, . . . , an in F, and b0, . . . , bn in F, prove
there exist unique c0, . . . , cn ∈F such that the polynomial p = Pn
i=0 cixi ∈
F[x] satisfies p(ai) = bi for 0 ≤i ≤n. Do this by setting up a system of n +
1 linear equations in n + 1 unknowns, and using the Vandermonde matrix V
and its determinant from Exercise 40. How is this result related to Lagrange’s
Interpolation Formula (§3.19)? Can you use that formula to describe the columns
of V −1?
42.
Given p = a0 + a1x + a2x2 + · · · + an−1xn−1 + xn ∈R[x], define the companion
matrix Cp ∈Mn(R) by setting Cp(j+1, j) = 1 for j ∈[n−1], Cp(i, n) = −ai−1 for
i ∈[n], and letting all other entries be zero. Prove that p = det(xIn −Cp) = χCp.
43.
Generalized Laplace Expansion. Given S ⊆[n], let inv(S) be the number of
ordered pairs (s, t) with s ∈S, t ∈[n], t ̸∈S, and s > t. Let sgn(S) = (−1)inv(S).
Fix a k-element subset I of [n]. Prove: for all A ∈Mn(R),
det(A) = sgn(I)
X
J⊆[n]:
|J|=k
sgn(J) det(A[I|J]) det(A[Ic|Jc]),
where A[I|J] denotes the matrix in Mn−k(R) obtained by erasing all rows in I
and all columns in J, and A[Ic|Jc] denotes the matrix in Mk(R) obtained by
erasing all rows not in I and all columns not in J. (Reduce to the case I = [k].)
Show that sgn(I) sgn(J) = (−1)
P
i∈I i+P
j∈J j. Show how to deduce the Laplace
expansion of det(A) along row i as a special case.
44.
Use Laplace expansions along columns to prove that A′A = (det A)In for all
A ∈Mn(R).
45.
Given a, b, c, d ∈R, find the classical adjoint of A =

a
b
c
d

. Use this to give
an explicit formula for A−1, when it exists.
46.
In M3(R), dind the classical adjoint and the inverse of


4
1
−2
3
0
5
7
−1
3

.
Determinants via Calculations
131
47.
In M4(Z7), find the classical adjoint and (if possible) the inverse of


2
3
1
0
1
5
3
1
2
2
6
4
3
4
5
1

.
48.
Use the explicit formula for A−1 to prove that if A is a unitriangular matrix (see
Exercise 30 of Chapter 4), then A−1 exists and is also unitriangular.
49.
Given b1, b2, b3 ∈R, solve


4
1
−2
3
6
−1
1
1
5




x1
x2
x3

=


b1
b2
b3

using Cramer’s Rule.
50.
Given the linear system of equations











2x1
+3x2
−x3
−x5
=
4
3x1
+5x2
+x3
+2x4
+x5
=
0
−x1
−3x4
+x5
=
1
x2
+4x3
+x4
=
−1
−2x2
−3x3
+4x5
=
6
use Cramer’s Rule to find x3.
51.
State and prove a version of Cramer’s Rule for solving the system xA = b, where
A ∈Mn(R) is invertible, b ∈Rn is a given row vector, and x ∈Rn is an unknown
row vector.
52.
Let A ∈Mn(R) have det(A) ̸= 0R. Use Cramer’s Rule and Laplace expansions
to derive the explicit formula for the entries of A−1.
53.
Prove: if A ∈Mn(R) is invertible, then det(A−1) = det(A)−1 in R.
54.
Matrices A, B ∈Mn(R) are called similar iff there exists an invertible S ∈Mn(R)
with B = S−1AS. Prove that similar matrices have the same trace, determinant,
and characteristic polynomial.
55.
For a field F, recall GLn(F) is the group of invertible matrices A ∈Mn(F). Prove
that SLn(F) = {A ∈Mn(F) : det(A) = 1F } is a normal subgroup of GLn(F).
SLn(F) is called the special linear group of degree n over F.
56.
Let A =
 a
b
c
d

and B =
 r
s
t
u

. Expand the definition of det(AB) to get
a sum of 8 terms, and show how terms cancel to produce a sum of 4 terms equal
to det(A) det(B). (This illustrates the proof in §5.13.)
57.
Let A, B ∈Mn(R). Give direct proofs of the product formula det(AB) =
det(A) det(B) for each of the following special cases:
(a) A is diagonal;
(b) A is triangular (cf. Exercise 27);
(c) A is a permutation matrix (every row and column of A has exactly one 1,
with all other entries 0);
(d) A can be factored as A = PLU for some P, L, U ∈Mn(R) with P a
permutation matrix, L lower-triangular, and U upper-triangular.
58.
Compute both sides of the Cauchy–Binet Formula for the matrices
A =


2
1
0
3
1
−1
4
3
5
−3
−2
0

,
B =


3
1
2
0
−2
3
1
1
2
3
−3
1

.
132
Advanced Linear Algebra
59.
Use the Cauchy–Binet Formula to prove the Cauchy–Schwarz inequality: for all
real x1, . . . , xn, y1, . . . , yn,

n
X
i=1
xiyi
 ≤
 n
X
i=1
x2
i
!1/2  n
X
i=1
y2
i
!1/2
.
60.
Illustrate the proof of the Cayley–Hamilton Theorem given in §5.15 for a general
matrix A ∈M3(R) by computing the entries of the matrices C, zCT, and
zCT adj(CT) appearing in that proof.
61.
This exercise fills in some details in the proof given in §5.15. (a) Show that
the set S of matrices defined in that proof is a subring of Mn(R), and S is
commutative. (b) Show that h : R →S, defined by h(r) = rIn for r ∈R, is a
ring homomorphism. (c) Prove that (zU)V = z(UV ) for all U, V ∈Mn(S) and
all z ∈(Rn)n.
62.
Suppose R and S are commutative rings, and F : R →S is a ring homomorphism.
Define F ∗: Mn(R) →Mn(S) by F ∗(A)i,j = F(Ai,j) for A ∈Mn(R) and i, j ∈[n].
(a) Prove F ∗is a ring homomorphism.
(b) Prove: for all A ∈Mn(R), det(F ∗(A)) = F(det(A)).
(c) Deduce the formula det( A ) = det(A) as a special case of (b).
63.
Suppose A ∈Mn(R[x]) has the property that for all i, j ∈[n], A(i, j) is either
zero or has degree at most di. Prove det(A) ∈R[x] is either zero or has degree at
most d1 + d2 + · · · + dn.
64.
Generalized Distributive Law. Let R be a ring and aij ∈R for 1 ≤i ≤m
and 1 ≤j ≤ni. Use the distributive laws in R and induction to prove
m
Y
i=1


ni
X
ji=1
aiji

=
n1
X
j1=1
n2
X
j2=1
· · ·
nm
X
jm=1
 m
Y
i=1
aiji
!
.
(This formula was used several times in §5.13 and §5.14.)
65.
Given A ∈Mn(R), use (5.1) to show that the characteristic polynomial χA is a
monic polynomial of degree n.
66.
Given A ∈Mn(R), show that the coefficient of xn−1 in χA is −Pn
i=1 A(i, i)
(the negative of the trace of A). Prove that the constant coefficient in χA is
(−1)n det(A).
67.
Suppose F is a field, A ∈Mn(F) has characteristic polynomial p ∈F[x], and
λ ∈F. Explain very carefully why p(λ) = det(λIn −A). Deduce that the set of
roots of p in F equals the set of eigenvalues of A in F.
68.
Given A ∈Mm(R) and B ∈Mn(R), prove det(A⊗B) = det(A)n det(B)m. (A⊗B
is defined in §4.15. First prove the formula for A ⊗In and Im ⊗B.)
69.
Prove facts (a) through (g) in §5.16.
70.
Prove: for A ∈Mn(R) and i ∈[n], per(A) = Pn
k=1 A(i, k) per(A[i|k]).
71.
Give an example with A, B ∈M2(R) to show that per(AB) = per(A) per(B) is
false in general.
Determinants via Calculations
133
72.
Pfaffians. For even n > 0, let SPfn be the set of f ∈Sn with f(1) < f(3) <
f(5) < · · · < f(n −1) and f(i) < f(i + 1) for all odd i ∈[n]. Given A ∈Mn(R)
with AT = −A, define the Pfaffian of A by the formula
Pf(A) =
X
f∈SPfn
sgn(f)A(f(1), f(2))A(f(3), f(4)) · · · A(f(n −1), f(n)).
(a) Compute Pf(A) for n ∈{2, 4, 6}.
(b) Prove: for even n > 0, Pf(A) is a sum of 1 · 3 · 5 · . . . · (n −1) terms.
(c) For i < j in [n], let A[[i, j]] be the matrix obtained from A by deleting row i,
row j, column i, and column j. Prove: for even n ≥4,
Pf(A) =
n
X
j=2
(−1)jA(1, j) Pf(A[[1, j]]).
(d) Prove that det(A) = Pf(A)2 for n ∈{2, 4, 6}.
(e) Can you prove det(A) = Pf(A)2 for all even n > 0?
73.
Let R be the non-commutative ring M2(R). Use formula (5.1) to define the
determinant of any A ∈Mn(R). Give specific examples to show that all but
two statements below are false. Prove the two true statements.
(a) For all r, s, t, u ∈R, det

r
s
t
u

= ru −st.
(b) For all A ∈Mn(R), det(A) = det(AT).
(c) Switching two columns of A ∈Mn(R) replaces det(A) by −det(A).
(d) Switching two rows of A ∈Mn(R) replaces det(A) by −det(A).
(e) For all A, B ∈Mn(R), det(AB) = det(A) det(B).
(f) For all A ∈Mn(R), det(A) = Pn
i=1(−1)i+1A(i, 1) det(A[i|1]).
(g) For all A ∈Mn(R), det(A) = Pn
j=1(−1)1+jA(1, j) det(A[1|j]).
74.
Let A ∈Mk,n(R) with k ≤n. Fix lists (i1, i2, . . . , ik) and (j1, j2, . . . , jk) in [n]k
and fix s ∈[k]. Prove
det(A[i1], A[i2], . . . , A[ik]) det(A[j1], A[j2], . . . , A[jk])
=
k
X
t=1
det(A[i1], . . . , A[jt], . . . , A[ik]) det(A[j1], . . . , A[is], . . . , A[jk]),
where the matrices on the right side arise from the matrices on the left side by
interchanging the positions of columns A[is] and A[jt].
75.
True or false? Explain each answer.
(a) For all A, B ∈Mn(R), det(A + B) = det(A) + det(B).
(b) For all A ∈Mn(R) and c ∈R, det(cA) = c det(A).
(c) For all A ∈Mn(C), if Ak = In for some k > 0, then | det(A)| = 1.
(d) For all upper-triangular A ∈Mn(R) and all strictly upper-triangular B ∈
Mn(R), det(A + B) = det(A) + det(B).
(e) For all n×n matrices A with integer entries, if det(A) = ±1, then every entry
of A−1 is also an integer.
(f) For all A ∈Mm,n(R) and B ∈Mn,m(R), if m > n, then det(AB) = 0.
(g) For all A ∈Mn(R), A−1 exists in Mn(R) iff det(A) ̸= 0R.
(h) For all A ∈Mn(R), if |A(i, j)| ≤K for all i, j ∈[n], then | det(A)| ≤n!Kn.
(i) For all n ∈Z>0 and all A, B ∈Mn(Z2), per(AB) = per(A) per(B).
6
Comparing Concrete Linear Algebra to Abstract
Linear Algebra
In introductions to linear algebra, we learn about column vectors, matrices, and the algebraic
operations on these objects: vector addition, multiplication of a vector by a scalar, matrix
addition, multiplication of a matrix by a scalar, matrix multiplication, matrix inversion,
matrix transpose, and so on. Later in linear algebra, we study abstract vector spaces
and linear maps between such spaces. The abstract setting provides a powerful tool for
theoretical work. But for computational applications, it is often more convenient to work
with column vectors and matrices. The goal of this chapter is to give a thorough explanation
of the relation between the concrete world of column vectors and matrices on the one hand,
and the abstract world of vector spaces and linear maps on the other hand. We build
a dictionary translating between these two worlds, which explains the precise connection
between each abstract concept and its concrete counterpart. The full dictionary linking
matrix theory to linear algebra appears in the summary for this chapter.
We are mostly concerned with the following three abstract ideas: vector spaces, linear
transformations, and bilinear maps. Given a vector v from an abstract n-dimensional vector
space V over a field F, we show how to represent v concretely by a column vector (n-tuple)
with entries from F. Given a linear transformation T : V →W mapping an abstract n-
dimensional F-vector space V to an abstract m-dimensional F-vector space W, we show
how to represent T concretely by an m×n matrix with entries from F. Given a bilinear form
B on an abstract n-dimensional vector space V , we show how to represent B concretely
by an n × n matrix with entries in F. For each of these constructions, we discuss how
the explicit, concretely defined operations on column vectors and matrices correspond to
abstract algebraic operations on vectors, linear maps, and bilinear maps.
All of the constructions for converting abstract entities to concrete entities depend
heavily on choosing ordered bases for the abstract vector spaces involved. Different choices of
bases lead to different concrete representations of a given vector, linear map, or bilinear form.
We might ask if some bases are better than others for computational or theoretical purposes.
For instance, since it is easier to compute with diagonal matrices compared to arbitrary
matrices, we could ask if there is a basis such that a given linear operator is represented
by a diagonal matrix. This leads us to a discussion of similarity, congruence, transition
matrices, change of coordinates, diagonalization, and triangularization. We conclude the
chapter by studying real and complex inner product spaces, orthogonal and unitary maps
and matrices, and orthonormal bases.
6.1
Column Vectors versus Abstract Vectors
In this chapter, we assume the background material on matrices covered in Chapter 4. For
each field F and each positive integer n, define a concrete vector space F n whose elements
DOI: 10.1201/9781003484561-6
134
Comparing Concrete Linear Algebra to Abstract Linear Algebra
135
are n-tuples (c1, c2, . . . , cn) with each ci ∈F. For example, if F = C and n = 3, some
specific elements of C3 are (i, π, e −3i), (7/2, 0, −i
√
3), and (0, 0, 1). We often identify the
n-tuple (c1, c2, . . . , cn) with the column vector


c1
c2
...
cn

.
On the other hand, let V be an abstract finite-dimensional vector space over F (as
defined in Section 1.3). Without more specific information about V , it is not clear at the
outset how to represent or describe particular elements in V . The best we can do is to
introduce generic letters like v, w, etc., to denote vectors in V .
The concept of linear combinations allows us to relate the concrete set F n to the abstract
set V . Suppose X = (x1, . . . , xn) is any ordered list of vectors in V . Define the linear
combination function LX : F n →V by setting
LX(c1, . . . , cn) = c1x1 + c2x2 + · · · + cnxn ∈V
for all (c1, . . . , cn) ∈F n.
The function LX maps an n-tuple of scalars (c1, . . . , cn) to the linear combination of
x1, . . . , xn having scalar coefficients c1, . . . , cn.
Let us find conditions ensuring that the function LX is surjective, or injective, or
bijective. First, the following conditions are logically equivalent.
(a) LX is surjective (onto).
(b) For every v ∈V , there exists w ∈F n with v = LX(w).
(c) For all v ∈V , there exist c1, . . . , cn ∈F with v = Pn
i=1 cixi.
(d) Every v ∈V can be written in at least one way as a linear combination of the list X.
(e) The list X spans V .
Second, we claim the following conditions are logically equivalent.
(a) LX is injective (one-to-one).
(b) For all w, y ∈F n, if LX(w) = LX(y), then w = y.
(c) For all ci, di ∈F, if Pn
i=1 cixi = Pn
i=1 dixi, then (c1, . . . , cn) = (d1, . . . , dn).
(d) Each v ∈V can be written in at most one way as a linear combination of the list X.
(e) The list X is linearly independent.
To see why linear independence of X is equivalent to the preceding conditions, assume
that X is linearly independent. Given ci, di ∈F with Pn
i=1 cixi = Pn
i=1 dixi, note that
Pn
i=1(ci −di)xi = 0. By the linear independence of X, we conclude ci −di = 0 and ci = di
for all i. Conversely, suppose LX is injective and Pn
i=1 cixi = 0 for some ci ∈F. Then
LX(c1, . . . , cn) = 0V = LX(0, . . . , 0) forces (c1, . . . , cn) = (0, . . . , 0), so all ci = 0. This
means that the list X is linearly independent.
Combining the preceding remarks on spanning and linear independence, we see that the
following conditions are logically equivalent.
(a) LX : F n →V is a bijection (one-to-one and onto).
(b) For every v ∈V , there exists a unique w ∈F n with v = LX(w).
(c) For all v ∈V , there exist unique c1, . . . , cn ∈F with v = Pn
i=1 cixi.
(d) Every v ∈V can be written in exactly one way as a linear combination of the list X.
(e) The list X is an ordered basis of V (meaning X spans V and X is linearly independent).
Assume that X = (x1, . . . , xn) is an ordered basis of V , so that LX is a bijection and
an invertible function. For all v ∈V , define [v]X = L−1
X (v) ∈F n. So, for all ci ∈F and all
v ∈V ,
[v]X = (c1, . . . , cn)
iff
v = c1x1 + c2x2 + · · · + cnxn.
For each v ∈V , there exist unique scalars c1, . . . , cn ∈F such that v = Pn
i=1 cixi. We call
the scalars c1, . . . , cn the coordinates of v relative to the ordered basis X, and we call [v]X
136
Advanced Linear Algebra
the coordinate vector of v relative to X. For example, take any index j in {1, 2, . . . , n}. Let
ej ∈F n be the n-tuple (0, . . . , 1, . . . , 0) that has a 1 in position j and 0s elsewhere. Since
LX(ej) = 0x1 + · · · + 1xj + · · · + 0xn = xj, we have [xj]X = ej.
Summary. For every ordered basis X of V , there is a bijection LX mapping the concrete
vector space F n to the abstract vector space V . The function LX maps an n-tuple
(c1, . . . , cn) to the linear combination c1x1 + · · · + cnxn. The inverse function L−1
X
maps
each abstract vector v ∈V to the n-tuple (coordinate vector) [v]X.
For all ci ∈F and v ∈V , [v]X = (c1, . . . , cn) iff v = c1x1 + · · · + cnxn.
The notation reminds us that the coordinate vector [v]X ∈F n depends on both the vector
v and the choice of ordered basis X. Changing to a different ordered basis Y replaces [v]X
with a new n-tuple [v]Y . The relationship between [v]X and [v]Y is explained in §6.10.
6.2
Examples of Computing Coordinates
Throughout this chapter, we use the real vector spaces and ordered bases shown in Table 6.1
as running examples of the computations discussed in the text. You may check that each
list displayed in the table really is an ordered basis for the associated vector space.
How to Compute Coordinate Vectors.
Input: a vector v ∈V and an ordered basis X = (x1, . . . , xn) of V .
Output: the coordinate vector [v]X = (c1, . . . , cn) ∈F n.
Method: Solve the vector equation v = c1x1 + · · · + cnxn for the unknown scalars c1, . . . , cn.
Upon expanding definitions of v, x1, . . . , xn, we often obtain a system of n linear scalar
equations in the n unknowns ci. We may solve this system by Gaussian elimination.
Example 1. Let v = (3, 3, 1) ∈R3. Since
v = 3(1, 0, 0) + 3(0, 1, 0) + 1(0, 0, 1) = 3e1 + 3e2 + 1e3,
we see that [v]X1 = (3, 3, 1) = v. More generally, for any w ∈F n, [w]X = w when X is the
standard ordered basis (e1, . . . , en).
On the other hand, to find [v]X2, we must solve the vector equation
(3, 3, 1) = c1(1, 1, 1) + c2(1, 2, 4) + c3(1, 3, 9).
Equating components gives a system of three linear equations



3
=
1c1 + 1c2 + 1c3
3
=
1c1 + 2c2 + 3c3
1
=
1c1 + 4c2 + 9c3.
Solving this system gives c1 = 2, c2 = 2, and c3 = −1. Therefore [v]X2 = (2, 2, −1).
To find coordinates relative to X3, we must solve the vector equation (3, 3, 1) =
c1(0, 2, 1) + c2(−1, 1, 3) + c3(1, 0, 4). The new system is



3
=
0c1 −1c2 + 1c3
3
=
2c1 + 1c2 + 0c3
1
=
1c1 + 3c2 + 4c3.
The solution is [v]X3 = (32/13, −25/13, 14/13).
Comparing Concrete Linear Algebra to Abstract Linear Algebra
137
TABLE 6.1
Real vector spaces and ordered bases used as examples in this chapter.
1. R3 = {(c1, c2, c3) : ci ∈R} (3-tuples or column vectors).
Basis X1 = (e1, e2, e3) = ((1, 0, 0), (0, 1, 0), (0, 0, 1)).
Basis X2 = ((1, 1, 1), (1, 2, 4), (1, 3, 9)).
Basis X3 = ((0, 2, 1), (−1, 1, 3), (1, 0, 4)).
2. P≤3 = {f ∈R[t] : f = 0 or deg(f) ≤3} (polynomials in t of degree at most 3).
Basis X1 = (1, t, t2, t3).
Basis X2 = ((t −2)3, (t −2)2, t −2, 1).
Basis X3 = (t + 3, 2t2 −4, t3 −t2, t3 + t2).
3. M2(R) =

a
b
c
d

: a, b, c, d ∈R

(2 × 2 matrices).
Basis X1 = (e11, e12, e21, e22) =
 
1
0
0
0

,

0
1
0
0

,

0
0
1
0

,

0
0
0
1
!
.
Basis X2 =
 
1
0
0
1

,

0
1
1
0

,

1
0
0
−1

,

0
−1
1
0
!
.
Basis X3 =
 
1
2
0
3

,

1
0
2
3

,

0
1
2
3

,

1
2
3
0
!
.
4. C = {a + ib : a, b ∈R} (complex numbers).
Basis X1 = (1, i).
Basis X2 = (eπi/6, e2πi/3).
Basis X3 = (3 + 4i, 2 −i).
Example 2. Let v = (t −3)3 ∈P≤3. To find [v]X1, solve (t −3)3 = c11 + c2t + c3t2 + c4t3
by expanding the left side into monomials and comparing coefficients. Since (t −3)3 =
t3 −9t2 + 27t −27, we get c1 = −27, c2 = 27, c3 = −9, c4 = 1, and [v]X1 = (−27, 27, −9, 1).
On the other hand, to find [v]X2, we must solve
(t −3)3 = d1(t −2)3 + d2(t −2)2 + d3(t −2) + d41
for unknowns d1, d2, d3, d4 ∈R. Expanding both sides gives
t3 −9t2 + 27t −27 = d1t3 + (d2 −6d1)t2 + (d3 −4d2 + 12d1)t + (d4 −2d3 + 4d2 −8d1).
Equating coefficients leads to the triangular system of linear equations







d1
=
1
−6d1
+d2
=
−9
12d1
−4d2
+d3
=
27
−8d1
+4d2
−2d3
+d4
=
−27.
Solving gives d1 = 1, d2 = −3, d3 = 3, d4 = −1, and [v]X2 = (1, −3, 3, −1). By solving
another system, you can check that [v]X3 = (27, 27, 32, −31).
138
Advanced Linear Algebra
Example 3. Let A =
 1
2
3
4

∈M2(R). We have A = 1e11 + 2e12 + 3e21 + 4e22, so that
[A]X1 = (1, 2, 3, 4). To find [A]X2, write

1
2
3
4

= A = d1

1
0
0
1

+ d2
 0
1
1
0

+ d3
 1
0
0
−1

+ d4
 0
−1
1
0

where d1, d2, d3, d4 are unknown scalars. Equating corresponding matrix entries on each
side, we obtain a system of linear equations







d1
+d3
=
1
d2
−d4
=
2
d2
+d4
=
3
d1
−d3
=
4.
Solving this system gives [A]X2 = (d1, d2, d3, d4) = (5/2, 5/2, −3/2, 1/2). Similarly, by
solving

1
2
3
4

= c1

1
2
0
3

+ c2

1
0
2
3

+ c3

0
1
2
3

+ c4

1
2
3
0

,
we find that [A]X3 = (1/3, 1/3, 2/3, 1/3).
Example 4. Let v = eπi/3 ∈C. By definition of complex exponentials, we have v =
cos(π/3)1 + sin(π/3)i = (1/2)1 + (
√
3/2)i, and hence [v]X1 = (1/2,
√
3/2). To find [v]X2, we
write eπi/3 = d1eπi/6 + d2e2πi/3 for real unknowns d1 and d2. Equating real and imaginary
parts of both sides leads to the 2 × 2 linear system

1/2
=
(
√
3/2)d1
+(−1/2)d2
√
3/2
=
(1/2)d1
+(
√
3/2)d2.
The solution is d1 =
√
3/2, d2 = 1/2, so [v]X2 = (
√
3/2, 1/2). As a side remark, note that
reversing the order of the basis X1 gives [v](i,1) = (
√
3/2, 1/2) = [v]X2. This shows that a
vector can have the same coordinates relative to two different ordered bases. By solving one
more 2 × 2 system, you can check that [v]X3 = ((1 + 2
√
3)/22, (4 −3
√
3)/22).
6.3
Operations on Column Vectors versus Abstract Vectors
Vector addition and scalar multiplication in F n are defined by the explicit formulas
(c1, . . . , cn) + (d1, . . . , dn) = (c1 + d1, . . . , cn + dn),
a(c1, . . . , cn) = (ac1, . . . , acn)
for all ci, di, a ∈F. On the other hand, all we know initially about the addition and scalar
multiplication operations in an abstract vector space V are the axioms listed in Table 1.4.
We can use the linear combination maps LX : F n →V to relate the concrete vector
space operations in F n to the abstract vector space operations in V . Let X = (x1, . . . , xn)
be any ordered list in V . For c = (c1, . . . , cn) ∈F n, d = (d1, . . . , dn) ∈F n, and a ∈F, we
use the vector space axioms in V to compute:
LX(c+d) = LX(c1+d1, . . . , cn+dn) =
n
X
i=1
(ci+di)xi =
n
X
i=1
cixi+
n
X
i=1
dixi = LX(c)+LX(d);
LX(ac) = LX(ac1, . . . , acn) =
n
X
i=1
(aci)xi = a
n
X
i=1
cixi = aLX(c).
Comparing Concrete Linear Algebra to Abstract Linear Algebra
139
This computation shows that LX : F n →V is always an F-linear map (vector space
homomorphism) from F n into V . Combining this fact with the results in §6.1, we see that
LX is a vector space isomorphism from F n to V iff X is an ordered basis for V . In this
case, the map L−1
X : V →F n, which sends each v ∈V to its coordinate vector [v]X in F n,
is also a vector space isomorphism (see Exercise 54 of Chapter 1). Linearity of this inverse
map means that
[v + w]X = [v]X + [w]X
and
[cv]X = c[v]X
for all v, w ∈V and c ∈F.
Example. Let us find [e22]X3 using computations from §6.2. Let A
=
 1
2
3
4

and B
=
 1
2
3
0

, and note e22
=
(1/4)A −(1/4)B. We already calculated
[A]X3 = (1/3, 1/3, 2/3, 1/3), while [B]X3 = (0, 0, 0, 1). Therefore,
[e22]X3 = [(1/4)(A −B)]X3 = (1/4)[A]X3 −(1/4)[B]X3 = (1/12, 1/12, 1/6, −1/6).
So far, we have seen that each ordered basis X = (x1, . . . , xn) of V gives us a vector
space isomorphism from the abstract n-dimensional space V to the concrete vector space
F n. This isomorphism is given by the coordinate map v 7→[v]X for v ∈V , and the inverse
isomorphism LX sends (c1, . . . , cn) ∈F n to Pn
i=1 cixi ∈V . We have LX(ej) = xj and
[xj]X = ej for all j. This says that the standard ordered basis E = (e1, . . . , en) of F n
corresponds to the ordered basis X of V under the isomorphism. Note carefully that we
have found not one, but many isomorphisms V ∼= F n, which are indexed by the ordered
bases of V . Many problems in linear algebra involve finding a good ordered basis X for V ,
so that the associated isomorphism V ∼= F n has additional properties useful for solving the
given problem.
Recall the theorem that any finite-dimensional vector space V has a unique dimension,
which is the length of any ordered basis of V . Using this, we show that two finite-dimensional
F-vector spaces are isomorphic iff they have the same dimension. On one hand, if V and
W both have dimension n, then we can compose the isomorphisms V ∼= F n and F n ∼= W
to conclude that V and W are isomorphic. Conversely, if V ∼= W, then any isomorphism
from V to W is a bijection mapping each ordered basis of V to an ordered basis of W. Thus
ordered bases of V and W must have the same length, so dim(V ) = dim(W). Although
we omit the proof, similar results hold for infinite-dimensional spaces. Specifically, vector
spaces V and W are isomorphic iff dim(V ) = dim(W), where the dimensions are viewed as
infinite cardinals.
We have constructed a family of isomorphisms V
∼= F n (along with their inverse
isomorphisms F n ∼= V ) parametrized by ordered bases of V . We may ask whether we
have found all such isomorphisms, and whether our parametrization via ordered bases is
unique. To state this question formally, note that we have a well-defined function ϕ, given
by the formula ϕ(X) = LX, that maps the set B of all ordered bases of V into the set I
of all vector space isomorphisms from F n onto V . We show that ϕ : B →I is a bijection,
which means that each L ∈I has the form ϕ(X) = LX for a unique X ∈B. In other words,
every isomorphism L : F n →V has the form LX for a unique ordered basis X of V .
First we prove that ϕ is one-to-one. Assume X = (x1, . . . , xn) and Y = (y1, . . . , yn) are
ordered bases of V such that ϕ(X) = ϕ(Y ), meaning LX = LY . We must show that X = Y .
This follows because xj = LX(ej) = LY (ej) = yj for all j between 1 and n.
Next, we prove that ϕ is surjective. Let L : F n →V be any vector space isomorphism.
We must find an ordered basis X such that L = LX = ϕ(X). We find X by applying L to
the standard ordered basis E = (e1, . . . , en) of F n. Define xj = L(ej) for j = 1, 2, . . . , n,
140
Advanced Linear Algebra
and define X = (x1, . . . , xn). This list X is an ordered basis of V since X is the image of the
ordered basis E of F n under an isomorphism L. To prove that the functions L and LX from
F n to V are equal, we check that they have the same effect on each input c = (c1, . . . , cn)
in the domain F n. Since c = Pn
j=1 cjej, linearity of L gives
L(c) =
n
X
j=1
cjL(ej) =
n
X
j=1
cjxj = LX(c).
This completes the proof that ϕ is a bijection.
6.4
Matrices versus Linear Maps
The next step in our comparison of concrete matrix theory to abstract linear algebra is estab-
lishing a correspondence between matrices and linear transformations. This correspondence
is fundamental and essential for all that follows. For any field F, let
Mm,n(F) = the set of all m × n matrices with entries in F.
For any F-vector spaces V and W, let
L(V, W) = the set of all F-linear maps T from V to W.
A function T : V →W belongs to L(V, W) iff T(v + v′) = T(v) + T(v′) and T(cv) = cT(v)
for all v, v′ ∈V and all c ∈F.
Suppose dim(V ) = n and dim(W) = m. We construct not one, but many bijections from
L(V, W) to Mm,n(F). These bijections are parametrized by ordered bases of V and W. Let
X = (x1, . . . , xn) be any ordered basis for V and Y = (y1, . . . , ym) be any ordered basis for
W. We define a bijection MX,Y : L(V, W) →Mm,n(F) that sends a linear transformation T
to a certain matrix MX,Y (T). We introduce special notation Y [T]X for the matrix MX,Y (T).
The matrix Y [T]X is called the matrix of T using the input basis X and the output basis Y .
Our plan is to build a correspondence between linear maps and matrices by composing
three bijections
L(V, W) −→W n −→(F m)n −→Mm,n(F), where:
(6.1)
L(V, W)
=
set of F-linear maps T : V →W;
W n
=
set of lists (w1, . . . , wn) with all wj ∈W;
(F m)n
=
set of lists (z1, . . . , zn) with all zj ∈F m;
Mm,n(F)
=
set of m × n matrices with entries in F.
The crucial observation for setting up this correspondence is that a linear map T with
domain V is completely determined by the values of T on the basis vectors in X. The first
map in (6.1) is the restriction map RX : L(V, W) →W n, defined by setting RX(T) =
(T(x1), T(x2), . . . , T(xn)) ∈W n for each T ∈L(V, W).
Let us check that RX is one-to-one and onto. First, suppose S, T ∈L(V, W) are two
linear maps such that RX(S) = RX(T). This means S(xj) = T(xj) for j = 1, 2, . . . , n. We
prove S = T by showing S(v) = T(v) for all v ∈V . Given v ∈V , write v = Pn
j=1 cjxj for
some cj ∈F. Then, use linearity of S and T to compute
S(v) = S


n
X
j=1
cjxj

=
n
X
j=1
cjS(xj) =
n
X
j=1
cjT(xj) = T


n
X
j=1
cjxj

= T(v).
Comparing Concrete Linear Algebra to Abstract Linear Algebra
141
Second, given any w = (w1, w2, . . . , wn) ∈W n, we must build T ∈L(V, W) such that
RX(T) = w. We define
T


n
X
j=1
cjxj

=
n
X
j=1
cjwj
for all cj ∈F.
(6.2)
It is routine to check that T is a well-defined linear map from V to W such that T(xj) = wj
for j = 1, 2, . . . , n. Hence, T ∈L(V, W) and RX(T) = (T(x1), . . . , T(xn)) = w. We now
know RX is a bijection. For each w ∈W n, the proof of surjectivity shows that R−1
X (w) is
the map T ∈L(V, W) defined by (6.2).
To continue, we define a bijection from W n to (F m)n. Starting with w = (w1, . . . , wn) ∈
W n, we apply the bijection L−1
Y
: W →F m to each element of the list w to obtain a list
([w1]Y , . . . , [wn]Y ) of n column vectors in F m. The inverse bijection takes a list (z1, . . . , zn) ∈
(F m)n and maps it to the list (LY (z1), . . . , LY (zn)) ∈W n.
To finish, recall from §4.1 that there is a bijection from (F m)n to Mm,n(F). This
bijection maps a list (z1, . . . , zn) of n column vectors in F m to the m × n matrix A whose
columns are z1, . . . , zn in this order. The inverse map sends each A ∈Mm,n(F) to the list
(A[1], . . . , A[n]) ∈(F m)n, where A[j] ∈F m is the jth column of A.
Composing the three bijections discussed above, we get a bijection MX,Y from L(V, W)
to Mm,n(F). Applying the formulas for each map, we see that T ∈L(V, W) is first
sent to the list (T(x1), . . . , T(xn)) ∈W n, which is then sent to the list of coordinate
vectors ([T(x1)]Y , . . . , [T(xn)]Y ) ∈(F m)n, which is finally sent to the matrix having these
coordinate vectors as columns. Writing Y [T]X = MX,Y (T), we see that the jth column of
this matrix is [T(xj)]Y for j = 1, 2, . . . , n.
The inverse bijection starts with a matrix A ∈Mm,n(F) and does three things. First,
replace the matrix by its list of columns (A[1], . . . , A[n]) ∈(F m)n. Second, replace this
list by the list of vectors (w1, . . . , wn) ∈W n, where wj = LY (A[j]) = Pm
i=1 A(i, j)yi for
j = 1, 2, . . . , n. Third, replace this list by the linear map T defined in (6.2). The net effect
is that A maps to T, where
T


n
X
j=1
cjxj

=
n
X
j=1
cj
m
X
i=1
A(i, j)yi =
m
X
i=1


n
X
j=1
A(i, j)cj

yi
for all cj ∈F.
(6.3)
Summary. Given an ordered basis X = (x1, . . . , xn) for V and an ordered basis Y =
(y1, . . . , ym) for W, we obtain a bijection from L(V, W) to Mm,n(F) sending the linear map
T to the matrix Y [T]X. By definition,
Y [T]X is the matrix whose jth column is the coordinate vector [T(xj)]Y .
To say this another way,
A = Y [T]X means T(xj) = Pm
i=1 A(i, j)yi for j = 1, 2, . . . , n.
Equivalently, A = Y [T]X iff T satisfies (6.3). The notation Y [T]X reminds us that the
bijection constructed here depends on the choice of ordered bases X and Y .
6.5
Examples of Matrices Associated with Linear Maps
We give some examples of computing the matrix of a linear map.
142
Advanced Linear Algebra
How to Compute the Matrix of a Linear Map.
Input: A linear map T : V →W, an ordered basis X = (x1, . . . , xn) of V , and an ordered
basis Y = (y1, . . . , ym) of W.
Output: the matrix Y [T]X of T using input basis X and output basis Y .
Method: Apply T to each input basis vector xj ∈X. Compute the coordinate vector of
T(xj) relative to the basis Y . Write this vector as column j of the matrix.
Example 1. (See Table 6.1 on page 137 for the notation used in these examples.)
Let T : P≤3 →P≤3 be the differentiation operator T(f) = df/dt for f ∈P≤3. By a theorem
from calculus, T is a linear map. Let us find A = X1[T]X1. To get the first column of
A, compute T(1) = 0, which has coordinates (0, 0, 0, 0) relative to X1. To get the second
column of A, compute T(t) = 1, which has coordinates (1, 0, 0, 0) relative to X1. To get the
third column of A, compute T(t2) = 2t, which has coordinates (0, 2, 0, 0) relative to X1. To
get the fourth column of A, compute T(t3) = 3t2, which has coordinates (0, 0, 3, 0) relative
to X1. Arranging these columns in a matrix, we have
A = X1[T]X1 =


0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0

.
Suppose we change the input basis to X2, but keep X1 as the output basis. Applying T to
the vectors (t −2)3, (t −2)2, (t −2)1, 1 produces the vectors 3(t −2)2 = 3t2 −12t + 12,
2(t−2) = 2t−4, 1, and 0, respectively. We find coordinates [3t2−12t+12]X1 = (12, −12, 3, 0),
and so on, leading to the matrix
X1[T]X2 =


12
−4
1
0
−12
2
0
0
3
0
0
0
0
0
0
0

.
On the other hand, taking X2 as both input basis and output basis gives
X2[T]X2 =


0
0
0
0
3
0
0
0
0
2
0
0
0
0
1
0

.
Example 2. Let T : M2(R) →M2(R) be the transpose map given by T(A) = AT for all
A ∈M2(R). This map is linear. We compute T(e11) = e11, T(e12) = e21, T(e21) = e12, and
T(e22) = e22. Therefore,
X1[T]X1 =


1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1

.
Similar computations show that:
X2[T]X2 =


1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
−1

;
X3[T]X3 =


0
1
1/2
1/2
1
0
−1/2
−1/2
0
0
1
0
0
0
0
1

;
Comparing Concrete Linear Algebra to Abstract Linear Algebra
143
X1[T]X3 =


1
1
0
1
0
2
2
3
2
0
1
2
3
3
3
0

;
X2[T]X1 =


1/2
0
0
1/2
0
1/2
1/2
0
1/2
0
0
−1/2
0
1/2
−1/2
0

.
Example 3. For any vector spaces V and W, the zero map 0 ∈L(V, W) is the linear
map sending every v ∈V to 0W . For all ordered bases X and Y , Y [0]X is the zero matrix
0 ∈Mm,n(F). On the other hand, for the identity map idV on any vector space V , we have
X[id]X = In (the identity matrix) for all ordered bases X of V . But if X and Y are distinct
ordered bases for V , then Y [id]X is not the identity matrix. For example, using the bases
of R3 in Table 6.1, we see that X1[id]X2 =


1
1
1
1
2
3
1
4
9

. We study matrices of the form
Y [id]X in more detail later (see §6.10).
Example 4. Let z = a + ib be a fixed complex number. Define Tz : C →C by Tz(w) = zw
for all w ∈C. You can check that Tz is an R-linear map (and also a C-linear map). Let
us compute X1[Tz]X1. For column 1, compute Tz(1) = z1 = z = a + ib, and note that
[z]X1 = (a, b). For column 2, compute Tz(i) = zi = −b + ia, and note that [zi]X1 = (−b, a).
So
X1[Tz]X1 =
 a
−b
b
a

.
Example 5. Define a linear map T : P≤3 →R3 by T(f) = (f(1), f(2), f(3)) for f ∈P≤3. Let
X = (1, t, t2, t3), Y = (e1, e2, e3), and Z = ((1, 1, 1), (1, 2, 3), (1, 4, 9)). Applying T to each
vector in X, we get T(1) = (1, 1, 1), T(t) = (1, 2, 3), T(t2) = (1, 4, 9), and T(t3) = (1, 8, 27).
Computing coordinates of these vectors relative to Y and Z, we find:
Y [T]X =


1
1
1
1
1
2
4
8
1
3
9
27

;
Z[T]X =


1
0
0
6
0
1
0
−11
0
0
1
6

.
If we change the input basis to X′ = (1, (t −2), (t −2)2, (t −2)3), we get:
Y [T]X′ =


1
−1
1
−1
1
0
0
0
1
1
1
1

;
Z[T]X′ =


1
−2
4
−2
0
1
−4
1
0
0
1
0

.
Example 6. Suppose V = F n, W = F m, and A is a given matrix in Mm,n(F). Define
a linear map LA : F n →F m by setting LA(v) = Av ∈F m for all column vectors v in
F n. The map LA is called left multiplication by the matrix A. Let E = (e1, . . . , en) be the
standard ordered basis of F n and E′ = (e′
1, . . . , e′
m) be the standard ordered basis of F m.
We show that E′[LA]E = A. To get the jth column of E′[LA]E, we apply LA to the jth
vector in E, obtaining LA(ej) = Aej = A[j]. Next, we take the coordinate vector of this
column vector relative to E′. For any column vector w ∈F m, [w]E′ = w, so the coordinate
vector is A[j]. Thus, the jth column of E′[LA]E equals the jth column of A for all j, proving
that E′[LA]E = A.
We can restate the result of this example by saying that the matrix of left multiplication
by A (using the standard ordered bases) is A itself. It follows that the map A 7→LA from
Mm,n(F) to L(F n, F m) is the inverse of the general bijection T 7→E′[T]E constructed
earlier, so this map is also a bijection. Bijectivity of the map A 7→LA means that every
linear transformation from F n to F m is left multiplication by a uniquely determined m × n
matrix.
144
Advanced Linear Algebra
6.6
Vector Operations on Matrices and Linear Maps
The set of matrices Mm,n(F) is a vector space with the following operations. Given A, B ∈
Mm,n(F) and c ∈F, A+B is the matrix with i, j-entry A(i, j)+B(i, j), and cA is the matrix
with i, j-entry cA(i, j). We can also describe these operations in terms of the columns of the
matrices using the formulas (A + B)[j] = A[j] + B[j] and (cA)[j] = c(A[j]) (see §4.7). The
operations on the right sides of the last two formulas are addition and scalar multiplication
of column vectors in the vector space F m.
Suppose V , W are given vector spaces, S and T are linear maps in L(V, W), and c ∈F.
Define S + T : V →W by (S + T)(v) = S(v) + T(v) for all v ∈V . Define cT : V →W
by (cT)(v) = c(T(v)) for all v ∈V . The operations on the right sides of these definitions
are the addition and scalar multiplication in the given abstract vector space W. You can
check that S + T and cT really are F-linear maps, so that we have the closure properties
S + T ∈L(V, W) and cT ∈L(V, W). Also, the zero function from V to W belongs to
L(V, W). It follows that L(V, W) is a subspace of the vector space of all functions from V to
W (see Exercise 8 in Chapter 4), so that L(V, W) is also a vector space using the operations
defined here.
Let X = (x1, . . . , xn) be an ordered basis for V and Y = (y1, . . . , ym) be an ordered basis
for W. We prove that the bijection MX,Y : L(V, W) →Mm,n(F) given by MX,Y (T) = Y [T]X
is F-linear, hence is a vector space isomorphism. Let S, T ∈L(V, W) and c ∈F. For j
between 1 and n, the jth column of Y [S + T]X is
[(S + T)(xj)]Y = [S(xj) + T(xj)]Y = [S(xj)]Y + [T(xj)]Y ,
which is the sum of the jth column of Y [S]X and the jth column of Y [T]X. This means
that MX,Y (S + T) = MX,Y (S) + MX,Y (T). Similarly, the jth column of Y [cT]X is
[(cT)(xj)]Y = [c(T(xj))]Y = c · [T(xj)]Y .
This is c times the jth column of Y [T]X, and therefore MX,Y (cT) = cMX,Y (T). To
summarize: given any ordered bases X of V and Y of W, the map T 7→Y [T]X is a vector
space isomorphism L(V, W) ∼= Mm,n(F).
Since isomorphic vector spaces have the same dimension, we deduce dim(L(V, W)) =
dim(Mm,n(F)) = mn. We can obtain an even stronger conclusion by recalling that
vector space isomorphisms send bases to bases. We have seen (§4.3) that the set
E = {eij : 1 ≤i ≤m, 1 ≤j ≤n} is a basis for Mm,n(F). Note that eij can be described
as the m × n matrix whose jth column is e[j]
ij = ei ∈F m, and whose kth column (for
each k ̸= j) is e[k]
ij = 0 ∈F m. The image of E under M −1
X,Y is a basis for L(V, W). Let
us describe the linear map Tij = M −1
X,Y (eij). Since Y [Tij]X = eij by definition, we see that
[Tij(xk)]Y = e[k]
ij = 0 for all k ̸= j, so that Tij(xk) = LY (0) = 0W for all k ̸= j. On the
other hand, [Tij(xj)]Y = e[j]
ij = ei implies Tij(xj) = LY (ei) = yi. Thus Tij sends the basis
vector xj to yi and all other basis vectors in X to zero. By linearity, Tij : V →W must be
given by the formula
Tij
 n
X
k=1
ckxk
!
= cjyi
for all ck ∈F.
(6.4)
Comparing Concrete Linear Algebra to Abstract Linear Algebra
145
6.7
Matrix Transpose versus Dual Maps
Given a matrix A ∈Mm,n(F), the transpose of A is the matrix AT ∈Mn,m(F) defined by
AT(i, j) = A(j, i) for 1 ≤i ≤n and 1 ≤j ≤m. To understand the significance of this
operation in the world of abstract linear algebra, we must first briefly discuss dual vector
spaces. (See Chapter 13 for a more complete treatment.)
Given an n-dimensional F-vector space V , the dual space V ∗is defined to be L(V, F), the
vector space of all linear maps from V into the field F (viewed as a 1-dimensional F-vector
space). Applying our general results on L(V, W) with W = F, we see that V ∗= L(V, F) is
isomorphic to the vector space M1,n(F) of 1 × n matrices or row vectors. More specifically,
let X = (x1, . . . , xn) be any ordered basis of V . The one-element list Z = (1F ) is an ordered
basis for the F-vector space F, as you can check. For j = 1, 2, . . . , n, define linear maps
fj ∈V ∗= L(V, F) by setting fj(xj) = 1F , setting fj(xk) = 0F for each k ̸= j, and extending
by linearity. Explicitly, fj(Pn
k=1 ckxk) = cj for all c1, . . . , cn ∈F. Comparing this to the
description of the maps Tij at the end of the last section, we see that X∗= (f1, . . . , fn)
is the ordered basis for V ∗= L(V, F) that corresponds to the standard ordered basis
for M1,n(F) under the isomorphism M −1
X,Z. X∗is called the dual basis of X. Note that
dim(V ∗) = dim(V ) = n (in the finite-dimensional case we are considering).
Next, let W be an m-dimensional F-vector space with ordered basis Y = (y1, . . . , ym)
and S ∈L(V, W) be a fixed linear map. Given any map g ∈W ∗= L(W, F), note that
g ◦S : V →F is a linear map from V to F; that is, g ◦S is an element of V ∗. Thus, S
induces a map S∗: W ∗→V ∗given by S∗(g) = g ◦S for g ∈W ∗. The map S∗is itself
linear, since for all g, h ∈W ∗and all c ∈F,
S∗(g + h) = (g + h) ◦S = (g ◦S) + (h ◦S) = S∗(g) + S∗(h);
S∗(cg) = (cg) ◦S = c(g ◦S) = cS∗(g).
So S∗∈L(W ∗, V ∗). From our discussion above, we have the dual bases Y ∗for W ∗and X∗
for V ∗. We claim that
X∗[S∗]Y ∗= (Y [S]X)T.
That is, transposing the matrix of S (computed using the input basis X and output basis
Y ) gives the matrix of S∗(computed using the input basis Y ∗and output basis X∗).
To prove this claim, set A = Y [S]X and B = X∗[S∗]Y ∗. We must show that B(i, j) =
A(j, i) whenever 1 ≤i ≤n and 1 ≤j ≤m. Write X∗= (f1, . . . , fn) and Y ∗= (g1, . . . , gm).
By definition, the jth column of B is
B[j] = [S∗(gj)]X∗= [gj ◦S]X∗.
To proceed, recall that [gj ◦S]X∗is the unique n-tuple (c1, . . . , cn) ∈F n such that
gj ◦S = c1f1 + · · · + cnfn.
To find the ith coordinate ci of B[j], evaluate both sides of the previous identity at xi.
The right side is ci, while the left side is gj(S(xi)). Therefore, B(i, j) = B[j](i) = ci =
gj(S(xi)). On the other hand, since A[i] = [S(xi)]Y , we have S(xi) = Pm
k=1 A[i](k)yk =
Pm
k=1 A(k, i)yk. Applying gj to both sides gives gj(S(xi)) = A(j, i). We conclude that
B(i, j) = A(j, i).
146
Advanced Linear Algebra
6.8
Matrix/Vector Multiplication versus Evaluation of Maps
Suppose A is an m×n matrix and z = (z1, . . . , zn) ∈F n is an n-tuple, regarded as an n×1
column vector. We can form the matrix-vector product Az, which is an m-tuple (m × 1
column vector) whose ith component is Pn
j=1 A(i, j)zj. As shown in Chapter 4, we can also
write this formula as Az = Pn
j=1 zjA[j], which expresses Az as a linear combination of the
columns A[j] of A.
On the other hand, given a linear map T : V →W and an abstract vector v ∈V , we
can apply the map T to v to obtain another vector w = T(v) ∈W. Let X = (x1, . . . , xn)
and Y = (y1, . . . , ym) be ordered bases for V and W, respectively. We prove that
[T(v)]Y = Y [T]X [v]X.
(6.5)
We can state this formula in words: the coordinate vector of the output of T at input v
(using basis Y ) is found by multiplying the matrix of T (using bases X and Y ) by the
coordinate vector of v (using basis X). The proof consists of three steps. First, writing
[v]X = (c1, . . . , cn), we know v = Pn
j=1 cjxj by definition of coordinates. Second, note that
T(v) = Pn
j=1 cjT(xj) by linearity of T. Third, applying the formula at the end of the
previous paragraph to A = Y [T]X and z = [v]X, we get
Y [T]X [v]X = Az =
n
X
j=1
zjA[j] =
n
X
j=1
cj[T(xj)]Y =


n
X
j=1
cjT(xj)


Y
= [T(v)]Y .
6.9
Matrix Multiplication versus Composition of Linear Maps
Given matrices B ∈Mp,m(F) and A ∈Mm,n(F), the matrix product BA ∈Mp,n(F) is
defined by
(BA)(i, j) =
m
X
k=1
B(i, k)A(k, j)
for i = 1, 2, . . . , p and j = 1, 2, . . . , n.
For linear maps T ∈L(V, W) and S ∈L(W, U), the composition S ◦T ∈L(V, U) is defined
by
(S ◦T)(v) = S(T(v))
for all v ∈V .
You can check that S ◦T really is linear.
We now compare the concrete operation of matrix multiplication to the abstract
operation of composition of linear maps. Let X
= (x1, . . . , xn), Y
= (y1, . . . , ym),
and Z = (z1, . . . , zp) be ordered bases for V , W, and U, respectively. Given linear
maps S and T as above, let A =
Y [T]X ∈Mm,n(F), B =
Z[S]Y
∈Mp,m(F), and
C = Z[S ◦T]X ∈Mp,n(F). We claim that C = BA, or in other notation,
Z[S ◦T]X = Z[S]Y Y [T]X.
(6.6)
We know (§4.7) that the jth column of BA is B(A[j]). So it suffices to show that
C[j] = B(A[j]) for j = 1, 2, . . . , n. This follows from the computation:
C[j] = [(S ◦T)(xj)]Z = [S(T(xj))]Z = Z[S]Y [T(xj)]Y = B(A[j]),
Comparing Concrete Linear Algebra to Abstract Linear Algebra
147
where we have used the definition of C, then the definition of S ◦T, then (6.5), then the
definition of A[j]. To summarize: the matrix of the composition of two linear maps is the
product of the matrices of the individual maps, whenever the output basis of the map acting
first equals the input basis of the map acting second.
As one consequence of this formula, suppose U = V , m = n = p, X = Z, and T ∈
L(V, W) is an invertible linear map with inverse T −1 ∈L(W, V ). Formula (6.6) gives
Y [T]X X[T −1]Y = Y [T ◦T −1]Y = Y [idW ]Y = In;
X[T −1]Y Y [T]X = X[T −1 ◦T]X = X[idV ]X = In.
These equations show that
(Y [T]X)−1 = X[T −1]Y .
(6.7)
Thus, matrix inversion corresponds to functional inversion of the associated linear map.
6.10
Transition Matrices and Changing Coordinates
Given an element v of some vector space V , we sometimes need to compute the coordinates
of v relative to more than one basis. Specifically, suppose X = (x1, . . . , xn) and Y =
(y1, . . . , yn) are ordered bases for V , we are given [v]X for some v ∈V , and we need to
compute [v]Y . Applying formula (6.5) to the identity map id : V →V , we see that
[v]Y = Y [id]X [v]X.
Here, Y [id]X is the matrix whose jth column is [id(xj)]Y = [xj]Y . We call this matrix
the transition matrix from X to Y , since left-multiplication by this matrix transforms
coordinates relative to X into coordinates relative to Y . Since id−1 = id, (6.7) shows that
(Y [id]X)−1 = X[id]Y . Thus, transition matrices between ordered bases are invertible, and
the transition matrix from Y to X is the inverse of the transition matrix from X to Y .
To compute the transition matrix Y [id]X: Compute the coordinates [xj]Y of each xj
relative to Y , and put these coordinates in the jth column of the transition matrix.
To change coordinates from [v]X to [v]Y : Compute the matrix-vector product [v]Y =
Y [id]X [v]X.
Example 1. For V = P≤3 (see Table 6.1), we have
X1[id]X2 =


−8
4
−2
1
12
−4
1
0
−6
1
0
0
1
0
0
0

.
For instance, the first column comes from the computation (t −2)3 = −8 + 12t −6t2 + 1t3.
You may check directly that
X2[id]X1 =


0
0
0
1
0
0
1
6
0
1
4
12
1
2
4
8

,
148
Advanced Linear Algebra
and this matrix is the inverse of the preceding one. Suppose v = 2t3 −4t + 1, so that
[v]X1 = (1, −4, 0, 2). Then
[v]X2 = X2[id]X1 [v]X1 = (2, 12, 20, 9),
which says that v = 2(t −2)3 + 12(t −2)2 + 20(t −2) + 9(1).
Example 2. For the three ordered bases of R3 in Table 6.1, it is immediate that
X1[id]X2 =


1
1
1
1
2
3
1
4
9

;
X1[id]X3 =


0
−1
1
2
1
0
1
3
4

.
So, for instance,
X2[id]X3 = X2[id]X1X1[id]X3 = (X1[id]X2)−1
X1[id]X3 =


−4.5
−4
5
7
4
−7
−2.5
−1
3

.
To check the third column of the answer, observe that (1, 0, 4) = 5(1, 1, 1) −7(1, 2, 4) +
3(1, 3, 9).
It turns out that all invertible matrices occur as transition matrices Y [id]X. More
precisely, suppose dim(V ) = n and Y is a fixed ordered basis for V . Let B be the set
of all ordered bases for V and GLn(F) be the set of all invertible n×n matrices with entries
in F. Define ϕ : B →GLn(F) by setting ϕ(Z) = Y [id]Z for each ordered basis Z ∈B. We
show that ϕ is a bijection.
For the proof, we define a map ϕ′ : GLn(F) →B that will be shown to be ϕ−1. For any
invertible matrix A ∈GLn(F) with columns A[1], . . . , A[n], let
ϕ′(A) = (LY (A[1]), . . . , LY (A[n])).
We first show that ϕ′(A) is an ordered basis for V , meaning that ϕ′(A) ∈B. Since A
is invertible, the column rank of A is n (see §4.13). Consequently, the list of columns
(A[1], . . . , A[n]) is an ordered basis for F n. Applying the isomorphism LY : F n →V to this
ordered basis, we see that ϕ′(A) is indeed an ordered basis for V . Next, let A ∈GLn(F)
and Z = (z1, . . . , zn) ∈B be arbitrary. Using the definition of Y [id]Z, we have the following
list of equivalent statements.
(a) ϕ(Z) = A.
(b) [zj]Y = A[j] for j = 1, 2, . . . , n.
(c) zj = LY (A[j]) for j = 1, 2, . . . , n.
(d) Z = ϕ′(A).
We conclude that ϕ′ is the two-sided inverse to ϕ.
The bijectivity of ϕ means that for any fixed ordered basis Y of V , every invertible matrix
in Mn(F) has the form Y [id]Z for a unique ordered basis Z of V . By similar methods, you
can check that for fixed Y , every invertible matrix in Mn(F) has the form X[id]Y for a
unique ordered basis X of V .
6.11
Changing Bases
Let V and W be vector spaces with dim(V ) = n and dim(W) = m. Suppose T ∈L(V, W)
is represented by a matrix A = Y [T]X relative to an ordered basis X for V and an ordered
Comparing Concrete Linear Algebra to Abstract Linear Algebra
149
basis Y for W. What happens to A if we change the basis for the domain V from X to
another ordered basis X′? Since T = T ◦idV , formula (6.6) gives the relation
Y [T]X′ = Y [T]X X[idV ]X′.
Writing A′ = Y [T]X′ and P = X[idV ]X′, this says that A′ = AP. As X′ ranges over
all possible ordered bases for V (while X remains fixed), we know that P ranges over all
invertible n × n matrices.
For any A, A′ ∈Mm,n(F), say that A is column-equivalent to A′ iff A′ = AP for some
invertible P ∈Mn(F). You can check that this defines an equivalence relation on Mm,n(F).
Since P is invertible iff P is a finite product of elementary matrices, we see that A′ is column-
equivalent to A iff A′ can be obtained from A by a finite sequence of elementary column
operations, which are encoded by the matrix P (see Chapter 4). For a matrix A = Y [T]X,
the equivalence class of A relative to column-equivalence is the set of all matrices Y [T]X′,
as X′ ranges over all ordered bases for V .
Next, we ask: what happens to A = Y [T]X if we change the basis for the codomain W
from Y to Y ′? Since T = idW ◦T, formula (6.6) gives
Y ′[T]X = Y ′[idW ]Y Y [T]X.
Writing A′ = Y ′[T]X and Q = Y ′[idW ]Y , this says that A′ = QA. As Y ′ ranges over all
possible ordered bases for W (while Y remains fixed), we know that Q ranges over all
invertible m × m matrices.
For all A, A′ ∈Mm,n(F), say that A is row-equivalent to A′ iff A′ = QA for some
invertible Q ∈Mm(F). This defines another equivalence relation on Mm,n(F). A′ is row-
equivalent to A iff A′ can be obtained from A by a finite sequence of elementary row
operations, which are encoded by the matrix Q. If A = Y [T]X, then the equivalence class of
A relative to row-equivalence is the set of all matrices Y ′[T]X, as Y ′ ranges over all ordered
bases for W.
Suppose we change the input basis from X to X′ and also change the output basis from
Y to Y ′. Then
Y ′[T]X′ = Y ′[idW ]Y Y [T]X X[idV ]X′.
Accordingly, we say that A, A′ ∈Mm,n(F) are equivalent (or, more precisely, row/column-
equivalent) iff A′ = QAP for some invertible matrices Q ∈Mm(F) and P ∈Mn(F) iff
A′ can be obtained from A by a finite sequence of elementary row and column operations.
The equivalence class of Y [T]X relative to this equivalence relation is the set of all matrices
Y ′[T]X′ as X′ and Y ′ range independently over all ordered bases for V and W, respectively.
Given a field F and any matrix A ∈Mm,n(F), we can find invertible matrices P and Q
such that QAP has i, i-entry equal to 1 for 1 ≤i ≤r = rank(A), and all other entries of
QAP are 0 (see §4.12). Taking A = Y [T]X, where T ∈L(V, W) is an arbitrary linear map,
we obtain the following result.
Projection Theorem. For any T ∈L(V, W), there exist ordered bases X′ = (x′
1, . . . , x′
n)
for V and Y ′ = (y′
1, . . . , y′
m) for W such that T(x′
i) = y′
i for 1 ≤i ≤rank(T), and T(x′
i) = 0
for all i > rank(T).
We can also give an abstract proof of this theorem that makes no mention of matrices;
see Exercise 46. In Chapter 18, we obtain a more general theorem by replacing the field F
of scalars by a type of ring called a principal ideal domain.
150
Advanced Linear Algebra
6.12
Algebras of Matrices versus Algebras of Linear Operators
Let F be a field. As defined in §1.3, an F-algebra is a structure (A, +, ·, ⋆) such that (A, +, ·)
is an F-vector space, (A, +, ⋆) is a ring, and c·(x⋆y) = (c·x)⋆y = x⋆(c·y) for all x, y ∈A
and all c ∈F. According to this definition, all F-algebras are assumed to be associative
and have a multiplicative identity. There are more general versions of F-algebras, but they
do not occur in this book. A map g : A →B between two F-algebras is called an F-algebra
homomorphism iff g is F-linear and a ring homomorphism, meaning that
g(x + y) = g(x) + g(y),
g(c · x) = c · g(x), g(x ⋆y) = g(x) ⋆g(y),
and g(1A) = 1B
for all x, y ∈A and all c ∈F. An F-algebra isomorphism is a bijective F-algebra
homomorphism.
In linear algebra, there are two prominent examples of F-algebras. First, we have the
concrete F-algebra Mn(F) = Mn,n(F) consisting of all square n × n matrices over the
field F. Using the standard matrix operations, Mn(F) is both a vector space (of dimension
n2) and a ring, and c(AB) = (cA)B = A(cB) for all A, B ∈Mn(F) and all c ∈F. The
multiplicative identity element is the identity matrix In. Second, we have the abstract F-
algebra L(V ) = L(V, V ) consisting of all F-linear operators T : V →V on an n-dimensional
vector space V . We know L(V ) is a vector space of dimension n2 under the standard
pointwise operations on linear maps. This vector space becomes a ring (as you can verify)
by defining the product of S, T ∈L(V ) to be the composition of functions S◦T. The identity
function idV is the identity element for L(V ).
We have already seen that the vector spaces Mn(F) and L(V ) are isomorphic (§6.6).
Indeed, for each pair of ordered bases X and Y of V , the map T 7→Y [T]X is a vector
space isomorphism L(V ) ∼= Mn(F). Here, we make the stronger statement that the F-
algebras Mn(F) and L(V ) are isomorphic. We obtain the required algebra isomorphisms by
letting X = Y . More specifically, for each ordered basis X of V , consider the vector space
isomorphism MX = MX,X : L(V ) →Mn(F) given by MX(T) = X[T]X for T ∈L(V ).
By (6.6), for all S, T ∈L(V ),
MX(S ◦T) = X[S ◦T]X = X[S]X X[T]X = MX(S)MX(T).
This says that the bijective linear map MX preserves products. Also, MX(idV ) = X[idV ]X =
In, so MX is an algebra isomorphism. Let us write [T]X as an abbreviation for X[T]X.
We review some previously established facts using the new notation. Suppose S, T ∈
L(V ), v ∈V , c ∈F, and X = (x1, . . . , xn) is an ordered basis for V . First, the following
statements are equivalent.
(a) A = [S]X.
(b) The jth column of A is [S(xj)]X for j = 1, 2, . . . , n.
(c) S(xj) = LX(A[j]) = Pn
i=1 A(i, j)xi for j = 1, 2, . . . , n.
Second, MX preserves the algebraic structure:
[S + T]X = [S]X + [T]X;
[cS]X = c[S]X;
[S ◦T]X = [S]X [T]X;
[S(v)]X = [S]X [v]X;
[0V ]X = 0n×n;
[idV ]X = In.
Third, S is an invertible operator in L(V ) iff [S]X is an invertible matrix in Mn(F), in
which case
[S−1]X = ([S]X)−1.
It follows from these remarks that [Sk]X = ([S]X)k holds for all integers k ≥0, and for all
negative integers k when S is invertible.
Comparing Concrete Linear Algebra to Abstract Linear Algebra
151
An element x in an F-algebra is called nilpotent iff xm = 0 for some m ≥1. For
example, a square matrix is nilpotent if multiplying the matrix by itself enough times gives
the zero matrix. A linear operator on V is nilpotent if applying the operator enough times
in succession gives the zero operator on V . From the identity [Sk]X = ([S]X)k, we see that
S ∈L(V ) is a nilpotent operator iff [S]X ∈Mn(F) is a nilpotent matrix.
6.13
Similarity of Matrices versus Similarity of Linear Maps
Two n × n matrices A and A′ are called similar iff there exists an invertible matrix P ∈
Mn(F) such that A′ = P −1AP. Similarity of matrices is readily seen to be an equivalence
relation on Mn(F). So, Mn(F) is the disjoint union of equivalence classes relative to the
similarity relation.
To understand the abstract significance of similarity of matrices, suppose T ∈L(V ) and
X is an ordered basis for V . Let A = [T]X. If we change the basis for V from X to X′,
what happens to the representing matrix A for T? Letting A′ = [T]X′ and P = X[idV ]X′,
formula (6.6) gives
A′ = X′[idV ]X X[T]X X[idV ]X′ = P −1AP.
In other words, the matrix [T]X′ is similar to the matrix [T]X. Holding X fixed and letting
X′ vary over all ordered bases for V , we know that P ranges over all invertible n×n matrices
(§6.10). Therefore, the set of all matrices similar to A, namely {P −1AP : P ∈GLn(F)},
equals the set of matrices [T]X′ that represent the given linear operator T relative to all
the possible ordered bases X′ for V .
Compare this to the following abstract version of similarity. Two linear maps T, T ′ :
V →V are called similar iff there exists an invertible linear map S : V →V such that
T ′ = S−1 ◦T ◦S. Similarity of linear maps defines an equivalence relation on the F-algebra
L(V ). Choosing a fixed ordered basis X for V and applying the isomorphism MX : L(V ) ∼=
Mn(F), we see that the linear maps T ′ and T are similar iff the matrices A′ = [T ′]X and
A = [T]X are similar. As T ′ ranges over all linear maps similar to T, A′ ranges over all
matrices similar to A.
Given a matrix A = [T]X ∈Mn(F), we now have two interpretations for its equivalence
class {P −1AP : P ∈GLn(F)} relative to similarity. First, this equivalence class equals the
set of all matrices of the form [T]X′ as X′ ranges over ordered bases for V . Second, this
equivalence class equals the set of all matrices [T ′]X where T ′ ranges over all linear maps
similar to T. The first interpretation of equivalence classes is fundamental in linear algebra,
for the following reason. If we can find a particularly simple matrix in the similarity class of
A = [T]X, then the action of the operator T is correspondingly simple for an appropriately
chosen ordered basis X′ for V . More specifically, if A′ = P −1AP is nice (in some sense)
when P is the matrix X[id]X′, then the relation A′ = [T]X′ implies that the action of T on
the basis X′ is nice in the same way. The next section describes some possibilities for the
“nice” matrices we might hope to find in the equivalence class of A under similarity.
6.14
Diagonalizability and Triangulability
Given a matrix A ∈Mn(F), we make the following definitions.
(a) A is diagonal iff A(i, j) = 0 for all i ̸= j.
(b) A is upper-triangular iff A(i, j) = 0 for all i > j.
152
Advanced Linear Algebra
(c) A is strictly upper-triangular iff A(i, j) = 0 for all i ≥j.
(d) A is lower-triangular iff A(i, j) = 0 for all i < j.
(e) A is strictly lower-triangular iff A(i, j) = 0 for all i ≤j.
For example, the matrices
 2
0
0
3

,
 2
1
0
3

,
 0
1
0
0

,
 2
0
1
3

,
 0
0
1
0

,
are diagonal, upper-triangular, strictly upper-triangular, lower-triangular, and strictly
lower-triangular, respectively.
Suppose V is an n-dimensional vector space and T ∈L(V ) is a linear map. T is called
diagonalizable iff there exists an ordered basis X = (x1, . . . , xn) for V such that [T]X is a
diagonal matrix. In this case, writing A = [T]X, we have
T(xj) =
n
X
i=1
A(i, j)xi = A(j, j)xj
for j = 1, 2, . . . , n.
This says that every xj in X is an eigenvector for T with eigenvalue A(j, j). Conversely, if
X is an ordered basis such that T(xj) = cjxj for each xj ∈X, then [T]X is the diagonal
matrix with c1, . . . , cn on the main diagonal. So far, we have shown: T is diagonalizable iff
there exists an ordered basis X for V consisting of eigenvectors for T.
Next, define T ∈L(V ) to be triangulable (resp. strictly triangulable) iff there exists an
ordered basis X = (x1, . . . , xn) for V such that A = [T]X is an upper-triangular matrix
(resp. strictly upper-triangular matrix). By definition, T(xj) = Pn
i=1 A(i, j)xi for j =
1, 2, . . . , n. So the upper-triangularity condition (A(i, j) = 0 for all i > j) is equivalent
to the identities T(xj) = Pj
i=1 A(i, j)xi for j = 1, 2, . . . , n. In other words, [T]X is upper-
triangular iff for j = 1, 2, . . . , n, T(xj) is a linear combination of x1, . . . , xj. Similarly, [T]X
is strictly upper-triangular iff T(x1) = 0 and each T(xj) for j = 2, 3, . . . , n is a linear
combination of x1, . . . , xj−1. Analogous considerations show that [T]X is lower-triangular
iff each T(xj) is a linear combination of xj, . . . , xn (similarly for strict lower-triangularity).
We can also phrase these results in terms of similarity of matrices. Given T ∈L(V ),
define A = [T]Y where Y is any ordered basis for V (e.g., Y might be the standard
ordered basis for V = F n). These conditions are equivalent: T is diagonalizable; A′ = [T]X
is diagonal for some ordered basis X; the similarity equivalence class of A contains a
diagonal matrix; P −1AP is diagonal for some invertible matrix P ∈Mn(F). Likewise,
these conditions are equivalent: T is triangulable; A′ = [T]X is upper-triangular for some
ordered basis X; the similarity equivalence class of A contains an upper-triangular matrix;
P −1AP is upper-triangular for some invertible P ∈Mn(F). We say that a matrix A is
diagonalizable (resp. triangulable) iff P −1AP is diagonal (resp. upper-triangular) for some
invertible P ∈Mn(F).
Let us look more closely at the matrix equation P −1AP = D, where A ∈Mn(F)
is given, P is an unknown invertible matrix in Mn(F), and D is an unknown diagonal
matrix in Mn(F). Say P has columns x1, x2, . . . , xn, and D has diagonal entries c1, . . . , cn.
The following conditions are equivalent: P −1AP = D; AP = PD; A(P [j]) = P(D[j]) for
j = 1, 2, . . . , n; Axj = cjxj for j = 1, 2, . . . , n. Also, invertibility of P is equivalent to
requiring that (x1, . . . , xn) is an ordered basis for F n. This calculation shows: a matrix
A ∈Mn(F) is diagonalizable iff F n has an ordered basis consisting of eigenvectors of A.
In this case, we diagonalize A by arranging n linearly independent eigenvectors of A as the
columns of a matrix P; then P −1AP = D is diagonal with the corresponding eigenvalues
as diagonal entries. So, any algorithm for finding eigenvectors and eigenvalues of A leads to
an algorithm for diagonalizing A, if possible. Some numerical methods for approximating
Comparing Concrete Linear Algebra to Abstract Linear Algebra
153
the eigenvalues and eigenvectors of A are covered in Chapter 10. As a complement to this
approach, Chapters 7 and 8 develop some easy-to-test sufficient conditions for a matrix to
be diagonalizable or triangulable.
Next, we discuss an abstract formulation of triangulability. To do so, we introduce the
notion of a flag of subspaces. Given an n-dimensional vector space V , a (complete) flag of
subspaces is a list (V0, V1, . . . , Vn) of subspaces of V such that {0} = V0 ⊆V1 ⊆· · · ⊆Vn = V
and dim(Vi) = i for each i. For example, if X = (x1, . . . , xn) is an ordered basis for V and
Vi is the subspace spanned by x1, . . . , xi, then (V0, . . . , Vn) is a flag of subspaces. Call this
flag the flag of subspaces associated with the ordered basis X. Conversely, we can obtain
an ordered basis X for V from a flag of subspaces by letting xi be any vector in Vi but
not in Vi−1. Note that X is not uniquely determined by the flag since we have choices for
each xi. We say that a linear map T ∈L(V ) stabilizes a flag of subspaces iff T[Vi] ⊆Vi
for i = 0, 1, . . . , n. T is said to strictly stabilize the flag iff T[Vi] ⊆Vi−1 for i = 1, 2, . . . , n.
From our earlier discussion of triangulability, we see that [T]X is (strictly) upper-triangular
iff T (strictly) stabilizes the flag of subspaces associated with X. Conversely, if T (strictly)
stabilizes some flag, and we form an ordered basis X from this flag as described above,
then [T]X is (strictly) upper-triangular. To summarize, T ∈L(V ) is (strictly) triangulable
iff there exists a flag of subspaces of V (strictly) stabilized by T.
We claim that a strictly triangulable operator T must be nilpotent. Letting V0 ⊆V1 ⊆
· · · ⊆Vn = V be a flag strictly stabilized by T, and setting Vi = {0} for all i < 0, we see by
induction on k ≥1 that T k[Vj] ⊆Vj−k for all j ≤n. In particular, T n[V ] = T n[Vn] ⊆V0 =
{0}, so that T n = 0 and T is nilpotent. Translating these comments into matrix terms, we
see that a strictly upper-triangular matrix A is nilpotent, with An = 0.
6.15
Block-Triangular Matrices and Invariant Subspaces
We say a matrix A in Mn(F) is block-triangular with two blocks iff there is a k between 1
and n −1 such that for all i, j with k < i ≤n and 1 ≤j ≤k, A(i, j) = 0. Such a matrix has
the form
 B
C
0
D

, where B ∈Mk(F), C ∈Mk,n−k(F), 0 is the (n −k) × k zero matrix,
and D ∈Mn−k(F).
Let W be a subspace of a vector space V . Given T ∈L(V ), we say W is a T-invariant
subspace of V iff T[W] ⊆W, which means that T(w) ∈W for all w ∈W. By linearity,
it is equivalent to require that T(x) ∈W for all x in some ordered basis for W. If W is
T-invariant, then the restricted linear map T|W : W →V can be viewed as a linear map
with codomain W. In other words, the restriction T|W belongs to the F-algebra L(W).
Furthermore, we have an induced map T ′ : V/W →V/W on the quotient vector space
V/W, defined by T ′(v + W) = T(v) + W for all v ∈V . This map is well-defined, because
for v, u ∈V , v + W = u + W implies v −u ∈W, hence T(v) −T(u) ∈W by linearity and
T-invariance of W, hence T(v) + W = T(u) + W. Since T ′ is also linear, T ′ belongs to the
F-algebra L(V/W).
We can relate these abstract concepts to block-triangular matrices. Suppose that W is
a T-invariant subspace of V with {0} ̸= W ̸= V , and X = (x1, . . . , xn) is any ordered
basis for V such that X1 = (x1, . . . , xk) is an ordered basis for W. You can check that
X2 = (xk+1 + W, . . . , xn + W) is an ordered basis for V/W (Exercise 56). We claim: the
matrix A = [T]X is block-triangular with block sizes k and n −k; the upper-left k × k block
154
Advanced Linear Algebra
of A is [T|W]X1; and the lower-right (n −k) × (n −k) block of A is [T ′]X2. Pictorially,
A = [T]X =

[T|W]X1
C
0
[T ′]X2

,
where C is some k × (n −k) matrix.
To prove this, let us first compute the column A[j] where 1 ≤j ≤k. On one hand, since
A = [T]X, we have T(xj) = Pn
i=1 A(i, j)xi, where this expression of T(xj) in terms of the
basis X is unique. On the other hand, letting B = [T|W]X1, we have
T(xj) = T|W(xj) =
k
X
i=1
B(i, j)xi =
k
X
i=1
B(i, j)xi + 0xk+1 + · · · + 0xn.
We have just written two expressions for T(xj) as linear combinations of the elements in the
basis X. Since the expansion in terms of a basis is unique, we conclude that A(i, j) = B(i, j)
for 1 ≤i ≤k, and A(i, j) = 0 for k < i ≤n. These conclusions hold for j = 1, 2, . . . , k, so
we have confirmed that the upper-left and lower-left blocks of A are given by B = [T|W]X1
and 0 ∈Mn−k,k(F), respectively.
Next, consider a column A[j] with k < j ≤n. Let D = [T ′]X2. On one hand, starting
from T(xj) = Pn
i=1 A(i, j)xi, the definition of T ′ shows that
T ′(xj + W) =
n
X
i=1
A(i, j)(xi + W) =
n
X
i=k+1
A(i, j)(xi + W).
The last equality holds because xi + W = 0 + W for i = 1, 2, . . . , k, since xi ∈W. On the
other hand, the definitions of D and X2 show that
T ′(xj + W) =
n−k
X
i=1
D(i, j −k)(xi+k + W) =
n
X
i=k+1
D(i −k, j −k)(xi + W).
Comparing these expressions, we see that A(i, j) = D(i −k, j −k), so that the lower-right
block of A is the matrix D = [T ′]X2.
Conversely, suppose X is any ordered basis for V such that A = [T]X is block-
triangular with diagonal blocks of size k and n −k, in this order. Let W be the subspace
spanned by X1 = (x1, . . . , xk). For j = 1, 2, . . . , k, the block-triangularity of A gives
T(xj) = Pn
i=1 A(i, j)xi = Pk
i=1 A(i, j)xi, so that T(xj) ∈W for all xj in X1. We conclude
that W is a T-invariant subspace of V .
6.16
Block-Diagonal Matrices and Reducing Subspaces
A matrix A ∈Mn(F) is block-diagonal with two blocks iff for some k between 1 and n −1,
some B ∈Mk(F), and some D ∈Mn−k(F), A has the form

B
0k×(n−k)
0(n−k)×k
D

.
To relate this concept to linear maps, we must first discuss direct sums. Given a vector
space V with subspaces W and Z, we say V is the direct sum of W and Z, and we write
Comparing Concrete Linear Algebra to Abstract Linear Algebra
155
V = W ⊕Z, to mean that V = W + Z and W ∩Z = {0}. Equivalently, V = W ⊕Z means
that for each v ∈V , there is exactly one way to write v = w + z where w ∈W and z ∈Z.
Suppose V = W ⊕Z, X1 is an ordered basis for W, and X2 is an ordered basis for Z. You
can check that the list X formed by concatenating X1 and X2 is an ordered basis for V .
This implies dim(V ) = dim(W ⊕Z) = dim(W) + dim(Z).
Given T ∈L(V ), we say that T is reducible iff there exist nonzero T-invariant subspaces
W and Z such that V = W ⊕Z. In this situation, consider ordered bases X1 = (x1, . . . , xk)
for W, X2 = (xk+1, . . . , xn) for Z, and X = (x1, . . . , xn) for V . We claim that
A = [T]X =
 B
0
0
D

,
where B = [T|W]X1 and D = [T|Z]X2.
On one hand, for each j
∈{1, 2, . . . , k}, xj is in the T-invariant subspace W, so
T(xj) = Pn
i=1 A(i, j)xi = Pk
i=1 A(i, j)xi. Also, T(xj) = T|W(xj) = Pk
i=1 B(i, j)xi. On
the other hand, for each j ∈{k + 1, . . . , n}, xj is in the T-invariant subspace Z, so T(xj) =
Pn
i=1 A(i, j)xi = Pn
i=k+1 A(i, j)xi. Also, T(xj) = T|Z(xj) = Pn−k
i=1 D(i, j −k)xi+k. The
claim follows from these remarks.
Conversely, suppose X = (x1, . . . , xn) is any ordered basis of V such that A = [T]X is
block-diagonal with two diagonal blocks of size k and n−k. Let W be the subspace spanned
by (x1, . . . , xk) and Z be the subspace spanned by (xk+1, . . . , xn). Since X is an ordered
basis, it follows that V = W ⊕Z. The block-diagonal form of A shows that W and Z are
T-invariant subspaces. To summarize: there exists a decomposition V = W ⊕Z into two
T-invariant subspaces with dim(W) = k iff there exists an ordered basis X of V such that
[T]X is block-diagonal with blocks of size k and n −k.
6.17
Idempotent Matrices and Projections
A matrix A ∈Mn(F) is called idempotent iff A2 = A. For example, given 0 ≤k ≤n,
the block-diagonal matrix Ik,n =
 Ik
0
0
0

∈Mn(F) is idempotent. Analogously, a linear
map T ∈L(V ) is called idempotent iff T ◦T = T. Applying the F-algebra isomorphism
T 7→[T]X, where X is an ordered basis of V , we see that a linear map T is idempotent iff
[T]X is an idempotent matrix.
Suppose V is an F-vector space and W, Z are subspaces of V such that V = W ⊕Z.
For all v ∈V , there exist unique w ∈W and z ∈Z with v = w + z. So, we can define a
map P = PW,Z : V →V by letting P(v) be the unique w ∈W appearing in the expression
v = w + z. We call P the projection of V onto W along Z. We claim that P is F-linear and
idempotent, with img(P) = W and ker(P) = Z. Given v, v′ ∈V and c ∈F, write v = w +z
and v′ = w′ + z′ for unique w, w′ ∈W and unique z, z′ ∈Z. By definition, P(v) = w and
P(v′) = w′. Note that v + v′ = (w + w′) + (z + z′) with w + w′ ∈W and z + z′ ∈Z.
So the definition of P gives P(v + v′) = w + w′ = P(v) + P(v′). Similarly, cv = cw + cz
with cw ∈W and cz ∈Z, so P(cv) = cw = cP(v). Thus P is F-linear. Given w ∈W, we
have w = w + 0 with w ∈W and 0 ∈Z. So P(w) = w for all w ∈W. Given v ∈V with
v = w + z as above, we see that P 2(v) = P(P(v)) = P(w) = w = P(v). This holds for all
v ∈V , so P 2 = P. Turning to the image of P, the identity w = P(w) for w ∈W shows
that W ⊆img(P). The reverse inclusion img(P) ⊆W is immediate from the definition of
P, so img(P) = W. As for the kernel, given z ∈Z, we have z = 0 + z with 0 ∈W and
z ∈Z. So, P(z) = 0, which shows that Z ⊆ker(P). Conversely, fix v ∈V with v ∈ker(P).
156
Advanced Linear Algebra
Write v = w + z with w ∈W and z ∈Z. Then w = P(v) = 0 shows that v = z ∈Z. So
ker(P) ⊆Z.
Let us compute [P]X, where X is the ordered basis of V obtained by concatenating
an ordered basis X1 = (x1, . . . , xk) for W with an ordered basis X2 = (xk+1, . . . , xn) for
Z. For j = 1, 2, . . . , k, xj is in W, so we know that P(xj) = xj = 1xj + P
i̸=j 0xi. For
j = k + 1, . . . , n, xj is in Z, so we know that P(xj) = 0 = Pn
i=1 0xi. We conclude that
[P]X = Ik,n. It also follows that W and Z are P-invariant subspaces of V with P|W = idW
and P|Z = 0L(Z).
Next, we show that for every idempotent T ∈L(V ), there exist unique subspaces W
and Z with V = W ⊕Z and T = PW,Z. To prove uniqueness, suppose we had subspaces
W, Z, W1, Z1 with V = W ⊕Z = W1 ⊕Z1 and PW,Z = T = PW1,Z1. Then Z = ker(PW,Z) =
ker(T) = ker(PW1,Z1) = Z1 and W = img(PW,Z) = img(T) = img(PW1,Z1) = W1. To prove
existence, assume T ∈L(V ) is idempotent, and define W = img(T) and Z = ker(T). Let us
first check that V = W ⊕Z. Given z ∈W ∩Z, we have z = T(v) for some v ∈V and also
T(z) = 0. So z = T(v) = T(T(v)) = T(z) = 0, proving W ∩Z = {0}. Given v ∈V , note
v = T(v) + (v −T(v)) where T(v) ∈W = img(T). Since T(v −T(v)) = T(v) −T(T(v)) =
T(v)−T(v) = 0, we have v −T(v) ∈Z = ker(T). So v ∈W +Z, and V = W ⊕Z. To finish,
we check that T = PW,Z. Fix v ∈V , and write v = w + z with w ∈W and z ∈Z. We have
T(z) = 0 and w = T(y) for some y ∈V . Applying T to v = T(y) + z gives
T(v) = T(T(y) + z) = T(T(y)) + T(z) = T(y) + 0 = T(y) = w = PW,Z(v),
as needed. To summarize: every idempotent T ∈L(V ) is the projection PW,Z determined by
unique subspaces W = img(T) and Z = ker(T), which satisfy V = W ⊕Z. T is idempotent
iff for some ordered basis X of V and some k ∈{0, . . . , n}, [T]X = Ik,n.
6.18
Bilinear Maps and Matrices
Let V be an n-dimensional F-vector space. A function B : V × V →F is called F-bilinear
iff for all v, v′, w ∈V and all c ∈F, B(v + v′, w) = B(v, w) + B(v′, w), B(w, v + v′) =
B(w, v) + B(w, v′), and B(cv, w) = cB(v, w) = B(v, cw). It follows from this definition and
induction that for all bilinear maps B and all xi, yj ∈V and all ci, dj ∈F,
B


r
X
i=1
cixi,
s
X
j=1
djyj

=
r
X
i=1
s
X
j=1
cidjB(xi, yj).
(6.8)
Let BL(V ) be the set of all F-bilinear maps on V . You can check that the zero map is F-
bilinear; the pointwise sum of two F-bilinear maps is F-bilinear; and any scalar multiple of
an F-bilinear map is F-bilinear. So, BL(V ) is a subspace of the vector space of all functions
from V × V to F, and therefore BL(V ) is a vector space.
For each ordered basis X = (x1, . . . , xn) of V , we define a map NX : BL(V ) →Mn(F)
that turns out to be a vector space isomorphism. Given B ∈BL(V ), we define NX(B) to
be the matrix [B]X with i, j-entry B(xi, xj) for i, j = 1, 2, . . . , n. We call [B]X the matrix
of the bilinear map B relative to the ordered basis X. You can check that NX is an F-linear
map. We show that NX is one-to-one and onto.
Comparing Concrete Linear Algebra to Abstract Linear Algebra
157
To prove NX is one-to-one, suppose B, C ∈BL(V ) are two bilinear maps with NX(B) =
NX(C); we must prove B = C. Comparing the entries of [B]X and [C]X, our assumption tells
us that B(xi, xj) = C(xi, xj) for all i, j ∈{1, 2, . . . , n}. We must show B(v, w) = C(v, w)
for all v, w ∈V . Fix v, w ∈V , and write v = Pn
i=1 cixi and w = Pn
j=1 djxj for some
ci, dj ∈F. Since B and C satisfy (6.8),
B(v, w) =
n
X
i=1
n
X
j=1
cidjB(xi, xj) =
n
X
i=1
n
X
j=1
cidjC(xi, xj) = C(v, w).
To prove NX is onto, let A ∈Mn(F) be a given matrix; we must find B ∈BL(V )
with NX(B) = A. In other words, we must construct an F-bilinear map on V satisfying
B(xi, xj) = A(i, j) for all i, j ∈{1, 2, . . . , n}. To do so, take any v, w ∈V and write
v = Pn
i=1 cixi and w = Pn
j=1 djxj for some ci, dj ∈F. Let
B(v, w) =
n
X
i=1
n
X
j=1
cidjA(i, j),
which gives a well-defined function since expansions in terms of the ordered basis X are
unique. We must now check that B is F-bilinear and B(xi, xj) = A(i, j) for all i, j ∈
{1, 2, . . . , n}. We prove one required identity and let you check the rest. Given v, w ∈W as
above, and given v′ = Pn
i=1 c′
ixi with c′
i ∈F, note that v +v′ = Pn
i=1(ci +c′
i)xi. Therefore,
the definition of B gives
B(v + v′, w)
=
n
X
i=1
n
X
j=1
(ci + c′
i)djA(i, j)
=
n
X
i=1
n
X
j=1
cidjA(i, j) +
n
X
i=1
n
X
j=1
c′
idjA(i, j) = B(v, w) + B(v′, w).
To summarize: for each ordered basis X of V , there is an F-vector space isomorphism
BL(V ) ∼= Mn(F) that sends a bilinear map B ∈BL(V ) to the matrix [B]X with i, j-entry
B(xi, xj). Therefore, dim(BL(V )) = n2, where n = dim(V ).
6.19
Congruence of Matrices
Two matrices A, A′ ∈Mn(F) are called congruent iff there exists an invertible matrix
P ∈Mn(F) with A′ = P TAP. Congruence of matrices defines an equivalence relation on
Mn(F), as you can check. To understand the abstract significance of congruence of matrices,
suppose B ∈BL(V ) and X = (x1, . . . , xn) and Y = (y1, . . . , yn) are ordered bases of V .
Given that A = [B]X, what happens to A if we change the basis from X to Y ? We claim
that A is replaced by a congruent matrix A′ = P TAP, where P = X[id]Y . In particular, as
Y ranges over all ordered bases of V , we know that P ranges over all invertible matrices in
Mn(F). So the set of matrices representing a bilinear map B relative to some ordered basis
of V equals the set of matrices congruent to A.
158
Advanced Linear Algebra
We prove the claim by computing A′ = [B]Y . We know yj = Pn
i=1 P(i, j)xi for j =
1, 2, . . . , n. For fixed r, s between 1 and n, the r, s-entry of A′ is
B(yr, ys)
=
B


n
X
i=1
P(i, r)xi,
n
X
j=1
P(j, s)xj


=
n
X
i=1
n
X
j=1
P(i, r)P(j, s)B(xi, xj)
=
n
X
i=1
n
X
j=1
P T(r, i)A(i, j)P(j, s) = (P TAP)(r, s).
So A′ = P TAP as claimed.
As in the case of similarity, for each congruence class in Mn(F), we would like to find a
matrix in that congruence class that is as simple as possible. The answer to this question
depends heavily on the field F. See Chapter 14 for more details.
6.20
Real Inner Product Spaces and Orthogonal Matrices
In this section, we consider real vector spaces. A real inner product space consists of a real
vector space V and a bilinear map B ∈BL(V ) such that B(v, w) = B(w, v) for all v, w ∈V ,
and B(v, v) > 0 for all nonzero v ∈V . For v, w ∈V , we write ⟨v, w⟩V = B(v, w). The most
common inner product space is Rn, taking the bilinear form B to be the dot product: for
all v = (v1, . . . , vn) and w = (w1, . . . , wn) in Rn,
B(v, w) = ⟨v, w⟩Rn = v • w = v1w1 + v2w2 + · · · + vnwn.
We see at once that [B]E = In, where E = (e1, . . . , en) is the standard ordered basis of Rn.
In any inner product space V , the norm or length of v ∈V is defined by ||v|| =
p
B(v, v).
An ordered basis X = (x1, . . . , xn) of V is called an orthonormal basis iff for all i, j between
1 and n, ⟨xi, xj⟩V is 1 if i = j and is 0 if i ̸= j. For example, the standard ordered basis E
of Rn is orthonormal.
Given inner product spaces V and W, a linear map T ∈L(V, W) is called orthogonal iff
for all v, v′ ∈V , ⟨T(v), T(v′)⟩W = ⟨v, v′⟩V . To check if a given linear map T is orthogonal,
it suffices to verify that ⟨T(xi), T(xj)⟩W = ⟨xi, xj⟩V for all xi, xj in a given ordered basis X
of V . To see why, suppose the condition holds for pairs of basis elements. Let v = Pn
i=1 cixi
and v′ = Pn
j=1 djxj be any vectors in V , where ci, dj ∈R. By linearity of T and bilinearity
of the inner product, we find that
⟨T(v), T(v′)⟩W
=
*
T
 n
X
i=1
cixi
!
, T


n
X
j=1
djxj


+
W
=
* n
X
i=1
ciT(xi),
n
X
j=1
djT(xj)
+
W
=
n
X
i=1
n
X
j=1
cidj⟨T(xi), T(xj)⟩W =
n
X
i=1
n
X
j=1
cidj⟨xi, xj⟩V
=
* n
X
i=1
cixi,
n
X
j=1
djxj
+
V
= ⟨v, v′⟩V .
Comparing Concrete Linear Algebra to Abstract Linear Algebra
159
Let O(V ) denote the set of all orthogonal linear maps from V to V . O(V ) is not a
subspace of the vector space L(V ). Instead, O(V ) is a subgroup of the group GL(V ) of all
invertible linear maps T ∈L(V ). O(V ) is called the orthogonal group on V . We check that
O(V ) ⊆GL(V ) and ask you to confirm the three closure conditions in the definition of a
subgroup (see §1.4). Given T ∈O(V ), we know T is a linear map from V to V . To see that
T is invertible, it suffices to check that ker(T) = {0V }. Given v ∈ker(T), orthogonality of
T gives ⟨v, v⟩V = ⟨T(v), T(v)⟩V = ⟨0, 0⟩V = 0, which forces v = 0 by the definition of an
inner product space.
We have seen that each ordered basis X = (x1, . . . , xn) of V induces a vector space
isomorphism LX : Rn →V . We now seek conditions for LX to be an orthogonal map (which
can be regarded as an isomorphism of inner product spaces). The following conditions are
equivalent.
(a) LX is orthogonal.
(b) ⟨LX(ei), LX(ej)⟩V = ⟨ei, ej⟩Rn for all i, j ∈{1, 2, . . . , n}.
(c) For all i, j ∈{1, 2, . . . , n}, ⟨xi, xj⟩V is 1 for i = j and is 0 for i ̸= j.
(d) X = (x1, . . . , xn) is an orthonormal basis of V .
So, LX is an inner product space isomorphism Rn ∼= V iff X is an orthonormal basis of V .
Let X = (x1, . . . , xn) be an orthonormal basis for the inner product space V . Recall there
is an R-algebra isomorphism from L(V ) to Mn(R) sending T ∈L(V ) to [T]X ∈Mn(R).
Let us describe the concrete group of matrices that corresponds to the abstract orthogonal
group O(V ) when we apply this isomorphism. The following conditions are equivalent, for
a given T ∈L(V ) with corresponding matrix A = [T]X.
(a) T is in O(V ).
(b) ⟨T(xi), T(xj)⟩V = ⟨xi, xj⟩V for all i, j between 1 and n.
(c) ⟨LX(A[i]), LX(A[j])⟩V is 1 for i = j and is 0 for i ̸= j.
(d) ⟨A[i], A[j]⟩Rn is 1 for i = j and is 0 for i ̸= j.
Conditions (c) and (d) are equivalent, since LX is an orthogonal map by the preceding
remarks. Call any matrix A ∈Mn(R) orthogonal iff A satisfies condition (d). Let On(R)
be the set of all orthogonal matrices in Mn(R). We have shown that the linear map T is
orthogonal iff its matrix A (relative to an orthonormal basis) is orthogonal.
Our definition of an orthogonal matrix A states that the column vectors A[1], . . . , A[n]
must be orthonormal. Geometrically, this condition says that each column of A is a unit
vector (has norm 1), and any two distinct columns of A are perpendicular. We can restate
this condition by noting that ⟨A[i], A[j]⟩Rn = A[i] • A[j] is the i, j-entry of the matrix ATA.
So, the following conditions are equivalent for a given A ∈Mn(R) with rows A[i] and
columns A[j]:
(a) A is an orthogonal matrix.
(b) The columns A[1], . . . , A[n] form an orthonormal list in Rn.
(c) A[i] • A[j] is 1 for i = j and is 0 for i ̸= j.
(d) (ATA)(i, j) is 1 for i = j and is 0 for i ̸= j.
(e) ATA = In (the identity matrix).
(f) A is invertible and A−1 = AT.
(g) AAT = In.
(h) (AAT)(i, j) is 1 for i = j and is 0 for i ̸= j.
(i) A[i] • A[j] is 1 for i = j and is 0 for i ̸= j.
(j) The rows A[1], . . . , A[n] form an orthonormal list in Rn.
(k) AT is an orthogonal matrix.
(l) A is invertible and A−1 is an orthogonal matrix.
For more on why (e), (f), and (g) are equivalent, see §4.6.
Consider a transition matrix P = Y [id]X between two orthonormal bases X and Y for
an inner product space V . By orthonormality of Y , the map L−1
Y
sending v ∈V to [v]Y is
160
Advanced Linear Algebra
orthogonal. By orthonormality of X, ⟨xi, xj⟩V is 1 for i = j and is 0 for i ̸= j. It follows
that the columns [x1]Y , . . . , [xn]Y of P are orthonormal in Rn, so that P is an orthogonal
matrix. You can check that every orthogonal matrix in On(R) has this form for some choice
of orthonormal bases X and Y . Define two matrices A, A′ ∈Mn(R) to be orthogonally
similar iff A′ = P −1AP = P TAP for some orthogonal matrix P ∈On(R). Orthogonal
similarity is an equivalence relation on Mn(R). Given T ∈L(V ), the equivalence class of
A = [T]X consists of all possible matrices that represent T relative to different choices of
orthonormal bases for V .
6.21
Complex Inner Product Spaces and Unitary Matrices
In this section, we consider the complex version of inner product spaces. A complex inner
product space consists of a complex vector space V and a map B : V × V →C, written
B(v, w) = ⟨v, w⟩V , satisfying the following identities for all v, v′, w ∈V and all c ∈C:
⟨v + v′, w⟩V =⟨v, w⟩V + ⟨v′, w⟩V ;
⟨cv, w⟩V = c⟨v, w⟩V ;
⟨w, v⟩V = ⟨v, w⟩V ;
⟨v, v⟩∈R>0 for all v ̸= 0.
The notation z stands for the complex conjugate of the complex number z, namely, a + ib =
a −ib for a, b ∈R. It follows from the preceding identities that
⟨w, v + v′⟩V = ⟨w, v⟩V + ⟨w, v′⟩V
and
⟨w, cv⟩V = c⟨w, v⟩V .
An ordered basis X = (x1, . . . , xn) of V is called orthonormal iff ⟨xi, xj⟩V is 1 if i = j and
is 0 if i ̸= j. For example, Cn is a complex inner product space with inner product
⟨v, w⟩Cn = v1w1 + v2w2 + · · · + vnwn
for v = (v1, . . . , vn), w = (w1, . . . , wn) ∈Cn.
Using this inner product, the standard ordered basis E = (e1, . . . , en) of Cn is orthonormal.
For complex inner product spaces V and W, T ∈L(V, W) is called a unitary map iff
⟨T(v), T(v′)⟩W = ⟨v, v′⟩V for all v, v′ ∈V . As in the real case, it suffices to check that
⟨T(xi), T(xj)⟩W = ⟨xi, xj⟩V for all xi, xj in an ordered basis X of V . Let U(V ) be the set
of all unitary maps in L(V ). You can check that U(V ) is a subgroup of GL(V ).
As in the real case, we see that the isomorphism LX : Cn →V is a unitary map iff X
is an orthonormal basis of V . Given an orthonormal basis X and a linear map T ∈L(V ),
T is in U(V ) iff A = [T]X is a matrix with orthonormal columns in Cn. Call any matrix
A ∈Mn(C) with orthonormal columns unitary, and let Un(C) be the set of all unitary
matrices in Mn(C). Recall that the conjugate-transpose of A is the matrix A∗with i, j-
entry A∗(i, j) = A(j, i). The orthonormality of the columns of A in Cn is equivalent to the
matrix identity A∗A = In, which is equivalent to AA∗= In (see §4.6). So, A is unitary iff
A is invertible with A−1 = A∗iff the rows of A are orthonormal in Cn.
As in the real case, a transition matrix P = Y [id]X between two orthonormal bases of a
complex inner product space V must be unitary. Moreover, all unitary matrices have this
form for some choice of orthonormal bases X and Y . Define two matrices A, A′ ∈Mn(C)
to be unitarily similar iff A′ = P −1AP = P ∗AP for some unitary P ∈Un(C). Unitary
similarity is an equivalence relation on Mn(C). Given T ∈L(V ), the equivalence class of
A = [T]X consists of all possible matrices that represent T relative to different choices of
orthonormal bases for V .
Comparing Concrete Linear Algebra to Abstract Linear Algebra
161
6.22
Summary
1.
Notation. Table 6.2 recalls the notation used in this chapter for various vector
spaces, algebras, and groups.
TABLE 6.2
Summary of notation.
Symbol
Meaning
F n
vector space of n-tuples (column vectors) with entries in F
Mm,n(F)
vector space of m × n matrices with entries in F
Mn(F)
F-algebra of n × n matrices
GLn(F)
group of invertible n × n matrices with entries in F
On(R)
group of orthogonal n × n real matrices (A−1 = AT)
Un(C)
group of unitary n × n complex matrices (A−1 = A∗)
V, W
abstract F-vector spaces
L(V, W)
vector space of F-linear maps T : V →W
L(V )
F-algebra of F-linear maps T : V →V
BL(V )
vector space of bilinear maps B : V × V →F
GL(V )
group of invertible F-linear maps T : V →V
O(V )
group of orthogonal linear maps on real inner product space V
U(V )
group of unitary linear maps on complex inner product space V
2.
Main Isomorphisms. Table 6.3 reviews the isomorphisms between abstract alge-
braic structures and concrete versions of these structures covered in this chapter.
3.
Linear Combination Maps. Given any list X = (x1, . . . , xn) of vectors in some
abstract F-vector space V , we have a linear map LX : F n →V given by
LX(c1, . . . , cn) = Pn
i=1 cixi for ci ∈F. LX is surjective iff X spans V . LX
is injective iff X is linearly independent. LX is bijective iff X is an ordered
basis for V . An F-vector space V has dimension n iff there exists a vector space
isomorphism V ∼= F n. Any two n-dimensional F-vector spaces are isomorphic.
The function X 7→LX is a bijection from the set of ordered bases for V onto the
set of vector space isomorphisms L : F n →V . The inverse bijection sends such
TABLE 6.3
Main isomorphisms between abstract linear-algebraic objects and concrete matrix-theoretic
objects.
Isomorphism
Type of iso.
Isomorphism depends on:
V ∼= F n
vector space
ordered basis X of V
L(V, W) ∼= Mm,n(F)
vector space
ordered bases X of V and Y of W
L(V ) ∼= Mn(F)
F-algebra
ordered basis X of V
BL(V ) ∼= Mn(F)
vector space
ordered basis X of V
GL(V ) ∼= GLn(F)
group
ordered basis X of V
V ∼= Rn or Cn
inner prod. space
orthonormal basis X of V
O(V ) ∼= On(R)
group
orthonormal basis X of V
U(V ) ∼= Un(C)
group
orthonormal basis X of V
162
Advanced Linear Algebra
an isomorphism L to the ordered basis (L(e1), . . . , L(en)), where e1, . . . , en are
the standard basis vectors in F n.
4.
Vector Spaces of n-tuples vs. Abstract Vectors. If X = (x1, . . . , xn) is an ordered
basis for an abstract F-vector space V , the coordinate map v 7→[v]X is an F-
vector space isomorphism V ∼= F n. In detail, for all c, ci ∈F and all v, w ∈V :
(c1, . . . , cn) = [v]X ⇔v = LX(c1, . . . , cn) ⇔v = c1x1 + · · · + cnxn;
[v + w]X = [v]X + [w]X;
[cv]X = c[v]X.
5.
Vector Spaces of Matrices and Linear Maps. If X
=
(x1, . . . , xn) and
Y = (y1, . . . , ym) are ordered bases for F-vector spaces V and W, then the map
T 7→Y [T]X is an F-vector space isomorphism L(V, W) ∼= Mm,n(F). In detail, for
all A ∈Mm,n(F), S, T ∈L(V, W), c ∈F:
A = Y [T]X ⇔∀j, A[j] = [T(xj)]Y ⇔∀j, T(xj) = LY (A[j]) =
n
X
i=1
A(i, j)yi;
Y [S + T]X = Y [S]X + Y [T]X;
Y [cT]X = c(Y [T]X).
If R ∈L(W, U) where U has ordered basis Z, then
Z[R ◦T]X = Z[R]Y Y [T]X.
6.
Algebras of Matrices and Operators. For each ordered basis X = (x1, . . . , xn) of
an F-vector space V , the map T 7→[T]X is an F-algebra isomorphism L(V ) ∼=
Mn(F). In detail, for all A ∈Mn(F), S, T ∈L(V ), c ∈F, v ∈V :
A = [T]X ⇔∀j A[j] = [T(xj)]X ⇔∀j, T(xj) = LX(A[j]) =
n
X
i=1
A(i, j)xi;
[S + T]X = [S]X + [T]X;
[cS]X = c[S]X;
[S ◦T]X = [S]X [T]X;
[S(v)]X = [S]X [v]X;
[0L(V )]X = 0Mn(F );
[idV ]X = In;
∀k ∈Z≥0, [Sk]X = ([S]X)k; this holds for all k ∈Z when S is invertible.
7.
Transition Matrices. For ordered bases X and Y of V , the transition matrix from
X to Y is the matrix A = Y [idV ]X defined by A[j] = [xj]Y for j = 1, 2, . . . , n.
We have xj = Pn
i=1 A(i, j)yi, A−1 = X[id]Y , and X[id]X = In. For each fixed
X, the map Y 7→Y [id]X is a bijection from the set of ordered bases of V to
the set of invertible matrices GLn(F). For each fixed Y , the map X 7→Y [id]X is
also a bijection between these sets. If P = Y [id]X and T ∈L(V ), then [T]X =
P −1[T]Y P.
8.
Bilinear Maps and Congruence of Matrices. For each ordered basis X of V , there
is a vector space isomorphism BL(V ) ∼= Mn(F) that sends a bilinear map B
to the matrix [B]X with i, j-entry B(xi, xj). Changing the basis from X to Y
replaces A = [B]X by A′ = [B]Y = P TAP, where P = X[id]Y . So, the set of all
matrices congruent to A is the set of matrices representing B relative to some
ordered basis of V .
Comparing Concrete Linear Algebra to Abstract Linear Algebra
163
9.
Inner Product Spaces. For each orthonormal basis X of a real inner product
space V , the map v 7→[v]X (and its inverse LX) are isomorphisms preserving
the inner product. Orthogonal maps correspond to orthogonal matrices under
the isomorphism T 7→[T]X, for T ∈L(V ). The transition matrix between two
orthonormal bases is orthogonal. The equivalence class of a matrix A = [T]X
under orthogonal similarity is the set of all matrices that represent T as we vary
the orthonormal basis X. Similar facts holds for complex inner product spaces,
replacing “orthogonal” by “unitary” throughout.
10.
Equivalence Relations on Matrices. Table 6.4 summarizes some commonly used
equivalence relations on rectangular and square matrices.
TABLE 6.4
Equivalence relations on matrices.
Equiv. relation
Definition of A ∼A′
Abstract significance
col. equivalence
∃P ∈GLn(F), A′ = AP
changes input basis in Y [T]X
row equivalence ∃Q ∈GLm(F), A′ = QA
changes output basis in Y [T]X
row/col equiv.
∃P ∈GLn(F), ∃Q ∈GLm(F),
changes input/output bases
A′ = QAP
in Y [T]X
similarity
∃P ∈GLn(F), A′ = P −1AP
go from [T]X to [T]Y
congruence
∃P ∈GLn(F), A′ = P TAP
change basis for bilinear map
orthogonal sim.
∃P ∈On(R), A′ = P −1AP = P TAP
orthonormal basis change
(F = R)
unitary sim.
∃P ∈Un(C), A′ = P −1AP = P ∗AP
orthonormal basis change
(F = C)
11.
Diagonalizability. These conditions on a linear map T : V →V are equivalent:
T is diagonalizable; [T]X is diagonal for some ordered basis X of V ; V has an
ordered basis consisting of eigenvectors of T; for any ordered basis Y of V , [T]Y is
similar to a diagonal matrix; for any ordered basis Y of V , P −1 [T]Y P is diagonal
for some invertible P ∈Mn(F).
12.
Triangulability. A flag of subspaces of V is a chain of subspaces {0} = V0 ⊆
V1 ⊆· · · ⊆Vn = V with dim(Vi) = i for all i. These conditions on a linear map
T : V →V are equivalent: T is triangulable; [T]X is upper-triangular for some
ordered basis X of V ; T stabilizes some flag of subspaces of V ; for any ordered
basis Y of V , [T]Y is similar to an upper-triangular matrix; for any ordered basis
Y of V , P −1 [T]Y P is upper-triangular for some invertible P ∈Mn(F).
13.
Reducibility and Invariant Subspaces. Given T ∈L(V ) and an ordered basis
X = (x1, . . . , xn) of V , the matrix A = [T]X has the block-triangular form
 Bk×k
C
0
D(n−k)×(n−k)

iff X1 = (x1, . . . , xk) spans a T-invariant subspace W
of V (which is a subspace with T(w) ∈W for all w ∈W). In this case, B =
[T|W]X1 and D = [T ′]X′
2, where T|W ∈L(W) is the restriction of T to W,
T ′ : V/W →V/W in L(V/W) is given by T ′(v + W) = T(v) + W for v ∈V ,
and X′
2 = (xk+1 + W, . . . , xn + W). Moreover, A = [T]X is block-diagonal with
two diagonal blocks of size k and n −k iff C = 0 iff X1 = (x1, . . . , xk) and
X2 = (xk+1, . . . , xn) both span T-invariant subspaces of V iff T is reduced by the
pair of subspaces W and Z spanned by X1 and X2. In this case, B = [T|W]X1
and D = [T|Z]X2.
164
Advanced Linear Algebra
14.
Idempotent Matrices and Projections. A linear map T ∈L(V ) is idempotent iff
T ◦T = T. Given V = W ⊕Z, the projection on W along Z is the map PW,Z that
sends each v ∈V to the unique w ∈W such that v = w + z for some z ∈Z. The
map PW,Z is idempotent with kernel Z and image W, and [PW,Z]X = Ik,n, where
X is obtained by concatenating ordered bases for W and Z. Every idempotent
T ∈L(V ) has the form PW,Z for unique subspaces W, Z with V = W ⊕Z; here,
W = img(T) and Z = ker(T).
15.
Computational Procedures. Assume X = (x1, . . . , xn) and Y = (y1, . . . , ym) are
ordered bases of F-vector spaces V and W.
• To compute coordinates [v]X when given v ∈V and X: solve the equation
v = c1x1 + · · · + cnxn for the unknowns cj in F; output the answer [v]X =
(c1, . . . , cn).
• To compute the matrix Y [T]X of a linear map when given T and X
and Y : for j = 1, 2, . . . , n, find T(xj); compute the coordinate vector of
T(xj) relative to Y , namely [T(xj)]Y ; and write these coordinates as the jth
column of the matrix.
• To find [T]X given T and X: perform the algorithm in the preceding item
taking Y = X.
• To find the transition matrix from X to Y : compute the coordinate
vector [xj]Y of each xj relative to Y , and write this vector as the jth column
of the transition matrix. Alternatively, compute the matrix inverse of the
transition matrix from Y to X if the latter matrix is already available. (For
instance, this occurs when X is the standard ordered basis for F n and each
yj ∈Y is presented as a column vector, so [yj]X = yj for all j.)
• To change coordinates from [v]X to [v]Y where [v]X and X and Y are
given: find the transition matrix Y [id]X from X to Y ; compute the matrix-
vector product [v]Y = Y [id]X [v]X.
• To find the matrix of a linear map relative to a new basis where
[T]X and X and Y are given: find the transition matrix P = X[id]Y and its
inverse P −1 = Y [id]X; compute the matrix product [T]Y = P −1[T]XP.
• To diagonalize a diagonalizable linear operator T: compute an ordered
basis X for V consisting of eigenvectors for T (for instance, by finding roots
of the characteristic polynomial and solving linear equations); output [T]X,
which is the diagonal matrix with the eigenvalues corresponding to each
xj ∈X appearing on the main diagonal.
• To diagonalize a diagonalizable matrix A ∈Mn(F): Find a linearly
independent list of n eigenvectors of A in F n, say x1, . . . , xn, with associated
eigenvalues c1, . . . , cn. Let P be the matrix with columns x1, . . . , xn. Let D
be the diagonal matrix with entries c1, . . . , cn on the diagonal. Then A is
similar to the diagonal matrix D via P −1AP = D.
• To compute the matrix [B]X of a bilinear map B: let the i, j-entry be
B(xi, xj) for all i, j ∈{1, 2, . . . , n}.
• To find the matrix of a bilinear map relative to a new basis: compute
P = X[id]Y and [B]Y = P T[B]XP.
Our dictionary for translating between matrix theory and linear algebra is presented in
Tables 6.2, 6.3, 6.4, and 6.5.
Comparing Concrete Linear Algebra to Abstract Linear Algebra
165
TABLE 6.5
Dictionary connecting concrete matrix theory and abstract linear algebra.
Concept in Matrix Theory
Connecting Formula
Concept in Linear Algebra
concrete vector space F n
isomorphisms LX : F n ∼= V
abstract F-vector space V with
of n-tuples
LX(c1, . . . , cn) = Pn
i=1 cixi
ordered basis X = (x1, . . . , xn)
n-tuple (c1, . . . , cn) ∈F n
[v]X = (c1, . . . , cn) iff
abstract vector v ∈V
v = c1x1 + · · · + cnxn
componentwise
[v + w]X = [v]X + [w]X,
abstract vector addition and
operations on n-tuples
[cv]X = c[v]X
scalar multiplication in V
set of all isomorphisms
bijections: X 7→LX and
set of all ordered
L : F n ∼= V
L 7→(L(e1), . . . , L(en))
bases X of V
vector space Mm,n(F)
isomorphisms T 7→Y [T]X
vector space L(V, W)
of m × n matrices
indexed by ordered bases
of linear maps from V to W
matrix A ∈Mm,n(F)
A = Y [T]X iff
linear map T ∈L(V, W) with
with columns A[j]
A[j] = [T(xj)]Y iff
X an ordered basis for V and
and entries A(i, j)
∀j, T(xj) = Pm
i=1 A(i, j)yi
Y an ordered basis for W
matrix addition
Y [S + T]X = Y [S]X + Y [T]X
addition of linear maps
(A + B)(i, j) = A(i, j)
+B(i, j)
(S + T)(v) = S(v) + T(v)
scalar multiple of a matrix
Y [cT]X = c(Y [T]X)
scalar multiple of linear map
(cA)(i, j) = c(A(i, j))
(cT)(v) = c · T(v)
matrix/vector multiplication
[T(v)]Y = Y [T]X [v]X
evaluation of linear map
(Av)i = Pn
j=1 A(i, j)vj
T on input v gives T(v)
matrix multiplication
Z[S ◦T]X = Z[S]Y
Y [T]X
composition of linear maps
(BA)(i, j) = P
k B(i, k)
A(k, j)
(S ◦T)(v) = S(T(v))
inverse of a matrix
X[T −1]Y = (Y [T]X)−1
inverse of a map
AA−1 = In = A−1A
T ◦T −1 = idW , T −1 ◦T = idV
matrix transpose
X∗[T ∗]Y ∗= (Y [T]X)T
dual map T ∗∈L(W ∗, V ∗)
AT(i, j) = A(j, i)
(X∗, Y ∗are dual bases)
T ∗(g) = g ◦T : V →F
m × n matrix A
E′[LA]E = A
left multiplication by A
(E, E′ are standard bases)
LA : F n →F m, LA(v) = Av
F-algebra Mn(F)
isomorphisms T 7→[T]X
F-algebra L(V )
identity matrix In
[idV ]X = In
identity map idV : V →V
idV (v) = v for v ∈V
zero matrix 0n
[0V ]X = 0n
zero map 0V : V →V
0V (v) = 0 ∈V for v ∈V
6.23
Exercises
Unless otherwise stated, assume that F is a field, V is an F-vector space with ordered
basis X = (x1, . . . , xn), and W is an F-vector space with ordered basis Y = (y1, . . . , ym).
Exercises involving R3, P≤3, M2(R), and C use the notation from Table 6.1 on page 137.
1.
For each vector space V in Table 6.1, prove that X1, X2, and X3 are ordered
bases for V . Compute dim(V ).
2.
For each vector v ∈R3, compute [v]X1, [v]X2, and [v]X3.
(a) (1, 2, 4)
(b) (−2, 5, 2)
(c) (0, 0, c)
166
Advanced Linear Algebra
3.
For each vector v ∈P≤3, compute [v]X1, [v]X2, and [v]X3.
(a) 1 + t + t2 + t3
(b) (t + 1)2
(c) t(3t −4)(2t + 1)
4.
For each vector v ∈M2(R), compute [v]X1, [v]X2, and [v]X3.
(a)

1
1
1
0

(b)

4
4
−1
−1

(c)

cos θ
−sin θ
sin θ
cos θ

5.
For each vector v ∈C, compute [v]X1, [v]X2, and [v]X3.
(a) −i
(b) 7 −5i
(c) eπi/4
6.
(a) Which vector v ∈R3 satisfies [v]X2 = (3, 2, 1)?
(b) Which vector v ∈P≤3 satisfies [v]X3 = (2, −1, 0, 5)?
(c) Which vector v ∈C satisfies [v]X3 = (2, −3)?
7.
For all c ∈R, find [(t + c)3]X1 and [(t + c)3]X2.
8.
(a) Find a nonzero A ∈M2(R) such that [A]X1 = 6[A]X3.
(b) Find all c ∈R such that for some nonzero A ∈M2(R), [A]X1 = c[A]X3.
9.
Let X′ be obtained from X by switching the positions of xi and xj. Show X′ is
an ordered basis of V . For v ∈V , how is [v]X′ related to [v]X?
10.
Let X′ be obtained from X by replacing xi by bxi, where b ∈F is nonzero. Show
X′ is an ordered basis of V . For v ∈V , how is [v]X′ related to [v]X?
11.
Let X′ be obtained from X by replacing xi by xi + axj, where a ∈F and j ̸= i.
Show X′ is an ordered basis of V . For v ∈V , how is [v]X′ related to [v]X?
12.
Define T : R3 →R2 by T(a, b, c) = (a + b −c, 3b + 2c) for a, b, c ∈R.
Let Y = ((1, 0), (0, 1)) and Z = ((1, 3), (−1, 2)).
(a) Find Y [T]X1 and Z[T]X1.
(b) Find Y [T]X2 and Z[T]X2.
(c) There is an ordered basis W of R2 such that W [T]X3 =

3
1
b
−1
1
c

. Find
W, b, and c.
13.
Define T : P≤3 →R3 by T(f) =

f(2), f ′(2),
R 2
0 f(t) dt

for f ∈P≤3. Check that
T is R-linear. For Y = (e1, e2, e3), compute Y [T]X1, Y [T]X2, and Y [T]X3. Do the
same for Y = ((1, 1, 1), (1, 2, 4), (1, 3, 9)).
14.
Let tr : M2(R) →R be the trace map, defined by tr(A) = A(1, 1) + A(2, 2) for
A ∈M2(R). Let Z = (1). Explain why tr ∈M2(R)∗. Find Z[tr]X1, Z[tr]X2, and
Z[tr]X3. Let T : M2(R) →M2(R) be the transpose map. Show that T ∗(tr) = tr.
15.
Define T : C →C by T(z) = iz for z ∈C. (a) Compute [T]X1, [T]X2, and [T]X3.
(b) Square each matrix found in (a), and discuss the answers. (c) Find X3[T]X1
and X1[T]X3.
16.
Let A =

2
1
−1
3

. Define three maps λA, ρA, κA : M2(R) →M2(R) by setting
λA(B) = AB, ρA(B) = BA, and κA(B) = AB −BA for all B ∈M2(R). Confirm
that λA, ρA, and κA are R-linear. Compute [λA]X1, [ρA]X1, and [κA]X1. Compute
[λA]X2, [ρA]X2, and [κA]X2.
17.
Given A ∈Mm,n(F), define λA : Mn,p(F) →Mm,p(F) by λA(B) = AB for B ∈
Mn,p(F). Describe the entries of Y [λA]X, where X and Y are standard ordered
bases for Mn,p(F) and Mm,p(F) (e.g., X = (e11, e12, . . . , e1p, e21, e22, . . . , enp)).
18.
Given A ∈Mn,p(F), define ρA : Mm,n(F) →Mm,p(F) by ρA(B) = BA for
B ∈Mm,n(F). Describe the entries of Y [ρA]X, where X and Y are standard
ordered bases for Mm,n(F) and Mm,p(F).
Comparing Concrete Linear Algebra to Abstract Linear Algebra
167
19.
Given A ∈Mn(F), define κA : Mn(F) →Mn(F) by κA(B) = AB −BA for
B ∈Mn(F). Let X = (e11, e12, . . . , enn) be the standard ordered basis of Mn(F).
For each i, j, compute [κeij]X. Illustrate by writing all entries of [κe23]X for n = 3.
20.
Show that each of the three bijections in (6.1) is F-linear. (This gives another
proof that the map T 7→Y [T]X is F-linear.)
21.
Using only the definitions of spanning and linear independence (not matrices),
prove that the maps Tij defined in (6.4) form a basis of the F-vector space
L(V, W).
22.
(a) Check that the map T defined in (6.2) is F-linear, well-defined, and sends xj
to wj for j = 1, 2, . . . , n. (b) Reprove (a) by showing that T = Lw ◦L−1
X , where
w = (w1, . . . , wn) ∈W n.
23.
Define a map M ′
X,Y : L(V, W) →Mn,m(F) by letting M ′
X,Y (T) be the n × m
matrix whose jth row (for j = 1, 2, . . . , n) is [T(xj)]Y viewed as a row vector.
(a) Is M ′
X,Y a vector space isomorphism? Explain.
(b) If V = W, X = Y , and m = n, is M ′
X,X an F-algebra isomorphism? Explain.
24.
Let B be the set of pairs (X, Y ) where X is an ordered basis for V and Y
is an ordered basis for W. Let I be the set of vector space isomorphisms
S : L(V, W) →Mm,n(F). Define a map ϕ : B →I by ϕ(X, Y ) = MX,Y , where
MX,Y (T) = Y [T]X. Prove ϕ is not injective, in general. Prove ϕ is not surjective,
in general.
25.
Show that each list is an ordered basis of P ∗
≤3.
(a) (f0, f1, f2, f3), where fi(c0 + c1t + c2t2 + c3t3) = ci for i = 0, 1, 2, 3.
(b) (g0, g1, g2, g3), where gi(p) = (d/dt)i(p)|t=0.
(c) (h0, h1, h2, h3), where hi(p) = p(i).
26.
Let D : P≤3 →P≤3 be the differentiation operator. For each basis X of P ∗
≤3
found in Exercise 25, compute [D∗]X.
27.
Suppose S ∈L(V, W) and U ∈L(W, Z). Prove (U ◦S)∗= S∗◦U ∗. Translate this
fact into an identity involving the matrix transpose.
28.
Define T : C →M2(R) by T(a + bi) =
 a
−b
b
a

for all a, b ∈R. Show that T
is a one-to-one R-algebra homomorphism.
29.
Prove (6.6) by expressing (S ◦T)(xj) as a specific linear combination of vectors
in Z and showing that the coefficient of zi in this combination is (BA)(i, j).
30.
Suppose T : V →W is an F-linear map. You are given Y [T]X but do not know
any other specific information about T, X, or Y .
(a) Explain how to use matrix algorithms to compute a basis for ker(T).
(b) Explain how to use matrix algorithms to compute a basis for img(T).
31.
Let X4 = ((1, 1, 1), (1, 0, −1), (1, 0, 1)), which is an ordered basis of R3. Compute
Xi[id]X4 and X4[id]Xi for i = 1, 2, 3.
32.
For V = P≤3, compute the transition matrix:
(a) from X3 to X1; (b) from X1 to X3; (c) from X3 to X2; (d) from X2 to X3.
33.
Compute transition matrices between all pairs of ordered bases for C listed in
Table 6.1.
34.
Compute transition matrices between all pairs of ordered bases for M2(R) listed
in Table 6.1.
168
Advanced Linear Algebra
35.
(a) Given [v]X3 = (5, −1, −1) in R3, find [v]X2.
(b) Given [v]X2 = (2, 1, 1, −2) in P≤3, find [v]X3.
(c) Given [v]X2 = (4, −5, 0, 1) in M2(R), find [v]X1 and [v]X3.
36.
Let P<n = {f ∈R[t] : f = 0 or deg(f) < n}. P<n has ordered bases X =
(1, t, t2, . . . , tn−1) and Y = (1, (t+c), (t+c)2, . . . , (t+c)n−1) for fixed c ∈R. Find
X[id]Y and Y [id]X.
37.
Let V consist of polynomials in R[t] of degree at most 4. Let X = (1, t, t2, t3, t4)
and Y = (1, t, t(t−1), t(t−1)(t−2), t(t−1)(t−2)(t−3)). Find X[id]Y and Y [id]X.
38.
Prove: for any given ordered basis Y of V , every invertible matrix in Mn(F) has
the form X[id]Y for a unique ordered basis X of V .
39.
Let A =


2
1
−1
2
3
0
4
1
−2

. Find the unique ordered basis Z of R3 such that
X3[id]Z = A. Find the unique ordered basis Y of R3 such that Y [id]X3 = A.
40.
Verify that each relation is an equivalence relation.
(a) column-equivalence on Mm,n(F)
(b) row-equivalence on Mm,n(F)
(c) row/column-equivalence on Mm,n(F)
(d) similarity on Mn(F)
(e) congruence on Mn(F)
(f) orthogonal similarity on Mn(R)
(g) unitary similarity on Mn(C)
41.
Find all the similarity equivalence classes in M2(Z2).
42.
Find all the congruence equivalence classes in M2(Z2).
43.
Find all similarity equivalence classes in Mn(F) consisting of a single element.
44.
Let D : P≤3 →P≤3 be given by D(f) = f ′ for f ∈P≤3. Find ordered bases X
and Y of P≤3 such that Y [D]X is diagonal with 0s and 1s on the diagonal. Find
an ordered basis X of P≤3 such that [D]X is diagonal, or explain why this cannot
be done.
45.
Let A ∈Mn(F) be an invertible matrix. Explicitly describe ordered bases X and
Y of F n such that Y [LA]X = In. Prove: if [LA]X = In for some ordered basis X,
then A = In.
46.
Prove the Projection Theorem (see §6.11) without using matrices.
47.
Let A ∈Mn(F) and P ∈GLn(F). Prove x ∈F n is an eigenvector of A with
associated eigenvalue c iff P −1x is an eigenvector of P −1AP with associated
eigenvalue c. Conclude that similar matrices have the same eigenvalues.
48.
Show that for all A ∈Mn(F) and all c ∈F, A is diagonalizable iff A + cIn is
diagonalizable.
49.
Show that a strictly upper-triangular nonzero matrix is not diagonalizable.
50.
Show that if c ∈F is the only eigenvalue of A ∈Mn(F), then A = cIn or A is
not diagonalizable.
51.
Show that if A ∈Mn(F) has no eigenvalues in F, then A is not triangulable.
52.
Show that A =
 0
−1
1
0

is not triangulable in M2(R), but A is diagonalizable
in M2(C).
Comparing Concrete Linear Algebra to Abstract Linear Algebra
169
53.
Give an example of a field F and A ∈Mn(F) such that A has an eigenvalue in
F, but A is not triangulable.
54.
Decide if each matrix A is diagonalizable using the given field F of scalars. If it
is, find a diagonal D and an invertible P with P −1AP = D.
(a) A = 1
7


32
12
−6
9
20
−3
9
6
11

, F = R.
(b) A =


1
1
0
−1
1
0
0
0
−1

, F = R.
(c) the matrix in (b), using F = C.
(d) A =


1
0
1
0
0
1
0
1
1
0
1
0
0
1
0
1

, F = Q.
(e) the matrix in (d), using F = Z2 (the integers mod 2).
55.
Given a complete flag of subspaces (V0, V1, . . . , Vn) of V , let xi be any element in
Vi but not in Vi−1, for i = 1, 2, . . . , n. Prove X = (x1, . . . , xn) is an ordered basis
of V .
56.
Let (x1, . . . , xn) be an ordered basis for V such that (x1, . . . , xk) is an ordered
basis for a subspace W. Prove (xk+1 +W, . . . , xn +W) is an ordered basis for the
quotient vector space V/W.
57.
Give an example of a map T ∈L(R2) such that the only T-invariant subspaces
of R2 are {0} and R2.
58.
Give an example of a map T ∈L(R2) such that R2 has exactly four T-invariant
subspaces.
59.
Does there exist T ∈L(R2) such that T has exactly three T-invariant subspaces?
Explain.
60.
Give an example of T ∈L(R3) such that R3 has infinitely many T-invariant
subspaces, but T is not a scalar multiple of id.
61.
Given positive integers i1, i2, . . . , ib with sum n, let
I1 = {1, 2, . . . , i1}, I2 = {i1 + 1, i1 + 2, . . . , i1 + i2}, . . . ,
Ib = {i1 + · · · + ib−1 + 1, . . . , n}.
Call a matrix A ∈Mn(F) block-triangular with block sizes i1, i2, . . . , ib iff for all
i ∈Ir and j ∈Is with r > s, A(i, j) = 0. Define a partial flag of type (i1, . . . , ib)
to be a chain of subspaces V1 ⊆V2 ⊆· · · ⊆Vb with dim(Vj) = i1 + i2 + · · · + ij
for j = 1, 2, . . . , b. Show that there exists an ordered basis X of V such that [T]X
is block-triangular with block sizes i1, i2, . . . , ib iff V has a partial flag of type
(i1, . . . , ib) stabilized by T.
62.
Define I1, . . . , Ib as in Exercise 61. Call a matrix A ∈Mn(F) block-diagonal with
block sizes i1, . . . , ib iff for all i ∈Ir and j ∈Is with r ̸= s, A(i, j) = 0. Find and
prove an abstract criterion on T ∈L(V ) that is equivalent to the existence of an
ordered basis X of V such that [T]X is block-diagonal with block sizes i1, . . . , ib.
63.
Give an example of a 4 × 4 real matrix A with all entries nonzero such that A is
similar to a block-triangular matrix, but A is not triangulable.
170
Advanced Linear Algebra
64.
Let A =


1/2
−1/2
−1/2
1/2
1/2
5/2
5/2
1/2
−1/2
−3/2
−3/2
−1/2
1/2
1/2
1/2
1/2

. Verify that A is idempotent. Find an
ordered basis X of R4 with [LA]X = Ik,4 for some k. Find an invertible P ∈M4(R)
with P −1AP = Ik,4.
65.
Given an idempotent matrix A ∈Mn(F), describe an algorithm for finding k and
P ∈GLn(F) with P −1AP = Ik,n.
66.
Projections of a Direct Sum. We say that V is the direct sum of subspaces
W1, . . . , Wk, and we write V = W1 ⊕· · · ⊕Wk, iff every v ∈V can be written
uniquely in the form v = w1+· · ·+wk with wi ∈Wi. (a) Given V = W1⊕· · ·⊕Wk,
define maps P1, . . . , Pk : V →V by setting Pi(v) = wi, where v = w1 + · · · + wk
as above. Show that each Pi is linear and idempotent with image Wi, PiPj = 0
for all i ̸= j, and P1 + · · · + Pk = idV . What is ker(Pi)? (b) Let X1, . . . , Xk be
ordered bases for W1, . . . , Wk. Let X be the concatenation of X1, . . . , Xk. Show
that X is an ordered basis of V , and find [Pi]X for each i.
67.
Suppose Q1, . . . , Qk are idempotent elements of L(V ) such that QiQj = 0 for all
i ̸= j and Q1 + · · · + Qk = idV .
(a) Write Wi = img(Qi) for i = 1, 2, . . . , k. Show that V = W1 ⊕· · · ⊕Wk.
(b) Define Pi as in Exercise 66. Show that Pi = Qi for all i.
68.
Suppose V = W ⊕Z, and let P = PW,Z be the associated projection.
(a) Prove: W is T-invariant iff PTP = TP in L(V ).
(b) Prove: W and Z are both T-invariant iff PT = TP in L(V ).
69.
Verify the following facts stated in §6.18.
(a) BL(V ) is a subspace of the vector space of all functions from V × V to F.
(b) The map NX defined in §6.18 is F-linear.
(c) Equation (6.8) holds.
(d) The map B defined at the end of §6.18 is F-bilinear, and B(xi, xj) = A(i, j).
70.
Check that each map B is R-bilinear, and compute [B]X for the given ordered
basis X.
(a) B
 a
c

,
 b
d

= det
 a
b
c
d

, using X = (e1, e2).
(b) For f, g ∈P≤3, B(f, g) =
R 1
0 f(t)g(t) dt, using X = (1, t, t2, t3).
(c) For z, w ∈C, B(z, w) = the real part of zw, using X = (1, i).
71.
Use your answers to each part of Exercise 70 to find [B]Y for the new ordered
basis Y . (a) Y = ((1, 2), (3, 4)). (b) Y = (t + 3, 2t2 −4, t3 −t2, t3 + t2).
(c) Y = (3 + 4i, 2 −i).
72.
For V = R3, we are given B ∈BL(V ) with [B]X2 =


2
1
0
−1
3
1
0
1
−1

.
(a) Find B((1, 0, −4), (1, 3, 7)).
(b) Find [B]X1.
(c) Find [B]X3.
73.
Given B ∈BL(V ) and v, w ∈V , show that [B(v, w)]1×1 = ([v]X)T[B]X[w]X.
74.
How does the matrix [B]X change if we switch the positions of xi and xj in X?
75.
How does the matrix [B]X change if we multiply xi by a nonzero c ∈F?
76.
Given B ∈BL(V ) and v, w ∈V , define Lv(w) = B(v, w) and Rv(w) = B(w, v).
(a) Prove: for all v ∈V , Lv and Rv are in V ∗.
(b) How is the row vector (1F )[Lxi]X related to the matrix [B]X?
Comparing Concrete Linear Algebra to Abstract Linear Algebra
171
(c) How is the row vector (1F )[Rxj]X related to the matrix [B]X?
(d) Describe how to use [B]X to compute (1F )[Lv]X and (1F )[Rv]X for any v ∈V .
77.
Let B be the set of ordered bases for V and I be the set of vector space
isomorphisms S : BL(V ) →Mn(F). Define ϕ : B →I by ϕ(X) = NX, where
NX(B) = [B]X for B ∈BL(V ). Prove or disprove: ϕ is a bijection.
78.
(a) For a real inner product space V , prove O(V ) is a subgroup of GL(V ).
(b) Prove On(R) is a subgroup of GLn(R).
79.
(a) For a complex inner product space V , prove U(V ) is a subgroup of GL(V ).
(b) Prove Un(C) is a subgroup of GLn(C).
80.
Fix an orthonormal basis X of a real inner product space V . (a) Show that every
orthogonal matrix P ∈On(R) has the form Y [id]X for a unique orthonormal
basis Y of V . (b) Show that every orthogonal matrix P ∈On(R) has the form
X[id]Y for a unique orthonormal basis Y of V . (c) Prove analogs of (a) and (b)
for complex inner product spaces.
81.
Let v = (v1, v2, v3) and w = (w1, w2, w3) be vectors in R3 with ||v|| = 1 = ||w||
and v • w = 0. Find two vectors z ∈R3 such that (v, w, z) is an orthonormal
basis of R3.
82.
(a) Show that P≤3 becomes a real inner product space if we define ⟨f, g⟩=
R 1
0 f(t)g(t) dt for f, g ∈P≤3. (b) Find an orthonormal basis (f0, f1, f2, f3) of P≤3
such that deg(fi) = i for 0 ≤i ≤3.
83.
(a) For each t ∈R, show that At =
 cos t
−sin t
sin t
cos t

is an orthogonal matrix.
(b) Show that every orthogonal matrix in O2(R) has the form At or At
 1
0
0
−1

for some t ∈R.
84.
Let B be the set of orthonormal bases for a real inner product space V . Let I be
the set of orthogonal vector space isomorphisms S : Rn →V . Define ϕ : B →I
by ϕ(X) = LX for all X ∈B. Prove or disprove: ϕ is a bijection.
85.
Let B be the set of ordered bases for V and I be the set of F-algebra isomorphisms
S : L(V ) →Mn(F). Define ϕ : B →I by ϕ(X) = MX, where MX(T) = [T]X for
T ∈L(V ). Is ϕ one-to-one? Is ϕ onto? Explain.
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
Part III
Matrices with Special Structure
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
7
Hermitian, Positive Definite, Unitary, and Normal
Matrices
We begin by recalling some fundamental facts about complex numbers.
• For every complex number z, there exist unique real numbers x and y such that z = x + iy.
We call x the real part of z and y the imaginary part of z, writing x = Re(z) and y = Im(z).
The formula z = x + iy is called the Cartesian decomposition of the complex number z.
• For each complex number z = x+iy with x, y ∈R, the complex conjugate of z is z = x−iy.
For all z, w ∈C, the following properties hold: z = z; z + w = z + w; zw = z · w = w · z;
Re(z) = (z + z)/2; Im(z) = (z −z)/2i; z is real iff z = z; z is pure imaginary (meaning
Re(z) = 0) iff z = −z.
• The magnitude of a complex number z = x + iy is the nonnegative real number |z| =
p
x2 + y2 = (zz)1/2. Every nonzero complex number can be written uniquely in the form
z = rw, where r is a positive real number and w is a complex number of magnitude 1. We
often write w = eiθ = cos θ + i sin θ for some (non-unique) real number θ. Then z = reiθ
is called the polar decomposition of z.
Now consider the set Mn(C) of all n×n matrices with complex entries. If n = 1, we can
identify Mn(C) with C. Our goal in this chapter is to build an analogy between C and Mn(C)
for n > 1. We define matrix versions of the concepts of real numbers, positive numbers, pure
imaginary numbers, complex numbers of magnitude 1, complex conjugation, the Cartesian
decomposition of a complex number, and the polar decomposition of a complex number.
This approach allows us to unify a diverse collection of fundamental results in matrix theory.
The analogy with C also provides some motivation for the introduction of certain special
classes of matrices — namely Hermitian, positive definite, unitary, and normal matrices —
that have many remarkable properties.
7.1
Conjugate-Transpose of a Matrix
The first step toward implementing the analogy between complex numbers and complex-
valued matrices is to introduce a matrix version of complex conjugation. One possibility is
to consider the operation on matrices that replaces each entry of a matrix by its complex
conjugate. However, it turns out to be much more fruitful to combine this operation with
the transpose map that interchanges the rows and columns of a matrix.
Formally, suppose A is an m × n matrix with complex entries A(i, j), where 1 ≤i ≤m
and 1 ≤j ≤n. Define the conjugate-transpose of A to be the n × m matrix A∗such that
A∗(i, j) = A(j, i)
for 1 ≤i ≤n and 1 ≤j ≤m.
DOI: 10.1201/9781003484561-7
175
176
Advanced Linear Algebra
For example, given A =

2 + i
3
5i
1 −2i
0
−1 + 3i

, we find A∗=


2 −i
1 + 2i
3
0
−5i
−1 −3i

. In the
case of a 1×1 matrix, the conjugate-transpose operation reduces to the complex conjugation
operation on C. Many algebraic properties of complex conjugation extend to the conjugate-
transpose.
Theorem on the Conjugate-Transpose.
(a) [conjugate-linearity] For all A, B ∈Mm,n(C) and all z ∈C, (A + B)∗= A∗+ B∗and
(zA)∗= z(A∗).
(b) [product reversal] For all A ∈Mm,n(C) and all B ∈Mn,p(C), (AB)∗= B∗A∗.
(c) [idempotence] For all A ∈Mm,n(C), (A∗)∗= A.
(d) [inverse rule] For all invertible A ∈Mn(C), A∗is invertible and (A∗)−1 = (A−1)∗.
(e) [trace rule] For all A ∈Mn(C), tr(A∗) = tr(A), where tr(A) = P
i A(i, i) is the sum of
the diagonal entries of A.
(f) [determinant rule] For all A ∈Mn(C), det(A∗) = det(A).
Proof. We prove a matrix identity by showing that the left side and the right side have the
same i, j-entry for all relevant i and j. The conclusions in (a) involve n × m matrices, so we
fix i, j with 1 ≤i ≤n and 1 ≤j ≤m. We compute
(A + B)∗(i, j) = (A + B)(j, i) = A(j, i) + B(j, i) = A(j, i) + B(j, i)
= A∗(i, j) + B∗(i, j) = (A∗+ B∗)(i, j).
So (A + B)∗= A∗+ B∗. For the second part of (a), compute
(zA)∗(i, j) = (zA)(j, i) = z(A(j, i)) = zA(j, i) = z(A∗(i, j)) = (zA∗)(i, j).
So (zA)∗= zA∗. In particular, if x is a real scalar, then (xA)∗= x(A∗).
In part (b), both sides of the conclusion are p × m matrices. Fix i, j with 1 ≤i ≤p and
1 ≤j ≤m. Compute
(AB)∗(i, j) = AB(j, i) =
n
X
k=1
A(j, k)B(k, i) =
n
X
k=1
A(j, k) · B(k, i)
=
n
X
k=1
B∗(i, k)A∗(k, j) = (B∗A∗)(i, j).
So (AB)∗= B∗A∗. For (c), fix i, j with 1 ≤i ≤m and 1 ≤j ≤n, and note that
(A∗)∗(i, j) = A∗(j, i) = A(i, j) = A(i, j).
So (A∗)∗= A. To prove (d), let A ∈Mn(C) be invertible, B = A−1, and I be the n × n
identity matrix. Apply part (b) to the equations AB = I = BA to obtain B∗A∗= I∗=
I = A∗B∗. Thus, B∗= (A−1)∗is the two-sided matrix inverse for A∗, as needed.
To prove (e), compute
tr(A∗) =
n
X
i=1
A∗(i, i) =
n
X
i=1
A(i, i) =
n
X
i=1
A(i, i) = tr(A).
Hermitian, Positive Definite, Unitary, and Normal Matrices
177
To prove (f), use the definition of determinants (Chapter 5) to calculate
det(A∗)
=
X
f∈Sn
sgn(f)A∗(f(1), 1)A∗(f(2), 2) · · · A∗(f(n), n)
=
X
f∈Sn
sgn(f)A(1, f(1)) A(2, f(2)) · · · A(n, f(n))
=
X
f∈Sn
sgn(f)AT(f(1), 1)AT(f(2), 2) · · · AT(f(n), n)
=
det(AT) = det(A).
The conjugate-transpose operation is related to the standard inner product on Cn. For
vectors v = (v1, . . . , vn) and w = (w1, . . . , wn) in Cn, the inner product of v and w is
defined to be ⟨v, w⟩= Pn
i=1 wivi. We often identify Cn with the space of column vectors
Mn,1(C) and identify 1 × 1 matrices with complex numbers. Using this convention, the
definition of matrix multiplication shows that ⟨v, w⟩= w∗v. Observe that v∗v = ⟨v, v⟩=
Pn
i=1 vivi = Pn
i=1 |vi|2 is a nonnegative real number, which is zero iff v = 0. We define
the Euclidean norm of v to be ||v|| = (v∗v)1/2 for all v ∈Cn. If we require v to be in
Rn, then the complex conjugates disappear and we recover the standard inner product (the
dot product) and the Euclidean norm on Rn. We have the following adjoint property of the
conjugate-transpose:
for all A ∈Mn(C) and all v, w ∈Cn, ⟨Av, w⟩= ⟨v, A∗w⟩.
This follows since ⟨Av, w⟩= w∗Av = w∗(A∗)∗v = (A∗w)∗v = ⟨v, A∗w⟩.
Given a matrix A ∈Mn(C), we can use the conjugate-transpose operation to define a
quadratic form on Cn associated with A. This quadratic form is the function Q : Cn →C
defined by Q(v) = v∗Av for v ∈Cn. To get a formula for Q(v) in terms of the components
of v, write v = (v1, . . . , vn) with each vk ∈C. The kth component of the column vector Av
is Pn
j=1 A(k, j)vj. Multiplying this column vector on the left by the row vector v∗, we see
that
Q(v) =
n
X
k=1
n
X
j=1
vkA(k, j)vj
for all v ∈Cn.
(7.1)
For example, if A =

1
2 −i
3i
0

and v = (3 + i, −4i), then
Q(v) = (3 −i)1(3 + i) + (3 −i)(2 −i)(−4i) + (4i)(3i)(3 + i) + (4i)0(−4i) = −46 −32i.
7.2
Hermitian Matrices
A complex number z is real iff z = z. By analogy, we make the following definition: a
complex-valued matrix A is Hermitian iff A∗= A. This equality requires A to be a square
matrix. We see from the definition of A∗that A ∈Mn(C) is Hermitian iff A(i, j) = A(j, i)
for all i, j between 1 and n. For example,
A =


2
1 + 8i
0
i
1 −8i
−1
3 −5i
7 + i/2
0
3 + 5i
0
1
−i
7 −i/2
1
π


178
Advanced Linear Algebra
is a Hermitian matrix. A real-valued matrix A is Hermitian iff A is symmetric (meaning
AT = A). For any Hermitian matrix A, the diagonal entries of A must all be real. This
follows from the requirement A(i, i) = A(i, i) for all i. A 1 × 1 complex matrix is Hermitian
iff the sole entry of that matrix is real. We regard Hermitian matrices as being matrix analogs
of real numbers. This analogy is strengthened by results proved later in this section.
Suppose A and B are n×n Hermitian matrices and x is a real scalar. Then A+B and xA
are Hermitian, since (A+B)∗= A∗+B∗= A+B and (xA)∗= x(A∗) = xA. More generally,
any real linear combination of Hermitian matrices is also Hermitian. On the other hand,
AB need not be Hermitian in general, since (AB)∗= B∗A∗= BA. This equation shows
that (AB)∗= AB holds iff AB = BA. Thus, the product of two commuting Hermitian
matrices is also Hermitian. If A ∈Mn(C) is Hermitian and B ∈Mn(C) is arbitrary, then
B∗AB is also Hermitian, because (B∗AB)∗= B∗A∗(B∗)∗= B∗AB.
If A is Hermitian and v, w ∈Cn, then ⟨Av, w⟩= ⟨v, A∗w⟩= ⟨v, Aw⟩. This fact is
sometimes expressed by saying that a Hermitian matrix is self-adjoint. Next, consider the
quadratic form Q associated with a Hermitian matrix A. We claim that for all v ∈Cn,
Q(v) = v∗Av is real. To confirm this, we need only check that Q(v) = Q(v). Since v∗Av
is a 1 × 1 matrix and A is Hermitian,
Q(v) = v∗Av = (v∗Av)∗= v∗A∗(v∗)∗= v∗Av = Q(v).
Another key fact about Hermitian matrices is that all eigenvalues of a Hermitian matrix
A must be real. To prove this, let λ ∈C be an eigenvalue of A with an associated nonzero
eigenvector v = (v1, . . . , vn) ∈Cn. The definition shows that v∗v = Pn
i=1 |vi|2 is a positive
real number. We have seen that v∗Av is real. Also v∗Av = v∗(λv) = λ(v∗v). Thus,
λ = (v∗Av)/(v∗v) is a quotient of two real numbers and is therefore real. In contrast,
a real-valued matrix B ∈Mn(R) need not have all real eigenvalues. For example, the
eigenvalues of B =

0
1
−1
0

are i and −i. This gives more evidence that the Hermitian
property (as opposed to the requirement of having all real entries) is a good generalization
of the property of being real. Note here that iB is Hermitian although every entry of iB is
pure imaginary.
The property of having a real-valued quadratic form characterizes the class of Hermitian
matrices. More precisely, if A ∈Mn(C) is a matrix such that Q(v) = v∗Av is real for all
v ∈Cn, then A must be Hermitian. To prove this, consider the unit vectors ek ∈Cn,
where ek has a 1 in position k and 0 elsewhere. Using (7.1), we see that Q(ek) = A(k, k)
must be real for 1 ≤k ≤n. Next, for all j ̸= k, we see similarly that Q(ej + ek) =
A(j, j) + A(j, k) + A(k, j) + A(k, k) is real, and therefore A(j, k) + A(k, j) is real for all
j ̸= k. Since we know Q(ej + iek) = A(j, j) + iA(j, k) + iA(k, j) + A(k, k) is real, it follows
that i(A(j, k) −A(k, j)) is real for all j ̸= k. Fix j ̸= k, and write A(j, k) = a + ib and
A(k, j) = c + id with a, b, c, d ∈R. The preceding conditions mean that b + d = 0 and
a −c = 0, so A(k, j) = c + id = a −ib = A(j, k) for all j ̸= k. We also have A(j, j) = A(j, j)
for all j, since A(j, j) is real. Thus, A = A∗as claimed.
On the other hand, a matrix A ∈Mn(R) with all real eigenvalues need not be Hermitian.
For instance, A =
 1
2
0
3

has eigenvalues 1 and 3 (the entries on the main diagonal), but
A ̸= A∗. Later, we introduce a matrix property called normality and prove that a normal
matrix with all real eigenvalues must be Hermitian.
Hermitian, Positive Definite, Unitary, and Normal Matrices
179
7.3
Hermitian Decomposition of a Matrix
The next theorem gives a matrix analog of the decomposition of a complex number z in the
form x + iy with x, y real.
Theorem on Hermitian Decomposition of a Matrix. For each A ∈Mn(C), there
exist unique Hermitian matrices X, Y ∈Mn(C) such that A = X + iY .
Proof. To prove existence, recall that complex conjugation can be used to recover the real
and imaginary parts of a complex number z = x+iy, since x = (z+z)/2 and y = (z−z)/2i.
Given the matrix A ∈Mn(C), we are led by analogy to consider the matrices X = (A+A∗)/2
and Y = (A−A∗)/2i. We have X∗= (A∗+A)/2 = X and Y ∗= 1
2i(A∗−A) =
1
−2i(A∗−A) =
Y , so X and Y are Hermitian. A calculation confirms that X+iY = A. To prove uniqueness,
suppose that A = X+iY = U+iV where X, Y, U, V are all Hermitian. We must prove X = U
and Y = V . Observe that X −U = i(V −Y ). Applying the conjugate-transpose operation to
both sides of this matrix identity, we get X −U = −i(V −Y ). Thus, i(V −Y ) = −i(V −Y ),
which implies V −Y = 0 and hence Y = V . Then X −U = i(V −Y ) = 0 leads to X = U,
as needed.
The preceding theorem can also be rephrased in terms of skew-Hermitian matrices. A
complex-valued matrix A is called skew-Hermitian iff A∗= −A. This concept is the analog
of a pure imaginary number in C. You can check that a real linear combination of skew-
Hermitian matrices is also skew-Hermitian. The zero matrix is the only matrix that is both
Hermitian and skew-Hermitian. A matrix B is Hermitian iff iB is skew-Hermitian, since
(iB)∗= −i(B∗). Combining this fact with the previous theorem, we see that any matrix
A ∈Mn(C) can be written uniquely in the form A = H +S, where H is Hermitian and S is
skew-Hermitian. More precisely, we have H = X = (A + A∗)/2 and S = iY = (A −A∗)/2.
H is called the Hermitian part of A, and S is called the skew-Hermitian part of A. The
decomposition A = X + iY = H + S is called the Hermitian decomposition or Cartesian
decomposition of the matrix A.
Consider a complex number z ∈C and its Cartesian decomposition z = x + iy with
x, y ∈R. Then xy = yx, since multiplication of real numbers is commutative. The analogous
property for complex matrices does not always hold. Suppose A = X + iY is the Cartesian
decomposition of A ∈Mn(C), so X = (A + A∗)/2 and Y = (A −A∗)/2i. A calculation with
the distributive law shows that
XY = 1
4i(AA + A∗A −AA∗−A∗A∗);
Y X = 1
4i(AA −A∗A + AA∗−A∗A∗).
Comparing these expressions, we see that XY = Y X holds iff A∗A −AA∗= −A∗A + AA∗
iff 2A∗A = 2AA∗iff A∗A = AA∗. The latter condition does not hold for every matrix; for
example, if A =

1
2
0
3

, then A∗=
 1
0
2
3

, AA∗=
 5
6
6
9

, and A∗A =
 1
2
2
13

.
We say that a matrix A ∈Mn(C) is a normal matrix iff A∗A = AA∗. We shall soon see
that normal matrices have nicer properties than general complex matrices.
180
Advanced Linear Algebra
7.4
Positive Definite Matrices
Our next goal is to find the matrix analogs of positive and negative real numbers. To begin,
recall that a matrix A ∈Mn(C) is Hermitian iff A = A∗iff Q(v) = v∗Av is real for all
v ∈Cn (§7.2). This motivates the following definition: we say a matrix A ∈Mn(C) is
positive definite iff v∗Av is a positive1 real number for all nonzero v ∈Cn. Say A is positive
semidefinite iff v∗Av is a nonnegative real number for all nonzero v ∈Cn. Similarly, A is
negative definite iff v∗Av is in R<0 for all nonzero v ∈Cn, and A is negative semidefinite
iff v∗Av is in R≤0 for all nonzero v ∈Cn. Positive definite matrices are Hermitian, since
positive real numbers are real; similarly for positive semidefinite, negative definite, and
negative semidefinite matrices.
Suppose A, B ∈Mn(C) are positive definite and c is a positive real scalar. Then A + B
and cA are positive definite, since for nonzero v ∈Cn, v∗(A + B)v = v∗Av + v∗Bv is a
sum of two positive real numbers and v∗(cA)v = c(v∗Av) is a product of two positive real
numbers. Similarly, any linear combination of positive definite matrices with positive real
coefficients is positive definite. Any linear combination of negative definite matrices with
positive real coefficients is negative definite. A is positive definite iff −A is negative definite.
Analogous properties hold for semidefinite matrices.
Another helpful fact about positive definite matrices is that all eigenvalues of a positive
definite matrix A are positive real numbers. To prove this, let λ ∈C be an eigenvalue of A and
v be an associated nonzero eigenvector. We know v∗v = Pn
i=1 |vi|2 is a positive real number.
By positive definiteness, v∗Av = v∗(λv) = λ(v∗v) is also in R>0. So λ = (v∗Av)/(v∗v)
is positive and real, being the ratio of two positive real numbers. The same argument
shows that the eigenvalues of a positive semidefinite matrix are all positive or zero; the
eigenvalues of a negative definite matrix are all negative real numbers; and the eigenvalues
of a negative semidefinite matrix are all in R≤0. Since the trace of a matrix is the sum of its
eigenvalues, we see that the trace of a positive definite matrix is a positive real number. Since
the determinant of a matrix is the product of its eigenvalues, we see that the determinant of
a positive definite matrix is a positive real number. In particular, positive definite matrices
are always invertible. In the negative definite case, the trace is always negative, but the
determinant is negative iff n (the size of the matrix) is odd; the determinant is positive for
n even. We prove in §7.15 that a matrix A ∈Mn(C) is positive definite iff A∗= A and for
all k between 1 and n, the matrix consisting of the first k rows and k columns of A has
positive determinant.
Every real number is either positive, negative, or zero. However, the corresponding
property is not true for Hermitian matrices with size n > 1. It is certainly possible for a
Hermitian matrix to have both positive and negative real eigenvalues; consider, for example,
diagonal matrices with real diagonal entries. Such matrices are neither positive definite
nor negative definite. A Hermitian matrix with both positive and negative eigenvalues is
sometimes called indefinite.
A matrix with all positive eigenvalues need not be Hermitian and hence need not be
positive definite; consider A =
 1
2
0
3

for an example. We show in §7.9 that a normal
matrix with all positive eigenvalues must be positive definite.
1In this section, “positive” always means “strictly positive” when applied to real numbers.
Hermitian, Positive Definite, Unitary, and Normal Matrices
181
7.5
Unitary Matrices
Next, we develop a matrix analog of the unit circle in C. Recall that a complex number
z ∈C is on the unit circle iff |z| = 1 iff |z|2 = 1 iff zz = 1 iff z−1 = z. Accordingly, we define
a matrix U ∈Mn(C) to be unitary iff UU ∗= I. A 1 × 1 matrix is unitary iff its sole entry
has modulus 1. Set u = det(U) ∈C, so det(U ∗) = u. Taking determinants in the formula
UU ∗= I, we see that |u|2 = uu = 1; thus, the determinant of a unitary matrix is a complex
number of modulus 1. In particular, det(U) ̸= 0, so U −1 exists. Multiplying UU ∗= I by
U −1, we see that U −1 = U ∗and hence U ∗U = I. Reversing this argument, we see that the
following three conditions on U ∈Mn(C) are equivalent:
UU ∗= I;
U ∗U = I;
U is invertible and U −1 = U ∗.
We could have taken any of these conditions as the definition of a unitary matrix.
If U is unitary, then U ∗is also unitary, since U ∗(U ∗)∗= U ∗U = I. If U is unitary, then
U −1 is also unitary, since U −1 = U ∗. If U is unitary, then the transpose U T is also unitary,
since U T(U T)∗= U T(U ∗)T = (U ∗U)T = IT = I. If U and V are unitary matrices of the
same size, then UV is unitary, since (UV )(UV )∗= UV V ∗U ∗= UIU ∗= UU ∗= I. If U
is a diagonal matrix such that each diagonal entry is a number zk ∈C of modulus 1, then
U is unitary, because z−1
k
= zk implies U −1 = U ∗for such a matrix U. In particular, the
identity matrix I is unitary. Let Un(C) denote the set of all unitary matrices in Mn(C). The
preceding remarks show that Un(C) is closed under identity, matrix product, and inverses.
Therefore, the set Un(C) of unitary matrices forms a subgroup of the multiplicative group
GLn(C) of invertible complex matrices.
Unitary matrices preserve inner products; more precisely, if U is unitary and v, w ∈Cn,
then ⟨Uv, Uw⟩= ⟨v, w⟩. This follows since ⟨Uv, Uw⟩= (Uw)∗(Uv) = w∗(U ∗U)v =
w∗Iv = w∗v = ⟨v, w⟩. Similarly, unitary matrices preserve norms, meaning that ||Uv|| =
||v|| for all unitary U and all v ∈Cn. This follows since ||Uv||2 = ⟨Uv, Uv⟩= ⟨v, v⟩= ||v||2.
All eigenvalues of a unitary matrix are complex numbers of modulus 1. To see this, let
U ∈Mn(C) be unitary with eigenvalue λ ∈C and associated nonzero eigenvector v. We
have ||v|| = ||Uv|| = ||λv|| = |λ| · ||v||. Dividing by the nonzero scalar ||v|| gives |λ| = 1, as
needed. Taking the product of all the eigenvalues, we get another proof that | det(U)| = 1
for U unitary.
A list of vectors v1, . . . , vm ∈Cn is called orthonormal iff ⟨vj, vk⟩= 0 for all j ̸= k and
⟨vj, vj⟩= 1 for all j. Consider the condition U ∗U = I, which can be used to define unitary
matrices. Let the columns of U be v1, . . . , vn ∈Cn. The j, k-entry of the product U ∗U is
found by taking the product of row j of U ∗and column k of U. Row j of U ∗is v∗
j, and
column k of U is vk. We conclude that (U ∗U)(j, k) = v∗
jvk = ⟨vk, vj⟩for all j, k. On the
other hand, I(j, k) = 1 for j = k and I(j, k) = 0 for j ̸= k. Comparing to the definition
of orthonormality, we see that a matrix U ∈Mn(C) is unitary iff U ∗U = I iff the columns
of U are orthonormal vectors in Cn. Applying similar reasoning to the condition UU ∗= I
(or applying the result just stated to U T), we see that a matrix U ∈Mn(C) is unitary iff
the rows of U are orthonormal vectors in Cn. Thus, the rows (resp. columns) of a unitary
matrix U are mutually perpendicular unit vectors in Cn, further strengthening the analogy
between U and the unit circle in C.
We have seen that a unitary matrix preserves inner products. Conversely, if U ∈Mn(C)
satisfies ⟨Uv, Uw⟩= ⟨v, w⟩for all v, w ∈Cn, then U is unitary. Proof: The condition on
scalar products says that w∗(U ∗U)v = w∗v for all v, w ∈Cn. Choosing v = ej and w = ek
(standard basis vectors) and carrying out the matrix multiplication on the left side, we see
that (U ∗U)(k, j) is 0 for k ̸= j and is 1 for k = j. So U ∗U = I and U is unitary.
182
Advanced Linear Algebra
Similarly, if U ∈Mn(C) satisfies ||Uv|| = ||v|| for all v ∈Cn, then U is unitary. Proof:
The condition on norms means that ⟨Uv, Uv⟩= ⟨v, v⟩for all v ∈Cn. We show that U
must preserve inner products, which implies that U is unitary by the previous result. Fix
v, w ∈Cn, and apply the assumed condition to the vectors v, w, v + w, and v + iw. We
conclude that:
⟨Uv, Uv⟩= ⟨v, v⟩;
⟨Uw, Uw⟩= ⟨w, w⟩;
⟨U(v + w), U(v + w)⟩= ⟨v + w, v + w⟩;
⟨U(v + iw), U(v + iw)⟩= ⟨v + iw, v + iw⟩.
For any complex scalars c, d and any vectors v, w ∈Cn, we have
⟨cv + dw, cv + dw⟩= (cv + dw)∗(cv + dw) = ||cv||2 + dcw∗v + cdv∗w + ||dw||2.
Applying this formula to the preceding expressions, we get:
⟨v + w, v + w⟩
=
||v||2 + w∗v + v∗w + ||w||2,
⟨Uv + Uw, Uv + Uw⟩
=
||Uv||2 + w∗U ∗Uv + v∗U ∗Uw + ||Uw||2,
⟨v + iw, v + iw⟩
=
||v||2 −iw∗v + iv∗w + ||w||2,
⟨Uv + iUw, Uv + iUw⟩
=
||Uv||2 −iw∗U ∗Uv + iv∗U ∗Uw + ||Uw||2.
Set a = w∗v = ⟨v, w⟩, b = w∗(U ∗U)v = ⟨Uv, Uw⟩, so v∗w = a and v∗(U ∗U)w = b.
Putting these values into the preceding formulas and recalling that ||Uv||2 = ||v||2 and
||Uw||2 = ||w||2, we get a + a = b + b and −i(a −a) = −i(b −b). These equations say that
2 Re(a) = 2 Re(b) and 2 Im(a) = 2 Im(b); so the complex numbers a and b are equal. This
means that ⟨Uv, Uw⟩= ⟨v, w⟩, hence U is unitary.
To summarize, all of the following conditions on a matrix U ∈Mn(C) are equivalent:
U is unitary; UU ∗= I; U ∗U = I; U is invertible and U −1 = U ∗; the rows of U are
orthonormal in Cn; the columns of U are orthonormal in Cn; U ∗is unitary; U −1 is unitary;
U T is unitary; U preserves inner products in Cn; U preserves norms in Cn. Also, if U is
unitary then the determinant and all eigenvalues of U have norm 1 in C, but the converse
does not hold in general. The failure of the converse can be demonstrated by considering an
upper-triangular matrix V with numbers of modulus 1 on the main diagonal and at least
one nonzero off-diagonal entry. Here, V ∗is lower-triangular, V −1 is upper-triangular, and
neither matrix is diagonal, so V ∗̸= V −1.
7.6
Unitary Similarity
Two square matrices A, B ∈Mn(C) are called similar iff there is an invertible matrix
S ∈Mn(C) such that B = S−1AS. We say A and B are unitarily similar iff there is a
unitary matrix U ∈Mn(C) such that B = U −1AU = U ∗AU. You can check that similarity
and unitary similarity are equivalence relations on Mn(C). The next theorem shows that
unitary similarity preserves the properties of being Hermitian, positive definite, or unitary.
Theorem on Unitary Similarity. Suppose B is unitarily similar to A.
(a) A is Hermitian iff B is Hermitian.
(b) A is positive definite iff B is positive definite.
(c) A is unitary iff B is unitary.
Hermitian, Positive Definite, Unitary, and Normal Matrices
183
Proof. Let B = U ∗AU where U is unitary. If A is Hermitian, then B∗= (U ∗AU)∗=
U ∗A∗U = U ∗AU = B, so B is Hermitian. If A is positive definite and v ∈Cn is nonzero,
then Uv ̸= 0 since U is invertible. Hence v∗Bv = v∗U ∗AUv = (Uv)∗A(Uv) is a positive
real number, so B is positive definite. If A is unitary, then B = U ∗AU is a product of unitary
matrices and is therefore unitary. The converse statements follow from the symmetry of
unitary similarity.
Next, we discuss the geometric significance of similarity and unitary similarity. Suppose
V is a finite-dimensional complex vector space (which we can take to be Cn with no loss
of generality), and suppose T : V →V is a linear operator on V . Suppose A is the matrix
representing T relative to some ordered basis of V . If we switch to some other ordered basis
of V , then the matrix representing T relative to the new basis is similar to A. The columns
of the similarity matrix S give the coordinates of the new basis relative to the old basis.
Conversely, any matrix similar to A is the matrix of T relative to some ordered basis of V .
For more details, see Chapter 6.
Unitary similarity has an analogous geometric interpretation. Here, we suppose V is
a finite-dimensional complex inner product space (which we can take to be Cn with the
standard inner product, without loss of generality), and we assume T is a linear operator
on V . Let A be the matrix of T relative to an orthonormal basis of V , which might be
the standard ordered basis (e1, . . . , en) of Cn. You can check that A is a length-preserving
matrix (meaning ||Ax|| = ||x|| for all x ∈Cn) iff T is a length-preserving map (meaning
||T(v)|| = ||v|| for all v ∈V ). If we switch to some other orthonormal basis of V , we obtain
a new matrix B such that B = U −1AU for some invertible matrix U. As pointed out above,
the columns of U are the coordinates of the new basis vectors relative to the old ones.
Since both bases are orthonormal, we see that the columns of U are orthonormal in Cn,
and hence U is unitary. Thus, B is unitarily similar to A. Conversely, any matrix that is
unitarily similar to A is the matrix of T relative to some orthonormal basis of V . So unitary
similarity corresponds to orthonormal change of basis, while ordinary similarity corresponds
to arbitrary change of basis. For more details, see §6.21.
A matrix A ∈Mn(C) is called unitarily diagonalizable iff there exists a unitary matrix
U ∈Mn(C) and a diagonal matrix D ∈Mn(C) such that U ∗AU = D. A linear operator T
on V is called unitarily diagonalizable iff the matrix of T relative to some orthonormal basis
is diagonal iff the matrix of T relative to any orthonormal basis is unitarily diagonalizable.
This means that there exists an orthonormal list of vectors (u1, . . . , un) in V and scalars
c1, . . . , cn ∈C such that T(uk) = ckuk for all k between 1 and n. In other words, an operator
T on V is unitarily diagonalizable iff the inner product space V has an orthonormal basis
consisting of eigenvectors of T.
In the case where V = Cn and A is the matrix of T relative to (e1, . . . , en), the matrix
equation U ∗AU = D is equivalent to AU = UD (since U ∗= U −1). Let the columns of U
be u1, . . . , un ∈Cn and the diagonal entries of D be c1, . . . , cn. The kth column of AU is
Auk, while the kth column of UD is ckuk. Thus, we get a matrix proof of the observation
in the previous paragraph: a matrix A ∈Mn(C) is unitarily diagonalizable iff there exist
n orthonormal vectors in Cn that are eigenvectors of A. In this case, the unitary matrix
achieving diagonalization has columns consisting of the orthonormal eigenvectors of A, and
the resulting diagonal matrix has the eigenvalues of A on its main diagonal.
184
Advanced Linear Algebra
7.7
Unitary Triangularization
Not every matrix can be unitarily diagonalized. For example, the matrix A =
 0
1
0
0

has
zero as its only eigenvalue. If A were similar (or unitarily similar) to a diagonal matrix,
that matrix must have zeroes on its main diagonal, since similar matrices have the same
eigenvalues. But then A would be similar to the zero matrix, forcing A itself to be zero.
We prove below that normality of A is a necessary and sufficient condition for unitary
diagonalizability of A.
On the other hand, if we weaken the concept of unitary diagonalizability, we can obtain
a result valid for all square matrices. A matrix A ∈Mn(C) is called triangulable iff there
is an invertible matrix S ∈Mn(C) such that S−1AS is an upper-triangular matrix. Call A
unitarily triangulable iff there is a unitary U ∈Mn(C) such that U −1AU = U ∗AU is an
upper-triangular matrix. Geometrically, if T : V →V is the linear operator represented by
the matrix A, then T is triangulable iff there exists an ordered basis (x1, . . . , xn) of V such
that T(xi) is in the span of (x1, . . . , xi) for i = 1, 2, . . . , n. T is unitarily triangulable iff
there exists an orthonormal basis of V with this property. In this situation, x1 must be an
eigenvector of T.
Schur’s Theorem on Unitary Triangularization. Every matrix A ∈Mn(C) can be
unitarily triangularized. Equivalently, for every linear operator T on an n-dimensional
complex inner product space V , there is an orthonormal basis X of V such that [T]X
(the matrix of T relative to X) is upper-triangular.
Proof. We prove the result for linear operators by induction on n ≥1. For n = 1, let x be
any nonzero vector in V . Letting X = (x/||x||), X is an orthonormal basis of V , and the
1 × 1 matrix [T]X is upper-triangular. Next, fix n > 1, and assume the theorem is already
known for all inner product spaces of dimension less than n. Since we are working over
the algebraically closed field C, we know that T has an eigenvalue c1 ∈C with associated
eigenvector x1 (take c1 to be any root of the characteristic polynomial of T). Dividing x1
by a scalar, we can ensure that ||x1|| = 1. We claim V = Cx1 ⊕W, where
W = x⊥
1 = {w ∈V : ⟨w, x1⟩= 0}.
To see this, consider the map S : V →C given by S(v) = ⟨v, x1⟩for v ∈V . S is a linear map
with kernel W and image C (since S(x1) = 1). So the Rank–Nullity Theorem shows that
n = dim(V ) = dim(W) +dim(C). As dim(C) = 1, we obtain dim(W) = n−1. On the other
hand, given dx1 ∈Cx1 ∩W with d ∈C, we have 0 = S(dx1) = ⟨dx1, x1⟩= d||x1||2 = d, so
Cx1 ∩W = {0}. This proves the claim.
It follows that every vector in V can be written uniquely in the form ax1 + w, where
a ∈C and w ∈W. Let P : V →W be the projection map defined by P(ax1 + w) = w, and
define a linear map T ′ : W →W by setting T ′(w) = P(T(w)) for w ∈W. By induction, we
can find an orthonormal basis X′ = (x2, . . . , xn) of W such that [T ′]X′ is upper-triangular.
Let X = (x1, x2, . . . , xn), which is readily verified to be an orthonormal basis for V . We
assert that
[T]X =


c1
∗· · · ∗
0
...
[T ′]X′
0

,
(7.2)
Hermitian, Positive Definite, Unitary, and Normal Matrices
185
which is an upper-triangular matrix. To verify this, recall that the jth column of [T]X
consists of the coordinates of T(xj) relative to the basis X. Since T(x1) = c1x1, the first
column of [T]X is (c1, 0, . . . , 0). For j > 1, write T(xj) = c1,jx1 + c2,jx2 + · · · + cn,jxn with
ci,j ∈C. Then
T ′(xj) = P(T(xj)) = c2,jx2 + · · · + cn,jxn.
Thus, the jth column of [T]X consists of some scalar c1,j followed by the (j −1)th column
of [T ′]X′. So [T]X has the form shown in (7.2).
7.8
Simultaneous Triangularization
Given two matrices A, B ∈Mn(C), Schur’s Theorem guarantees that there are unitary
matrices U and V such that U ∗AU and V ∗BV are upper-triangular matrices. However,
there is no guarantee that one single matrix U can triangularize both A and B at the
same time. If such a unitary matrix does exist, we say that A and B can be simultaneously
unitarily triangularized.
Theorem on Simultaneous Triangularization of Two Commuting Matrices. If
A, B ∈Mn(C) satisfy AB = BA, then A and B can be simultaneously unitarily
triangularized. Equivalently, if T and U are two commuting linear operators on a complex
inner product space V , then there exists an orthonormal basis X of V such that [T]X and
[U]X are both upper-triangular matrices.
Proof. We use induction on n = dim(V ), the case n = 1 being immediate. Assume n > 1
and that the theorem holds for linear maps on spaces of dimension less than n. If the
required result is to be true, consideration of the first column of [T]X and [U]X shows that
we need to find a unit vector x1 ∈Cn that is simultaneously an eigenvector for T and an
eigenvector for U. To do this, fix an eigenvalue c1 of T. Let Z = {v ∈V : T(v) = c1v},
which is a nonzero subspace of V , namely, the eigenspace associated with the eigenvalue c1.
We claim that U maps Z into Z. Given v ∈Z, commutativity of T and U gives
T(U(v)) = U(T(v)) = U(c1v) = c1(U(v)),
so that U(v) ∈Z. It follows that the restriction U|Z : Z →Z is a linear operator on Z.
Thus, U has an eigenvector in Z (with associated eigenvalue d1, say). Let x1 ∈Z be a unit
eigenvector of U. Since x1 ∈Z, x1 is also an eigenvector for T.
We continue as in the proof of Schur’s Theorem. Let W = x⊥
1 , so that V = Cx1 ⊕W.
Let P : Cx1 ⊕W →W be the projection given by P(ax1 + w) = w for a ∈C and w ∈W.
Consider the two linear operators on W defined by T ′ = (P ◦T)|W and U ′ = (P ◦U)|W.
We intend to apply our induction hypothesis to the (n −1)-dimensional space W. Before
doing so, we must check that the new operators T ′ and U ′ on W still commute. To verify
this, we first show that P ◦T ◦P = P ◦T. Given any v = ax1 + w in V (where a ∈C and
w ∈W),
P(T(v)) = P(T(ax1 + w)) = P(aT(x1) + T(w)) =
P(ac1x1) + P(T(w)) = P(T(w)) = P(T(P(v))).
(7.3)
Similarly, P ◦U ◦P = P ◦U. It follows that
T ′ ◦U ′
=
(P ◦T ◦P ◦U)|W = (P ◦T ◦U)|W
=
(P ◦U ◦T)|W = (P ◦U ◦P ◦T)|W = U ′ ◦T ′.
186
Advanced Linear Algebra
The induction hypothesis, applied to T ′ and U ′ acting on W, produces an orthonormal
basis X′ = (x2, . . . , xn) of W such that [T ′]X′ and [U ′]X′ are both upper-triangular. Letting
X = (x1, x2, . . . , xn), we see as in the proof of Schur’s Theorem that X is an orthonormal
basis of V such that [T]X and [U]X are both upper-triangular.
We can extend this result to an arbitrary (possibly infinite) collection of commuting
matrices or operators.
Theorem on Simultaneous Triangularization of Commuting Operators. Suppose
V is an n-dimensional complex inner product space and {Ti : i ∈I} is an indexed collection
of linear operators on V such that Ti ◦Tj = Tj ◦Ti for all i, j ∈I. Then there exists an
orthonormal basis X of V such that [Ti]X is upper-triangular for all i ∈I.
Proof. We prove this result by repeating the previous induction proof. The key point is that
we must find a unit vector in V that is simultaneously an eigenvector for every operator
Ti. To do this, consider a nonzero subspace Z of V of minimum positive dimension such
that Ti[Z] ⊆Z for all i ∈I. Such a subspace must exist, since Ti[V ] ⊆V and V is finite-
dimensional. Let T be a fixed operator from the collection {Ti : i ∈I}. Let Z′ be a nonzero
eigenspace for T|Z. The reasoning used before shows that Ti[Z′] ⊆Z′ for all i ∈I, since
each Ti commutes with T. By minimality of dim(Z), we see that Z′ = Z. Thus, Z itself
is part of an eigenspace for T, so every nonzero z ∈Z is an eigenvector for T. But T was
arbitrary, so every nonzero z ∈Z is an eigenvector for every Ti in the given collection.
Choose any such z and normalize it to get a common unit eigenvector x1 for all the Ti.
The rest of the proof is the same as before. In particular, any two maps from the collection
{T ′
i = (P ◦Ti)|W : i ∈I} commute (by the same calculation done previously), so the
induction hypothesis does apply to give us the basis X′ needed to complete the induction
step.
7.9
Normal Matrices and Unitary Diagonalization
Recall that a matrix A ∈Mn(C) is unitarily diagonalizable iff there exists a unitary matrix
U such that U ∗AU is a diagonal matrix. On the other hand, A is normal iff AA∗= A∗A.
The next theorem states a surprising connection between these two concepts.
Spectral Theorem on Normal Matrices and Unitary Diagonalizability. For all
A ∈Mn(C), A is unitarily diagonalizable if and only if A is normal.
Proof. First assume that A is unitarily diagonalizable, so there is a unitary matrix U and
a diagonal matrix D with D = U ∗AU. Since D∗= U ∗A∗U is also diagonal, we see that
DD∗= D∗D. Then
AA∗= (UDU ∗)(UD∗U ∗) = U(DD∗)U ∗= U(D∗D)U ∗= (UD∗U ∗)(UDU ∗) = A∗A,
so A is normal.
Conversely, suppose A ∈Mn(C) is a normal matrix. Then AA∗= A∗A by definition,
so A and A∗can be simultaneously unitarily triangularized. Choose a unitary matrix U
such that C = U ∗AU and D = U ∗A∗U are both upper-triangular. On one hand, C is
an upper-triangular matrix. On the other hand, C = U ∗AU = (U ∗A∗U)∗= D∗is the
conjugate-transpose of an upper-triangular matrix, so C is a lower-triangular matrix. This
means that C must be a diagonal matrix, so we have unitarily diagonalized A.
Hermitian, Positive Definite, Unitary, and Normal Matrices
187
Here are some useful corollaries of the Spectral Theorem.
Theorem on Unitary Diagonalization of Special Matrices. Hermitian matrices, skew-
Hermitian matrices, positive definite matrices, negative definite matrices, real symmetric
matrices, and unitary matrices can always be unitarily diagonalized.
Proof. By the Spectral Theorem, it suffices to check that each type of matrix mentioned
in the theorem must be normal. Any Hermitian matrix A is normal, since A = A∗implies
AA∗= A2 = A∗A. In particular, positive definite, negative definite, and real symmetric
matrices are normal. Any unitary matrix U is normal, since UU ∗= I = U ∗U. Any skew-
Hermitian matrix S is normal, since S∗= −S implies SS∗= −S2 = S∗S.
There are normal matrices that are neither Hermitian, skew-Hermitian, nor unitary. For
example, consider A =
 1 + i
0
0
4

or any matrix unitarily similar to A.
The nature of the eigenvalues of a normal matrix leads to a characterization of the
properties of being Hermitian, positive definite, or unitary.
Theorem on Eigenvalues of Normal Matrices. Suppose A ∈Mn(C) is normal.
(a) A is Hermitian iff all eigenvalues of A are real.
(b) A is positive definite iff all eigenvalues of A are strictly positive real numbers.
(c) A is unitary iff all eigenvalues of A are complex numbers of modulus 1.
(d) A is invertible iff all eigenvalues of A are nonzero.
Proof. In each part, the forward implication holds with no restriction on A, as we saw earlier
in the chapter. For normal matrices, we can prove both directions of each implication using
the Spectral Theorem. Given a normal matrix A, write D = U ∗AU where U is unitary
and D is a diagonal matrix with the eigenvalues of A on its diagonal. Unitary similarity
preserves the properties of being Hermitian, positive definite, unitary, or invertible, so A
has one of these properties iff D does. It is immediate from the definitions that a diagonal
matrix D is Hermitian iff all diagonal entries of D are real; D is positive definite iff all
diagonal entries of D are strictly positive; D is unitary iff all diagonal entries of D have
norm 1 in C; and D is invertible iff all diagonal entries of D are nonzero.
We know that every positive real number has a positive square root. Analogy leads us
to the following matrix result.
Theorem on Square Roots of Positive Definite Matrices. If A ∈Mn(C) is positive
definite, then there exists a positive definite matrix B such that B2 = A.
This positive definite square root of A is unique, as shown later (§7.11).
Proof. There is a unitary matrix U and a diagonal matrix D with D = U ∗AU. Each diagonal
entry of D is a positive real number; let E be the diagonal matrix obtained by replacing
each such entry by its positive square root. Then E2 = D. Setting B = UEU ∗, we compute
B2 = (UEU ∗)(UEU ∗) = UE(U ∗U)EU ∗= UE2U ∗= UDU ∗= A.
Since E is positive definite and B is unitarily similar to E, B is also positive definite.
A similar proof shows that every Hermitian matrix A has a Hermitian cube root, which
is a Hermitian matrix C such that C3 = A.
188
Advanced Linear Algebra
7.10
Polynomials and Commuting Matrices
We sometimes need to know which matrices in Mn(C) commute with a given matrix A.
For example, we proved that commutativity of A and B is a sufficient condition for A and
B to be simultaneously triangulable. In general, it is difficult to characterize the set of all
matrices that commute with A. However, we can describe a particular collection of matrices
that always commute with A. Consider a polynomial p ∈C[x] with complex coefficients, say
p = c0 + c1x + c2x2 + · · · + cdxd where all ci ∈C. We can replace x by A in this polynomial
to obtain a matrix p(A) = c0I + c1A + c2A2 + · · · + cdAd ∈Mn(C). For any polynomial p,
the matrix A commutes with p(A), because
Ap(A) = c0A + c1A2 + c2A3 + · · · + cdAd+1 = p(A)A.
More generally, suppose C is any matrix that commutes with a given matrix A. We show
by induction that C commutes with Ak for all integers k ≥0. This holds for k = 0 and
k = 1, since C commutes with I and A. Assuming the result holds for some k, note that
CAk+1 = C(AkA) = (CAk)A = (AkC)A = Ak(CA) = Ak(AC) = Ak+1C.
Next, observe that if C commutes with U and V , then C commutes with U + V . If C
commutes with U and r is a scalar, then C commutes with rU. It follows that if C commutes
with matrices U0, . . . , Um, then C commutes with any linear combination of U0, . . . , Um.
Taking Uk = Ak, we deduce that if C commutes with A, then C commutes with p(A) for
every polynomial p. For example, if A is invertible, then C = A−1 commutes with A (since
AA−1 = I = A−1A), so A−1 commutes with every polynomial in A.
We have shown that for all matrices A, C ∈Mn(C), if C commutes with A, then p(A)
commutes with C for every polynomial p. We can apply this result with A replaced by C
and C replaced by p(A) to conclude: if AC = CA, then p(A) commutes with q(C) for all
polynomials p, q ∈C[x].
Here are a few more facts that can be proved by similar methods. First, suppose B =
U ∗AU for some unitary matrix U. Then p(B) = U ∗p(A)U and p(A) = Up(B)U ∗for all
polynomials p ∈C[x]. We prove this for the particular polynomials p = xk by induction on
k ≥0, then deduce the result for general p by linearity. Second, suppose D is a diagonal
matrix with diagonal entries d1, . . . , dn. The same proof technique shows that p(D) is a
diagonal matrix with diagonal entries p(d1), . . . , p(dn).
Theorem on Polynomial Characterization of Normality. For all A ∈Mn(C), A is
normal iff A∗= p(A) for some polynomial p ∈C[x]. If A is normal and invertible, then A−1
is a polynomial in A.
Proof. First assume A∗= p(A) for some polynomial p. Every polynomial in A commutes
with A, so A is normal. Conversely, assume A is normal. Choose a unitary U such that D =
U ∗AU is diagonal. Let d1, . . . , dn be the diagonal entries of D. We know that D∗= U ∗A∗U,
where D∗has diagonal entries d1, . . . , dn. Suppose we can find a polynomial p such that
p(D) = D∗. Then a calculation shows that p(A) = p(UDU ∗) = Up(D)U ∗= UD∗U ∗= A∗,
so the proof is complete once p is found.
To construct p, we use Lagrange’s Interpolation Formula (see §3.19): if x1, . . . , xm are
any m distinct numbers in C, and y1, . . . , ym are arbitrary elements of C, then there exists
a polynomial p ∈C[x] such that p(xi) = yi for all i. You can check that
p(x) =
m
X
j=1
yj
Q
i:i̸=j(x −xi)
Q
i:i̸=j(xj −xi)
Hermitian, Positive Definite, Unitary, and Normal Matrices
189
is a polynomial with the required values. To apply this formula in the current situation,
note that di = dj iff di = dj. We can take the xi to be the distinct complex numbers that
occur on the main diagonal of D, and we can take yi = xi for each i. Choosing p as above,
it then follows that p(di) = di for all i. Accordingly, p(D) = D∗and p(A) = A∗.
In the case where A is normal and invertible, D−1 is the diagonal matrix with diagonal
entries d−1
1 , . . . , d−1
n . Using Lagrange’s Interpolation Formula as before, we can find a
polynomial q ∈C[x] such that q(di) = d−1
i
for all i. Then q(D) = D−1, and hence
q(A) = A−1.
Using the Cayley–Hamilton Theorem, you can check that A−1 is a polynomial in A for
any invertible matrix A, even if A is not normal.
7.11
Simultaneous Unitary Diagonalization
We say that a collection of matrices {Ai : i ∈I} ⊆Mn(C) can be simultaneously unitarily
diagonalized iff there exists a unitary matrix U (independent of i) such that Di = U ∗AiU
is diagonal for all i ∈I.
Theorem on Simultaneous Unitary Diagonalization. A collection {Ai : i ∈I} of
matrices in Mn(C) is simultaneously unitarily diagonalizable if and only if every Ai is a
normal matrix and AiAj = AjAi for all i, j ∈I.
Proof. First assume {Ai : i ∈I} is simultaneously unitarily diagonalizable. Since each Ai
is unitarily diagonalizable, we know each Ai is normal by a previous theorem. Next, choose
a unitary U such that Di = U ∗AiU is diagonal for all i ∈I. Diagonal matrices commute,
so DiDj = DjDi for all i, j ∈I. Therefore,
AiAj = (UDiU ∗)(UDjU ∗) = U(DiDj)U ∗= U(DjDi)U ∗= (UDjU ∗)(UDiU ∗) = AjAi
for all i, j ∈I.
Next, assume every Ai is normal and AiAj = AjAi for all i, j ∈I. We recycle some ideas
from the earlier proof that a normal matrix is unitarily diagonalizable. The key idea in that
proof was to simultaneously unitarily upper-triangularize A and A∗, and then observe that
the resulting triangular matrices must actually be diagonal. To use this idea here, we need
to invoke our previous theorem about simultaneous upper-triangularization of a collection
of matrices (§7.8). Consider the collection of matrices F = {Ai : i ∈I} ∪{A∗
i : i ∈I}. Each
Ai is normal, so A∗
i = pi(Ai) for certain polynomials pi ∈C[x]. To use our theorem, we need
to know that F is a commuting family. For any i, j ∈I, Ai commutes with Aj by hypothesis.
So, by our results on polynomials, Ai commutes with pj(Aj) = A∗
j; pi(Ai) = A∗
i commutes
with Aj; and pi(Ai) = A∗
i commutes with pj(Aj) = A∗
j. This covers all possible pairs of
matrices in F. So our theorem does apply, and we know there is a unitary matrix U such
that Di = U ∗AiU and Ei = U ∗A∗
i U are upper-triangular matrices for all i ∈I. We finish
the proof as before: Di is both upper-triangular and lower-triangular, since Di = E∗
i ; so Di
is a diagonal matrix for all i ∈I. Thus the collection {Ai : i ∈I} has been simultaneously
unitarily diagonalized. (In fact, we have diagonalized the larger collection F.)
As an illustration of this theorem, we prove the uniqueness part of the Theorem on
Square Roots of Positive Definite Matrices. Let A ∈Mn(C) be a positive definite matrix.
We show that there exists a unique positive definite matrix B such that B2 = A. Recall
how existence of B follows from the Spectral Theorem. First, we find a unitary U such
190
Advanced Linear Algebra
that D = U ∗AU is diagonal with positive real diagonal entries d1, . . . , dn. Then we let
E be diagonal with diagonal entries √d1, . . . , √dn, and put B = UEU ∗. By Lagrange
interpolation, choose a polynomial p ∈C[x] such that p(di) = √di for all i. It follows
that p(D) = E and hence p(A) = B. Now, suppose C is any positive definite matrix such
that C2 = A. Since B = p(C2) is a polynomial in C, B and C commute. Both B and C
are normal, so we can simultaneously unitarily diagonalize B and C. Let V be a unitary
matrix such that D1 = V ∗BV and D2 = V ∗CV are both diagonal. D1 and D2 are positive
definite (since B and C are), so all diagonal entries of D1 and D2 are positive. Furthermore,
D2
1 = V ∗AV = D2
2, so that D1(k, k)2 = D2(k, k)2 ∈R for all k. Since every positive real
number has a unique positive square root, it follows that D1(k, k) = D2(k, k) for all k.
So D1 = D2, which implies B = V D1V ∗= V D2V ∗= C. A similar proof shows that for
all integers k ≥1, every positive definite matrix in Mn(C) has a unique positive definite
(2k)th root, and every Hermitian matrix has a unique Hermitian (2k −1)th root. We use
the notation
√
A to denote the unique positive definite square root of a positive definite
matrix A,
3√
B to denote the unique Hermitian cube root of a Hermitian matrix B, and so
on.
7.12
Polar Decomposition: Invertible Case
For every nonzero complex number z, there exist r, u ∈C such that r is a positive real
number, |u| = 1, and z = ru = ur. We call this the polar decomposition of z. Analogy
suggests the following result: for every invertible normal matrix A ∈Mn(C), there exist
R, U ∈Mn(C) such that R is positive definite, U is unitary, and A = RU = UR. The
techniques developed so far lead to a quick proof, as follows. By normality of A, find a
unitary V and a diagonal D such that D = V ∗AV . Each diagonal entry di = D(i, i) is a
nonzero complex number (since A is invertible). Let di = riui be the polar decomposition
of di. Let R′ and U ′ be diagonal matrices with diagonal entries ri and ui, so R′ is positive
definite and U ′ is unitary. Observe that D = R′U ′ = U ′R′. Putting R = V R′V ∗and
U = V U ′V ∗, we then have A = RU = UR where R is positive definite and U is unitary.
The next theorem generalizes this result to the case where A is invertible but not
necessarily normal.
Polar Decomposition Theorem for Invertible Matrices. If A ∈Mn(C) is invertible,
then there exist unique matrices R, U ∈Mn(C) such that R is positive definite, U is unitary,
and A = RU. This factorization is called the polar decomposition of A.
Proof. To motivate the proof, consider the polar decomposition z = ru of a nonzero complex
number z. To find r given z, we could calculate zz = |z|2 = r2|u|2 = r2 and then take square
roots to find r. Then u must be r−1z. Given the invertible matrix A, we are therefore led
to consider the matrix AA∗. This matrix is positive definite, since for nonzero v ∈Cn,
v∗AA∗v = ||A∗v||2 is a positive real number. Let R be the unique positive definite square
root of AA∗and U = R−1A. Then A = RU. Since R∗= R and RR = AA∗, the fact that
U is unitary follows from the calculation
UU ∗= R−1AA∗(R−1)∗= R−1(RR)(R∗)−1 = (R−1R)(RR−1) = I.
To see that R and U are uniquely determined by A, suppose A = SW where S is positive
definite and W is unitary. Then AA∗= SWW ∗S∗= SIS = S2. So S = R since the
positive definite matrix AA∗has a unique positive definite square root. It follows that
W = S−1A = R−1A = U.
Hermitian, Positive Definite, Unitary, and Normal Matrices
191
In the polar decomposition A = RU, R and U need not commute. In fact, RU = UR
holds iff A is a normal matrix (compare this to the corresponding fact for the Cartesian
decomposition of A, which motivated the definition of normality). If A is normal, then
RU = UR follows from the unitary diagonalization proof given in the first paragraph of
this section. Conversely, suppose R and U commute. Since R and U are both normal, it
suffices to prove that the product of two commuting normal matrices is also normal. Suppose
B and C are normal commuting matrices. By normality, B∗is a polynomial in B and C∗is
a polynomial in C, so the four matrices B, B∗, C, C∗all commute with one another. Then
(BC)(BC)∗= BCC∗B∗= C∗B∗BC = (BC)∗(BC),
so BC is normal.
For A invertible but not normal, we can obtain a dual polar decomposition A = U1R1,
where R1 is positive definite and U1 is unitary. It suffices to take the conjugate-transpose
of the polar decomposition A∗= R2U2, which gives A = U ∗
2 R∗
2 where U ∗
2 is unitary and
R∗
2 = R2 is positive definite. As above, U1 and R1 are uniquely determined by A.
7.13
Polar Decomposition: General Case
The complex number 0 can be written in polar form as 0 = ru = ur, where r = 0 and u is
any complex number of modulus 1. Analogy suggests the following result.
Polar Decomposition Theorem for Non-Invertible Matrices. For any non-invertible
matrix A ∈Mn(C), there exist a positive semidefinite R and a unitary U such that A = UR,
where R (but not U) is uniquely determined by A.
Proof. Finding R and proving its uniqueness is not difficult, given what we did earlier for
invertible A: if A = UR is to hold, we must have A∗A = R∗U ∗UR = R2, forcing R to be
the unique positive semidefinite square root of the matrix A∗A (where A∗A is readily seen
to be positive semidefinite). However, finding U becomes trickier, since neither A nor R is
invertible in the present situation.
To proceed, we need the observation that R and A have the same null space. To see this,
fix v ∈Cn, and compute (using R2 = A∗A and R = R∗)
Rv = 0 ⇔||Rv||2 = 0 ⇔v∗R∗Rv = 0 ⇔v∗A∗Av = 0 ⇔||Av||2 = 0 ⇔Av = 0.
By the Rank–Nullity Theorem, the image of R and the image of A have the same dimension.
Let V ⊆Cn be the image of R and W ⊆Cn be the image of A. We define a map ϕ : V →W
as follows. Given z ∈V , write z = Rx for some x ∈Cn, and set ϕ(z) = Ax. We must check
that ϕ is well-defined, i.e., that the value ϕ(z) does not depend on the choice of x such that
Rx = z. If x′ is another vector such that Rx′ = z, then R(x−x′) = 0, hence A(x−x′) = 0,
so Ax = Ax′. Thus, ϕ is well-defined, and this map satisfies ϕ(Rx) = Ax for all x ∈Cn.
You can check that ϕ is a linear map. Furthermore, ϕ is length-preserving, since
||ϕ(Rx)||2 = ||Ax||2 = x∗A∗Ax = x∗R2x = x∗R∗Rx = ||Rx||2
for all x ∈Cn. We can write Cn = V ⊕V ⊥= W ⊕W ⊥(see Exercise 26). Extend ϕ to a
linear operator u on Cn by sending an orthonormal basis of V ⊥to an orthonormal basis
of W ⊥and extending by linearity; the operator u is readily seen to be length-preserving.
Let U be the matrix of u relative to the standard ordered basis of Cn, so that u(x) = Ux
192
Advanced Linear Algebra
(matrix-vector product) for all x ∈Cn. Since u is a length-preserving map, U is a length-
preserving (unitary) matrix. Furthermore, URx = u(Rx) = ϕ(Rx) = Ax for all x ∈Cn.
Thus, A = UR, and the proof is complete. Moreover, the proof reveals the extent of the
non-uniqueness of U: the action of U is forced on the image V of R =
√
A∗A, but U can
act as an arbitrary length-preserving map from V ⊥to W ⊥.
Reasoning as in the invertible case, UR = RU holds iff A is normal. By forming the
conjugate-transpose of the polar decomposition A∗= U2R2, we see that every A ∈Mn(C)
can be written as a product R1U1, where R1 =
√
AA∗is positive semidefinite and U1 is
unitary.
The polar decomposition can be written in the following equivalent form.
Singular Value Decomposition. Any matrix A ∈Mn(C) can be factored in the form
A = PDQ∗, where Q and P are unitary, and D is a diagonal matrix with nonnegative real
entries. This factorization is called the singular value decomposition of A.
Proof. Given A ∈Mn(C), start with a polar decomposition A = UR with U unitary and R
positive semidefinite. Unitarily diagonalize R, say D = Q∗RQ for some unitary matrix Q.
Then R = QDQ∗, P = UQ is a unitary matrix, and A = UR = UQDQ∗= PDQ∗.
In the singular value decomposition A = PDQ∗, the entries on the diagonal of D are the
eigenvalues of R (occurring with the correct multiplicities), and R is uniquely determined
by A. It follows that the diagonal entries of D (disregarding order) are uniquely determined
by the matrix A; these entries are called the singular values of A. By passing from matrices
to the linear maps they represent, we obtain the following geometric interpretation of the
matrix factorization A = PDQ∗. Every linear transformation on Cn is the composition of
an isometry (length-preserving linear map), followed by a rescaling of the coordinate axes by
nonnegative real numbers, followed by another isometry. The multiset of rescaling factors
is uniquely determined by the linear map. We give another proof of the singular value
decomposition, extended to rectangular matrices, in §9.12.
7.14
Interlacing Eigenvalues for Hermitian Matrices
Given a Hermitian matrix B ∈Mn(C) with n > 1, let A ∈Mn−1(C) be the submatrix
of B obtained by deleting the last row and column of B. Then A is also Hermitian, so all
eigenvalues of A and B are real numbers. The next theorem shows that each eigenvalue of
A is interleaved (or “interlaced”) between two consecutive eigenvalues of B.
Interlacing Theorem for Hermitian Matrices. Let B ∈Mn(C) be a Hermitian matrix
with eigenvalues b1 ≤b2 ≤· · · ≤bn. Let A be B with the last row and column deleted. Let
A have eigenvalues a1 ≤a2 ≤· · · ≤an−1. Then
b1 ≤a1 ≤b2 ≤a2 ≤b3 ≤a3 ≤· · · ≤bn−1 ≤an−1 ≤bn,
Proof. Step 1. We prove the theorem for all matrices B of the form
B =


a1
c1
a2
0
c2
0
...
...
an−1
cn−1
c1
c2
. . .
cn−1
an


,
(7.4)
Hermitian, Positive Definite, Unitary, and Normal Matrices
193
where A is diagonal, a1, . . . , an−1 are distinct and in increasing order, an ∈R, and every
ck ∈C is nonzero. Using the definition of determinants in §5.2, we see that any matrix of
the form (7.4) has characteristic polynomial
χB(t) = det(tIn −B) =
n
Y
k=1
(t −ak) −
n−1
X
k=1
(t −a1) · · · (t −ak−1)|ck|2(t −ak+1) · · · (t −an−1)
(7.5)
(see Exercise 66). We also know χB(t) = Qn
k=1(t −bk) is a monic polynomial of degree n
in R[t] whose roots are the eigenvalues of B (§5.15).
To proceed, we evaluate the polynomial χB(t) at t = aj, where 1 ≤j ≤n −1. Putting
t = aj in (7.5), the first product becomes 0, and all summands indexed by k with k ̸= j
also become 0 because of the factor (t −aj). Thus,
χB(aj) = −(aj −a1) · · · (aj −aj−1)|cj|2(aj −aj+1) · · · (aj −an−1).
Since the ak are distinct and occur in increasing order, and since |cj|2 ̸= 0, we see that χB(aj)
is negative for j = n−1, n−3, n−5, . . ., and χB(aj) is positive for j = n −2, n −4, n −6, . . ..
Since χB(t) is monic of degree n, limx→∞χB(x) = +∞, while limx→−∞χB(x) is +∞for
n even and is −∞for n odd. We illustrate this situation in Figure 7.1 in the cases n = 6
and n = 7. Since polynomial functions are continuous, we can apply the Intermediate Value
Theorem from calculus to conclude that each of the n intervals (−∞, a1), (ak, ak+1) for
1 ≤k < n −1, and (an−1, ∞) contains at least one root of χB(t). Since χB(t) has exactly
n real roots, we see that each interval contains exactly one eigenvalue of B. Therefore
b1 < a1 < b2 < a2 < · · · < bn−1 < an−1 < bn, as needed.
+
a1
a2
a3
a4
a5
+
_
(n=6)
+
_
+
_
+
a5
a1
a2
a3
a4
+
a6
(n=7)
_
+
_
+
_
_
FIGURE 7.1
Signs of χB(t) at the eigenvalues of A and in the limit as x →±∞.
Step 2. We generalize Step 1 by dropping the assumption that a1, . . . , an−1 are distinct.
We show that for each multiple eigenvalue of A of multiplicity s + 1, say aj = aj+1 = · · · =
aj+s, aj is an eigenvalue of B of multiplicity s. Inspection of the right side of (7.5) shows
that the polynomial χB(t) is divisible by (t −aj)s. On the other hand, dividing χB(t) by
(t −aj)s and then setting t = aj, we obtain
−
j+s
X
k=j
(ak −a1) · · · (ak −aj−1)|ck|2(ak −aj+s+1) · · · (ak −an−1)
= −(aj −a1) · · · (aj −aj−1)(aj −aj+s+1) · · · (aj −an−1)(|cj|2 + · · · + |cj+s|2) ̸= 0.
Thus, aj is a root of χB(t) of multiplicity s. The proof of Step 2 can now be completed by
a sign-change analysis similar to Step 1 (see Exercise 67).
194
Advanced Linear Algebra
Step 3. We generalize Step 2 by dropping the assumption that every ck be nonzero.
Proceed by induction on n ≥2. If n = 2 and c1 = 0, the eigenvalues of B are a1 and
a2 (in some order), and the interlacing inequalities immediately follow. If n > 2 and some
ck = 0, let B′ (resp. A′) be obtained from B (resp. A) by deleting row k and column k.
Since ck = 0, we see from (7.4) that χB(t) = (t −ak)χB′(t) and χA(t) = (t −ak)χA′(t). By
induction, the eigenvalues of B′ and A′ satisfy the required interlacing property. To obtain
the eigenvalues of B and A, we insert one more copy of ak into both lists of eigenvalues.
You can check that the interlacing property still holds for the new lists.
Step 4. We prove the result for general Hermitian B ∈Mn(C). As A is unitarily
diagonalizable, there is a unitary matrix U ∈Mn−1(C) such that U ∗AU is diagonal with
entries a1 ≤· · · ≤an−1 on the diagonal. Add a zero row at the bottom of U, and then add
the column en on the right end to get a unitary matrix V ∈Mn(C). The matrix V ∗BV
has the same eigenvalues as B and is of the form (7.4). By Step 3, the required interlacing
property holds.
7.15
Determinant Criterion for Positive Definite Matrices
Given A ∈Mn(C), we can quickly decide whether A is Hermitian (A∗= A), unitary (A∗A =
I), or normal (A∗A = AA∗) via matrix computations that do not require knowing the
eigenvalues of A. On the other hand, the definition of a positive definite matrix (v∗Av ∈R>0
for all nonzero v ∈Cn) cannot be checked so easily. The next theorem gives a determinant
condition on A that is necessary and sufficient for A to be positive definite. While computing
determinants of large matrices is time-consuming, it may be even harder to find all the
eigenvalues of A. The theorem uses the following notation. For k between 1 and n and
A ∈Mn(C), let A[k] be the matrix in Mk(C) consisting of the first k rows and k columns
of A.
Determinant Criterion for Positive Definite Matrices. For all A ∈Mn(C), A is
positive definite if and only if A∗= A and det(A[k]) is a strictly positive real number for
k = 1, 2, . . . , n.
Proof. First assume A ∈Mn(C) is positive definite, so A∗= A holds. Fix k between 1 and
n. We show that A[k] is positive definite, which implies det(A[k]) is real and positive. Fix
a nonzero v ∈Ck. Let w ∈Cn be v with n −k zeroes appended. Then v∗A[k]v = w∗Aw
is a positive real number. So A[k] is positive definite.
Next, assume B ∈Mn(C), B∗= B, and det(B[k]) ∈R>0 for 1 ≤k ≤n. We prove B
is positive definite by induction on n. In the base case n = 1, we see that B ∈M1(C) is
positive definite iff B(1, 1) ∈R>0 iff det(B[1]) ∈R>0. Fix n > 1, and assume the result
is known for matrices in Mn−1(C). Let A = B[n −1] ∈Mn−1(C) be obtained from B
by deleting the last row and column. Note A∗= A and det(A[k]) = det(B[k]) ∈R>0
for 1 ≤k ≤n −1, so the induction hypothesis shows that A is positive definite. Let the
eigenvalues of A be the positive real numbers a1 ≤a2 ≤· · · ≤an−1 and the eigenvalues of
B be the real numbers b1 ≤b2 ≤· · · ≤bn. By the Interlacing Theorem in §7.14, we know
b1 ≤a1 ≤b2 ≤a2 ≤· · · ≤an−1 ≤bn. Thus, every eigenvalue of B, except possibly b1,
is ≥a1 and hence is positive. On the other hand, det(B) = b1b2 · · · bn > 0 by hypothesis.
Since b2, . . . , bn > 0, it follows that b1 > 0 as well. Since B is normal and all its eigenvalues
are positive real numbers, B is positive definite. This completes the induction step.
Hermitian, Positive Definite, Unitary, and Normal Matrices
195
7.16
Summary
1.
Definitions. Given any complex matrix A, the conjugate-transpose A∗is the
matrix obtained by transposing A and replacing each entry by its complex
conjugate. A matrix A ∈Mn(C) is Hermitian iff A∗= A; A is unitary iff
A∗= A−1; A is normal iff AA∗= A∗A; A is positive definite iff v∗Av is a
positive real number for all nonzero v ∈Cn; A is positive semidefinite iff v∗Av
is a nonnegative real number for all v ∈Cn.
2.
Properties of Conjugate-Transpose. Whenever the matrix operations are defined,
the following identities hold: (A+B)∗= A∗+B∗; (cA)∗= c(A∗); (AB)∗= B∗A∗;
(A∗)∗= A; (A−1)∗= (A∗)−1; tr(A∗) = tr(A); det(A∗) = det(A); ⟨Av, w⟩=
w∗Av = ⟨v, A∗w⟩.
3.
Hermitian Matrices. The following conditions on a matrix A ∈Mn(C) are
equivalent: A is Hermitian; A = A∗; v∗Av is real for all v ∈Cn; A is normal
with all real eigenvalues. Real linear combinations of Hermitian matrices are
Hermitian, as are products of commuting Hermitian matrices. Every matrix
B ∈Mn(C) can be written uniquely in the form B = X + iY , where X and
Y are Hermitian matrices; X and Y commute iff B is normal. This is called the
Cartesian decomposition or Hermitian decomposition of B.
4.
Positive Definite Matrices. The following conditions on a matrix A ∈Mn(C)
are equivalent: A is positive definite; v∗Av is real and positive for all nonzero
v ∈Cn; A is normal with all positive real eigenvalues; A∗= A and for all k
between 1 and n, the matrix consisting of the first k rows and columns of A has
positive determinant. Positive linear combinations of positive definite matrices
are positive definite.
5.
Unitary Matrices. The following conditions on a matrix U in Mn(C) are
equivalent: U is unitary; UU ∗= I; U ∗U = I; U −1 = U ∗; U ∗is unitary; U −1 is
unitary; U T is unitary; the rows of U are orthonormal vectors in Cn; the columns
of U are orthonormal vectors in Cn; ⟨Uv, Uw⟩= ⟨v, w⟩for all v, w ∈Cn; U
preserves inner products; ||Uv|| = ||v|| for all v ∈Cn; U is length-preserving;
U is the matrix of a length-preserving linear map relative to an orthonormal
basis; U is normal with all eigenvalues having modulus 1 in C. For U unitary, we
have | det(U)| = 1 in C. Unitary similarity is an equivalence relation on square
matrices that corresponds to an orthonormal change of basis on the associated
vector space.
6.
Unitary Triangularization and Diagonalization. A matrix A ∈Mn(C) is unitarily
triangulable iff U ∗AU = U −1AU is upper-triangular for some unitary matrix U;
A is unitarily diagonalizable iff U ∗AU is diagonal for some unitary matrix U.
Schur’s Theorem says every matrix A ∈Mn(C) can be unitarily triangularized.
The Spectral Theorem says A can be unitarily diagonalized iff A is normal iff
there exist n orthonormal eigenvectors for A. In this case, the eigenvectors are
the columns of the diagonalizing matrix U, and the diagonal entries of U ∗AU are
the eigenvalues of A. The Spectral Theorem applies, in particular, to Hermitian,
positive definite, and unitary matrices (which are all normal).
7.
Simultaneous Triangularization and Diagonalization. A collection {Ai : i ∈I} of
n × n matrices can be simultaneously triangularized (resp. diagonalized) iff there
exists a single invertible matrix S (independent of i ∈I) such that S−1AiS is
upper-triangular (resp. diagonal) for all i ∈I. A commuting collection of n × n
196
Advanced Linear Algebra
complex matrices can be simultaneously unitarily triangularized. A collection of
normal matrices can be simultaneously unitarily diagonalized iff the matrices in
the collection commute.
8.
kth Roots of Matrices. For any even k ≥1, every positive definite matrix A has a
unique positive definite kth root, which is a positive definite matrix B such that
Bk = A. For any odd k ≥1, every Hermitian matrix A has a unique Hermitian
kth root. In each case, B = p(A) for some polynomial p; we can find B by unitarily
diagonalizing A and taking positive (resp. real) kth roots of the positive (resp.
real) diagonal entries.
9.
Polar Decomposition. For every matrix A in Mn(C), there exist a positive
semidefinite matrix R and a unitary matrix U such that A = UR. The matrix
R =
√
A∗A is always uniquely determined by A; U is unique if A is invertible. U
and R commute iff A is normal. There is a dual decomposition A = R′U ′ even
when A is not normal.
10.
Singular Value Decomposition. Every matrix A ∈Mn(C) can be written in the
form A = PDQ∗, where P and Q are unitary and D is a diagonal matrix with
nonnegative real entries. The entries of D are uniquely determined by A (up
to rearrangement); they are called the singular values of A. This result says
that every linear transformation of Cn is the composition of an isometry, then a
nonnegative rescaling of orthonormal axes, then another isometry.
11.
Characterizations of Normality. The following conditions on A in Mn(C) are
equivalent: A is normal; AA∗= A∗A; A is unitarily diagonalizable; A has
n orthonormal eigenvectors; the unique Hermitian matrices X and Y in the
Cartesian decomposition A = X + iY commute; A∗= p(A) for some polynomial
p; the matrices U and R in the polar decomposition A = UR commute.
12.
Interlacing Theorem. Given B ∈Mn(C) with B∗= B, let A be B with the last
row and column erased. If B has eigenvalues b1 ≤· · · ≤bn and A has eigenvalues
a1 ≤· · · ≤an−1, then b1 ≤a1 ≤b2 ≤a2 ≤· · · ≤bn−1 ≤an−1 ≤bn.
7.17
Exercises
1.
Prove the properties of complex conjugation stated in the introduction to this
chapter.
2.
Let A =

1 −i
0
2 + 2i
3
−1 + 3i
5i

and B =

i
−2 + i
1
1 + 3i

.
Compute A∗, B∗, AA∗, A∗A, (BA)∗, A∗B∗, B−1, (B∗)−1, and (B−1)∗.
3.
For A ∈Mm,n(C), define A ∈Mm,n(C) to be the matrix with i, j-entry A(i, j).
Which properties in the Theorem on the Conjugate-Transpose (§7.1) have analogs
for A? Prove your answers.
4.
Let QA be the quadratic form associated with a matrix A ∈Mn(C) (see §7.1).
(a) Prove: for all A ∈Mn(R), there exists a symmetric matrix B ∈Mn(R) with
QA = QB. (b) Does part (a) hold if we replace R by C? Explain. (c) Given
A ∈Mn(C), must there exist a Hermitian B ∈Mn(C) with QA = QB?
5.
Decide whether each matrix is Hermitian, unitary, positive definite, or normal
(select all that apply, and explain).
Hermitian, Positive Definite, Unitary, and Normal Matrices
197
(a) the n × n identity matrix
(b) the n × n zero matrix
(c)

0
i
−i
0

(d)

5
−
√
3
−
√
3
7

(e)

7
1
3
3

(f)


1/3
2/3
2/3
2/3
−2/3
1/3
2/3
1/3
−2/3


(g)


−1 + i
−4 −2i
6
−4 −2i
3
2 −2i
6
2 −2i
−2 −i


(h)


3
i
0
i
3
i
0
i
3


6.
For a 2 × 2 matrix A =
 a
b
c
d

with a, b, c, d ∈C, find algebraic conditions on
a, b, c, d that are equivalent to A being: (a) Hermitian (b) unitary (c) positive
definite (d) positive semidefinite (e) normal.
7.
For a fixed t ∈C, call a matrix A ∈Mn(C) t-Hermitian iff A∗= tA.
(a) Prove that the set of t-Hermitian matrices is a real subspace of Mn(C).
(b) Suppose A is t-Hermitian, B is s-Hermitian, and AB = BA. Prove AB is
st-Hermitian.
8.
(a) Prove: if A ∈Mn(C) is skew-Hermitian, then the associated quadratic form
Q takes values in {ib : b ∈R}. (b) Is the converse of (a) true? Prove or give a
counterexample. (c) Must a matrix with all pure imaginary eigenvalues be skew-
Hermitian? Explain.
9.
Find the Hermitian decomposition of each matrix.
(a)

2
5
−1
3

(b)

1 + 3i
2 −5i
3 + 4i
−1 −2i

(c)

3
7
7
3

(d)

2i
1 + 3i
−1 + 3i
−5i

(e)


0
i
2i
−i
0
3i
2i
3i
4


10.
Given nonzero v ∈Cn, let A = I −(2/v∗v)(vv∗) ∈Mn(C).
(a) Compute A for v = (1, 2) and v = (1, 7, 5, 5).
(b) Show that A is Hermitian and unitary.
(c) Show that Av = −v and Aw = w for all w ∈Cn with ⟨w, v⟩= 0.
(d) Describe all diagonal matrices unitarily similar to A.
11.
Let F be a field in which 1F + 1F ̸= 0F . Prove: for all A ∈Mn(F), there exist
unique B, C ∈Mn(F) with A = B +C, BT = B, and CT = −C. Does this result
hold when 1F + 1F = 0F ? Explain.
12.
Let A ∈Mn(C) be positive definite. (a) Show AT is positive definite. (b) Show
A−1 is positive definite. (c) For k ∈Z>0, must Ak be positive definite? Explain.
13.
Give an example of an n × n matrix A with n ≥2 satisfying the indicated
properties, or explain why no such example exists.
(a) A is positive definite, but A(i, j) is a negative real number for all i, j.
(b) A is positive definite, but A(i, j) is pure imaginary for all i ̸= j.
(c) A is positive semidefinite and negative semidefinite and nonzero.
(d) A is negative definite, but det(A) > 0.
(e) All eigenvalues of A are real and negative, but A is not negative definite.
14.
Suppose A ∈Mn(C) satisfies v∗Av ∈R>0 for all v in a fixed basis of Cn. Give
an example to show that A need not be positive definite.
15.
Suppose A ∈Mn(R) satisfies AT = A and vTAv ≥0 for all nonzero v ∈Rn.
Prove A is positive semidefinite. Give a specific example where this result fails if
we omit the hypothesis AT = A.
198
Advanced Linear Algebra
16.
Use the determinant criterion in §7.15 to find all c ∈C such that the following
matrices are positive definite.
(a)
 2
c
c
3

(b)


4
c
0
c
4
c
0
c
4


(c)


c
1
2
0
1
c
1
2
2
1
c
1
0
2
1
c


17.
Suppose A ∈Mn(C) satisfies A∗= A and det(A[k]) ≥0 for 1 ≤k ≤n. Give an
example to show that A need not be positive semidefinite.
18.
Suppose A ∈Mn(C) satisfies A∗= A, det(A[k]) > 0 for 1 ≤k < n, and det(A) ≥
0. Prove A is positive semidefinite.
19.
State and prove a determinant characterization of negative definite matrices.
20.
Suppose f : R2 →R has continuous second partial derivatives, and f has a local
minimum at (a, b) ∈R2. (a) Prove that A =

fxx(a, b)
fxy(a, b)
fyx(a, b)
fyy(a, b)

is positive
semidefinite. Deduce that fxxfyy −f 2
xy ≥0 and fxx, fyy ≥0 at (a, b). (For any
nonzero v = (v1, v2) ∈R2, study g(t) = f((a, b) + t(v1, v2)) for t ∈R.) (b) State
and prove a version of (a) for a local maximum of f.
21.
Generalize the previous exercise to functions f : Rn →R.
22.
How many diagonal unitary matrices are there in Mn(R)?
23.
Give an example of a unitary matrix U with each property, or explain why this
cannot be done. (a) U T = −U. (b) U(1, 1) = 2. (c) U ∗= U ̸= I. (d) U is
upper-triangular and not diagonal. (e) U is symmetric with all entries nonzero.
24.
Let U be any 2×2 real unitary matrix. Show that the first row of U has the form
(cos θ, sin θ) for some θ ∈[0, 2π). Given such a first row, find all possible second
rows of U.
25.
Let Z be any subspace of Cn. Prove there exists an orthonormal basis of Z by
induction on dim(Z). (Use the claim in §7.7.)
26.
Let W be a subspace of Cn with orthonormal basis (w1, . . . , wk). Define the
orthogonal complement W ⊥to be the set of z ∈Cn such that ⟨z, w⟩= 0 for all
w ∈W. Prove z ∈W ⊥iff ⟨z, wi⟩= 0 for 1 ≤i ≤k. Prove W ⊥is a subspace
of Cn. Prove W ∩W ⊥= {0}. Prove dim(W) + dim(W ⊥) = n. Conclude that
Cn = W ⊕W ⊥.
27.
Let W be a subspace of Cn with orthonormal basis (w1, . . . , wk). We can extend
this list to a basis (w1, . . . , wk, xk+1, . . . , xn) of Cn. For k + 1 ≤j ≤n, define
yj = xj −
j−1
X
r=1
(w∗
rxj)wr,
wj = yj/||yj||.
Use induction to prove these facts: (a) Each yj is nonzero. (b) ⟨yj, ws⟩= 0 for
all s < j. (c) (w1, . . . , wj) spans the same subspace as (w1, . . . , wk, xk+1, . . . , xj)
for k + 1 ≤j ≤n. (d) (w1, . . . , wn) is an orthonormal basis for Cn. So, every
orthonormal list in Cn can be extended to an orthonormal basis of Cn.
28.
Fix A ∈Mn(C). Prove: if there exists U ∈Un(C) such that A is similar to U,
then A−1 is similar to A∗.
29.
Prove or disprove the converse of the statement in the previous exercise.
30.
Find all A ∈Mn(C) such that A is the only matrix unitarily similar to A.
Hermitian, Positive Definite, Unitary, and Normal Matrices
199
31.
Given A ∈Mn(C) and f ∈Sn, define B ∈Mn(C) by B(i, j) = A(f(i), f(j)) for
all i, j between 1 and n. Prove A and B are unitarily similar.
32.
Given any subgroup H of GLn(C), call two matrices A, B ∈Mn(C) H-similar
iff B = S−1AS for some S ∈H. (a) Show that H-similarity is an equivalence
relation on Mn(C). (b) Give an example of subgroups H ̸= K in GLn(C) such
that H-similarity coincides with K-similarity.
33.
With as little calculation as possible, explain why each pair of matrices cannot be
unitarily similar. (a) A =

2
1
1
2

, B =
 −1
−8
1
5

. (b) A =
 0.6
−0.8
0.8
0.6

,
B =

−3.4
−1.6
10.4
4.6

. (c) A =

2
2
2
5

, B =

9
2
−12
−2

.
34.
Suppose e1 · · · ek is any finite sequence of 1s and ∗s. Prove: for all A, B ∈Mn(C),
if A is unitarily similar to B, then tr(Ae1Ae2 · · · Aek) = tr(Be1Be2 · · · Bek).
For example, if e1e2e3e4 = ∗1 ∗∗, the condition states that tr(A∗AA∗A∗) =
tr(B∗BB∗B∗). (It can be shown that, conversely, if the equality of traces holds
for all such sequences e1 · · · ek, then A and B are unitarily similar.)
35.
Prove that any normal matrix A ∈Mn(C) is unitarily similar to AT. Is every
A ∈Mn(C) unitarily similar to AT?
36.
For each matrix A, find a unitary U and an upper-triangular T with T = U ∗AU.
(a)

2.5
−0.5
0.5
3.5

(b)


5
−108
−36
39
−127
−26
−36
72
−25


(c)


4
1
0
0
1
4
1
0
0
1
4
1
0
0
1
4


37.
Define A ∈M5(C) by A(i, j) = i for 1 ≤i, j ≤5.
(a) Find a matrix S ∈GL5(C) such that S−1AS is diagonal.
(b) Find a matrix U ∈U5(C) such that U ∗AU is upper-triangular.
(c) Is there a matrix V ∈U5(C) such that V ∗AV is diagonal? Why?
38.
Give an example of a matrix A ∈M2(R) for which there does not exist any
unitary U ∈M2(R) with U ∗AU upper-triangular.
39.
Suppose A, B
∈
Mn(C) are unitarily similar. Prove Pn
i,j=1 |A(i, j)|2
=
Pn
i,j=1 |B(i, j)|2. (Look at the trace of AA∗.)
40.
Suppose A is unitarily similar to two upper-triangular matrices T1 and T2. Prove
P
i<j |T1(i, j)|2 = P
i<j |T2(i, j)|2.
41.
Give an example of upper-triangular matrices T1 and T2 in M3(C) such that T1
and T2 are unitarily similar, T1(i, i) = T2(i, i) for i = 1, 2, 3, but T1 ̸= T2.
42.
Give an example of upper-triangular matrices T1 and T2 with the same diagonal
such that P
i<j |T1(i, j)|2 = P
i<j |T2(i, j)|2, but T1 and T2 are not unitarily
similar.
43.
Let A ∈Mn(C) have eigenvalues c1, . . . , cn
∈C. Prove A is normal iff
Pn
i,j=1 |A(i, j)|2 = Pn
i=1 |ci|2.
44.
Which pairs of matrices can be simultaneously unitarily triangularized? Which
can be simultaneously unitarily diagonalized? Decide without calculating any
eigenvalues.
(a) A =

1
4
3
1

, B =

0
12
9
0

.
200
Advanced Linear Algebra
(b) A =
 3
−1
0
2

, B =
 1
1
1
0

.
(c) A =
 a
b
b
a

, B =
 c
d
d
c

, where a, b, c, d ∈R.
45.
For even k > 0, show that for each positive definite (resp. semidefinite) A ∈
Mn(C), there exists a unique positive definite (resp. semidefinite) B ∈Mn(C)
with Bk = A.
46.
For odd k > 0, show that for each Hermitian A ∈Mn(C), there exists a unique
Hermitian B ∈Mn(C) with Bk = A.
47.
Compute a numerical approximation to the unique positive definite square root
of each positive definite matrix.
(a)

5
−1
−1
5

(b)


3
2
1
2
3
2
1
2
3


(c)

2
1 + i
1 −i
2

48.
Prove A ∈Mn(C) is positive definite iff there is an invertible B ∈Mn(C) with
A = B∗B.
49.
(a) Give an example of a matrix A ∈M2(C) such that B2 = A has no solution
B ∈M2(C). (b) For each n > 2, find A ∈Mn(C) with no square root in Mn(C).
50.
Fix n > 1. (a) Show that In ∈Mn(C) has infinitely many square roots in Mn(C).
(b) Show that 0 ∈Mn(C) has infinitely many square roots in Mn(C).
51.
Let T ∈M2(C) be upper-triangular. Find all upper-triangular U ∈M2(C) with
U 2 = T. Do the same for M3(C). Do there exist matrices T, U such that U 2 = T,
T is upper-triangular, but U is not upper-triangular?
52.
Let T ∈Mn(C) be upper-triangular with distinct nonzero diagonal entries. Prove
there exist exactly 2n upper-triangular U ∈Mn(C) with U 2 = T.
53.
Prove that for n > 1, a matrix A ∈Mn(C) cannot have a finite odd number of
square roots in Mn(C).
54.
Find (with proof) an example of a matrix A ∈M2(C) that has exactly two square
roots in M2(C).
55.
(a) Suppose A ∈Mn(C) has eigenvalues c1, . . . , cn, B ∈Mn(C) has eigenvalues
d1, . . . , dn, and AB = BA. Prove there exists f ∈Sn such that A + B has
eigenvalues c1 + df(1), . . . , cn + df(n).
(b) With the setup in (a), what can you say about the eigenvalues of AB?
(c) Give an example to show that (a) can fail without the hypothesis AB = BA.
56.
For each matrix A, find a polynomial p ∈C[x] with A∗= p(A), or explain why
this is impossible. (a)


2
0
0
0
3 + 4i
0
0
0
2

(b)
 5
5
2
2

(c)

5
2
−2
5

(d)


1
1 + 2i
1
−1 + 2i
−1 −2i
1
−1 + 2i
−1
1
1 −2i
1
−1 −2i
1 −2i
−1
1 + 2i
1


57.
Prove: for any invertible A ∈Mn(C), A−1 = p(A) for some p ∈C[x].
58.
Suppose A ∈Mn(C) is normal. Must A = p(A) for some p ∈C[x]?
59.
Suppose A = p(A) for some p ∈C[x]. Must A be normal?
60.
Suppose A ∈Mn(C) is normal. Must AT = p(A) for some p ∈C[x]?
Hermitian, Positive Definite, Unitary, and Normal Matrices
201
61.
Suppose AT = p(A) for some p ∈C[x]. Must A be normal?
62.
Find a polar decomposition A = UR for each matrix A.
(a)
 1
1
1
−1

(b)
 2
2
2
2

(c)


−5
0
0
0
1
0
0
0
−7

(d)


0
0
0
2
0
0
0
1
0


63.
Find a singular value decomposition and the singular values for each matrix in
Exercise 62.
64.
Prove the following abstract version of the singular value decomposition: given a
linear map T : V →W between two n-dimensional complex inner product spaces,
there exist an orthonormal basis X = (x1, . . . , xn) for V and an orthonormal
basis Y = (y1, . . . , yn) for W and nonnegative real numbers c1, . . . , cn such that
T(xi) = ciyi for 1 ≤i ≤n.
65.
Prove this singular value decomposition theorem for rectangular matrices: given
A ∈Mm,n(C), there exist unitary matrices P ∈Mm(C) and Q ∈Mn(C) and a
matrix D ∈Mm,n(C) with D(i, i) ∈R≥0 for all i and D(i, j) = 0 for i ̸= j, such
that A = PDQ∗.
66.
Verify (7.5) on page 193.
67.
In Step 2 of §7.14, assume aj−1 < aj = · · · = aj+s < aj+s+1. Compute the sign
of lim
ϵ→0+ χB(aj −ϵ)/ϵs and the sign of lim
ϵ→0+ χB(aj+s + ϵ)/ϵs. Use this to complete
the proof of Step 2.
68.
For the matrix A =


1
4
0
0
4
7
−1
−5
0
−1
1
0
0
−5
0
5

, verify the Interlacing Theorem
in §7.14 by computing the eigenvalues of A[k] for k = 1, 2, 3, 4.
69.
True or false? Explain each answer. Assume matrices are in Mn(C) unless
otherwise stated.
(a) Every diagonal matrix is normal.
(b) A is positive semidefinite iff −A is negative semidefinite.
(c) For all A ∈Mm,n(C), AA∗is positive definite.
(d) For all A ∈Mm,n(C), A∗A is positive semidefinite.
(e) Every real symmetric matrix is unitarily diagonalizable.
(f) The product of two Hermitian matrices is always Hermitian.
(g) The product of two positive definite matrices is always positive definite.
(h) The product of two unitary matrices is always unitary.
(i) The square of a Hermitian matrix is always positive semidefinite.
(j) Every negative definite matrix has a negative definite inverse.
(k) The inverse of an invertible normal matrix must be normal.
(l) If A is normal, then p(A) is normal for all p ∈C[x].
(m) Every positive definite matrix has a unique square root.
(n) For fixed n > 1, In is the only matrix that is both unitary and Hermitian.
(o) A normal matrix with all entries in R>0 must be positive definite.
(p) If every complex eigenvalue of A has modulus 1, then A must be unitary.
(q) If A ∈M2(C) has A(1, 1) ≥0 and det(A) ≥0, then A must be positive
semidefinite.
8
Jordan Canonical Forms
Let V be an n-dimensional vector space over a field F. Given any linear operator T : V →V
and any ordered basis X of V , recall from Chapter 6 that there is a matrix A = [T]X in
Mn(F) that represents T relative to the basis X. If we switch from X to another ordered
basis Y , the matrix A is replaced by a similar matrix of the form [T]Y = P −1AP, where
P ∈Mn(F) is some invertible matrix (specifically, P is the matrix of idV relative to the
input basis Y and output basis X).
A fundamental question in linear algebra is how to pick the ordered basis Y to make
the matrix [T]Y as simple as possible. From the viewpoint of matrix computations, the
simplest n × n matrices are diagonal matrices D, which satisfy D(i, j) = 0F for all
i ̸= j. A linear map T is called diagonalizable iff there exists an ordered basis Y such
that [T]Y is a diagonal matrix. Regrettably, for n > 1, not all linear maps on V can
be diagonalized. For example, suppose T is a nilpotent linear operator, which means that
T k = T ◦T ◦· · · ◦T (k factors) = 0 for some positive integer k. Suppose [T]Y were a
diagonal matrix D. On one hand, [T k]Y = [0]Y = 0. On the other hand, [T k]Y = [T]k
Y = Dk.
Since D is diagonal, Dk = 0 forces every diagonal entry of D to be zero, so that D = 0 and
T = 0. But for n > 1, there are many examples of nonzero nilpotent linear maps (as we see
below). These maps cannot be diagonalized.
In this chapter, we prove that linear maps on a vector space using complex scalars (F =
C) can always be represented by certain nearly-diagonal matrices called Jordan canonical
forms. More precisely, for any field F, c ∈F, and m ∈Z>0, define the Jordan block
J(c; m) =


c
1
0
· · ·
0
0
c
1
· · ·
0
0
0
c
· · ·
0
0
0
0
· · ·
1
0
0
0
· · ·
c


m×m
.
This matrix has m diagonal entries equal to c, m−1 copies of 1F on the next higher diagonal,
and all other entries zero. When m = 1, J(c; 1) is the 1×1 matrix [c]. Next, define a Jordan
canonical form to be any matrix J that has the block-diagonal structure
J =


J(c1; m1)
0
. . .
0
0
J(c2; m2)
. . .
0
...
...
...
0
0
. . .
J(cs; ms)


(8.1)
for some m1, . . . , ms in Z>0 and c1, . . . , cs in F (repetitions may occur here). Writing
blk-diag(A1, . . . , As) to denote a block-diagonal matrix with diagonal blocks A1, . . . , As,
we have J = blk-diag(J(c1; m1), . . . , J(cs; ms)). We can now state our main result.
Jordan Canonical Form Theorem for Linear Maps. For any linear map T on a finite-
dimensional complex vector space V , there is an ordered basis X of V such that the matrix
J = [T]X is a Jordan canonical form. Furthermore, if Y is any ordered basis such that
DOI: 10.1201/9781003484561-8
202
Jordan Canonical Forms
203
J′ = [T]Y is also a Jordan canonical form, then J′ is obtained from J by rearranging the
Jordan blocks in (8.1).
Here is a reformulation of the theorem as a statement about matrices.
Jordan Canonical Form Theorem for Matrices. For any A ∈Mn(C), there exists an
invertible P ∈Mn(C) such that J = P −1AP is a Jordan canonical form. If J′ = Q−1AQ
is any Jordan canonical form similar to A, then J′ is obtained by rearranging the Jordan
blocks of J.
Our approach to proving the Jordan Canonical Form Theorems begins by proving the
following classification result for nilpotent linear maps over any field.
Theorem Classifying Nilpotent Linear Maps. Suppose V is an n-dimensional vector
space over any field F and T : V →V is a nilpotent linear map. There exist unique integers
m1 ≥m2 ≥· · · ≥ms > 0 such that for some ordered basis X of V ,
[T]X = blk-diag(J(0F ; m1), J(0F ; m2), . . . , J(0F ; ms)).
We prove this theorem by analyzing partition diagrams that give a geometric represen-
tation of the sequence (m1, . . . , ms). The next proof ingredient is Fitting’s Lemma, which
(roughly speaking) takes an arbitrary linear map T : V →V and breaks V into two pieces,
such that T restricted to one piece is nilpotent, and T restricted to the other piece is an
isomorphism. Combining this result with the classification of nilpotent maps, we obtain the
Jordan Canonical Form Theorems.
We continue by discussing methods for actually computing the Jordan canonical form
of a specific linear map or matrix. Then we describe an application of this theory to the
solution of systems of linear ordinary differential equations. We conclude by studying a more
abstract version of the Jordan canonical form that is needed in the study of Lie algebras.
Specifically, we prove that every linear map on a finite-dimensional complex vector space
V can be written uniquely as the sum of a diagonalizable linear map and a nilpotent linear
map that commute with each other.
8.1
Examples of Nilpotent Maps
We begin our analysis of nilpotent linear maps by constructing some specific examples of
nonzero nilpotent maps. Let F be any field and V be an F-vector space with ordered basis
X = (x1, . . . , xn). Recall that we can define an F-linear map T : V →V by choosing
any elements y1, . . . , yn ∈V , declaring that T(xj) = yj for 1 ≤j ≤n, and then setting
T(Pn
j=1 cjxj) = Pn
j=1 cjyj for all cj ∈F. In other words, we can build a unique linear map
by sending basis elements anywhere and then extending by linearity. Furthermore, writing
T(xj) = yj = Pn
i=1 aijxi for aij ∈F, we know that the matrix A = [T]X has i, j-entry aij.
To illustrate this procedure, suppose n = 5 and we decide that T(x1) = 0, T(x2) = x1,
T(x3) = x2, T(x4) = x3, and T(x5) = x4. The following arrow diagram illustrates the
action of T on the basis X:
0
T←x1
T←x2
T←x3
T←x4
T←x5.
For a general vector v = c1x1 + c2x2 + c3x3 + c4x4 + c5x5 with cj ∈F,
T(v) = c1 · 0 + c2x1 + c3x2 + c4x3 + c5x4.
204
Advanced Linear Algebra
Applying T again, we find that
T 2(v)
=
T(T(v)) = 0 + c3x1 + c4x2 + c5x3,
T 3(v)
=
c4x1 + c5x2,
T 4(v)
=
c5x1,
T 5(v)
=
0.
Since T 5(v) = 0 for all v ∈V , T 5 = 0 and T is nilpotent. On the other hand, the powers
T k for 1 ≤k < 5 are nonzero maps. In general, we say a nilpotent map T has index of
nilpotence m iff m is the least positive integer with T m = 0. The matrix of T relative to
the ordered basis X is


0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0


= J(0; 5).
Generalizing this example, for any ordered basis X = (x1, . . . , xn), we may define a
linear map T on V by setting T(x1) = 0 and T(xj) = xj−1 for 2 ≤j ≤n:
0
T←x1
T←x2
T←x3
T←· · ·
T←xn−1
T←xn.
Since T(x1) = 0 and T(xj) = 1xj−1+P
k̸=j−1 0xk for j > 1, we see that the matrix [T]X has
1F as its (j −1, j)-entry (for 2 ≤j ≤n) and 0F elsewhere. In other words, [T]X = J(0; n).
Extending by linearity, we have
T
 n
X
k=1
ckxk
!
=
n
X
k=2
ckxk−1
for all ck ∈F.
Iterating the map T gives
T i
 n
X
k=1
ckxk
!
=
n
X
k=i+1
ckxk−i
for all i ≥0.
We see that T i = 0 for all i ≥n, and T is nilpotent of index n.
Next, consider an example where n = 9 and T acts on basis vectors as shown here:
0
T←x1
T←x2
T←x3
0
T←x4
T←x5
0
T←x6
T←x7
0
T←x8
0
T←x9
(8.2)
You can check that [T]X = blk-diag(J(0; 3), J(0; 2), J(0; 2), J(0; 1), J(0; 1)), T ̸= 0, T 2 ̸= 0,
but T 3 = 0. So T is a nilpotent map of index 3.
For our next example, let n = 4 and define T on the basis by T(x1) = 0, T(x2) = x1,
T(x3) = 2x1, and T(x4) = 3x1. We see that T is nilpotent of index 2, and
[T]X =


0
1
2
3
0
0
0
0
0
0
0
0
0
0
0
0

.
This matrix is not a Jordan canonical form. However, if we let Z = (x1, x2, x3−2x2, x4−3x2),
then Z is an ordered basis of V with [T]Z = blk-diag(J(0; 2), J(0; 1), J(0; 1)).
Jordan Canonical Forms
205
8.2
Partition Diagrams
We intend to show that every nilpotent linear map on V can be described by an arrow
diagram like (8.2), if we choose the right ordered basis for V . To discuss these arrow diagrams
more precisely, we introduce the idea of partition diagrams.
A partition of an integer n ≥0 is a sequence µ = (µ1, µ2, . . . , µs), where each µi is a
positive integer, µ1 ≥µ2 ≥· · · ≥µs, and µ1 + µ2 + · · · + µs = n. We let ℓ(µ) be the length
of the sequence µ. For example, µ = (3, 2, 2, 1, 1) is a partition of n = 9 with ℓ(µ) = 5. For
any c ∈F and partition µ, define
J(c; µ) = blk-diag(J(c; µ1), J(c; µ2), . . . , J(c; µs)).
Given any partition µ, the diagram of µ is the set
D(µ) = {(i, j) ∈Z × Z : 1 ≤i ≤ℓ(µ), 1 ≤j ≤µi}.
We visualize D(µ) by drawing a picture with a box in row i and column j for each (i, j) in
D(µ). For example, the diagram of (3, 2, 2, 1, 1) is the set
{(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (3, 1), (3, 2), (4, 1), (5, 1)},
which is drawn as follows:
.
For all k ≥1, let µ′
k be the number of boxes in column k of the diagram of µ. In our
example, µ′
1 = 5, µ′
2 = 3, µ′
3 = 1, and µ′
k = 0 for all k > 3. In general, µ′
1 = ℓ(µ) and µ′
k = 0
for all k > µ1.
We can recover a partition µ from its diagram D(µ). A partition diagram D(µ) can
be reconstructed from the sequence of column lengths (µ′
1, µ′
2, . . .). We can compute these
column lengths if we are given the sequence of partial sums (µ′
1, µ′
1 + µ′
2, µ′
1 + µ′
2 + µ′
3, . . .),
where the kth entry in the sequence counts the number of boxes in the first k columns. So,
for any partitions µ and ν,
µ = ν
iff
for all k ≥1, µ′
1 + · · · + µ′
k = ν′
1 + · · · + ν′
k.
(8.3)
This observation is the key fact needed to establish the uniqueness properties of Jordan
canonical forms.
8.3
Partition Diagrams and Nilpotent Maps
Let V be an n-dimensional vector space over any field F. For each partition µ of n and
each ordered basis X = (x1, . . . , xn) of V , we can build a nilpotent linear map T = TX,µ
as follows. Fill the boxes in the diagram of µ with the elements of X, row by row, working
from left to right in each row. Define the map T by sending each basis vector in the leftmost
column to zero, sending every other basis vector to the element to its immediate left, and
206
Advanced Linear Algebra
extending by linearity. For example, given µ = (5, 5, 3, 1, 1, 1) and X = (x1, . . . , x16), we
first fill D(µ) as shown here:
x1 x2 x3 x4 x5
x6 x7 x8 x9 x10
x11 x12 x13
x14
x15
x16
Then TX,µ is the unique linear map on V that sends basis vectors x1, x6, x11, x14, x15, and
x16 to 0, x2 to x1, x7 to x6, x9 to x8, and so on. You can check that T is nilpotent of index
5, and [T]X = J(0; (5, 5, 3, 1, 1, 1)).
Let us describe the preceding construction more formally. Given a partition µ of n and
an ordered basis X = (x1, . . . , xn), we can view the ordered basis X as an indexed set
X = {x(i, j) : (i, j) ∈D(µ)} by setting x(i, j) = xj+µ1+µ2+···+µi−1 for all (i, j) ∈D(µ).
Here, x(i, j) is the basis vector placed in cell (i, j) of the diagram. We define T = TX,µ on
the basis X by setting T(x(i, 1)) = 0V for 1 ≤i ≤ℓ(µ) and T(x(i, j)) = x(i, j −1) for all
(i, j) ∈D(µ) with j > 1. Extending by linearity, we see that
T


X
(i,j)∈D(µ)
c(i, j)x(i, j)

=
ℓ(µ)
X
i=1
µi
X
j=2
c(i, j)x(i, j −1)
for all c(i, j) ∈F.
(8.4)
Returning to the ordered basis (x1, . . . , xn), we see that T sends x1, xµ1+1, xµ1+µ2+1, etc.,
to zero, and T sends every other xk to xk−1. This observation is equivalent to the statement
that [T]X = J(0; µ).
Our goal is to prove that for every nilpotent linear map T on V , there exists a unique
partition µ of n = dim(V ) such that [T]X = J(0; µ) for some ordered basis X of V . By
the preceding remarks, this is equivalent to proving that for every nilpotent linear map
T on V , there exists a unique partition µ and a (not necessarily unique) ordered basis X
with T = TX,µ. Before giving this proof, we show how the partition diagram D(µ) encodes
information about the image and null space of TX,µ and its powers.
8.4
Computing Images via Partition Diagrams
Given any linear map T : V →V , recall that the image of T (also called the range of T)
is the subspace img(T) = T[V ] = {T(v) : v ∈V }. The null space of T (also called the
kernel of T) is the subspace Null(T) = ker(T) = {v ∈V : T(v) = 0V }. Given a nilpotent
map of the special form T = TX,µ, let us see how to use the diagram of µ to compute the
dimensions of img(T k) and Null(T k) for all k ≥1. We use µ = (5, 5, 3, 1, 1, 1) as a running
example to illustrate the general discussion.
We begin by giving a visual representation of the formula (8.4) defining TX,µ. Represent
a vector v = P
(i,j)∈D(µ) c(i, j)x(i, j) in V by filling each cell (i, j) ∈D(µ) with the scalar
c(i, j) = ci,j ∈F that multiplies the basis vector x(i, j) indexed by that cell. Applying T
shifts all these coefficients one cell to the left, with coefficients c(i, 1) falling off the left edge
Jordan Canonical Forms
207
and new zero coefficients coming into the right cell of each row. For example,
T












c1,1c1,2c1,3c1,4c1,5
c2,1c2,2c2,3c2,4c2,5
c3,1c3,2c3,3
c4,1
c5,1
c6,1












=
c1,2c1,3c1,4c1,5 0
c2,2c2,3c2,4c2,5 0
c3,2c3,3 0
0
0
0
.
(8.5)
It is immediate from this picture that the image of T consists of all F-linear combinations
of the linearly independent basis vectors
{x(1, 1), x(1, 2), x(1, 3), x(1, 4), x(2, 1), x(2, 2), x(2, 3), x(2, 4), x(3, 1), x(3, 2)},
so that this set is a basis of img(T). In general, we see pictorially or via (8.4) that the set
{x(i, j −1) : 1 ≤i ≤ℓ(µ), 2 ≤j ≤µi} = {x(i, j) : (i, j) ∈D(µ) and (i, j + 1) ∈D(µ)}
is a basis for img(TX,µ). These are the basis vectors occupying all cells of D(µ) excluding
the rightmost cell in each row.
What happens if we apply T again? In our example,
T 2












c1,1c1,2c1,3c1,4c1,5
c2,1c2,2c2,3c2,4c2,5
c3,1c3,2c3,3
c4,1
c5,1
c6,1












=
c1,3c1,4c1,5 0
0
c2,3c2,4c2,5 0
0
c3,3 0
0
0
0
0
.
So img(T 2) consists of all F-linear combinations of x(1, 1), x(1, 2), x(1, 3), x(2, 1), x(2, 2),
x(2, 3), and x(3, 1), and these vectors form a basis for this subspace. Similarly, for any X
and µ, a basis for img(T 2
X,µ) is the set of all x(i, j) occupying cells (i, j) ∈D(µ) excluding
the two rightmost cells in each row.
Continuing to iterate (8.4), we see that for each positive integer k,
T k
X,µ


X
(i,j)∈D(µ)
c(i, j)x(i, j)

=
X
(i,j)∈D(µ):(i,j−k)∈D(µ)
c(i, j)x(i, j −k)
=
X
(i,j)∈D(µ):(i,j+k)∈D(µ)
c(i, j + k)x(i, j)
for all c(i, j) ∈F,
(8.6)
which shows that {x(i, j) : (i, j) ∈D(µ) and (i, j + k) ∈D(µ)} is an F-basis for img(T k
X,µ).
We get this basis by ignoring the k rightmost cells in each row of D(µ) and taking all
remaining x(i, j). In particular, taking k = µ1 −1 and k = µ1 in (8.6) shows that TX,µ is a
nilpotent map of index µ1.
208
Advanced Linear Algebra
8.5
Computing Null Spaces via Partition Diagrams
Next, let us see how to use partition diagrams to find bases for the null spaces
Null(T k
X,µ). For our running example of µ = (5, 5, 3, 1, 1, 1), inspection of (8.5) reveals
that v = P
(i,j)∈D(µ) c(i, j)x(i, j) is sent to zero by T iff
c(1, 2) = c(1, 3) = c(1, 4) = c(1, 5)
=c(2, 2) = c(2, 3) = c(2, 4) = c(2, 5)
=c(3, 2) = c(3, 3) = 0.
Pictorially,
Null(T) =























c1,1 0
0
0
0
c2,1 0
0
0
0
c3,1 0
0
c4,1
c5,1
c6,1
: c(i, 1) ∈F























.
(8.7)
In general, Null(TX,µ) consists of all F-linear combinations of basis vectors residing in the
leftmost column of D(µ), so that {x(i, 1) : 1 ≤i ≤ℓ(µ) = µ′
1} is a basis for Null(TX,µ).
Similarly, applying T 2 to v produces output zero iff c(i, j) = 0 for all j > 2, so that a
basis of Null(T 2) consists of all basis vectors in the first two columns of D(µ). In general,
for all k ≥1, the set
{x(i, j) : (i, j) ∈D(µ) and 1 ≤j ≤k}
(8.8)
is a basis for Null(T k
X,µ), and therefore
dim(Null(T k
X,µ)) = µ′
1 + µ′
2 + · · · + µ′
k
for all k ≥1.
(8.9)
To prove this assertion without pictures, note from (8.6) that T k
X,µ(v) = 0 iff c(i, j) = 0 for
all (i, j) ∈D(µ) such that (i, j −k) ∈D(µ), which holds iff v is a linear combination of
those x(i, j) satisfying (i, j) ∈D(µ) and 1 ≤j ≤k.
8.6
Classification of Nilpotent Maps (Stage 1)
We are now ready to prove the following classification theorem.
Theorem Classifying Nilpotent Linear Maps. Suppose V is an n-dimensional vector
space over any field F and T : V →V is a nilpotent linear map. There exists a unique
partition µ of n such that [T]X = J(0; µ) for some ordered basis X of V .
As we saw at the end of §8.3, the conclusion is equivalent to the existence of a unique
partition µ such that for some ordered basis X of V , T = TX,µ.
We can prove uniqueness of µ very quickly. Suppose T : V →V is nilpotent, and
T = TX,µ = TY,ν for some partitions µ and ν and some ordered bases X and Y of V . For
all k ≥1, (8.9) shows that
µ′
1 + · · · + µ′
k = dim(Null(T k)) = ν′
1 + · · · + ν′
k.
So µ = ν follows from (8.3).
Jordan Canonical Forms
209
Proving existence of µ and X is more subtle. We use induction on n = dim(V ). For the
base case n = 0, we must have V = {0} and T = 0. Then T = TX,µ holds if we choose
X = ∅and µ = (), which is a partition of n = 0 of length zero. If you prefer to start the
induction at n = 1, you can check that for n = 1 we have T = TX,µ, where X = {x} is any
nonzero vector in V and µ = (1) is the unique partition of 1. Incidentally, this shows that
the ordered basis X is not unique, in general.
In the rest of the proof, we assume n = dim(V ) > 1 and that the existence assertion is
already known to hold for all F-vector spaces of smaller dimension than n. Given a nilpotent
linear map T : V →V , we build µ and X such that T = TX,µ in three stages. In Stage 1,
we consider the subspace V1 = img(T) = T[V ]. Since dim(V ) > 0, we cannot have V1 = V ;
otherwise T[V ] = V implies T k[V ] = V ̸= {0} for all k > 0, contradicting the nilpotence of
T. Furthermore, T maps V1 = T[V ] into itself, so we can consider the restricted function
T|V1 : V1 →V1, given by T|V1(v) = T(v) for all v ∈V1. The map T|V1 is linear and
nilpotent. Since V1 is a proper subspace of V , we know n1 = dim(V1) < dim(V ), and the
induction hypothesis gives us an ordered basis X1 of V1 and a partition ν = (ν1, . . . , νs) of
n1 such that T|V1 = TX1,ν. For example, D(ν) and X1 might look like this:
D(ν) =
0
T |V1
←x1,1
T |V1
←x1,2
T |V1
←x1,3
T |V1
←x1,4
0
T |V1
←x2,1
T |V1
←x2,2
T |V1
←x2,3
T |V1
←x2,4
0
T |V1
←x3,1
T |V1
←x3,2
8.7
Classification of Nilpotent Maps (Stage 2)
Continuing the proof, our goal in the induction step is to prove that T : V →V has the
form TX,µ for some partition µ of n. If this conclusion were true, we know from §8.4 that
D(ν) consists of all cells of D(µ) excluding the rightmost cell of each row. Stage 2 of the
proof is to recover the rows of the unknown partition µ of length 1, if there are any.
To do this, we study V2 = V1 + Null(T) = {u + v : u ∈V1, v ∈Null(T)}, which is a
subspace of V containing V1 = img(T). Write n2 = dim(V2). The subspace V2 is spanned by
X1 ∪Null(T), so we can extend the ordered basis X1 for V1 to an ordered basis X2 for V2
by adding appropriate vectors from Null(T) to the end of the list X1. Suppose t additional
vectors are needed, so n2 = n1 + t. Let ρ be the partition of n2 obtained by adding t parts
of size 1 to the end of ν. Note that T|V2 is nilpotent and maps V2 to itself (in fact, it maps
V2 into V1). We can see that T|V2 = TX2,ρ since both sides are linear maps having the
same effect on basis vectors in X2; in particular, both sides send all new vectors in X2 \ X1
to zero. For example, D(ρ) and X2 might look like this, where new boxes added to ν are
marked with stars:
D(ρ) =
⋆
⋆
⋆
0
T |V2
←x1,1
T |V2
←x1,2
T |V2
←x1,3
T |V2
←x1,4
0
T |V2
←x2,1
T |V2
←x2,2
T |V2
←x2,3
T |V2
←x2,4
0
T |V2
←x3,1
T |V2
←x3,2
0
T |V2
←x4,1
0
T |V2
←x5,1
0
T |V2
←x6,1
210
Advanced Linear Algebra
We claim that n2 = n −s, where s = ℓ(ν) is the number of rows in D(ν). To prove the
claim, recall that V2 = img(T) + Null(T), so (using Exercise 31)
n2 = dim(V2) = dim(img(T)) + dim(Null(T)) −dim(img(T) ∩Null(T)).
(8.10)
By the Rank–Nullity Theorem (see §1.8), dim(img(T)) + dim(Null(T)) = dim(V ) = n. On
the other hand, Null(T|V1) = V1 ∩Null(T) = img(T) ∩Null(T). Applying (8.9) to the map
T|V1 = TX1,ν (with k = 1), we get
dim(img(T) ∩Null(T)) = dim(Null(T|V1)) = ν′
1 = ℓ(ν) = s.
Putting these formulas into (8.10) gives n2 = n −s, as claimed.
8.8
Classification of Nilpotent Maps (Stage 3)
We can now find a partition µ of n and an ordered basis X of V for which T = TX,µ. Define
µ by adding 1 to each of the first s parts of ρ, so µ = (ν1 + 1, ν2 + 1, . . . , νs + 1, 1, . . . , 1)
and µ ends with t parts equal to 1. Extend the indexed basis X2 = {x(i, j) : (i, j) ∈D(ρ)}
to an indexed set X = {x(i, j) : (i, j) ∈D(µ)} by letting x(i, µi) be any vector in V with
T(x(i, µi)) = x(i, µi −1), for 1 ≤i ≤s. Such vectors must exist, since (i, µi −1) ∈D(ν)
means that x(i, µi −1) ∈X1 ⊆img(T). For example, D(µ) and X might look like this,
where new boxes added to ρ are marked with stars:
D(µ) =
⋆
⋆
⋆
0
T←x1,1
T←x1,2
T←x1,3
T←x1,4
T←x1,5
0
T←x2,1
T←x2,2
T←x2,3
T←x2,4
T←x2,5
0
T←x3,1
T←x3,2
T←x3,3
0
T←x4,1
0
T←x5,1
0
T←x6,1
If we can show that the indexed set X is a basis of V , then T = TX,µ follows since both linear
maps agree on all basis vectors in X. By the claim in the last section, µ is a partition of
n2+s = n = dim(V ). Hence, if we can show that the indexed set X = {x(i, j) : (i, j) ∈D(µ)}
is linearly independent, then the n vectors x(i, j) must be distinct and form a basis of V .
To check linear independence, assume that for some scalars c(i, j) ∈F,
0 =
X
(i,j)∈D(µ)
c(i, j)x(i, j).
We must show every c(i, j) is zero. Applying the linear map T gives
0 =
X
(i,j)∈D(µ):(i,j+1)∈D(µ)
c(i, j + 1)x(i, j) =
X
(i,j)∈D(ν)
c(i, j + 1)x(i, j)
(see (8.5) and (8.6)). Since X1 = {x(i, j) : (i, j) ∈D(ν)} is known to be linearly independent
already (by induction), we see that c(i, j + 1) = 0 for all (i, j) ∈D(ν). Our original linear
combination now reduces to
0 =
ℓ(µ)
X
i=1
c(i, 1)x(i, 1) =
X
(i,j)∈D(ρ):j=1
c(i, j)x(i, j).
Jordan Canonical Forms
211
But X2 = {x(i, j) : (i, j) ∈D(ρ)} is linearly independent by construction, so we deduce
that c(i, 1) = 0 for all i. Hence the indexed set X is linearly independent, and the existence
proof is finally complete.
Here is one corollary of the classification of nilpotent linear maps. If V is n-dimensional
and T : V →V is a nilpotent linear map, then T n = 0 (so T has index of nilpotence at
most n). To see why this holds, note T = TX,µ for some ordered basis X and some partition
µ of n. The diagram D(µ) can have at most n nonempty columns. By (8.9), Null(T n) has
dimension µ′
1 + · · · + µ′
n = n, so Null(T n) must be all of V . This means that T n = 0, as
claimed.
8.9
Fitting’s Lemma
We prove the existence part of the Jordan Canonical Form Theorem by combining the
classification of nilpotent linear maps with a result called Fitting’s Lemma. Before stating
this result, we need some preliminary concepts. Let V be a vector space over a field F.
Given two subspaces W and Z of V , the sum of these subspaces is the subspace W + Z =
{w +z : w ∈W, z ∈Z}. We say this sum is a direct sum, denoted W ⊕Z, iff W ∩Z = {0V }.
You can check that for a direct sum W ⊕Z, if X1 is an ordered basis of W and X2 is an
ordered basis of Z, then the list consisting of the vectors in X1 followed by the vectors in
X2 is an ordered basis of W ⊕Z. In particular, dim(W ⊕Z) = dim(W) + dim(Z) when W
and Z are finite-dimensional.
Given any linear map S : V →V , we say a subspace W of V is S-invariant iff S(w) ∈W
for all w ∈W. In this situation, the restriction of S to W, denoted S|W, maps W into
itself and is a linear map from W to W. Now assume n = dim(V ) is finite. Recall that
Null(S) and img(S) are always subspaces of V , and by the Rank–Nullity Theorem in §1.8,
dim(Null(S)) + dim(img(S)) = n = dim(V ). If Null(S) ∩img(S) = {0}, then we have a
direct sum Null(S) ⊕img(S) of dimension n, which must therefore be the entire space V .
However, there is no guarantee that Null(S) and img(S) have zero intersection. For example,
if S is the nilpotent map defined in (8.5), then Null(S)∩img(S) is a 3-dimensional subspace
spanned by x(1, 1), x(2, 1), and x(3, 1).
We can rectify this situation by imposing an appropriate hypothesis on S. Specifically,
if S : V →V is a linear map such that Null(S) = Null(S2), then V = Null(S)⊕img(S). By
the comments in the preceding paragraph, it is enough to show that Null(S)∩img(S) = {0}.
Suppose z ∈Null(S) ∩img(S). On one hand, S(z) = 0. On the other hand, z = S(y) for
some y ∈V . Now, S2(y) = S(S(y)) = S(z) = 0 means that y ∈Null(S2) = Null(S). Then
z = S(y) = 0, so Null(S) ∩img(S) = {0}.
Fitting’s Lemma: Given an n-dimensional F-vector space V and a linear map U : V →V ,
there exist U-invariant subspaces Z and W of V such that V = Z ⊕W, U|Z is nilpotent,
and U|W is an isomorphism.
In the case where Null(U) = {0}, we have dim(img(U)) = n by the Rank–Nullity
Theorem, so img(U) = V and U is an isomorphism. So we may take Z = {0} and W = V
in this situation. Next, consider the case where Null(U) is a nonzero subspace of V . Since
Null(U k) ⊆Null(U k+1) for all k ≥1, we have a chain of subspaces
{0} ̸= Null(U) ⊆Null(U 2) ⊆Null(U 3) ⊆· · · ⊆Null(U k) ⊆· · · ⊆V.
All these subspaces are finite-dimensional, so there is some m ≥1 with Null(U k) = Null(U m)
for all k ≥m. In particular, taking k = 2m, we can apply our earlier result to the
212
Advanced Linear Algebra
linear map S = U m to conclude that V = Null(S) ⊕img(S) = Null(U m) ⊕img(U m).
Let Z = Null(U m) ̸= {0} and W = img(U m). Given z ∈Z, we know U m(z) = 0, so
U m(U(z)) = U(U m(z)) = U(0) = 0, so U(z) ∈Z. Given w ∈W, we know w = U m(v)
for some v ∈V , so U(w) = U(U m(v)) = U m(U(v)) ∈W. Thus Z and W are U-invariant
subspaces. By the very definition of Z, U|Z is a nilpotent linear map of index at most m.
On the other hand, suppose w ∈W is in Null(U|W). Then U(w) = 0, hence U m(w) = 0
and w ∈Null(U m) ∩img(U m) = {0}. So ker(U|W) = {0}, which means U|W : W →W is
injective and hence surjective. Thus, U|W is an isomorphism.
8.10
Existence of Jordan Canonical Forms
In this section, we prove the existence assertion in the Jordan Canonical Form Theorem.
Before doing so, we need some basic facts about eigenvalues. Given a field F and a matrix
A ∈Mn(F), a scalar c ∈F is called an eigenvalue of A iff Av = cv for some nonzero n × 1
column vector v. Any such v is called an eigenvector of A associated with the eigenvalue c.
The eigenvalues of A are the roots of the characteristic polynomial χA = det(xIn−A) ∈F[x]
in the field F. This polynomial has degree n, so A has at most n distinct eigenvalues in F.
For the field F = C of complex numbers, the polynomial χA always has a complex root by
the Fundamental Theorem of Algebra. So every n×n complex matrix A has between 1 and
n complex eigenvalues. For A ∈Mn(F), let Spec(A) (the spectrum of A) be the set of all
eigenvalues of A. When A is triangular, Spec(A) is the set of scalars appearing on the main
diagonal of A.
Suppose V is an n-dimensional F-vector space, and T : V →V is a linear map. An
eigenvalue of T is a scalar c ∈F such that T(v) = cv for some nonzero v ∈V . Any such v
is called an eigenvector of T associated with the eigenvalue c. If X is any ordered basis of
V and A = [T]X is the matrix of T relative to X, then T and A have the same eigenvalues.
This follows since T(v) = cv iff A[v]X = c[v]X, where [v]X is the column vector giving
the coordinates of v relative to X. In particular, every linear map T on an n-dimensional
complex vector space has between 1 and n complex eigenvalues. Let Spec(T) (the spectrum
of T) be the set of all eigenvalues of T.
Jordan Canonical Form Theorem (Existence Part).
For every finite-dimensional
complex vector space V and every linear map T : V →V , there exists an ordered basis X
of V such that [T]X is a Jordan canonical form.
The proof is by induction on n = dim(V ). For n = 0 and n = 1, the result follows
immediately. Now assume n > 1 and the result is already known for all complex vector
spaces of smaller dimension than n.
Pick a fixed eigenvalue c of the given linear map T, and let v ̸= 0 be an associated
eigenvector. Let U = T −c idV , where idV denotes the identity map on V . Applying Fitting’s
Lemma to the linear map U, we get a direct sum V = Z ⊕W where Z and W are U-
invariant (hence also T-invariant) subspaces of V such that U|Z is nilpotent and U|W is an
isomorphism. On one hand, U|Z is nilpotent, so we know there is an ordered basis X1 of Z
and a partition µ of the integer k = dim(Z) such that [U|Z]X1 = J(0; µ). It follows that
[T|Z]X1 = [U|Z + c idZ]X1 = [U|Z]X1 + [c idZ]X1 = J(0; µ) + cIk = J(c; µ).
On the other hand, v cannot belong to W, since otherwise U|W(v) = U(v) = 0 = U|W(0)
contradicts the fact that U|W is an isomorphism. So W ̸= V and dim(W) < dim(V ). By
the induction hypothesis, there exists an ordered basis X2 of W such that [U|W]X2 is a
Jordan Canonical Forms
213
Jordan canonical form matrix J1. By the same calculation used above, we see that [T|W]X2
is the matrix J2 obtained from J1 by adding c to every diagonal entry. This new matrix is
also a Jordan canonical form. Finally, taking X to be the concatenation of X1 and X2, we
know X is an ordered basis of V such that [T]X = blk-diag(J(c; µ), J2). This matrix is a
Jordan canonical form, so the induction proof is complete.
The existence of Jordan canonical forms for linear maps implies the existence of Jordan
canonical forms for matrices, as follows. Given A ∈Mn(C), let T : Cn →Cn be the linear
map defined by T(v) = Av for all column vectors v ∈Cn. Choose an ordered basis X of Cn
such that J = [T]X is a Jordan canonical form. We know J = P −1AP for some invertible
P ∈Mn(C), so A is similar to a Jordan canonical form.
The only special feature of the field C needed in this proof was that every linear map
on a nonzero finite-dimensional C-vector space has an eigenvalue in C. This follows from
the fact that for F = C, all non-constant polynomials in F[x] split into products of linear
factors. Any field F with this property is called algebraically closed. The Jordan Canonical
Form Theorem holds for vector spaces with scalars coming from any algebraically closed
field.
8.11
Uniqueness of Jordan Canonical Forms
Jordan Canonical Form Theorem (Uniqueness Part). Suppose V is an n-dimensional
complex vector space, T : V →V is a linear map, and X and Y are ordered bases of V
such that A = [T]X and B = [T]Y are both Jordan canonical forms. Then B is obtained
from A by rearranging the Jordan blocks in A.
Before proving this statement, we consider an example that conveys the idea of the
proof. Suppose X = (x1, . . . , x14) and A = [T]X is the Jordan canonical form
blk-diag(J(7; 2), J(7; 2), J(7; 1), J(−4; 5), J(i; 3), J(i; 1)).
This matrix is triangular, so listing entries on the main diagonal gives Spec(T) = Spec(A) =
{7, −4, i}. Some eigenvectors of A and T associated with the eigenvalue 7 are x1, x3, and
x5. In fact, you can check that the set of all eigenvectors for this eigenvalue is the set of
all nonzero C-linear combinations of x1, x3, and x5. On the other hand, x2 and x4 are not
eigenvectors for the eigenvalue 7, even though these basis vectors are associated with Jordan
blocks with 7 on the diagonal.
The key to the uniqueness proof is that the subspace of V spanned by x1, x2, x3, x4, x5
can be described using just the linear map T, not the ordered basis X or the matrix A. To
see how this is done, consider the linear map U = T −7 idV . The matrix of U relative to X
is
C = [U]X = A−7I14 = blk-diag(J(0; 2), J(0; 2), J(0; 1), J(−11; 5), J(−7+i; 3), J(−7+i; 1)).
We can also write C = blk-diag(J(0; µ), C1), where µ = (2, 2, 1) and C1 is a triangular
matrix with all diagonal entries nonzero. What is the null space of C14? We compute
C14 = blk-diag(05×5, C2), where C2 = C14
1
is also a triangular matrix with all diagonal
entries nonzero. It follows that Null(C14) consists of all column vectors v ∈C14 with
v1, . . . , v5 arbitrary and v6 = v7 = · · · = v14 = 0. Translating back to the linear map
T, this means that Null((T −7 idV )14) is the subspace of V with basis (x1, x2, x3, x4, x5).
Since the restriction of T −7 idV to this subspace is nilpotent, we can appeal to the known
uniqueness result for nilpotent maps to see that the partition µ = (2, 2, 1) is unique.
214
Advanced Linear Algebra
We now turn to the general uniqueness proof. Assume the setup in the theorem
statement. First, Spec(A) = Spec(T) = Spec(B), so that A and B have the same set
of diagonal entries (ignoring multiplicities), namely the set of eigenvalues of the map T.
Let c ∈Spec(T) be any fixed eigenvalue. Write U = T −c idV . Solely for notational
convenience, we can reorder the ordered basis X and the ordered basis Y so that all the
Jordan blocks for c in A and B occur first, with the block sizes weakly decreasing, say
A = blk-diag(J(c; µ), A′) for some partition µ of some k and B = blk-diag(J(c; ν), B′) for
some partition ν of some m. Since c is fixed but arbitrary, uniqueness follows if we can show
k = m and µ = ν. Writing the reordered bases as X = (x1, . . . , xn) and Y = (y1, . . . , yn),
we claim that X1 = (x1, . . . , xk) and Y1 = (y1, . . . , ym) are both bases for the same subspace
Z = Null(U n) of V . Assuming this claim is true, it follows that k = m, [U|Z]X1 = J(0; µ),
and [U|Z]Y1 = J(0; ν). Since U|Z is a nilpotent linear map, our previously proved uniqueness
result for nilpotent operators allows us to conclude that µ = ν.
So we need only prove the claim that X1 = (x1, . . . , xk) is a basis for Z = Null(U n)
(the corresponding assertion for Y1 is proved in the same way). Since U = T −c idV ,
[U]X = blk-diag(J(0; µ), A′−cIn−k) where A′−cIn−k is an upper-triangular matrix with no
zeroes on its diagonal (because all Jordan blocks for c occur in the first k rows of A). Taking
the nth power of U, we get [U n]X = [U]n
X = blk-diag(0k×k, A′′), where A′′ ∈Mn−k(C) is
triangular with nonzero entries on its diagonal. From the form of the matrix [U n]X, we
see that U n sends x1, . . . , xk, and every linear combination of these vectors to zero. So the
span of X1 is contained in Null(U n) = Z. For the reverse inclusion, suppose v = Pn
i=1 cixi
(where ci ∈C) is not in the span of X1. This means there exists s > k with cs ̸= 0; choose
the largest index s with this property. Since [U n]X is upper-triangular with a nonzero entry
in the s, s-position, there are scalars di with U n(v) = P
i<s dixi + csA′′(s, s)xs ̸= 0. Hence
v ̸∈Z, and Z is the subspace spanned by the list X1. Since this list is linearly independent
(being a sublist of the ordered basis X), X1 is an ordered basis of Z.
The uniqueness of Jordan canonical forms for linear maps implies the uniqueness result
for matrices, as follows. Given A ∈Mn(C), let T : Cn →Cn be the linear map T(v) = Av
for v ∈Cn. The set of matrices similar to A is exactly the set of matrices [T]X as X
ranges over all ordered bases of Cn (see Chapter 6). So all Jordan canonical forms in the
similarity class of A represent the map T relative to appropriate ordered bases, and these
forms therefore differ from one another only by rearranging the Jordan blocks.
8.12
Computing Jordan Canonical Forms
In this section, we discuss how to compute a Jordan canonical form of a specific matrix
A ∈Mn(C). More specifically, we want to find a Jordan canonical form J ∈Mn(C) and
an invertible matrix P with J = P −1AP. If all we need is the matrix J, we can proceed
as follows. Examining the uniqueness proof given above, we observe first that the diagonal
entries of J must be the eigenvalues of A. These eigenvalues can be found, in principle, by
computing the roots of the characteristic polynomial of A. In reality, for large n, it may be
difficult or impossible to find the exact eigenvalues of A. But for the present discussion, let
us assume that Spec(A) can be found.
For each c ∈Spec(A), we need to find the Jordan blocks in J of the form J(c; m).
Assuming these blocks occur in decreasing order of size, they collectively constitute a Jordan
matrix of the form J(c; µ) for some unknown partition µ depending on c. Looking at the
uniqueness proof again, we are led to consider the null space Z of the matrix (A −cIn)n.
The proof shows that the nilpotent linear map U : Z →Z defined by U(z) = Az −cz for
Jordan Canonical Forms
215
z ∈Z has matrix J(0; µ) relative to some ordered basis of Z. We can find µ by using (8.9),
which states that µ′
1 +· · ·+µ′
k = dim(Null(U k)) for all k ≥1. Translating back to matrices,
we need only compute bases for all the null spaces Null((A −cIn)k) for 1 ≤k ≤n, which
can be done by Gaussian elimination. Letting dk be the dimension of the kth null space
(with d0 = 0), we compute µ′
k = dk −dk−1 for all k ≥1, from which D(µ) and hence µ are
readily found.
Finding the transition matrix P requires more work. Since J is now known, one
straightforward but inefficient approach is to treat the entries of P as unknowns, which
can be found by solving the linear system PJ = AP. Another method first finds bases
for the various null spaces Null((A −cIn)n). Using these basis vectors as the columns of a
matrix Q, we have Q−1AQ = blk-diag(A1, . . . , Ak), where each Ai has a single eigenvalue ci
and Ai −ciI is nilpotent. Considering each block separately, we are reduced to solving the
following problem: given a nilpotent matrix B ∈Mm(C), find a specific invertible matrix R
and a partition µ of m such that R−1BR = J(0; µ).
This problem can be solved by a recursive algorithm that implements the three-
stage inductive proof of the Classification Theorem for Nilpotent Linear Maps. First, we
recursively find an ordered basis X1 for V1 = img(B) and a partition ν = (ν1, . . . , νs) such
that the map (x 7→Bx : x ∈V1) has matrix J(0; ν) relative to the basis X1. Second, we
compute a basis for Null(B) and use appropriate vectors from this basis to augment X1 to
a basis X2 for V2 = img(B) + Null(B). If t new basis vectors are added, we define ρ by
adding t new parts of size 1 to the end of ν. Third, writing X2 = {x(i, j) : (i, j) ∈D(ρ)},
we solve linear equations to find vectors x(i, νj + 1) such that Bx(i, νj + 1) = x(i, νj) for
1 ≤i ≤s. We define µ = (ν1 + 1, . . . , νs + 1, 1, . . . , 1) (where there are t parts of size 1) and
X = {x(i, j) : (i, j) ∈D(µ)}. Finally, we obtain R by placing the column vectors x(i, j) into
a matrix, starting with the first row of D(µ) and working left to right, top to bottom. You
may check that every step in this paragraph can be implemented by solving an explicitly
computable system of linear equations. Admittedly, the entire algorithm requires a very
substantial amount of computation.
For example, consider the matrix
A =


−2
−7
2
1
7
−8
−3.5
−1.5
−0.5
0.5
5
0
−1
−4
1
1
4
−5
6.5
18.5
−10.5
−0.5
−14
31
−0.5
−0.5
−3.5
0.5
4
3
3
3
−3
0
−3
5


.
(8.11)
Using a computer algebra system, we compute χA = (x + 1)2(x −2)4, so Spec(A) =
{−1, 2}. Row reduction of (A + I)6 shows that the null space of this matrix consists of
column vectors
{(s, s −t, s + t, −s −2t, s, t) : s, t ∈C},
so v1 = (1, 1, 1, −1, 1, 0) and v2 = (0, −1, 1, −2, 0, 1) form a basis for this null space.
Similarly, row reduction of (A −2I)6 yields a null space
{(a −b/4 + c/4 + 3d/4, b/4 + 3c/4 −7d/4, a, b, c, d) : a, b, c, d ∈C},
so
v3 = (1, 0, 1, 0, 0, 0), v4 = (−1, 1, 0, 4, 0, 0), v5 = (1, 3, 0, 0, 4, 0), v6 = (3, −7, 0, 0, 0, 4)
216
Advanced Linear Algebra
form a basis for this null space. Letting P1 be the matrix with columns v1, . . . , v6, we find
A1 = P −1
1
AP1 =


−1
−1
0
0
0
0
0
−1
0
0
0
0
0
0
0
1
3
5
0
0
−1
2.5
1.5
3.5
0
0
−1
0.5
3.5
3.5
0
0
0
0
0
2


.
By inspection, replacing v2 by −v2 converts the upper 2×2 block to J(−1; 2). To deal with
the lower 4 × 4 block (corresponding to the eigenvalue 2 of A), we consider the nilpotent
matrix
B =


−2
1
3
5
−1
0.5
1.5
3.5
−1
0.5
1.5
3.5
0
0
0
0

.
A recursive call to the algorithm produces a basis x(1, 1)
=
(2, 1, 1, 0), x(1, 2)
=
(2.5, 1.75, 1.75, 0) for img(B) and associated partition ν = (2). Row-reduction of B shows
that Null(B) has a basis ((1, 2, 0, 0), (3, 0, 2, 0)). Since the first basis vector is not a linear
combination of x(1, 1) and x(1, 2), we set x(2, 1) = (1, 2, 0, 0) and ρ = (2, 1). Finally, we
solve Bv = x(1, 2) to obtain x(1, 3) = (2, 1, 1, 0.5). Now, taking
P2 =


1
0
0
0
0
0
0
−1
0
0
0
0
0
0
2
2.5
2
1
0
0
1
1.75
1
2
0
0
1
1.75
1
0
0
0
0
0
0.5
0


, P = P1P2 =


1
0
2
2.5
3.5
−1
1
1
4
7
0.5
2
1
−1
2
2.5
2
1
−1
2
4
7
4
8
1
0
4
7
4
0
0
−1
0
0
2
0


,
we find that P −1AP = blk-diag(J(−1; 2), J(2; 3), J(2; 1)).
8.13
Application to Differential Equations
This section describes an application of the Jordan Canonical Form Theorem to the solution
of homogeneous systems of linear ordinary differential equations. We let x denote a column
vector of unknown continuously differentiable functions x1, . . . , xn : R →C. Given a fixed
matrix A ∈Mn(C), our goal is to find all solutions to x′ = Ax, which is equivalent to the
system of scalar differential equations
x′
i(t) =
n
X
j=1
A(i, j)xj(t)
for 1 ≤i ≤n and t ∈R.
This system can be solved easily if A is a diagonal matrix. In this case, x′
i(t) = A(i, i)xi(t)
has general solution xi(t) = bieA(i,i)t, where bi is any constant. More generally, if A is
a Jordan block J(c; n) for some c ∈C, we can backsolve for xn(t), xn−1(t), . . . , x1(t) as
follows. First, x′
n(t) = cxn(t) implies xn(t) = bnect for some constant bn. Second, x′
n−1(t) =
cxn−1(t) + xn(t) = cxn−1(t) + bnect has general solution xn−1(t) = bntect + bn−1ect where
bn−1 is any constant. Third, x′
n−2(t) = cxn−2(t) + xn−1(t) has general solution xn−2(t) =
Jordan Canonical Forms
217
(bn/2)t2ect + bn−1tect + bn−2ect where bn−2 is any constant. Continuing backwards through
the block, you can check by induction that
xn−k(t) =
k
X
i=0
bn−i
(k −i)!tk−iect
for 0 ≤k < n,
(8.12)
where bn, . . . , b1 ∈C are arbitrary constants.
Given A = blk-diag(J(c1; n1), . . . , J(ck; nk)), we can use (8.12) to solve for the first n1
functions x1, . . . , xn1. The same formula applies to recover the functions xn1+1, . . . , xn1+n2,
starting with xn1+n2 and working backwards, and similarly for all later blocks.
Finally, consider the case where A ∈Mn(C) is an arbitrary matrix. Using the algorithms
in the previous section, we can find a Jordan canonical form J and an invertible matrix P in
Mn(C) with J = P −1AP. Introduce a new column vector y of unknown functions y1, . . . , yn
by the linear change of variable y = P −1x, x = Py. By linearity of the derivative, x′ = Py′,
and we see that x solves x′ = Ax iff Py′ = A(Py) iff y′ = (P −1AP)y iff y solves y′ = Jy.
So, once we compute P and J, y is given by the formulas above, and then x = Py is the
solution to the original system. We must point out that there may be more efficient ways of
solving x′ = Ax, especially when A has special structure, but a full discussion of this point
is beyond the scope of this text.
For example, let us solve x′ = Ax, where A is the matrix in (8.11). We computed a
Jordan canonical form of A to be blk-diag(J(−1; 2), J(2; 3), J(2; 1)). Let y = P −1x, where
P is the matrix found at the end of §8.12. Then y′ = Jy has general solution
y1(t) = c1te−t + c2e−t,
y2(t) = c1e−t,
y3(t) = (c3/2)t2e2t + c4te2t + c5e2t,
y4(t) = c3te2t + c4e2t,
y5(t) = c3e2t,
y6(t) = c6e2t.
where c1, . . . , c6 ∈C are arbitrary constants, and the original system has solution x = Py.
8.14
Minimal Polynomials
Let V be an n-dimensional vector space over any field F and T : V →V be a linear map.
Recall that T is in the F-algebra L(V ) of all F-linear maps from V to V , which is finite-
dimensional. Therefore, we know from §3.20 that there exists a unique monic polynomial
mT ∈F[x] of least degree such that mT (T) = 0, and mT divides all polynomials g ∈F[x]
such that g(T) = 0. We can find mT by searching the list of powers (idV , T, T 2, T 3, . . .) for
the lowest power T k that is a linear combination of preceding powers of T. Similarly, any
matrix A ∈Mn(F) has a minimal polynomial mA. For any fixed ordered basis X of V ,
the map sending T ∈L(V ) to [T]X ∈Mn(F) is an algebra isomorphism (see Chapter 6).
It follows that the linear map T has the same minimal polynomial as the matrix [T]X, for
any choice of X. By letting X vary over all ordered bases of V , we conclude from this that
similar matrices have the same minimal polynomial.
Suppose A is a block-diagonal matrix blk-diag(A1, . . . , As). For any f ∈F[x], we have
f(A) = blk-diag(f(A1), . . . , f(As)). Therefore, f(A) = 0 iff every f(Ai) = 0 iff mAi divides
f for 1 ≤i ≤s. It follows that mA = lcm(mA1, . . . , mAs). In particular, suppose A is a
diagonal matrix. Then we can take each Ai to be a 1 × 1 matrix. The minimal polynomial
of the matrix [c] is x −c, for any c ∈F. Hence, for diagonal A, mA = lcm1≤i≤n(x −
A(i, i)). Since the diagonal entries of A are the eigenvalues of A, we can also write this
as mA = Q
c∈Spec(A)(x −c). More generally, any diagonalizable matrix A is similar to a
218
Advanced Linear Algebra
diagonal matrix with the eigenvalues in Spec(A) appearing on the main diagonal (each with
a certain multiplicity). It follows that
for diagonalizable A ∈Mn(F),
mA =
Y
c∈Spec(A)
(x −c).
So for all A ∈Mn(F), if A is diagonalizable, then mA splits into distinct linear factors in
F[x].
The converse statement also holds. We use Jordan canonical forms to prove the
converse for F = C; a different proof is needed for general fields F (Exercise 58). Assume
A ∈Mn(C) is not diagonalizable. We know A is similar to a Jordan canonical form
J = blk-diag(J(c1; n1), . . . , J(cs; ns)) that has the same minimal polynomial as A. The
matrix J cannot be diagonal, so some ni > 1. We know mJ = lcm1≤i≤s mJ(ci;ni). So it
suffices to show that mJ(c;k) = (x −c)k.
Evaluating the polynomial g = (x −c)k at x = J(c; k) gives
g(J(c; k)) = (J(c; k) −cIk)k = J(0; k)k = 0.
The last equality can be seen by a matrix calculation, or by noting J(0; k) is the matrix of
the linear map TX,(k), which is nilpotent of index k. So mJ(c;k) must divide g. By unique
factorization in C[x], the monic divisors of g in C[x] are (x −c)i where 0 ≤i ≤k. Since
J(0; k)i ̸= 0 for i < k, we see that g itself must be the minimal polynomial of J(c; k).
This proof tells us how to compute mA from a Jordan canonical form of A. We have mA =
Q
c∈Spec(A)(x−c)k(c), where k(c) is the maximum size of any Jordan block J(c; k) appearing
in a Jordan canonical form similar to A. On the other hand, consider the characteristic
polynomial χA = det(xIn −A). Similar matrices have the same characteristic polynomial,
so χA = χJ. The characteristic polynomial of a Jordan block J(c; k) is (x −c)k, since
xIn −J(c; k) is a k × k triangular matrix with all diagonal entries equal to x −c. For any
block-diagonal matrix B = blk-diag(B1, . . . , Bs), computing the determinant shows that
χB = Qs
i=1 χBi. Taking B = J here, we see that χA = Qs
i=1(x −ci)ni. Comparing to the
earlier formula for mA, we deduce the Cayley–Hamilton Theorem for complex matrices: for
all A ∈Mn(C), the minimal polynomial mA divides the characteristic polynomial χA in
C[x].
Analogous results hold for linear maps T ∈L(V ). In particular, T is diagonalizable
iff mT splits into distinct linear factors in F[x], in which case mT = Q
c∈Spec(T )(x −c).
We use this theorem to prove that if T ∈L(V ) is diagonalizable and W is a T-invariant
subspace of V , then T|W is diagonalizable. To prove this, write g = mT and f = mT |W
in F[x]. We know g(T) is the zero operator on V . Restricting to W, g(T|W) = g(T)|W
is the zero operator on W. So f, the minimal polynomial of T|W, must divide g in F[x].
By diagonalizability of T, g splits into a product of distinct linear factors in F[x]. Since f
divides g, the unique factorization of f in F[x] must also be a product of distinct linear
factors, which are a subset of the factors for g. By the theorem, T|W is diagonalizable.
8.15
Jordan–Chevalley Decomposition of a Linear Operator
Here, we prove an abstract version of the Jordan Canonical Form Theorem that is needed
in the theory of Lie algebras.
Jordan Canonical Forms
219
Jordan–Chevalley Decomposition Theorem. Suppose V is an m-dimensional complex
vector space and T : V →V is a linear map. There exist unique linear maps Td and Tn on
V such that Td is diagonalizable, Tn is nilpotent, T = Td + Tn, and Td ◦Tn = Tn ◦Td.
Td is called the diagonalizable part of T, Tn is called the nilpotent part of T, and T = Td+Tn
is called the Jordan–Chevalley decomposition of T.
To prove existence of Td and Tn with the stated properties, write Spec(T) = {c1, . . . , cs}.
By the Jordan Canonical Form Theorem, we can pick an ordered basis X of V such that
[T]X = blk-diag(J(c1; µ(1)), J(c2; µ(2)), . . . , J(cs; µ(s))), where each µ(i) is a partition of
some positive integer mi. Let X1 consist of the first m1 vectors in the list X, let X2 consist
of the next m2 vectors in X, and so on. Define a linear map Td on the basis X by letting
Td(zi) = cizi for all zi ∈Xi, and extending by linearity. Define Tn = T −Td, which is linear
since it is a linear combination of two linear maps. Evidently T = Td + Tn. Also, [Td]X is
the diagonal matrix blk-diag(c1Im1, c2Im2, . . . , csIms), so Td is diagonalizable. On the other
hand,
[Tn]X = [T]X −[Td]X = blk-diag(J(0; µ(1)), J(0; µ(2)), . . . , J(0; µ(s))).
It follows that every basis vector in X gets sent to zero after applying Tn at most
max(m1, . . . , ms) times, so that Tn is nilpotent. Finally, for each i, the scalar multiple
of the identity ciImi commutes with J(0; µ(i)). So the matrices [Td]X and [Tn]X commute
since each diagonal block of the first matrix commutes with the corresponding block of the
second matrix. It follows that the linear maps Td and Tn commute, completing the existence
proof.
Turning to the uniqueness proof, let T = T ′
d+T ′
n be any decomposition of T into the sum
of a diagonalizable linear map T ′
d and a nilpotent linear map T ′
n that commute with each
other. We must prove T ′
d = Td and T ′
n = Tn. Keep the notation of the previous paragraph.
We saw in the uniqueness proof in §8.11 that each Xi is a basis of the T-invariant subspace
Zi = Null((T −ci idV )m). Now, T ′
d commutes with T, since
T ′
d ◦T = T ′
d ◦(T ′
d + T ′
n) = T ′
d ◦T ′
d + T ′
d ◦T ′
n = T ′
d ◦T ′
d + T ′
n ◦T ′
d = (T ′
d + T ′
n) ◦T ′
d = T ◦T ′
d.
Then T ′
d also commutes with T −ci idV and any power of this linear map. It follows
that each Zi is a T ′
d-invariant subspace, since z ∈Zi implies (T −ci idV )m(T ′
d(z)) =
T ′
d((T −ci idV )m(z)) = T ′
d(0) = 0. Similarly, T ′
n commutes with T, and so each Zi is a
T ′
n-invariant subspace.
It follows that [T ′
d]X = blk-diag(B1, . . . , Bs) and [T ′
n]X = blk-diag(C1, . . . , Cs) for
certain matrices Bi, Ci ∈Mmi(C). Because Bi commutes with ciImi for all i, T ′
d commutes
with Td. Since T ′
d also commutes with T, T ′
d commutes with Tn = T −Td. Similarly, T ′
n
commutes with Td and Tn. Now, T ′
d+T ′
n = T = Td+Tn gives T ′
d−Td = Tn−T ′
n. The left side
of this equation is a diagonalizable linear map, since it is the difference of two commuting
diagonalizable linear maps (Exercise 60). The right side of this equation is a nilpotent
linear map, since it is the difference of two commuting nilpotent linear maps (Exercise 61).
The two sides of the equation are equal, so we are considering a diagonalizable nilpotent
linear map. The only such map is zero (as we saw in the introduction to this chapter), so
T ′
d −Td = 0 = Tn −T ′
n. Thus, T ′
d = Td and Tn = T ′
n, proving uniqueness.
The result just proved leads to the following Jordan–Chevalley Decomposition Theorem
for Matrices: for any matrix A ∈Mn(C), there exist unique matrices B, C ∈Mn(C) such
that A = B + C, BC = CB, B is diagonalizable, and C is nilpotent.
220
Advanced Linear Algebra
8.16
Summary
1.
Definitions. Given a vector space V and a linear map T : V →V , T is nilpotent iff
T k = 0 for some positive integer k. The least such k is the index of nilpotence of
T. T is diagonalizable iff for some ordered basis X of V , [T]X is a diagonal matrix.
Spec(T) is the set of eigenvalues of T. The Jordan block J(c; k) is a k × k matrix
with all diagonal entries equal to c, all entries 1 on the next higher diagonal,
and zeroes elsewhere. A Jordan canonical form is a block-diagonal matrix with
diagonal blocks J(c1; k1), . . . , J(cs; ks). A partition of n is a weakly decreasing
sequence µ = (µ1, . . . , µs) of positive integers with sum n. We write J(c; µ) =
blk-diag(J(c; µ1), . . . , J(c; µs)). The diagram of µ consists of s rows of boxes, with
µi boxes in row i; µ′
k is the height of column k in this diagram. Formally, the
diagram of µ is D(µ) = {(i, j) : 1 ≤i ≤s, 1 ≤j ≤µi}.
2.
Nilpotent Maps and Partition Diagrams. Given a partition µ and a basis
X = {x(i, j) : (i, j) ∈D(µ)} for V , we obtain a nilpotent linear map TX,µ on
V by sending x(i, 1) to zero for all i, sending x(i, j) to x(i, j −1) for all i and all
j > 1, and extending by linearity. A basis for Null(T k
X,µ) consists of x(i, j) with
(i, j) ∈D(µ) and 1 ≤j ≤k; so dim(Null(T k
X,µ)) = µ′
1 + · · · + µ′
k. A basis for
img(T k
X,µ) consists of all x(i, j) with (i, j) ∈D(µ) and (i, j + k) ∈D(µ).
3.
Classification of Nilpotent Maps. Given any field F, any n-dimensional F-vector
space V , and any nilpotent linear map T : V →V , there exists a unique partition
µ of n such that T = TX,µ (equivalently, [T]X = J(0; µ)) for some ordered basis
X of V .
4.
Jordan Canonical Form Theorem. For all algebraically closed fields F (such as
C), all finite-dimensional F-vector spaces V , and all linear maps T : V →V ,
there exists an ordered basis X of V such that [T]X is a Jordan canonical
form blk-diag(J(c1; m1), . . . , J(cs; ms)). For any other ordered basis Y of V ,
if [T]Y = blk-diag(J(d1; n1), . . . , J(dt; nt)), then s = t and the Jordan blocks
J(dk; nk) are a rearrangement of the Jordan blocks J(ci; ni). Every matrix
A ∈Mn(F) is similar to a Jordan canonical form J, and any other Jordan form
similar to A is obtained by reordering the Jordan blocks of J.
5.
Computing Jordan Forms. The diagonal entries in any Jordan form of a linear
map T (or matrix A) are the eigenvalues of T (or A). For each eigenvalue c, let
Uc = T −c idV (or Uc = A−cIn). We can find an ordered basis Xc for the subspace
Zc = Null(U n
c ) such that [Uc|Zc]Xc = J(0; µ(c)). Letting X be the concatenation
of the bases Xc, [T]X is a Jordan canonical form. Column k of D(µ(c)) has size
dim(Null(U k
c )) −dim(Null(U k−1
c
)).
6.
Application to Differential Equations. One way to solve x′ = Ax is to find an
invertible P and a Jordan form J with J = P −1AP. The substitution x = Py,
y = P −1x converts the system x′ = Ax to y′ = Jy, which can be solved by
backsolving each Jordan block.
7.
Minimal Polynomials and Jordan Forms. A matrix A ∈Mn(F) is diagonaliz-
able iff the minimal polynomial mA splits into distinct linear factors in F[x];
similarly for a linear map on an n-dimensional vector space. Given A ∈Mn(C),
mA = Q
c∈Spec(A)(x −c)k(c), where k(c) is the maximum size of any Jordan block
J(c; k) appearing in a Jordan form similar to A. The characteristic polynomial
χA = Q
c∈Spec(A)(x −c)m(c), where m(c) is the total size of all Jordan blocks
Jordan Canonical Forms
221
J(c; k) in a Jordan form of A. The Cayley–Hamilton Theorem follows: mA divides
χA. If T ∈L(V ) is diagonalizable and W is a T-invariant subspace of V , then
the restriction T|W is also diagonalizable.
8.
Jordan–Chevalley Decomposition. Given a linear map T on a finite-dimensional
vector space V over an algebraically closed field F, there exist unique linear maps
Td and Tn on V such that Td is diagonalizable, Tn is nilpotent, Td ◦Tn = Tn ◦Td,
and T = Td + Tn.
8.17
Exercises
Unless otherwise specified, assume in these exercises that F is a field, V is a finite-
dimensional F-vector space, and T : V →V is a linear map.
1.
Decide whether each linear map is nilpotent. If not, explain why not. If so, find
the index of nilpotence.
(a) T : R3 →R3 given by T(a, b, c) = (c, 0, b) for a, b, c ∈R.
(b) T : R3 →R3 given by T(a, b, c) = (0, c, b) for a, b, c ∈R.
(c) D : V →V given by D(f) = df/dx, where V is the vector space of real
polynomials of degree at most 4.
2.
Decide whether each linear map is nilpotent. If not, explain why not. If so, find
the index of nilpotence.
(a) T : M3(R) →M3(R) given by T(A) = AT (the transpose map).
(b) T : C2 →C2 given by T(a, b) = (−3a + 9b, −a + 3b) for a, b ∈C.
(c) T : C2 →C2 given by T(a, b) = (−a + 3b, −a + 3b) for a, b ∈C.
(d) T(x) = Ax for x ∈R10, where A = blk-diag(J(0; 4), J(0; 3)T, J(0; 3)).
3.
Decide whether each linear map is diagonalizable. If not, explain why not. If so,
find an ordered basis X such that [T]X is diagonal.
(a) T : R2 →R2 given by T(a, b) = (a, a + b) for a, b ∈R.
(b) T : R2 →R2 given by T(a, b) = (a + b, a + b) for a, b ∈R.
(c) Tθ : R2 →R2 given by Tθ(a, b) = (a cos θ −b sin θ, a sin θ +b cos θ) for a, b ∈R,
where 0 < θ < π/2 is fixed.
(d) Tθ : C2 →C2 given by the same formula in (c) for a, b ∈C.
4.
Decide whether each linear map is diagonalizable. If not, explain why not. If so,
find an ordered basis X such that [T]X is diagonal.
(a) D(f) = df/dx for f a real polynomial of degree at most 4.
(b) T : M2(R) →M2(R) given by T(A) = AT (the transpose map).
5.
Prove or disprove: for all A ∈Mn(F), if A is nilpotent, then A is not invertible.
6.
Prove or disprove: for all A ∈Mn(F), if A is not invertible, then A is nilpotent.
7.
Use the uniqueness part of the Jordan Canonical Form Theorem to prove that
A ∈Mn(C) is diagonalizable iff every Jordan block in any Jordan canonical form
similar to A has size 1.
8.
Suppose that A ∈Mn(F) has only one eigenvalue c and A ̸= cIn. Prove that A is
not diagonalizable. Deduce that nonzero nilpotent maps are not diagonalizable.
9.
For A = J(0; (5, 5, 5, 5, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1)), how many Jordan canonical forms
are similar to A?
222
Advanced Linear Algebra
10.
For A = blk-diag(J(2; (2, 2, 2)), J(3; 3), J(4; (3, 3, 2, 2))), how many Jordan canon-
ical forms are similar to A?
11.
Describe the Jordan canonical form matrices that are not similar to any Jordan
canonical form besides themselves.
12.
Suppose V is finite-dimensional and T : V →V is a linear map such that for all
v ∈V , there exists a positive integer k(v) (depending on v) with T k(v)(v) = 0.
Prove that T is nilpotent.
13.
Give an example to show that the result of the previous exercise can fail if V is
infinite-dimensional.
14.
In the example at the end of §8.1, verify that Z is an ordered basis of V and
[T]Z = J(0; (2, 1, 1)).
15.
For each linear map T defined on the basis (x1, . . . , xn), decide if T is nilpotent.
If it is, find an ordered basis Z and a partition µ such that [T]Z = J(0; µ).
(a) n = 5, T(xi) = xi+1 for 1 ≤i < 4, T(x5) = 0.
(b) n = 4, T(x1) = x3, T(x2) = 0, T(x3) = x4, T(x4) = x1.
(c) n = 8, T(xi) = x⌊i/2⌋for all i, letting x0 = 0.
(d) n = 11, T(xi) = x(2i mod 12) for 1 ≤i < 12, letting x0 = 0.
16.
For each linear map T defined on the basis (x1, . . . , xn), decide if T is nilpotent.
If it is, find an ordered basis Z and a partition µ such that [T]Z = J(0; µ).
(a) n = 7, T(x1) = T(x5) = x2, T(x7) = x4, T(x2) = T(x4) = T(x6) = x3,
T(x3) = 0.
(b) n = 3, T(x1) = x2 + 2x3, T(x2) = x1 + 2x3, T(x3) = x1 + 2x2.
(c) n = 4, T(xi) = x1 + x2 + x3 −3x4 for 1 ≤i ≤4.
(d) T(xi) = xi+1 + xi+2 + · · · + xn for 1 ≤i < n, T(xn) = 0.
(e) n = 15, T(xi) = x(2i mod 16) for 1 ≤i < 15, letting x0 = 0.
17.
(a) Suppose X = (x1, . . . , xn) is an ordered basis for V and T is a linear map on
V such that for 1 ≤j ≤n, either T(xj) = 0 or there exists i < j with T(xj) = xi.
Prove T is nilpotent. (b) More generally, assume that for all j, T(xj) = Pj−1
i=1 aijxi
for some aij ∈F. Without using matrices or Jordan forms, prove T is nilpotent.
18.
List all partitions of 5. For each partition µ, state the elements of the set D(µ),
draw a picture of D(µ), and compute µ′
k for all k ≥1.
19.
Find the partition µ, given that
(µ′
1 + µ′
2 + · · · + µ′
k : k ≥1) = (8, 12, 16, 19, 21, 23, 25, 26, 27, 28, 29, 30, 30, 30, . . .).
20.
For a certain linear map TX,µ, it is known that the dimensions of img(T k
X,µ) for
k ≥1 are 15, 11, 8, 5, 3, 2, 1, 0. What can be said about µ?
21.
Prove: for all partitions µ of n, µ′
k is the number of µi ≥k. Conclude that
µ′ = (µ′
1, µ′
2, . . . : µ′
k > 0) is a partition of n, and µ′′ = µ.
22.
Let X = (x1, x2, x3, x4) be an ordered basis of V . For each partition µ of 4,
compute TX,µ(xi) for 1 ≤i ≤4 and TX,µ(v), where v = P4
i=1 cixi.
23.
Let T = TX,µ where µ = (6, 3, 3, 3, 2, 1, 1). For all k ≥1, describe a basis for
Null(T k) and a basis for img(T k).
24.
Let T = TX,µ where µ = (11, 8, 6, 6, 3, 2, 2, 2, 1, 1). For all k ≥1, compute the
dimensions of Null(T k) and img(T k).
25.
Assume dim(V ) = 1. Check directly that for any nilpotent linear map T : V →V ,
T = T(x),(1) for all nonzero x ∈V .
Jordan Canonical Forms
223
26.
Given a partition µ and r, k > 0, use a visual analysis of D(µ) to find a basis for
Null(T r
X,µ) ∩img(T k
X,µ). Illustrate for µ = (5, 5, 3, 1, 1, 1), r = 3, k = 2.
27.
Given a partition µ and r, k > 0, find a basis for Null(T r
X,µ)+img(T k
X,µ). Illustrate
for µ = (5, 5, 3, 1, 1, 1), r = 1, k = 3.
28.
Let µ = (5, 5, 3, 1, 1, 1) and X = (x1, . . . , x16). For each k ≥2, find a partition ν
and an ordered basis Z such that T k
X,µ = TZ,ν.
29.
For a partition µ and k > 0, describe how to find Z and ν such that T k
X,µ = TZ,ν.
30.
Let [T]X = J(0; (2, 2, 1)), where X = (x1, . . . , x5). Let Z = (z1, . . . , z5) be another
ordered basis given by zj = P5
i=1 cijxi for some cij ∈F. Find necessary and
sufficient conditions on the cij to ensure that [T]Z = J(0; (2, 2, 1)).
31.
Prove: for any two subspaces W and Z of a finite-dimensional vector space V ,
dim(W + Z) + dim(W ∩Z) = dim(W) + dim(Z). (One approach is to apply the
Rank–Nullity Theorem to the map f : W ×Z →W +Z given by f(w, z) = w −z
for w ∈W and z ∈Z.)
32.
Let T(a, b, c) = (2a + b −3c, 2a + b −3c, 2a + b −3c) for a, b, c ∈R.
(a) Check that T is nilpotent.
(b) Find a basis X1 for V1 = img(T) and a partition ν such that T|V1 = TX1,ν.
(c) Extend X1 to a basis X2 of V2 = img(T) + Null(T), and find a partition ρ
with T|V2 = TX2,ρ.
(d) Extend X2 to get an indexed basis X of R3 and a partition µ with T = TX,µ.
(e) Check that X′
2 = ((1, −2, 0), (0, 3, 1)) is a basis of V2 such that T|V2 = TX′
2,ρ.
What happens if we try to solve part (d) starting from the basis X′
2?
33.
Let T(a, b, c, d) = (−a + b + c + d, a + b −c + d, −a + b + c + d, −a −b + c −d) for
a, b, c, d ∈R. Follow the proof in the text to find X and µ such that T = TX,µ.
34.
Let T(a, b, c, d, e, f) = (0, a + 2b + d −3f, 0, 2a + 2b + c + d −3f, 0, 3a + 2b + 2c +
d + e −3f) for a, b, c, d, e, f ∈R. Follow the proof in the text to find X and µ
such that T = TX,µ. (In Stage 1, you can use the results of Exercise 32.)
35.
Let a linear map T : R9 →R9 be defined on the standard basis by T(ek) = e⌊k/3⌋
for 1 ≤k ≤9, with e0 = 0. Follow the proof in the text to find a partition µ and
ordered basis X with T = TX,µ.
36.
Let T be a nilpotent linear map on V , n = dim(V ), and j = dim(Null(T)), so
dim(img(T)) = n−j. What are the possible values of k = dim(Null(T) ∩img(T))?
For each feasible k, construct an explicit example of a map T achieving this k.
37.
Direct Sums. Let W1, . . . , Wk be subspaces of V and Xi be an ordered basis of
Wi for 1 ≤i ≤k. Let W = W1 + · · · + Wk and X be the concatenation of the
lists X1, . . . , Xk. Prove the following conditions are equivalent:
(a) For each w ∈W, there exist unique wi ∈Wi with w = w1 + · · · + wk.
(b) For 2 ≤i ≤k, Wi ∩(W1 + W2 + · · · + Wi−1) = {0}.
(c) X is an ordered basis for W.
(d) X is a linearly independent list.
When any of these conditions holds, we say W is the direct sum of the Wi, written
W = W1 ⊕W2 ⊕· · · ⊕Wk.
38.
Give an example of three subspaces W1, W2, W3 of a vector space V such that
W1 ∩W2 = W1 ∩W3 = W2 ∩W3 = {0}, but the sum W1 + W2 + W3 is not direct.
39.
Assume n = dim(V ) < ∞, S : V →V is linear, and img(S) = img(S2). Must
V = Null(S) ⊕img(S) follow? Explain.
224
Advanced Linear Algebra
40.
For each linear map U on R4, find explicit U-invariant subspaces Z and W
satisfying the conclusions of Fitting’s Lemma.
(a) U(a, b, c, d) = (a, b, 0, 0)
(b) U(a, b, c, d) = (0, c, b, a)
(c) U(a, b, c, d) = (a + b, c + d, c + d, a + b)
(d) U(a, b, c, d) = (c −a, d −a, c −a, 2d −b −a)
41.
Let V be the infinite-dimensional real vector space of all sequences (x0, x1, x2, . . .)
under componentwise operations. For each linear map U : V →V , prove that the
conclusion of Fitting’s Lemma does not hold for U.
(a) U((x0, x1, x2, . . .)) = (x1, x2, x3, . . .) for all xi ∈R.
(b) U((x0, x1, x2, . . .)) = (0, x0, x1, x2, . . .) for all xi ∈R.
42.
Eigenspaces. For each eigenvalue c of a linear map T on V , define the eigenspace
Ec = {v ∈V : T(v) = cv}. Thus, Ec consists of zero and all eigenvectors of T
associated with the eigenvalue c. Show each Ec is a subspace of V .
43.
For a complex vector space V , let X = (x1, . . . , xn) be an ordered basis of V such
that [T]X = blk-diag(J(c; µ), B), where B is a triangular matrix with no diagonal
entries equal to c. Show that (x1, xµ1+1, xµ1+µ2+1, . . .) is an ordered basis of Ec.
Deduce that for Spec(T) = {c1, . . . , ck}, the sum Ec1 + · · · + Eck is a direct sum
(see Exercise 37) in the complex case.
44.
For any field F, show that the sum of the eigenspaces of T is a direct sum. (If
not, study a relation v1 + · · · + vi = 0 with vj ∈Ecj, vi ̸= 0, and i minimal.)
45.
Give an example to show that the sum of all eigenspaces of V may be a proper
subspace W of V . If dim(V ) = n and | Spec(T)| = k, what is the minimum
possible dimension of W?
46.
Generalized
Eigenspaces. Given a linear map T
on an n-dimensional
vector
space
V
and
c
∈
Spec(T),
the
generalized
eigenspace
for
c
is
Gc
=
Null((T −c idV )n).
Prove:
if
F
is
algebraically
closed,
then
V = L
c∈Spec(T ) Gc.
47.
Prove: for all A ∈Mn(C), A and AT are similar to the same Jordan canonical
forms and hence are similar to each other.
48.
For c ̸= 0 and k ≥1, compute J(c; k)−1. What is a Jordan canonical form similar
to this matrix?
49.
Compute a Jordan canonical form similar to each matrix.


1
1
1
1
0
1
1
1
0
0
1
1
0
0
0
1

,


3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

,


1
0
1
0
0
1
0
1
1
0
1
0
0
1
0
1

,


4
−2
9
−2
1
1
4
−1
0
0
2
0
1
−1
5
1

.
50.
For each matrix A in Exercise 49, find an invertible matrix P such that P −1AP
is a Jordan canonical form.
51.
Write a program to find the Jordan canonical form of a given complex matrix.
52.
Solve the following systems of ordinary differential equations.
(a) x′
1 = 3x1, x′
2 = x1 + 3x2.
(b) x′
1 = x2, x′
2 = −x1.
(c) x′
1 = x1 + 2x2 + 3x3, x′
2 = x2 + 2x3, x′
3 = x3.
(d) x′
1 = x1 + x2, x′
2 = x2 + x3, x′
3 = x3 + x4, x′
4 = x1 + x4.
(e) x′
1 = x′
3 = x1 + 2x2 + 2x3 + x4, x′
2 = x′
4 = 2x1 + x2 + x3 + 2x4.
53.
Check by induction on k that the functions in (8.12) satisfy x′ = J(c; n)x.
Jordan Canonical Forms
225
54.
Find the minimal polynomial and characteristic polynomial of each matrix in
Exercise 49.
55.
(a) Prove or disprove: for all A ∈Mn(C), if χA splits into distinct linear factors
in C[x] then A is diagonalizable. (b) Prove or disprove: for all A ∈Mn(C), if A
is diagonalizable then χA splits into distinct linear factors in C[x].
56.
Suppose W is a T-invariant subspace of V and h ∈F[x] is any polynomial. Prove
W is also h(T)-invariant.
57.
Primary
Decomposition
Theorem. For any field F
and linear map
T : V →V , suppose mT = pe1
1 · · · pes
s
∈F[x], where p1, . . . , ps are distinct
monic irreducible polynomials in F[x]. Define Wi = ker(pei
i (T)) for 1 ≤i ≤s.
(a) Show that each Wi is a T-invariant subspace of V .
(b) Show that V = W1 + W2 + · · · + Ws. (Define gi = mT /pei
i
∈F[x] and
explain why there exist hi ∈F[x] with Ps
i=1 higi = 1. For v ∈V , show
v = Ps
i=1 hi(T)gi(T)(v) where the ith summand is in Wi.)
(c) Show that V = W1 ⊕W2 ⊕· · · ⊕Ws, a direct sum. (If the sum is not direct,
choose t minimal such that 0 = w1 + w2 + · · · + wt with wi ∈Wi and wt ̸= 0.
Contradict minimality of t.)
58.
Prove: for any field F, if mT splits into a product of distinct linear factors in
F[x], then T is diagonalizable. (Use Exercise 57.)
59.
Let S and T be diagonalizable linear maps on an F-vector space V . Prove
there exists an ordered basis X such that [S]X and [T]X are both diagonal iff
S ◦T = T ◦S. So, two diagonalizable operators can be simultaneously diagonal-
ized iff the operators commute. (For the hard direction, choose an ordered basis
Y such that [S]Y = blk-diag(c1In1, . . . , csIns), where c1, . . . , cs are the distinct
eigenvalues of S. Break Y into sublists Y1, . . . , Ys of length n1, . . . , ns, and let
W1, . . . , Ws be the subspaces spanned by these sublists. Explain why each Wi is
T-invariant. Show that there is an ordered basis Xi for Wi such that [S|Wi]Xi
and [T|Wi]Xi are both diagonal.)
60.
(a) Prove: if S and T are diagonalizable linear maps on V and S ◦T = T ◦S,
then aS + bT is diagonalizable for any a, b ∈F. (b) Give a specific example of
diagonalizable linear maps S and T such that S −T is not diagonalizable.
61.
(a) Suppose S and T are commuting nilpotent linear maps on an F-vector space
V . Prove S −T is nilpotent. (b) Must (a) hold if S and T do not commute? Prove
or give a counterexample.
62.
Deduce the Jordan–Chevalley decomposition for matrices from the corresponding
result for linear maps.
63.
Find the Jordan–Chevalley decomposition of each matrix in Exercise 49.
64.
Find the Jordan–Chevalley decomposition of the matrix A in (8.11).
65.
Given A =
 0
−1
1
0

, show that there exist infinitely many pairs of matrices
B, C with A = B + C, B diagonalizable, and C nilpotent. Why doesn’t this
contradict the uniqueness assertion in the Jordan–Chevalley Theorem?
66.
Given T ∈L(V ), show that there exist polynomials f, g ∈F[x] with no constant
term such that Td = f(T) and Tn = g(T).
9
Matrix Factorizations
Matrices with special structure — such as diagonal matrices, triangular matrices, and
unitary matrices — are simpler to work with than general matrices. Many algorithms have
been developed in numerical linear algebra to convert an input matrix into a form with
specified special structure by using a sequence of carefully chosen matrix operations. These
algorithms can often be described mathematically as providing factorizations of matrices
into products of structured matrices. This chapter proves the existence and uniqueness
properties of several matrix factorizations and explores the algebraic and geometric ideas
leading to these factorizations.
The first factorization, called the QR factorization, writes a complex matrix as a product
of a unitary matrix Q and an upper-triangular matrix R. More generally, Q can be replaced
by a rectangular matrix with orthonormal columns. To obtain such factorizations, we study
the Gram–Schmidt algorithm for converting a linearly independent list of vectors into an
orthonormal list of vectors. Another approach to QR factorizations involves Householder
transformations, which generalize geometric reflections in R2 or R3. By cleverly choosing
and composing Householder matrices, we can force the entries of a matrix below the main
diagonal to become 0, one column at a time.
The second family of factorizations, called LU decompositions, writes certain square
matrices as products LU with L lower-triangular and U upper-triangular. When such a
product exists, we can arrange for one of the matrices (L or U) to have all 1s on its main
diagonal. In general, a triangular matrix is called unitriangular iff all of its diagonal entries
are 1. Not every matrix has an LU factorization, but an appropriate permutation of the
rows can convert any invertible matrix to a matrix of the form LU. Similarly, by permuting
both rows and columns, any matrix can be converted to a matrix with an LU factorization.
These LU factorizations are closely connected to the Gaussian Elimination Algorithm, which
reduces a matrix to a simpler form by systematically making entries become 0.
One motivation for finding QR and LU factorizations is the efficient solution of a linear
system of equations Ax = b. If A is lower-triangular, this system is readily solved by forward
substitution, in which we solve for the components x1, . . . , xn of x in this order. Similarly, if A
is upper-triangular, the system can be solved quickly by backward substitution, in which we
solve for xn, . . . , x1 in this order. If A = LU with L lower-triangular and U upper-triangular,
we can solve Ax = LUx = b by first solving Ly = b for y via forward substitution, then
solving Ux = y for x via backward substitution. Given A = QR with Q unitary and R
upper-triangular, we can solve Ax = QRx = b by noting that Q−1 = Q∗(which can be
computed quickly from Q), so Ax = b iff Rx = Q∗b. The latter system can be solved by
backward substitution.
The chapter concludes with two more matrix factorization results. The Cholesky
factorization expresses a positive semidefinite matrix as a product LL∗, where L is lower-
triangular. This factorization is closely related to the QR and LU factorizations, and it has
applications to least-squares approximation problems. The singular value decomposition
expresses a matrix A as a product of a unitary matrix, a diagonal matrix with nonnegative
diagonal entries, and another unitary matrix. These factorizations all play a fundamental
role in numerical linear algebra.
DOI: 10.1201/9781003484561-9
226
Matrix Factorizations
227
9.1
Approximation by Orthonormal Vectors
To prepare for our study of the QR factorization, we first discuss how to approximate
a vector as a linear combination of orthonormal vectors. Let V
be a complex inner
product space. This means V is a complex vector space with an inner product (the analog
of the dot product in Rn) satisfying these conditions for all x, y, z ∈V and c ∈C:
⟨x + y, z⟩= ⟨x, z⟩+ ⟨y, z⟩; ⟨cx, y⟩= c⟨x, y⟩; ⟨y, x⟩= ⟨x, y⟩; and for x ̸= 0, ⟨x, x⟩∈R>0. The
most frequently used complex inner product space is Cn with the standard inner product
⟨v, w⟩= Pn
k=1 vkwk for v, w ∈Cn.
For any x ∈V , the length of x is ||x|| =
p
⟨x, x⟩. The distance between vectors x, y ∈V is
||x−y||. Two vectors x, y ∈V are called orthogonal iff ⟨x, y⟩= 0, or equivalently ⟨y, x⟩= 0.
Orthogonal vectors x and y satisfy the Pythagorean Identity ||x + y||2 = ||x||2 + ||y||2, since
||x + y||2 = ⟨x + y, x + y⟩= ⟨x, x⟩+ ⟨x, y⟩+ ⟨y, x⟩+ ⟨y, y⟩= ||x||2 + 0 + 0 + ||y||2.
A list of vectors (u1, . . . , uk) in V is called orthogonal iff ⟨ur, us⟩= 0 for all r ̸= s in [k].
This list is called orthonormal iff the list is orthogonal and ⟨ur, ur⟩= 1 for all r in [k].
Geometrically, an orthonormal list consists of mutually perpendicular unit vectors in V .
Theorem on Independence of Orthogonal Vectors. Any orthogonal list of nonzero
vectors in an inner product space V is linearly independent.
In particular, every orthonormal list of vectors is linearly independent.
Proof. Let (u1, . . . , uk) be an orthogonal list of nonzero vectors in V . Suppose c1u1 + · · · +
ckuk = 0 for given c1, . . . , ck ∈C. Fix r between 1 and k and take the inner product of this
linear combination with ur. We get
0 = ⟨0, ur⟩= ⟨c1u1 + · · · + ckuk, ur⟩=
k
X
j=1
cj⟨uj, ur⟩= cr⟨ur, ur⟩.
Since ur is nonzero, ⟨ur, ur⟩> 0, and so cr = 0. This holds for all r, so (u1, . . . , uk) is
linearly independent.
Theorem on Approximation by Orthonormal Vectors. Suppose (u1, . . . , uk) is an
orthonormal list in a complex inner product space V . Let U be the subspace spanned by
(u1, . . . , uk). For each v ∈V , there is a unique vector x ∈U minimizing the distance ||v−x||
from x to v, namely x = Pk
r=1⟨v, ur⟩ur. Also, v −x is orthogonal to every u ∈U.
We call x the orthogonal projection of v onto U.
Proof. Define x = Pk
r=1⟨v, ur⟩ur ∈U. Given any s between 1 and k, we compute
⟨v −x, us⟩= ⟨v, us⟩−⟨x, us⟩= ⟨v, us⟩−
k
X
r=1
⟨v, ur⟩⟨ur, us⟩= ⟨v, us⟩−⟨v, us⟩= 0.
Given any u ∈U, write u = Pk
s=1 dsus with ds ∈C, and note
⟨v −x, u⟩=
k
X
s=1
ds⟨v −x, us⟩=
k
X
s=1
ds · 0 = 0.
228
Advanced Linear Algebra
Hence, v −x is orthogonal to every u ∈U. In particular, for any y ∈U, x −y is in the
subspace U, so the Pythagorean Identity gives
||v −y||2 = ||(v −x) + (x −y)||2 = ||v −x||2 + ||x −y||2.
Now ||x −y||2 = ⟨x −y, x −y⟩≥0, with equality iff x −y = 0. So ||v −y||2 ≥||v −x||2
for all y ∈U, with equality iff y = x. So x as defined here is the unique vector in U with
minimum distance to v.
The preceding theorem shows that any v ∈V can be decomposed into a sum v = x +
(v−x), where x = Pk
r=1⟨v, ur⟩ur ∈U and v −x is orthogonal to every vector in U. Observe
that v is in the subspace U spanned by (u1, . . . , ur) iff v itself is the unique vector in U
with minimum distance to v iff x = v iff v −x = 0 iff v = Pk
r=1⟨v, ur⟩ur. This gives an
algorithm for detecting when a given v ∈V is a linear combination of the orthonormal list
(u1, . . . , uk) and finding the coefficients of the linear combination when it exists.
For example, let u1 =
1
2(1, 1, 1, 1), u2 =
1
2(1, −1, 1, −1), and u3 =
1
2(−1, −1, 1, 1) in
V = C4. You can check that (u1, u2, u3) is an orthonormal list. Given v = (−5, 1, 3, 9),
we compute ⟨v, u1⟩= 4, ⟨v, u2⟩= −6, and ⟨v, u3⟩= 8. Then x = 4u1 −6u2 + 8u3 = v,
so v ∈U and we have written v as a specific linear combination of u1, u2, u3. For v =
(2, −3, 0, 1), we compute ⟨v, u1⟩= 0, ⟨v, u2⟩= 2, and ⟨v, u3⟩= 1. Here, x = 0u1 + 2u2 +
1u3 = 1
2(1, −3, 3, −1) is the orthogonal projection of v onto U. We can see directly that
v −x =
1
2(3, −3, −3, 3) is orthogonal to x, u1, u2, u3, and hence to every vector in U.
Although v −x is not a unit vector, we can change it into one by dividing by its length,
obtaining u4 = 1
2(1, −1, −1, 1). We now have a longer orthonormal list (u1, u2, u3, u4), which
(being linearly independent) must be a basis of C4. This illustrates the basic step in the
Gram–Schmidt Orthonormalization Algorithm, which we describe in the next section.
9.2
Gram–Schmidt Orthonormalization Algorithm
Given a complex inner product space V , a finite-dimensional subspace U, and a vector
v ∈V , our test for determining whether v is in the subspace U requires us to know an
orthonormal basis of U. This raises the question of how to convert an arbitrary basis for a
subspace U into an orthonormal basis of U.
Let (v1, . . . , vk) be any linearly independent list of vectors in V . We describe a process,
called the Gram–Schmidt Orthonormalization Algorithm, that produces an orthonormal
list (u1, . . . , uk) such that for all r between 1 and k, the sublist (v1, . . . , vr) spans the
same subspace as the sublist (u1, . . . , ur). The algorithm executes the following steps for
s = 1, 2, . . . , k in this order. Upon starting step s, vectors u1, . . . , us−1 have already been
found. Calculate xs = Ps−1
r=1⟨vs, ur⟩ur (taking x1 = 0), and set us = (vs −xs)/||vs −xs||.
This algorithm computes the next vector us by subtracting from vs its orthogonal projection
xs onto the span of the preceding vectors, then normalizing to get a unit vector.
To see that the algorithm has the required properties, we prove by induction on r that
(u1, . . . , ur) is an orthonormal basis for the subspace Vr spanned by (v1, . . . , vr), for all r
between 1 and k. In the base case (r = 1), x1 = 0 and v1 ̸= 0, so u1 = v1/||v1|| is a
well-defined unit vector. Because u1 is a nonzero scalar multiple of v1, (u1) is a basis of V1.
For the induction step, fix r with 1 < r ≤k, and assume (u1, . . . , ur−1) is an orthonormal
basis of Vr−1. On one hand, since (v1, . . . , vr) is linearly independent by hypothesis, vr is not
in the subspace Vr−1 spanned by (v1, . . . , vr−1). Since (u1, . . . , ur−1) spans this subspace by
Matrix Factorizations
229
assumption, the orthogonal projection xr is not equal to vr. Thus vr −xr ̸= 0, and hence
we may divide this vector by its length to get a unit vector ur. On the other hand, we
proved earlier that vr −xr is orthogonal to every vector in the space Vr−1, and the same
is true of the scalar multiple ur of vr −xr. In particular, ur is orthogonal to u1, . . . , ur−1,
proving orthonormality (and hence linear independence) of the list (u1, . . . , ur). Finally,
since xr is in Vr−1 and ur = ||vr −xr||−1(vr −xr), we see that ur is in the subspace Vr
spanned by (v1, . . . , vr). Since the r linearly independent vectors u1, . . . , ur all belong to the
r-dimensional subspace Vr, these vectors must be an orthonormal basis for this subspace,
completing the induction step.
For example, suppose we are given the linearly independent vectors
v1 = (2, 2, 2, 2), v2 = (4, −1, 4, −1), v3 = (−4, −2, 0, 2), v4 = (2, −3, 0, 1)
in V = C4. We execute the Gram–Schmidt algorithm as follows:
x1 = 0,
u1 = v1/||v1|| = 1
2(1, 1, 1, 1);
x2 = ⟨v2, u1⟩u1 = 3u1 = 1
2(3, 3, 3, 3),
v2 −x2 = 1
2(5, −5, 5, −5),
u2 = 1
2(1, −1, 1, −1);
x3 = ⟨v3, u1⟩u1 + ⟨v3, u2⟩u2 = −2u1 −2u2 = (−2, 0, −2, 0),
v3 −x3 = (−2, −2, 2, 2),
u3 = 1
2(−1, −1, 1, 1);
and (see the example at the end of §9.1)
x4 = 0u1 + 2u2 + 1u3,
v4 −x4 = 1
2(3, −3, −3, 3),
u4 = 1
2(1, −1, −1, 1).
The output is the orthonormal basis (u1, u2, u3, u4) of C4.
We close with a few remarks about the Gram–Schmidt orthonormalization process.
First, this algorithm provides a constructive proof of the theorem that every subspace of a
finite-dimensional complex inner product space has an orthonormal basis. Second, we can
use the algorithm to see that every orthonormal list in a finite-dimensional inner product
space V can be extended to an orthonormal ordered basis of V (Exercise 15). Third, the
algorithm can be applied to real inner product spaces by restricting to real scalars. Fourth,
the formulas in the algorithm remain valid for an infinite-dimensional complex inner product
space. In this setting, the formulas transform a countably infinite linearly independent
sequence (v1, v2, . . . , vn, . . .) into an infinite orthonormal sequence (u1, u2, . . . , un, . . .) such
that (v1, . . . , vn) and (u1, . . . , un) span the same subspace for all positive integers n.
However, calling this process an “algorithm” is misleading, since the calculation does not
terminate in finitely many steps.
Finally, we can apply the algorithm to a list of vectors (v1, . . . , vk) that may be
linearly dependent. If the list is linearly dependent and r is minimal such that vr is a
linear combination of v1, . . . , vr−1, then the algorithm detects this by computing that the
orthogonal projection xr is equal to vr, hence vr−xr = 0. In this case, we leave ur undefined,
discard vr, and continue processing vr+1, vr+2, and so on. If further linear dependencies
exist, they are detected as they arise. In the end, the algorithm tells us exactly which vectors
vr depend linearly on preceding vectors and provides an orthonormal list (of length at most
k) that spans the same subspace as the list (v1, . . . , vk).
230
Advanced Linear Algebra
9.3
Gram–Schmidt QR Factorization
We now recast our results on the Gram–Schmidt Orthonormalization Algorithm as a
theorem about matrix factorizations.
Theorem on QR Factorizations (Full Rank Case). Suppose a matrix A ∈Mn,k(C) has
k linearly independent columns. There exist unique matrices Q ∈Mn,k(C) and R ∈Mk(C)
such that A = QR, Q has orthonormal columns, and R is upper-triangular with all diagonal
entries in R>0. When A has real entries, Q and R are real as well.
Proof. To prove existence of the factorization, we consider the complex inner product space
Cn (viewed as a set of column vectors) and look at the linearly independent list of vectors
(v1, . . . , vk), where vj = A[j] is the jth column of the given matrix A. The Gram–Schmidt
Orthonormalization Algorithm produces an orthonormal list (u1, . . . , uk) such that, for 1 ≤
j ≤k, the subspace Vj spanned by (v1, . . . , vj) equals the subspace spanned by (u1, . . . , uj).
Let Q ∈Mn,k(C) be the matrix with columns Q[j] = uj ∈Cn for 1 ≤j ≤k. Define an
upper-triangular matrix R ∈Mk(C) by setting R(i, j) = ⟨vj, ui⟩for 1 ≤i ≤j ≤k, and
R(i, j) = 0 for all i > j. Note that Q and R have real entries if A does.
To check that A = QR, we use facts established in §4.7 and §9.1. First, the jth
column of QR is Q(R[j]). Second, Q(R[j]) is a linear combination of the columns of
Q with coefficients given by the entries of R[j]. Specifically, the jth column of QR is
R(1, j)Q[1] + R(2, j)Q[2] + · · · + R(k, j)Q[k]. Using the definitions of Q and R, the jth
column of QR is
⟨vj, u1⟩u1 + ⟨vj, u2⟩u2 + · · · + ⟨vj, uj⟩uj.
Since vj is in Vj, which is the span of (u1, . . . , uj), the linear combination just written must
be equal to vj (see §9.1). This means that the jth column of QR is vj = A[j] for all j, so
A = QR as needed.
We see that the diagonal entries of R are strictly positive real numbers as follows. Recall
from the description of the orthonormalization algorithm that uj = (vj −xj)/||vj −xj||,
where xj
is the orthogonal projection of vj
onto the subspace Vj−1. We know
that vj −xj is orthogonal to everything in Vj−1, hence is orthogonal to xj. Then
||vj −xj||2 = ⟨vj −xj, vj −xj⟩= ⟨vj, vj −xj⟩−⟨xj, vj −xj⟩= ⟨vj, vj −xj⟩. So, the diag-
onal entry R(j, j) is ⟨vj, uj⟩= ⟨vj, vj −xj⟩/||vj −xj|| = ||vj−xj||2
||vj−xj|| = ||vj −xj|| ∈R>0.
Before discussing uniqueness, we consider an example. Suppose A =


2
4
−4
2
−1
−2
2
4
0
2
−1
2

.
By the calculations in §9.2 and the definitions above, A = QR holds for
Q =


1/2
1/2
−1/2
1/2
−1/2
−1/2
1/2
1/2
1/2
1/2
−1/2
1/2

,
R =


4
3
−2
0
5
−2
0
0
4

.
Returning to the general case, we now prove uniqueness of Q and R. Assume A = QR =
Q1R1, where Q, Q1 ∈Mn,k(C) both have orthonormal columns and R, R1 ∈Mk(C) are
both upper-triangular with all diagonal entries in R>0. Let the columns of A be v1, . . . , vk,
the columns of Q be u1, . . . , uk, and the columns of Q1 be z1, . . . , zk. We show by strong
induction on j that uj = zj and R[j] = R[j]
1
for 1 ≤j ≤k. Fix j in this range, and
Matrix Factorizations
231
assume us = zs and R[s] = R[s]
1
is already known for all s with 1 ≤s < j. On one hand,
consideration of the jth column of A = QR shows (as in the existence proof) that
vj = R(1, j)u1 + · · · + R(j −1, j)uj−1 + R(j, j)uj.
Taking the inner product of each side with ui, we see that R(i, j) = ⟨vj, ui⟩for 1 ≤i ≤j.
Since R is upper-triangular, R(i, j) = 0 for j < i ≤k.
On the other hand, looking at the jth column of A = Q1R1 gives
vj
=
R1(1, j)z1 + · · · + R1(j −1, j)zj−1 + R1(j, j)zj
=
R1(1, j)u1 + · · · + R1(j −1, j)uj−1 + R1(j, j)zj.
Since the list (z1, . . . , zj−1, zj) = (u1, . . . , uj−1, zj) is orthonormal, taking the inner product
of each side with ui gives R1(i, j) = ⟨vj, ui⟩= R(i, j) for 1 ≤i < j. Since R1 is upper-
triangular, R1(i, j) = 0 for j < i ≤k. Now, equating the two expressions for vj and
cancelling Pj−1
i=1⟨vj, ui⟩ui, we deduce that R(j, j)uj = R1(j, j)zj. Taking the length of both
sides gives |R(j, j)| = |R1(j, j)|. Since both R(j, j) and R1(j, j) are positive real numbers,
we conclude finally that R(j, j) = R1(j, j) and uj = zj.
We can extend the QR factorization to arbitrary matrices, as follows. Define a
rectangular matrix R to be upper-triangular iff R(i, j) = 0 for all i > j.
Theorem on QR Factorizations (General Case). Suppose a matrix A ∈Mn,k(C) has
column rank r. There exist matrices Q ∈Mn,r(C) and R ∈Mr,k(C) such that A = QR, Q
has orthonormal columns, and R is upper-triangular with all diagonal entries in R≥0.
Proof. Let the columns of A be v1, . . . , vk. Let the columns of Q be the orthonormal list
(u1, . . . , ur) produced when the Gram–Schmidt algorithm is applied to the list (v1, . . . , vk).
We have r < k if some vector vj is a linear combination of previous vectors vi. Let R(i, j) =
⟨vj, ui⟩for 1 ≤i ≤j ≤k, and R(i, j) = 0 for r ≥i > j ≥1. As in the earlier existence
proof, A = QR follows from the fact that each vj is in the span of the orthonormal list
(u1, . . . , umin(j,r)). If some vj is in the span of (u1, . . . , uj−1), then R(j, j) is 0. Otherwise,
R(j, j) ∈R>0 by the reasoning used before.
By deleting the columns of A corresponding to those vj that depend linearly on earlier
vi, and deleting the same columns of R, we obtain a factorization A′ = QR′ of the type
initially discussed, with A′ ∈Mn,r(C) having full column rank and R′ ∈Mr,r(C) being
square and upper-triangular with all diagonal entries strictly positive.
9.4
Householder Reflections
Our next goal is to derive a version of the QR factorization in which an arbitrary matrix
A ∈Mn,k(C) is factored as A = QR, where Q ∈Mn(C) is a unitary matrix and R ∈Mn,k(C)
is an upper-triangular matrix. We can obtain such a factorization by modifying the proof
given earlier based on Gram–Schmidt orthonormalization (Exercise 19). This section and
the next one describe Householder’s Algorithm for reaching this factorization, which has
certain computational advantages over algorithms using the Gram–Schmidt Algorithm.
The key idea is that we can apply a sequence of reflections to transform A into an
upper-triangular matrix by forcing the required zeroes to appear, one column at a time. In
Rn, a reflection is a linear map that sends a given nonzero vector v to −v and sends w to
232
Advanced Linear Algebra
w for each vector w perpendicular to v. The subspace of all such vectors w, which is the
orthogonal complement of {v}, forms the mirror through which v is being reflected.
Let us define reflections formally in the complex inner product space Cn with the
standard inner product ⟨x, y⟩= Pn
k=1 xkyk = y∗x for x, y ∈Cn. Fix a nonzero vector
v ∈Cn. Starting with any basis (v, v2, . . . , vn) of Cn, use the Gram–Schmidt Algorithm to
obtain an orthonormal ordered basis B = (u1, u2, . . . , un) of Cn, where u1 = v/||v||. There
is a unique linear map Tv : Cn →Cn defined by setting
Tv(c1u1 + c2u2 + · · · + cnun) = −c1u1 + c2u2 + · · · + cnun
(9.1)
for all c1, . . . , cn ∈C. Observe that Tv sends any scalar multiple of u1 to its negative,
whereas Tv sends any linear combination of u2, . . . , un to itself. In particular, Tv(v) = −v
and Tv(w) = w for all w orthogonal to v. These formulas uniquely determine the linear
map Tv and show that Tv does not depend on the choice of the orthonormal basis B. The
matrix of Tv relative to the ordered basis B is diagonal with diagonal entries −1, 1, . . . , 1.
This matrix is unitary and equal to its own inverse, so ||Tv(z)|| = ||z|| for all z ∈Cn, and
T −1
v
= Tv.
Define a matrix Qv = In −(2/||v||2)vv∗∈Mn(C). We claim that Tv(y) = Qvy for all
y ∈Cn. By linearity, it suffices to verify this for all y ∈{v, u2, . . . , un}. For y = v, recall
that ||v||2 = ⟨v, v⟩= v∗v and compute
Qvv = Inv −

2
||v||2 vv∗

v = v −
2
v∗v v(v∗v) = v −2v = −v = Tv(v).
For y = uj with 2 ≤j ≤n, note v∗uj = ⟨uj, v⟩= ⟨uj, ||v||u1⟩= 0, so
Qvuj = Inuj −

2
||v||2 vv∗

uj = uj −
2
||v||2 v(v∗uj) = uj = Tv(uj).
So the claim holds. We call Tv the Householder transformation determined by v, and we call
Qv the Householder matrix determined by v. We also refer to Tv and Qv as Householder
reflections to emphasize the geometric character of these maps and matrices.
You can check from the explicit formula that Qv is unitary (Q∗
vQv = QvQ∗
v = In) and
self-inverse (Q−1
v
= Qv); these facts also follow from the corresponding properties of Tv
or [Tv]B. It is also routine to confirm that for any nonzero scalar c ∈C, Tcv = Tv and
Qcv = Qv. Note that if v is a unit vector, the formula for Qv simplifies to Qv = In −2vv∗,
and we can always arrange this by replacing v by an appropriate scalar multiple of itself.
We also define T0 = idCn and Q0 = In.
The following lemma shows how to find Householder reflections sending a given vector
x to certain other vectors having the same length as x.
Lemma on Householder Reflections. For any x, y ∈Cn with x ̸= y and ||x|| = ||y||
and ⟨x, y⟩∈R, there exists v ∈Cn with Qvx = y. In fact, Qvx = y iff v is a nonzero scalar
multiple of x −y.
Proof. Fix x ̸= y in Cn with ||x|| = ||y|| and ⟨x, y⟩∈R. Define v = x −y and w = x + y;
the geometric motivation for choosing v and w in this way is suggested in Figure 9.1. The
hypothesis ⟨x, y⟩∈R implies ⟨x, y⟩= ⟨x, y⟩= ⟨y, x⟩, so that
⟨v, w⟩= ⟨x −y, x + y⟩= ⟨x, x⟩−⟨y, x⟩+ ⟨x, y⟩−⟨y, y⟩= ||x||2 −||y||2 = 0.
Since w is orthogonal to v, Tv(w) = w. Now x = (x −y)/2 + (x + y)/2 = v/2 + w/2, so
Tv(x) = Tv(v/2 + w/2) = 1
2Tv(v) + 1
2Tv(w) = (−v)/2 + w/2 = (y −x)/2 + (x + y)/2 = y
(cf. Figure 9.1). Thus, Qvx = y, and hence Qcvx = y for any nonzero c ∈C.
Matrix Factorizations
233
w/2
mirror
x
y
v/2
−v/2
FIGURE 9.1
Finding a reflection that sends x to y.
To prove the converse, assume z ∈Cn satisfies Tz(x) = y. Then Tz(y) = T −1
z
(y) = x,
so Tz(v) = Tz(x −y) = Tz(x) −Tz(y) = y −x = −v. From the defining formula for Tz
(see (9.1) with v there replaced by z), we find that the only vectors sent to their negatives
by Tz are scalar multiples of z. So v is a nonzero scalar multiple of z and vice versa.
Observe that the v specified in the lemma is not unique even if we insist that v be a
unit vector, since we can still replace v by eiθv for any θ ∈[0, 2π). In order for v to exist,
we must assume ||x|| = ||y|| and ⟨x, y⟩∈R (see Exercise 27).
9.5
Householder QR Factorization
Given A ∈Mn,k(C), Householder’s QR algorithm finds a factorization A = QR, where
Q ∈Mn(C) is unitary and R ∈Mn,k(C) is upper-triangular with nonnegative diagonal
entries, by determining a sequence of Householder reflections and a diagonal unitary matrix
whose product is Q. The main idea is to use the lemma of §9.4 repeatedly to create the zero
entries in R one column at a time.
To begin, let x = A[1] ∈Cn be the first column of A. Write x(1) = A(1, 1) = seiθ
for some nonnegative real s and θ ∈[0, 2π). Define y = −||x||eiθe1 ∈Cn, where e1 is
the standard basis vector (1, 0, . . . , 0). Note that ||y|| = ||x|| and ⟨x, y⟩= −seiθ||x||e−iθ
is real. (Exercise 34 discusses why the negative sign is present in the definition of y.) Let
v1 = x −y, and consider the matrix Qv1A. The first column of this matrix is Qv1x = y,
which is a multiple of e1 and hence has zeroes in rows 2 through n. Since Q0 = In, this
conclusion holds even if x = 0 or x = y.
At the next stage, let A2 be the submatrix of Qv1A obtained by deleting the first row
and column. Repeat the construction in the previous paragraph, taking x ∈Cn−1 to be the
first column of A2, to obtain v′
2 ∈Cn−1 such that Qv′
2A2 ∈Mn−1,k−1(C) has zeroes in the
first column below row 1. Construct v2 ∈Cn by preceding v′
2 with a zero. The first row and
column of v2v∗
2 contain all zeroes, so Qv2 is a block-diagonal matrix with diagonal blocks I1
and Qv′
2. It follows that Qv2Qv1A has all zeroes below the diagonal in the first two columns.
We continue similarly to process the remaining columns. After j steps, we have found
a matrix Qvj · · · Qv1A with zeroes below the diagonal in the first j columns. Repeat
the construction in step 1, taking x ∈Cn−j to be the first column of the lower-right
(n −j) × (k −j) submatrix of the current matrix. We thereby obtain v′
j+1 ∈Cn−j such
234
Advanced Linear Algebra
that Qv′
j+1 sends x to a multiple of e1 ∈Cn−j. Let vj+1 ∈Cn be v′
j+1 preceded by j zeroes,
so that Qvj+1 is block-diagonal with diagonal blocks Ij and Qv′
j+1. Then Qvj+1Qvj · · · Qv1A
has all zeroes below the diagonal in the first j + 1 columns.
After s = min(n−1, k) steps, we have a relation Qvs · · · Qv1A = R′, where R′ ∈Mn,k(C)
is upper-triangular. Left-multiplying both sides by an appropriate diagonal matrix D with
all diagonal entries of the form eiθ, we can obtain a factorization DQvs · · · Qv1A = R where
R is upper-triangular with nonnegative real diagonal entries. Solving for A and recalling
that Q−1
vj = Qvj for all j, we get A = QR with Q = Qv1Qv2 · · · QvsD−1. The matrix Q is
unitary, being a product of unitary matrices.
We illustrate Householder’s QR algorithm on the matrix
A =


−5
3
−9
1
−4
1
3
12
8
3
−2
0
4
9
1
−4

.
In the first step, x = (−5, −4, 8, 4)T, ||x|| = 11, x(1) = −5 = 5eiπ, so y = −||x||eiπe1 =
(11, 0, 0, 0). Using v1 = x −y = (−16, −4, 8, 4)T, we find that
Qv1 = 1
11


−5
−4
8
4
−4
10
2
1
8
2
7
−2
4
1
−2
10

,
Qv1A = 1
11


121
41
21
−69
0
13
63
112
0
29
−82
40
0
97
−19
−24

.
In the second step, we look at the lower-right 3×3 block and take x = (13/11, 29/11, 97/11)T,
||x|| = 9.27941, x(1) = (13/11)ei0, so y = (−9.27941, 0, 0)T. Taking v′
2 = x −y gives
v2 = (0, 10.4612, 2.63636, 8.81818)T,
Qv2 =


1
0
0
0
0
−0.127
−0.284
−0.950
0
−0.284
0.928
−0.239
0
−0.950
−0.239
0.199

, Qv2Qv1A =


11
3.727
1.909
−6.273
0
−9.279
3.030
−0.257
0
0
−8.134
1.006
0
0
−4.001
−10.981

.
In the third step, x
=
(−8.134, −4.001)T, ||x||
=
9.065, y
=
(9.065, 0)T, and
v3 = (0, 0, −17.199, −4.001)T, so
Qv3 =


1
0
0
0
0
1
0
0
0
0
−0.897
−0.441
0
0
−0.441
0.897

, Qv3Qv2Qv1A =


11
3.727
1.909
−6.273
0
−9.279
3.030
−0.257
0
0
9.065
3.944
0
0
0
−10.297

.
Finally, let D be diagonal with diagonal entries 1, −1, 1, −1 and Q = Qv1Qv2Qv3D−1. Then
A = QR holds with
Q =


−0.455
0.506
−0.728
0.086
−0.364
0.254
0.492
0.749
0.727
0.031
−0.363
0.581
0.363
0.824
0.309
−0.306

,
R =


11
3.727
1.909
−6.273
0
9.279
−3.030
0.257
0
0
9.065
3.944
0
0
0
10.297

.
(9.2)
Matrix Factorizations
235
9.6
LU Factorization
In the next few sections, we discuss decompositions of a square matrix into the product of
a lower-triangular matrix and an upper-triangular matrix. In order to assure the existence
and uniqueness of such a decomposition, we must impose certain hypotheses on the given
matrix A. For any field F and A ∈Mn(F), let A[k] denote the matrix in Mk(F) consisting
of the first k rows and columns of A.
LU Factorization Theorem. For all A ∈Mn(F), there exist a lower-unitriangular matrix
L ∈Mn(F) and an invertible upper-triangular matrix U ∈Mn(F) such that A = LU iff for
all k between 1 and n, det(A[k]) ̸= 0F . L and U are unique if they exist.
Proof. On one hand, assume A = LU for some matrices L and U with the stated properties.
In particular, 0 ̸= det(U) = Qn
i=1 U(i, i). Fix k between 1 and n. By writing A = LU as a
product of block matrices
 A[k]
B
C
D

=
 L[k]
0
M
N
  U[k]
V
0
W

,
(9.3)
we see that A[k] = L[k]U[k], where L[k] is lower-unitriangular and U[k] is upper-triangular.
Taking determinants gives det(A[k]) = det(L[k]) det(U[k]) = Qk
i=1 L(i, i) Qk
i=1 U(i, i) ̸= 0.
So the determinant condition on A is necessary for the LU factorization to exist.
On the other hand, to prove uniqueness, assume A = LU = L1U1 where both L, U
and L1, U1 have the properties stated in the theorem. Since L1 and U are invertible, we
can write L−1
1 L = U1U −1. The matrix L−1
1 L is lower-unitriangular, and the matrix U1U −1
is upper-triangular. These two matrices are equal, so L−1
1 L is upper-triangular and lower-
unitriangular. The only such matrix is In, which implies L = L1 and U = U1.
Next, we assume det(A[k]) ̸= 0 for 1 ≤k ≤n and prove existence of the factorization
A = LU by the following recursive computation. Define L(i, i) = 1F for i ∈[n] and L(i, j) =
U(j, i) = 0F for 1 ≤i < j ≤n. We determine the remaining entries of L one column at a
time, and we simultaneously determine the remaining entries of U one row at a time. Fix k
between 1 and n, and make the induction assumption that we have already determined all
entries in the first k −1 columns of L and all entries in the first k −1 rows of U. Assume
further that A(i, j) = (LU)(i, j) holds for all i, j ∈[n] such that i ≤k −1 or j ≤k −1. For
the base case k = 1, these assumptions are vacuously true.
To continue, we first recover U(k, k) as follows. Comparing the k, k-entries of A and LU,
we see that these entries are equal iff
A(k, k) =
n
X
r=1
L(k, r)U(r, k) =
k−1
X
r=1
L(k, r)U(r, k) + L(k, k)U(k, k).
(9.4)
By hypothesis, every entry in the sum from r = 1 to k −1 is already known. Since we chose
L(k, k) = 1, equation (9.4) holds iff we define
U(k, k) = A(k, k) −
k−1
X
r=1
L(k, r)U(r, k).
(9.5)
236
Advanced Linear Algebra
We now know (among other things) that A(i, j) = (LU)(i, j) for all i, j ∈[k] and that the
zero blocks displayed in (9.3) are present in L and U. So the equation A[k] = L[k]U[k] is
valid. Taking determinants shows that 0 ̸= det(A[k]) = det(L[k]) det(U[k]) = Qk
i=1 U(i, i).
Since the field F has no zero divisors, we conclude that U(k, k) ̸= 0F .
The next step is to compute the unknown entries in column k of L, which are the entries
L(i, k) as i ranges from k + 1 to n. Comparing the i, k-entries of A and LU for each i in
this range, we see that these entries agree iff
A(i, k) =
n
X
r=1
L(i, r)U(r, k) =
k−1
X
r=1
L(i, r)U(r, k) + L(i, k)U(k, k).
The entries L(i, r) and U(r, k) for 1 ≤r ≤k −1 are already known, and we have seen that
U(k, k) ̸= 0F . Thus, A(i, k) = (LU)(i, k) holds iff we define
L(i, k) = U(k, k)−1
"
A(i, k) −
k−1
X
r=1
L(i, r)U(r, k)
#
.
(9.6)
We now know all of column k of L, and we have ensured that A(i, j) = (LU)(i, j) whenever
i < k or j ≤k.
The last step is to compute the unknown entries in row k of U, which are the entries
U(k, j) as j ranges from k + 1 to n. Comparing the k, j-entries of A and LU for each j in
this range, we see that these entries agree iff
A(k, j) =
n
X
r=1
L(k, r)U(r, j) =
k−1
X
r=1
L(k, r)U(r, j) + L(k, k)U(k, j).
The entries L(k, r) and U(r, j) for 1 ≤r ≤k −1 are already known, and we chose
L(k, k) = 1F . So A(k, j) = (LU)(k, j) holds iff we define
U(k, j) = A(k, j) −
k−1
X
r=1
L(k, r)U(r, j).
(9.7)
We now know all of row k of U, and we have ensured that A(i, j) = (LU)(i, j) whenever
i ≤k or j ≤k. This completes step k of the induction, and the existence proof is now
finished.
The factorization A = LU just proved, where L is lower-unitriangular and U is upper-
triangular and invertible, is sometimes called the Doolittle factorization of A. Some variants
of this factorization can be found by changing the conditions on the diagonal entries of L
and U. For example, assuming det(A[k]) ̸= 0 for 1 ≤k ≤n, there exist unique L, D, U in
Mn(F) such that L is lower-unitriangular, D is diagonal and invertible, and U is upper-
unitriangular. With the same assumption on A, there exist unique L, U ∈Mn(F) such
that L is lower-triangular and invertible, and U is upper-unitriangular; this is called the
Crout factorization of A. In the field F = C (or any field where every element has a square
root), we can write A = LU with L(k, k) = U(k, k) for all k by setting L(k, k) = U(k, k)
in (9.4) and solving for U(k, k). We ask the reader to derive these variant factorizations in
Exercise 37.
Matrix Factorizations
237
9.7
Example of the LU Factorization
We illustrate the computation of an LU factorization using the matrix
A =


3
1
−5
2
−9
−1
14
−5
15
9
−23
12
3
9
−9
7

,
which does satisfy the determinant condition det(A[k]) ̸= 0 for 1 ≤k ≤4. Initially, the
unknown factors L and U look like this:
L =


1
0
0
0
a
1
0
0
b
d
1
0
c
e
f
1

,
U =


q
r
s
t
0
u
v
w
0
0
x
y
0
0
0
z

.
Comparing the entries of the matrix equation A = LU leads to the following sixteen scalar
equations in the unknowns a, b, c, . . . , z:
3 = q
1 = r
−5 = s
2 = t
−9 = aq
−1 = ar + u
14 = as + v
−5 = at + w
15 = bq
9 = br + du
−23 = bs + dv + x
12 = bt + dw + y
3 = cq
9 = cr + eu
−9 = cs + ev + fx
7 = ct + ew + fy + z
We see q = 3, then (working down the first column) a = −3, b = 5, c = 1, and (looking
at the first row) r = 1, s = −5, t = 2. From the 2, 2-entry, we now see u = −1 −ar = 2.
Working down the second column, 9 = 5 + 2d gives d = 2, and 9 = 1 + 2e gives e = 4.
Moving along the second row, 14 = 15+v gives v = −1, and −5 = −6+w gives w = 1. The
3, 3-entry gives −23 = −25 −2 + x, so x = 4. Continuing similarly, we find f = 0, y = 0,
and z = 1. So, the Doolittle version of the LU factorization is
A =


3
1
−5
2
−9
−1
14
−5
15
9
−23
12
3
9
−9
7

=


1
0
0
0
−3
1
0
0
5
2
1
0
1
4
0
1

·


3
1
−5
2
0
2
−1
1
0
0
4
0
0
0
0
1

.
We can force U to be unitriangular by factoring out an appropriate diagonal matrix:
A =


1
0
0
0
−3
1
0
0
5
2
1
0
1
4
0
1

·


3
0
0
0
0
2
0
0
0
0
4
0
0
0
0
1

·


1
1/3
−5/3
2/3
0
1
−1/2
1/2
0
0
1
0
0
0
0
1

.
Then we can absorb the diagonal matrix into L to obtain the Crout factorization of A:
A =


3
0
0
0
−9
2
0
0
15
4
4
0
3
8
0
1

·


1
1/3
−5/3
2/3
0
1
−1/2
1/2
0
0
1
0
0
0
0
1

.
238
Advanced Linear Algebra
9.8
LU Factorizations and Gaussian Elimination
There is a close relationship between LU factorizations and Gaussian elimination. To see
how this occurs, let us perform elementary row operations on the matrix A in §9.7 to create
zeroes below the diagonal of A. The first step is to remove the −9 in the 2, 1-entry by adding
3 times row 1 of A to row 2 of A. Recall from §4.9 that this row operation can be achieved
by multiplying A on the left by the elementary matrix E1 obtained from I4 by adding 3
times row 1 to row 2. The new matrix is
E1A =


1
0
0
0
3
1
0
0
0
0
1
0
0
0
0
1

·


3
1
−5
2
−9
−1
14
−5
15
9
−23
12
3
9
−9
7

=


3
1
−5
2
0
2
−1
1
15
9
−23
12
3
9
−9
7

.
We continue by adding −5 times row 1 to row 3 using an elementary matrix E2, which
produces a new matrix
E2(E1A) =


1
0
0
0
0
1
0
0
−5
0
1
0
0
0
0
1

·


3
1
−5
2
0
2
−1
1
15
9
−23
12
3
9
−9
7

=


3
1
−5
2
0
2
−1
1
0
4
2
2
3
9
−9
7

.
Next, we add −1 times row 1 to row 4, obtaining
E3(E2E1A) =


1
0
0
0
0
1
0
0
0
0
1
0
−1
0
0
1

·


3
1
−5
2
0
2
−1
1
0
4
2
2
3
9
−9
7

=


3
1
−5
2
0
2
−1
1
0
4
2
2
0
8
−4
5

.
Moving to column 2, we add −2 times row 2 to row 3:
E4(E3E2E1A) =


1
0
0
0
0
1
0
0
0
−2
1
0
0
0
0
1

·


3
1
−5
2
0
2
−1
1
0
4
2
2
0
8
−4
5

=


3
1
−5
2
0
2
−1
1
0
0
4
0
0
8
−4
5

.
Then we add −4 times row 2 to row 4:
E5(E4E3E2E1A) =


1
0
0
0
0
1
0
0
0
0
1
0
0
−4
0
1

·


3
1
−5
2
0
2
−1
1
0
0
4
0
0
8
−4
5

=


3
1
−5
2
0
2
−1
1
0
0
4
0
0
0
0
1

.
There is already a zero in the 4, 3-position, so we are finished.
Observe that the final matrix (E5E4E3E2E1)A is precisely the upper-triangular matrix
U found in §9.7. Solving for A, we find that A = LU where L = E−1
1 E−1
2 E−1
3 E−1
4 E−1
5 . The
inverse of each elementary matrix Ei is the lower-triangular matrix obtained by negating
the unique nonzero entry of Ei not on the diagonal. Using this observation, you can confirm
that L as defined here agrees with the L found in §9.7. Note that for i > j, L(i, j) is the
negative of the multiple of row j that was added to row i to make the i, j-entry of A become
zero.
Now consider what happens when we perform Gaussian elimination on a general matrix
A ∈Mn(F) satisfying det(A[k]) ̸= 0 for k between 1 and n. The algorithm proceeds as
follows, assuming no divisions by zero occur (we justify this assumption later):
Matrix Factorizations
239
for j=1 to n-1 do
for i=j+1 to n do
m(i,j) = A(i,j)/A(j,j);
replace row i of A by row i minus m(i,j) times row j;
end for;
end for.
To describe this algorithm using elementary matrices, let E(i, j, c) be the elementary matrix
with i, j-entry equal to c, diagonal entries equal to 1, and other entries equal to zero (for
i ̸= j in [n] and c ∈F). For each column index j in the outer loop, the inner loop modifies
the current version of the matrix A by left-multiplying by a product of elementary matrices
Cj = E(n, j, −m(n, j))E(n −1, j, −m(n −1, j)) · · · E(j + 1, j, −m(j + 1, j)),
(9.8)
which create zeroes in column j below the diagonal. The net effect of the algorithm is to
replace A by the matrix
Cn−1 · · · C2C1A = U,
which is an upper-triangular matrix. Solving for A, we can write A = LU with
L = C−1
1 C−1
2
· · · C−1
n−1In.
Observe that
C−1
j
= E(j + 1, j, m(j + 1, j)) · · · E(n −1, j, m(n −1, j))E(n, j, m(n, j)).
To find the entries of L, we can start with In and work from right to left, applying the
elementary row operations encoded by each elementary matrix E(i, j, m(i, j)) as j goes
from n −1 down to 1 and (for each j) i goes from n down to j + 1. Because the elementary
operations are applied in this particular order, you can check that the final result is a lower-
unitriangular matrix L with i, j-entry L(i, j) = m(i, j) for all i > j in [n]. To summarize,
in the Doolittle factorization A = LU, the entries of L encode the multipliers needed when
we use Gaussian elimination to reduce A to the upper-triangular matrix U.
To complete our analysis, we justify the assumption that the computation m(i, j) =
A(i, j)/A(j, j) in the algorithm never involves a division by zero. We show this by induction
on the outer loop variable j. Fix j with 1 ≤j < n, and assume no divisions by zero occurred
in the first j −1 iterations of the outer loop. At this point, the algorithm has transformed
the original matrix A into a new matrix
Cj−1 · · · C2C1A = Uj−1,
where Lj−1 = Cj−1 · · · C2C1 is lower-unitriangular, and Uj−1 has zeroes below the diagonal
in the first j −1 columns. Write Lj−1A = Uj−1 as a product of block matrices
 Lj−1[j]
0
∗
∗
  A[j]
∗
∗
∗

=
 Uj−1[j]
∗
∗
∗

,
(9.9)
where each upper-left block is j×j. We see that Lj−1[j]A[j] = Uj−1[j]. Taking determinants
and noting that Uj−1[j] is upper-triangular, we get
0 ̸= det(A[j]) = det(Lj−1[j]) det(A[j]) = det(Uj−1[j]) =
jY
k=1
Uj−1(k, k).
So the value of A(j, j) used in the jth iteration of the outer loop, namely Uj−1(j, j), is
nonzero, as claimed.
240
Advanced Linear Algebra
9.9
Permuted LU Factorizations
Our next goal is to generalize the analysis in the preceding section to the case where
A ∈Mn(F) is an invertible matrix, but A does not satisfy the condition that all det(A[k])
are nonzero. Such an A cannot have an LU factorization, but we prove there is a permutation
matrix P ∈Mn(F) such that PA has an LU factorization. Recall that a permutation matrix
P has exactly one 1 in each row and column, with all other entries of P equal to 0. For
F = C, the columns of P are orthonormal (relative to the standard inner product on Cn),
so that P ∗P = P TP = In. It follows that P is invertible with P −1 = P T; the same result
holds for any field F (Exercise 39). By §4.8, PA is obtained from A by reordering the rows
of A. Specifically, for each i, j with P(i, j) = 1, row i of PA is row j of A.
Permuted
LU
Factorization
Theorem
(Invertible
Case). For any invertible
A ∈Mn(F), there exist a permutation matrix P ∈Mn(F), a lower-unitriangular matrix
L ∈Mn(F), and an upper-triangular invertible matrix U ∈Mn(F) with PA = LU, or
equivalently A = P TLU.
Proof. The idea of the proof is to reduce A to the upper-triangular matrix U via Gaussian
elimination. The algorithm used before still works, with one modification. When we try to
compute the multipliers m(i, j) to process column j of the current matrix, we may find
that A(j, j) is zero. Recall that the current matrix (when the jth iteration of the outer
loop begins) can be written Uj−1 = Lj−1A, where A is the original input matrix, Lj−1 is
lower-unitriangular, and Uj−1 has zeroes below the diagonal in the first j −1 columns. Since
A and Lj−1 are both invertible, Uj−1 is invertible. Writing Uj−1 in block form as
Uj−1 =
 Uj−1[j −1]
∗
0
∗

,
(9.10)
it follows by taking determinants that for some k ≥j, Uj−1(k, j) ̸= 0. Choose k minimal
with this property and interchange rows j and k at the start of the jth iteration of the
outer loop. The rest of the algorithm proceeds as before.
In terms of elementary matrices, the modified elimination algorithm yields a factorization
Cn−1Pn−1 · · · C3P3C2P2C1P1A = U,
(9.11)
where Cj has the form (9.8), and each Pj is an elementary permutation matrix obtained
from In by switching row j with some row kj ≥j. To obtain the required factorization
PA = LU, we need to move all the permutation matrices Pj to the right through all the
matrices Cr. This manipulation requires some care, since Pj and Cr do not commute in
general.
Consider a product PjCr where 1 ≤r < j ≤n. Each Cr is a product of elementary
matrices of the form E(s, r, m) with s > r and m ∈F. Suppose Pj is obtained by switching
rows j and k ≥j of In. We claim PjE(s, r, m) = E(s′, r, m)Pj, where s′ = k if s = j, s′ = j
if s = k, and s′ = s otherwise. We verify that PjE(j, r, m) = E(k, r, m)Pj, leaving the other
assertions in the claim as exercises. It suffices to show that PjE(j, r, m)B = E(k, r, m)PjB
for any matrix B ∈Mn(F). The left side is the matrix obtained from B by first adding m
times row r to row j, then switching rows j and k. The right side is the matrix obtained
from B by first switching rows j and k, then adding m times row r to row k. Since r is
different from j and k, the effect of these two operations on B is the same. The rest of the
claim is verified similarly.
Matrix Factorizations
241
By using the claim repeatedly to move Pj to the right past each factor in Cr, we see that
for all r < j, PjCr = C′
rPj where C′
r is also a product of elementary matrices E(s′, r, m)
with s′ ranging from r+1 to n in some order (cf. (9.8)). Using this in (9.11), we can first write
P2C1 = C′
1P2 and continue by rewriting C3P3C2C′
1P2P1 as C3C′
2C′′
1 P3P2P1. Ultimately, we
reach a new factorization
˜Cn−1 · · · ˜C2 ˜C1(Pn−1 · · · P2P1)A = U,
where each ˜Cj has the form (9.8) (with factors appearing in a different order). The product
Pn−1 · · · P2P1 is a permutation matrix P. We now have PA = LU, where L = ˜C−1
1
· · · ˜C−1
n−1
is lower-unitriangular.
Finally, we extend our results to the case of a general square matrix A that need not be
invertible.
Permuted LU Factorization Theorem (General Case). For any A ∈Mn(F), there
exist permutation matrices P, Q ∈Mn(F), a lower-unitriangular L ∈Mn(F), and an upper-
triangular U ∈Mn(F) with rank(A) = rank(U), such that PAQ = LU.
Proof. Examining the preceding proof, we see that the only place we needed invertibility of
A was when we used (9.10) to show Uj−1(k, j) ̸= 0 for some k ≥j. Consider what happens
when our algorithm reduces a general matrix A to the matrix Uj−1 given in (9.10).
Case 1: There is a nonzero entry somewhere in the lower-right block shown in (9.10),
say Uj−1(k, p) ̸= 0 for some k, p ≥j. We can multiply the current matrix on the right by
the permutation matrix Qj that switches columns j and p, and then multiply on the left (as
before) by the permutation matrix Pj that switches rows j and k, to create a matrix with a
nonzero entry in the j, j-position. The algorithm proceeds to create zeroes below this entry,
as before.
Case 2: All entries in the lower-right block of (9.10) are zero. Then the algorithm
terminates at this point with a factorization
Cj−1Pj−1 · · · C1P1AQ1Q2 · · · Qj−1 = Uj−1 = U,
where U is upper-triangular since Uj−1[j −1] is upper-triangular and rows j through n of
Uj−1 contain all zeroes. Moving the matrices P1, . . . , Pj−1 to the right (as in the invertible
case) and rearranging, we obtain the required factorization PAQ = LU. Since P, Q, and
L are all invertible, and multiplying on the left or right by an invertible matrix does not
change the rank, we have rank(A) = rank(U).
9.10
Cholesky Factorization
Cholesky Factorization Theorem. For every positive semidefinite matrix A ∈Mn(C),
there exists a lower-triangular matrix L with nonnegative diagonal entries such that A =
LL∗. If A is positive definite (hence invertible), then L is unique.
Proof. Recall from §7.11 that a positive semidefinite matrix A ∈Mn(C) has a unique
positive semidefinite square root, which is a positive semidefinite matrix B ∈Mn(C) such
that A = B2. Since B must be Hermitian, we have A = B∗B. Now, B has a QR factorization
B = QR, where Q ∈Mn(C) is unitary and R ∈Mn(C) is upper-triangular with nonnegative
diagonal entries. Let L = R∗, which is lower-triangular with nonnegative diagonal entries.
Since Q is unitary, we have A = B∗B = (QR)∗QR = R∗Q∗QR = R∗R = LL∗, as needed.
242
Advanced Linear Algebra
To prove uniqueness when A is invertible, suppose A = LL∗= MM ∗where both
L, M ∈Mn(C) are lower-triangular with nonnegative diagonal entries. Taking determinants,
we see that L and M are both invertible. Then M −1L = M ∗(L∗)−1, where the left side is
a lower-triangular matrix and the right side is an upper-triangular matrix. Since the two
sides are equal, M −1L must be a diagonal matrix D with strictly positive diagonal entries.
Then L = MD, so MM ∗= LL∗= MDD∗M ∗= MD2M ∗. Left-multiplying by M −1 and
right-multiplying by (M ∗)−1 gives I = D2. Since all diagonal entries of D are positive real
numbers, this forces D = I and M = L, completing the uniqueness proof.
One approach to computing the Cholesky factorization of a positive definite matrix A is
to observe that A = LL∗is one of the variations of the LU factorization from §9.6, in which
L is lower-triangular, L∗= U is upper-triangular, and L(k, k) = L∗(k, k) for all k ∈[n].
We can recover the columns of L (and hence the rows of U = L∗) one at a time using the
formulas in §9.6. Specifically, for k = 1 to n, we first calculate
L(k, k) =
v
u
u
tA(k, k) −
k−1
X
r=1
L(k, r)L∗(r, k)
and then compute
L(i, k) = L(k, k)−1
"
A(i, k) −
k−1
X
r=1
L(i, r)L∗(r, k)
#
for i = k+1, . . . , n. Since we proved above that there is a factorization of the form A = LL∗,
the uniqueness of the LU factorization ensures that the U computed by the LU algorithm
must equal L∗. In particular, there is no need to use (9.7) to solve for the entries of U in
this setting.
For example, consider A =


4
2
−1
2
5
1
−1
1
3

. By the determinant test in §7.15, A is
positive definite. We seek a factorization A = LL∗, say


4
2
−1
2
5
1
−1
1
3

=


r
0
0
s
u
0
t
v
w




r
s
t
0
u
v
0
0
w

.
(Since A is real and positive definite, L must be real and L∗= LT.) Comparing 1, 1-entries,
we find r2 = 4 and r = 2. Working down the first column, sr = 2 and tr = −1, so s = 1
and t = −1/2. Moving to the 2, 2-entry, s2 + u2 = 5 becomes 1 + u2 = 5, so u = 2. Looking
at the 3, 2-entry, 1 = st + vu implies 1 = −1/2 + 2v, so v = 3/4. Finally, t2 + v2 + w2 = 3
gives 1/4 + 9/16 + w2 = 3, so w =
p
35/16 =
√
35/4. We can check the computation by
confirming that


4
2
−1
2
5
1
−1
1
3

=


2
0
0
1
2
0
−1/2
3/4
√
35/4




2
1
−1/2
0
2
3/4
0
0
√
35/4

.
9.11
Least Squares Approximation
This section describes an application of the Cholesky factorization to the solution of a least
squares approximation problem. Our goal is to solve a linear system of the form Ax = b,
Matrix Factorizations
243
where A ∈Mm,n(C) is a given matrix with m ≥n, b ∈Cm is a given column vector, and
x ∈Cn is unknown. This linear system typically has many more equations than unknowns,
so we would not expect there to be any exact solutions. Instead, we seek an approximate
solution x that minimizes the length of the error vector Ax −b. The squared length of this
error vector is Pm
i=1 |(Ax)i −bi|2, which is the sum of the squares of the errors in each
individual equation.
Theorem on Least Squares Approximation. Suppose m ≥n and A ∈Mm,n(C) has
rank n (so the n columns of A are linearly independent). For each b ∈Cm, there exists a
unique x ∈Cn minimizing ||Ax −b||2, and this x is the unique solution to the square linear
system (A∗A)x = A∗b.
The linear equations in the system (A∗A)x = A∗b are called normal equations for x,
and x is called the least squares approximate solution to Ax = b.
Proof. We first check that A∗A ∈Mn(C) is invertible. Fix z ∈Cn with A∗Az = 0; it suffices
to show z = 0 (see Table 4.1). Note that ||Az||2 = ⟨Az, Az⟩= (Az)∗(Az) = z∗A∗Az = 0, so
Az = 0. If z were not zero, then writing Az = 0 as a linear combination of the n columns of
A (as explained in §4.7) would show that these columns were linearly dependent, contrary
to hypothesis. So z = 0, A∗A is invertible, and the normal equations have a unique solution
x = (A∗A)−1(A∗b).
Now let y ∈Cn be arbitrary. Since x solves the normal equations, the vectors Ax−b and
A(y−x) are orthogonal: ⟨Ax−b, A(y−x)⟩= (y−x)∗A∗(Ax−b) = (y−x)∗(A∗Ax−A∗b) = 0.
Using the Pythagorean Identity, we compute
||Ay −b||2 = ||A(y −x) + (Ax −b)||2 = ||A(y −x)||2 + ||Ax −b||2 ≥||Ax −b||2.
This inequality shows that among all vectors in Cn, the x chosen above minimizes ||Ax−b||2.
Furthermore, equality holds iff ||A(y −x)||2 = 0, which implies A(y −x) = 0, hence (as seen
in the last paragraph) y −x = 0 and y = x. So the vector x minimizing the sum of the
squares of the errors is unique, as needed.
To see how the Cholesky factorization can be used here, note that A∗A is positive
definite, since for all nonzero v ∈Cn, Av ̸= 0 and so v∗A∗Av = ⟨Av, Av⟩= ||Av||2 is
a positive real number. We can therefore write A∗A = LL∗for a unique lower-triangular
L ∈Mn(C). The normal equations characterizing the least squares solution x become
LL∗x = A∗b. If we have found L, we can quickly solve the normal equations by first solving
Lw = A∗b using forward substitution, then solving L∗x = w using backward substitution.
We must point out, however, that solving least squares problems via the normal equations
can lead to issues of numerical stability. Exercise 52 describes another approach to finding
x based on a QR factorization of A.
9.12
Singular Value Decomposition
Singular Value Decomposition Theorem. For any A ∈Mm,n(C), there exist a unitary
matrix U ∈Mn(C), a unitary matrix V ∈Mm(C), and a diagonal matrix D ∈Mm,n(R)
with diagonal entries d1 ≥d2 ≥· · · ≥dmin(m,n) ≥0, such that A = V DU ∗. The numbers dj
are uniquely determined by A; the nonzero dj are the positive square roots of the nonzero
eigenvalues of A∗A.
We call d1, . . . , dmin(m,n) the singular values of A.
244
Advanced Linear Algebra
Proof. We proved the singular value decomposition for square matrices as a consequence
of the polar decomposition in §7.13. This section gives an independent proof valid for
rectangular matrices. Fix A ∈Mm,n(C). The matrix A∗A ∈Mn(C) is positive semidefinite,
since for any x ∈Cn, x∗A∗Ax = (Ax)∗(Ax) = ⟨Ax, Ax⟩≥0. We saw in Chapter 7 that
positive semidefinite matrices are normal, hence unitarily diagonalizable via an orthonormal
basis of eigenvectors, and all their eigenvalues are nonnegative real numbers. Let the
eigenvalues of A∗A (with multiplicity) be c1 ≥c2 ≥· · · ≥cn ≥0. Let u1, u2 . . . , un ∈Cn be
associated unit eigenvectors, so (u1, . . . , un) is an orthonormal basis of Cn and A∗Auj = cjuj
for 1 ≤j ≤n. Let dj = √cj ≥0 for 1 ≤j ≤n and r be the maximal index with cj > 0. For
1 ≤j ≤r, define vj = d−1
j Auj ∈Cm. We claim (v1, . . . , vr) is an orthonormal list in Cm.
To see why, fix i, j between 1 and r and compute
⟨vj, vi⟩= (d−1
i Aui)∗(d−1
j Auj) = d−1
i d−1
j u∗
i A∗Auj = d−1
i d−1
j u∗
i (cjuj) = (dj/di)⟨uj, ui⟩.
Since the uk are orthonormal, we get ⟨vj, vi⟩= 0 if j ̸= i, and ⟨vj, vi⟩= di/di = 1 if j = i.
Using the Gram–Schmidt Orthonormalization Algorithm, we can extend the orthonor-
mal list (v1, . . . , vr) to an orthonormal basis (v1, . . . , vr, . . . , vm) of Cm. Let U ∈Mn(C)
be the matrix with columns u1, . . . , un and V ∈Mm(C) be the matrix with columns
v1, . . . , vm. Both U and V are unitary matrices, since they are square matrices with
orthonormal columns (see §7.5). Let D ∈Mm,n(C) have diagonal entries D(j, j) = dj
for 1 ≤j ≤min(m, n), and all other entries 0. To prove that A = V DU ∗, we prove the
equivalent identity AU = V D (recall U ∗= U −1 since U is unitary). Fix j between 1 and n.
On the left side, the jth column of AU is A(U [j]) = Auj. On the right side, the jth column
of V D is V (D[j]) = D(j, j)V [j] = djvj. If j ≤r, then djvj = Auj holds by definition of vj. If
r < j ≤n, then cj = dj = 0. So ||Auj||2 = u∗
jA∗Auj = u∗
j(cjuj) = 0, hence Auj = 0 = djvj.
We conclude AU = V D since all columns agree.
To prove the uniqueness of the dj, let A = V DU ∗be any factorization with the properties
given in the theorem statement (not necessarily using the matrices U, D, V constructed
above). Since V is unitary, A∗A = (V DU ∗)∗(V DU ∗) = UD∗V ∗V DU ∗= UD∗DU ∗. Since
U is unitary, A∗A and D∗D are similar and so have the same eigenvalues (counted with
multiplicity). Letting the nonzero diagonal entries of D be d1, . . . , dr, you can check that
D∗D has eigenvalues d2
1, . . . , d2
r, together with n−r eigenvalues equal to zero. So the nonzero
diagonal entries of D are the positive square roots of the nonzero eigenvalues of A∗A, as
claimed.
We remark that the matrices V and U appearing in the singular value decomposition
of A are not unique in general. We can obtain different matrices U by picking different
orthonormal bases for each eigenspace of A∗A when forming the list (u1, . . . , un). Each
choice of U leads to a unique list (v1, . . . , vr), but then we can get different matrices V by
extending this list to an orthonormal basis of Cm in different ways.
In terms of linear transformations, the singular value decomposition can be phrased as
follows. For any linear map T : Cn →Cm, there exist an orthonormal basis (u1, . . . , un)
of Cn, an orthonormal basis (v1, . . . , vm) of Cn, and nonnegative real numbers d1 ≥· · · ≥
dmin(m,n) such that T(uj) = djvj for 1 ≤j ≤min(m, n), and T(uj) = 0 for m < j ≤n.
We can also view the factorization A = V DU ∗geometrically, by saying that the linear
transformation x 7→Ax acts as the composition of an isometry on Cn encoded by the
unitary matrix U ∗, followed by a nonnegative rescaling of the coordinate axes encoded by
D (which deletes the last n −m coordinates when n > m, and adds m −n zero coordinates
when m > n), followed by an isometry on Cm encoded by the unitary matrix V .
Matrix Factorizations
245
9.13
Summary
1.
Orthogonality and Orthonormality. A list (u1, . . . , uk) in an inner product space
V is orthogonal iff ⟨ui, uj⟩= 0 for all i ̸= j in [k]. The list is orthonormal iff it
is orthogonal and ⟨ui, ui⟩= 1 for all i in [k]. Orthogonal lists of nonzero vectors
are linearly independent. Orthogonal vectors x, y ∈V satisfy the Pythagorean
Identity ||x + y||2 = ||x||2 + ||y||2. A square matrix Q is unitary iff its columns
are orthonormal iff Q∗Q = I iff Q∗= Q−1.
2.
Orthogonal Projections. Let (u1, . . . , uk) be an orthonormal list in an inner
product space V . Let U be the subspace spanned by this list. For any v ∈V ,
there exists a unique x ∈U minimizing ||v −x||, namely x = Pk
r=1⟨v, ur⟩ur, and
v −x is orthogonal to every vector in U. We call x the orthogonal projection of v
onto U. The vector v is in U iff v = x iff v = Pk
r=1⟨v, ur⟩ur.
3.
Gram–Schmidt Orthonormalization. Given a finite or countably infinite linearly
independent sequence (v1, v2, . . .) in an inner product space V , the Gram–Schmidt
Orthonormalization Algorithm computes an orthonormal sequence (u1, u2, . . .)
such that (v1, . . . , vr) and (u1, . . . , ur) span the same subspace Vr for all
r ≥1. Having found u1, . . . , us−1, we calculate xs = Ps−1
r=1⟨vs, ur⟩ur and
us = (vs −xs)/||vs −xs||. When dim(V ) < ∞, we see that every subspace of
V has an orthonormal basis, and every orthonormal list in V can be extended to
an orthonormal basis of V . When vs depends linearly on preceding vectors in the
list, the algorithm detects this by computing xs = vs.
4.
QR Factorization via the Gram–Schmidt Algorithm. Given A ∈Mn,k(C) with
k linearly independent columns, there exist unique matrices Q ∈Mn,k(C) and
R ∈Mk(C) such that A = QR, Q has orthonormal columns, and R is upper-
triangular with strictly positive diagonal entries. When A is real, Q and R are
real. If A has column rank r < k, we can write A = QR where Q ∈Mn,r(C)
has orthonormal columns and R ∈Mr,k(C) is upper-triangular with nonnegative
diagonal entries. The columns of Q are the orthonormal vectors obtained by
applying Gram–Schmidt orthonormalization to the columns of A. The entries of
R are given by R(i, j) = ⟨vj, ui⟩for i ≤j, and R(i, j) = 0 for i > j.
5.
Householder Reflections. Given nonzero v ∈Cn, the Householder matrix for v
is Qv = In −(2/||v||2)vv∗∈Mn(C). These matrices are unitary and self-inverse
(Q∗
v = Q−1
v
= Qv), Qvv = −v, and Qvw = w for all w ∈Cn orthogonal to v. For
any x ̸= y in Cn, there exists v ∈Cn with Qvx = y iff ||x|| = ||y|| and ⟨x, y⟩∈R.
Specifically, when x and y satisfy these conditions, Qvx = y iff v is a nonzero
scalar multiple of x −y.
6.
Householder’s QR Algorithm. The Householder algorithm reduces A ∈Mn,k(C)
to an upper-triangular matrix R ∈Mn,k(C) by applying s = min(n −1, k)
Householder reflections to create zeroes one column at a time. At the jth
step, we find a Householder reflection sending the partial column vector
(A(j, j), A(j + 1, j), . . . , A(n, j))T to a vector whose last n −j entries are 0. The
output of the algorithm is a factorization A = QR, where Q is the product of s
Householder matrices and a diagonal unitary matrix.
7.
LU Factorizations. For any field F and A ∈Mn(F), there exist a lower-
unitriangular matrix L ∈Mn(F) and an invertible upper-triangular matrix
U ∈Mn(F) such that A = LU iff for all k ∈[n], det(A[k]) ̸= 0. When L
246
Advanced Linear Algebra
and U exist, they are unique. This is Doolittle’s LU factorization; we obtain
Crout’s LU factorization by making U unitriangular instead of L. We can also
write A = LDU with both L and U unitriangular and D diagonal.
8.
Recursive Formula for LU Factorizations. When the Doolittle factorization
A = LU exists, we can find L and U recursively as follows:
set L(i,i)=1 and L(i,j)=0=U(j,i) for all i<j in [n];
for k=1 to n do
U(k,k)=A(k,k)-sum(r=1 to k-1) L(k,r)U(r,k);
for i=k+1 to n do
L(i,k)=(1/U(k,k))[A(i,k)-sum(r=1 to k-1) L(i,r)U(r,k)];
end for;
for j=k+1 to n do
U(k,j)=A(k,j)-sum(r=1 to k-1) L(k,r)U(r,j);
end for;
end for.
9.
LU Factorization via Gaussian Elimination. Given A ∈Mn(F) with det(A[k]) ̸=
0 for all k ∈[n], the Gaussian elimination algorithm proceeds as follows:
for j=1 to n-1 do
for i=j+1 to n do
m(i,j) = A(i,j)/A(j,j);
replace row i of A by row i minus m(i,j) times row j;
end for;
end for.
In the Doolittle factorization A = LU, U is the output of the elimination
algorithm, and the entries L(i, j) below the diagonal are the multipliers m(i, j)
used in the algorithm.
10.
Permuted LU Factorizations. For any A ∈Mn(F), there exist permutation
matrices P, Q ∈Mn(F), a lower-unitriangular matrix L ∈Mn(F), and an upper-
triangular matrix U ∈Mn(F) with PAQ = LU and rank(A) = rank(U). When
A is invertible, we may take Q = In. These factorizations can be achieved by
using Gaussian elimination with row and column interchanges.
11.
Cholesky Factorization. Every positive semidefinite matrix A ∈Mn(C) can be
factored as A = LL∗for some lower-triangular L ∈Mn(C) with nonnegative
diagonal entries. When A is positive definite, L is unique.
12.
Least Squares Approximation. Given b ∈Cm and A ∈Mm,n(C) of rank n with
m ≥n, there exists a unique x ∈Cn minimizing ||Ax −b||2, namely the unique
solution to the normal equations A∗Ax = A∗b.
13.
Singular Value Decomposition. Any A ∈Mm,n(C) can be factored as A = V DU ∗,
where V ∈Mm(C) and U ∈Mn(C) are unitary, and D ∈Mm,n(R) is diagonal
with diagonal entries d1 ≥d2 ≥· · · ≥0. The dj are called the singular values
of A and are uniquely determined by A. The nonzero dj are the positive square
roots of the nonzero eigenvalues of A∗A.
Matrix Factorizations
247
9.14
Exercises
Assume V is a complex inner product space in these exercises.
1.
Prove: for all x, y in V , ||x + y||2 + ||x −y||2 = 2||x||2 + 2||y||2.
2.
Give a specific example of vectors x, y with ||x+y||2 = ||x||2+||y||2 and ⟨x, y⟩̸= 0.
3.
Prove: for all x, y ∈V , if ||x + y||2 = ||x||2 + ||y||2 = ||x + iy||2, then ⟨x, y⟩= 0.
4.
Let L =


5
0
0
0
−3
2
0
0
7
−1
−4
0
8
−3
−5
2

, U =


1
3
2
−4
0
1
5
−3
0
0
1
6
0
0
0
1

, b =


10
6
−12
4

.
(a) Solve Lx = b by forward substitution.
(b) Solve Ux = b by backward substitution.
(c) Solve Ax = b, where A = LU.
5.
Let Q =


0.28
0
0
0.96
0
0.6
0.8
0
−0.96
0
0
0.28
0
0.8
−0.6
0

, R =


2
3
−1
−1
0
5
3
−1
0
0
2
−2
0
0
0
2

, b =


1
3
2
−2

.
Confirm that Q is unitary. Solve Ax = b efficiently, where A = QR.
6.
Suppose A = LU, where A, L, U ∈Mn(C) with L lower-unitriangular and U
upper-triangular. If we know L and U, how many multiplications and divisions
in C are required to solve Ax = b?
7.
Suppose A = QR, where A, Q, R ∈Mn(C) with Q unitary and R upper-
triangular. If we know Q and R, how many multiplications and divisions in C
are required to solve Ax = b?
8.
Apply the Gram–Schmidt algorithm to v1 = (1, 1, 1), v2 = (1, 2, 4), v3 = (1, 3, 9)
to compute an orthonormal basis of C3.
9.
Let V be the subspace of C4 spanned by v1 = (3, 1, 5, −1), v2 = (2, 0, 1, 3), and
v3 = (1, −1, −1, −1). Apply the Gram–Schmidt algorithm to the list (v1, v2, v3)
to compute an orthonormal basis of V . Find the orthogonal projection of each
vector onto V : e1, (−2, 2, 3, −2), (1, 2, 3, 4), (24, −2, 15, 7).
10.
Let V be the real inner product space of continuous functions f : [0, 1] →R,
with inner product ⟨f, g⟩=
R 1
0 f(x)g(x) dx for f, g ∈V . Apply Gram–Schmidt
orthonormalization to the list (1, x, x2, x3) to obtain an orthonormal basis for the
subspace W spanned by this list. Find the orthogonal projection of f(x) = ex onto
W and the minimum distance from f to W. Do the same for f(x) = sin(2πx).
11.
Repeat Exercise 10, but let V consist of continuous functions f : [−1, 1] →R,
with ⟨f, g⟩=
R 1
−1 f(x)g(x) dx for f, g ∈V .
12.
Given a list of orthonormal vectors (u1, . . . , uk) and c1, . . . , ck ∈C, prove ||c1u1 +
· · ·+ckuk||2 = |c1|2 +· · ·+|ck|2. How does this formula change if we only assume
u1, . . . , uk are orthogonal vectors?
13.
Let (u1, . . . , uk) be an orthonormal list in V
and v
∈
V . Prove that
Pk
j=1 |⟨v, uj⟩|2 ≤||v||2, with equality iff v is in the span of u1, . . . , uk.
14.
Let V
=
R[x] be the real inner product space of polynomials, with in-
ner product
DPn
i=0 aixi, Pm
j=0 bjxjE
= Pmin(m,n)
i=0
aibi for ai, bj ∈R. Suppose
248
Advanced Linear Algebra
(f0, f1, f2, . . . , fn, . . .) is any infinite list of elements of V with deg(fn) = n for
all n ≥0. Show that applying Gram–Schmidt orthonormalization to this list
produces the sequence (±1, ±x, ±x2, . . . , ±xn, . . .). Given a finite subset S of
Z≥0, what is the orthogonal projection of g ∈R[x] onto the subspace spanned by
(xj : j ∈S)?
15.
Use the Gram–Schmidt algorithm to prove that any orthonormal list (u1, . . . , uk)
in a finite-dimensional inner product space V can be extended to an orthonormal
ordered basis of V .
16.
Show that if R, R1 ∈Mk(C) are upper-triangular with positive real diagonal
entries, then RR−1
1
is upper-triangular with positive real diagonal entries. Show
that the only unitary upper-triangular matrix in Mk(C) with positive real
diagonal entries is Ik. Deduce the uniqueness of the factorization A = QR in
the case where A ∈Mk(C) is a square matrix.
17.
For each matrix, use Gram–Schmidt orthonormalization to find the QR factor-
izations described in §9.3. (a)
 1
2
1
1

(b)
 3
4
4
3

(c)


2
1
3
3
1
5
6
2
−1


(d)


1
0
1
0
3
3
3
3
2
0
2
0

(e)


1
2
3
4
1
3
2
4
1
4
2
3
2
3
1
4


18.
Let A ∈Mn,k(R) have rank k. How many ways can we write A = QR where
Q ∈Mn,k(R) has orthonormal columns and R ∈Mk(R) is upper-triangular? (We
do not assume R has positive diagonal entries here.)
19.
Use the Gram–Schmidt algorithm to prove: any A ∈Mn,k(C) can be factored as
A = QR, where Q ∈Mn(C) is unitary and R ∈Mn,k(C) is upper-triangular.
20.
Formulate and prove a uniqueness result for the factorization A = QR of an
arbitrary matrix A ∈Mn,k(C) described in §9.3.
21.
Modified Gram–Schmidt Algorithm. Given a linearly independent list
(v1, . . . , vk) in an inner product space V , the Gram–Schmidt orthonormalization
algorithm in §9.2 produces an orthonormal list (u1, . . . , uk). Show that the
following algorithm also transforms (v1, . . . , vk) into (u1, . . . , uk). Loop from j = 1
to k. Replace the current vector vj by vj/||vj||, and then for each r from j + 1
to k, replace vr by vr −⟨vr, vj⟩vj. Output the final values of (v1, . . . , vk). (This
version of Gram–Schmidt can be shown to be more stable numerically compared
to the original version.)
22.
Estimate the number of real multiplications, divisions, and square root extractions
needed to compute the QR factorization of a matrix A ∈Mn(R) via the Gram–
Schmidt Orthonormalization Algorithm.
23.
Estimate the number of real multiplications, divisions, and square root extractions
needed to compute the QR factorization of a matrix A ∈Mn(R) via Householder’s
Algorithm.
24.
Describe how to adapt the Gram–Schmidt algorithm to transform a linearly inde-
pendent list (v1, . . . , vk) into an orthogonal list (u1, . . . , uk) using a computation
that avoids extractions of square roots.
25.
Draw a picture in R2 illustrating the action of the reflection T(−4,3) on each
of these vectors: (−4, 3), (3, 4), (1, 0), (0, 1), (1, −2). Confirm your answers
algebraically by multiplying each column vector by the matrix Q(−4,3).
Matrix Factorizations
249
26.
Find a Householder matrix Qv in M3(R) that fixes each point in the plane
2x1 −3x2 + x3 = 0. Draw a sketch showing this plane and the vectors v,
w = (1, −1, 1), and Qvw.
27.
(a) Explain why the lemma in §9.4 fails to hold without the hypothesis ||x|| = ||y||.
(b) Does the lemma hold without the hypothesis ⟨x, y⟩∈R?
(c) Suppose x = y. Describe all v ∈Cn with Qvx = y.
28.
For each x, y ∈Cn, find v such that Qvx = y or explain why this cannot be done.
(a) x = (5, 12), y = (13, 0). (b) x = (1, i), y = (
√
2, 0). (c) x = (i, 1), y = (
√
2, 0).
(d) x = (3, 2, 6), y = (2, 6, 3). (e) x = (3, 2, 3), y = (2, 3, 2).
(f) x = (3, 2, 6), y = (b, 0, 0) for different choices of b ∈C.
29.
For each matrix in Exercise 17, use Householder’s Algorithm to compute the
QR factorization. In each case, write Q as an explicit product of Householder
reflections and a diagonal unitary matrix.
30.
Apply Gram–Schmidt orthonormalization to the columns of the matrix A in §9.5
to obtain the matrices Q and R in (9.2).
31.
Givens Rotations. For any i ̸= j in [n] and θ ∈[0, 2π), define the Givens
rotation matrix G = G(i, j; θ) ∈Mn(R) by letting G(i, i) = G(j, j) = cos θ,
G(j, i) = sin θ, G(i, j) = −sin θ, G(k, k) = 1 for all k ̸= i, j, and letting all other
entries of G be zero. Left-multiplication by G(i, j; θ) rotates the plane spanned
by ei and ej counterclockwise through an angle of θ.
(a) Show that G(i, j; θ) is orthogonal with inverse G(i, j; −θ).
(b) Let A ∈Mn(R) have A(j, i) ̸= 0. Find θ so that G(i, j; θ)A has j, i-entry 0.
32.
Givens QR Algorithm. Describe an algorithm for computing a factorization
A = QR of a matrix A ∈Mn,k(R) that reduces A to an upper-triangular matrix
R by left-multiplying by a sequence of Givens rotation matrices (see Exercise 31).
How many matrices are needed in general?
33.
Show that every unitary matrix Q
∈
Mn(C) has a factorization Q
=
Qv1 · · · Qvn−1D, where D is a diagonal unitary matrix and each Qvj is a
Householder matrix. Can the matrix D be omitted? Explain.
34.
In numerical computations, one source of inaccuracy is subtractive cancellation, in
which two nearly equal real numbers are subtracted. Explain how the minus sign
in the formula for y in §9.5 avoids subtractive cancellation in the computation of
the first component of v1.
35.
Find Doolittle’s LU factorization of each matrix using the recursive formulas
in §9.6. (a)

3
−2
−12
13

(b)


2
−3
1
−4
10
6
6
−13
−3

(c)


2
3
−2
4
1
6.5
−9
3
−5
2.5
−9
−8
−2
−3
5
−4


36.
Find an LU factorization of each matrix using Gaussian elimination.
(a)
 2
8
6
19

(b)


3
2
1
2
10/3
5/3
−5
8/3
7/3

(c)


2
10
−13
8
−10
−35
58
−46
−6
−24
39.2
−35.4
0.8
−41
16.4
21.4


37.
Justify the existence and uniqueness assertions for the variants of the LU
factorization described in the last paragraph of §9.6.
38.
State and prove a theorem regarding factorizations of square matrices of the form
A = UL, where U is upper-triangular and L is lower-triangular. Give one proof by
250
Advanced Linear Algebra
modifying the formulas in §9.6, and a second proof that deduces UL factorizations
from LU factorizations.
39.
Show that for any field F and any permutation matrix P ∈Mn(F), P T = P −1.
40.
For each matrix A, find a factorization PA = LU or PAQ = LU as in §9.9.
(a)
 0
2
3
1

(b)


0
0
2
1
−1
2
3
3
3

(c)


0
0
0
1
0
1
0
0
0
0
0
3
0
5
0
0

.
41.
Extend the results on LU factorizations and permuted LU factorizations to
rectangular matrices A ∈Mm,n(F).
42.
Without using Gaussian elimination or permuted LU factorizations, prove by
induction on n that for every invertible A ∈Mn(F), there exists a permutation
matrix P ∈Mn(F) such that det((PA)[k]) ̸= 0 for 1 ≤k ≤n.
43.
Suppose A ∈Mn(C) has rank r and det(A[k]) ̸= 0 for 1 ≤k ≤r. Prove that A has
an LU factorization in which L is lower-unitriangular and U is upper-triangular
with the last n −r diagonal entries equal to zero.
44.
Show that any A ∈Mn(F) can be factored as A = LQU, where L is lower-
triangular and invertible, U is upper-triangular and invertible, and Q ∈Mn(F)
has at most one 1 in each row and column, and all other entries of Q are 0. (Use
elementary matrices to reduce A.)
45.
Find a Cholesky factorization of each positive semidefinite matrix.
(a)

4
−2i
2i
2

(b)


9
15
0
15
26
1
0
1
5

(c)


16
−4
−12
4
−4
1
3
−1
−12
3
19
−1
4
−1
−1
4


46.
Give an example to show that the Cholesky factorization of a non-invertible
positive semidefinite matrix need not be unique.
47.
Count the number of multiplications, divisions, and square root extractions in
C needed to compute the Cholesky factorization of an n × n positive definite
matrix using the formulas in §9.10. How do these operation counts compare to
those needed to find the LU decomposition of an n×n matrix using the formulas
in §9.6?
48.
Consider the inconsistent linear system x + y = 3, 2x −y = 0, x −2y = 0.
Use the normal equations to find the least squares approximate solution to this
system. Graph the equations in the system and the least squares solution in a
single picture.
49.
Find the least squares approximate solution to the system











3x
+7y
−2z
=
4
5x
−3y
+2z
=
8
−x
−5z
=
2
−2x
+y
+3z
=
−1
7x
−4y
+z
=
0
by setting up and solving the normal equations.
50.
Given the system of two equations ax = b and cx = d with (a, c) ̸= (0, 0), what
is the least squares approximate solution to this system?
Matrix Factorizations
251
51.
Given n equations ajx = bj with a1, . . . , an, b1, . . . , bn ∈C and not all aj are 0,
what is the least squares approximate solution to this system?
52.
Suppose we need the least squares approximate solution to Ax = b, where
A ∈Mn,k(C) has rank k, and we know a factorization A = QR with the
properties stated in §9.3. Show that this solution can be found efficiently by
solving Rx = Q∗b.
53.
Given n real data points (x1, y1), . . . , (xn, yn), consider the problem of finding real
parameters m and b such that the line y = mx + b best approximates the given
data. Formulate this as a least squares problem, and solve the normal equations
to obtain specific formulas for m and b. Specify exactly what quantity is being
minimized by the optimal choice of m and b.
54.
Follow the proof in §9.12 to find a singular value decomposition for each matrix.

1.824
1.032
−1.968
2.176

,

3
3
3
−1
−1
−1

,


1
1
1
1
1
−1
1
−1
1
1
−1
−1

,


2
0
2
0
0
0
0
0
2
0
2
0
0
0
0
i

.
55.
Penrose Properties. (a) Prove that for every A ∈Mm,n(C), there exists at
most one matrix B ∈Mn,m(C) with these four properties, called the Penrose
properties.
I: ABA = A;
II: BAB = B;
III: (AB)∗= AB;
IV: (BA)∗= BA.
(If B and C have these properties, show B = (BA)∗(CA)∗C(AC)∗(AB)∗= C.)
(b) Show that if A ∈Mn(C) is invertible, then B = A−1 has the four Penrose
properties.
56.
Pseudoinverses. Suppose A ∈Mm,n(C) has singular value decomposition A =
V DU ∗. Let D′ ∈Mn,m(C) be the matrix obtained from DT by inverting the
nonzero diagonal entries. The matrix A+ = UD′V ∗∈Mn,m(C) is called the
pseudoinverse of A. (a) Show that the pseudoinverse does not depend on the
choice of singular value decomposition of A, by showing that A+ has the four
Penrose properties in Exercise 55. (b) Without using the Penrose properties,
show that A+ = A−1 when A ∈Mn(C) is invertible.
57.
For each matrix A in Exercise 54, find the pseudoinverse A+ (see Exercise 56).
Compute AA+ and A+A in each case.
58.
Let A = V DU ∗be a singular value decomposition of A ∈Mm,n(C) such that D
has k nonzero entries on the diagonal. (a) Show that the last n −k columns of U
are an orthonormal basis of the null space of A. (b) Show that the first k columns
of V are an orthonormal basis of the image of A.
59.
Let A ∈Mm,n(C). Use singular value decompositions to prove that the largest
singular value of A is sup{||Ax|| : x ∈Cn, ||x|| = 1}.
10
Iterative Algorithms in Numerical Linear Algebra
In applications of linear algebra to science, engineering, and other areas, we often need
to find numerical solutions to a huge linear system of equations or to approximate the
eigenvalues of a large square matrix. The branch of numerical analysis called numerical
linear algebra studies algorithms for solving such problems efficiently while minimizing the
effects of rounding errors.
Consider the problem of solving the linear system Ax = b, where A ∈Mn(C) is a
given invertible matrix, b ∈Cn is a given column vector, and x ∈Cn is a column vector
of unknowns. Gaussian elimination and related algorithms solve this system by a finite
sequence of steps, which (in the absence of roundoff errors) ultimately terminates with
the exact solution. This chapter discusses a different class of algorithms for solving such
problems, called iterative algorithms. The idea is to start with some initial approximation
x0 to the sought-for solution x, and then to apply some recursive formula xk+1 = f(xk)
(depending on the problem instance) to compute an infinite sequence x0, x1, x2, . . . , xk, . . .
of approximate solutions. We can then ask if this sequence converges (under an appropriate
definition of convergence) to the true solution x, and try to find bounds on how far away a
given approximation xk is to x.
In order to give a precise mathematical analysis of such algorithms, we need a formal
way to measure the distance between two column vectors in Cn. We define this distance
using the idea of a vector norm. The related concept of a matrix norm helps us understand
how multiplication by a fixed matrix affects the distances between vectors.
The analysis of iterative algorithms constitutes a vast subfield of numerical linear
algebra, and we only have space to give a short introduction to this subject in this chapter.
We discuss three basic iterative methods for solving linear systems (the algorithms of
Richardson, Jacobi, and Gauss–Seidel) and analyze them using norms and the spectral
radius of a matrix. We also study the power method, which can be used to approximate the
largest eigenvalue of a given square matrix. To find other eigenvalues of the matrix, we can
use variations of the power method or a technique called deflation.
10.1
Richardson’s Algorithm
As a first illustration of an iterative method for solving the linear system Ax = b, we describe
Richardson’s Algorithm. The input to the algorithm is a matrix A ∈Mn(C), a column vector
b ∈Cn, and a vector x0 ∈Cn that represents an initial guess or approximation to the true
solution vector x. (We may take x0 = 0 if we have no initial information about x.) The
algorithm proceeds by forming a sequence of approximate solutions x0, x1, x2, . . . , xk, . . .,
where for all k ≥0,
xk+1 = xk + (b −Axk).
After computing each vector xk, we find the output error vector b−Axk and add this vector
to xk to obtain the next input vector xk+1. Ideally, we would reach the true solution x from
DOI: 10.1201/9781003484561-10
252
Iterative Algorithms in Numerical Linear Algebra
253
xk by adding the input error vector x −xk to xk, but we do not know this error vector.
However, if the matrix A is close to the identity matrix in some sense, then we would expect
the output error vector b −Axk = A(x −xk) to be close to the input error vector. In this
case, we would expect the approximate solutions xk to approach the true solution x. We
make this informal analysis rigorous in §10.11, where we also derive a precise bound for the
size of the error vector x −xk.
Here is a small example to illustrate the execution of Richardson’s Algorithm. Let
A =

1.1
0.3
−0.4
0.8

and b =

2
−3

. Starting with x0 = 0, we successively compute
x1 = x0 + (b −Ax0) =

2
−3

,
x2 = x1 + (b −Ax1) =

2.7
−2.8

,
x3 = x2 + (b −Ax2) =

2.57
−2.48

,
x4 = x3 + (b −Ax3) =

2.487
−2.468

,
x5 = x4 + (b −Ax4) =

2.4917
−2.4988

,
x6 = x5 + (b −Ax5) =

2.50047
−2.50308

.
On the other hand, by Gaussian elimination or Cramer’s Rule, we find the exact solution
is x =

2.5
−2.5

. We see that the sequence is converging rapidly to the exact solution.
However, suppose we take A =

2
3
1
4

and b =

2
−3

. Starting the algorithm at
x0 = 0, we compute the sequence of approximations

2
−3

,
 9
4

,
 −19
−24

,
 93
88

,
 −355
−360

,
 1437
1432

, . . . ,
which does not appear to be converging. The true solution here is

3.4
−1.6

.
10.2
Jacobi’s Algorithm
We introduce the next iterative algorithm for solving Ax = b, called Jacobi’s Algorithm,
with a specific example. Suppose A =


5
1
−1
−1
5
2
2
2
4

, b =


4
−1
3

, and x =


r
s
t

. We
are trying to solve a system of three linear equations in three unknowns. The basic idea is
to solve the ith equation for the ith unknown, as shown here:



5r
+s
−t
=
4
−r
+5s
+2t
=
−1
2r
+2s
+4t
=
3
⇒



r
=
(4 −s + t)/5
s
=
(−1 + r −2t)/5
t
=
(3 −2r −2s)/4
We use the rewritten equations to define a sequence of vectors xk = [rk sk tk]T by choosing
any initial vector x0 = [r0 s0 t0]T, then computing



rk+1
=
(4 −sk + tk)/5
sk+1
=
(−1 + rk −2tk)/5
tk+1
=
(3 −2rk −2sk)/4
for all k ≥0.
(10.1)
254
Advanced Linear Algebra
If we start with x0 = 0, we compute the following sequence of approximate solutions:


0.8
−0.2
0.75

,


0.99
−0.34
0.45

,


0.958
−0.182
0.425

,


0.921
−0.178
0.362

,


0.908
−0.161
0.379

,


0.908
−0.170
0.376

,


0.909
−0.169
0.381

, . . . .
These vectors appear to be converging rapidly to the exact solution x = [0.91 −0.17 0.38]T.
Now we describe how Jacobi’s Algorithm works in general. The input to the algorithm is
a matrix A ∈Mn(C), a column vector b ∈Cn, and an initial approximation x0 ∈Cn. This
algorithm requires that all diagonal entries A(i, i) be nonzero. In fact, it is preferable for the
diagonal entry in each row to be large in magnitude compared to the other entries in its row.
If the initial matrix does not satisfy this requirement, we may adjust A by permuting rows
and columns to make the requirement hold. Row permutations correspond to reordering the
linear equations in the system Ax = b, and column permutations correspond to reordering
the unknown components of x.
Write A = Q + R, where Q consists of the diagonal entries of A, and R consists of the
off-diagonal entries of A. Formally, let Q(i, i) = A(i, i) for 1 ≤i ≤n, let all other entries
of Q be zero, let R(i, j) = A(i, j) for 1 ≤i, j ≤n with i ̸= j, and let all other entries
of R be zero. Since Q is a diagonal matrix with nonzero entries on the diagonal, we can
quickly compute Q−1 by inverting each diagonal entry. We now compute the sequence of
approximations x0, x1, x2, . . . , xk, . . . by setting
xk+1 = Q−1(b −Rxk)
for all k ≥0.
(10.2)
To see that this matches the description of the algorithm given in the example above, note
that the ith row of Ax = b is the equation
b(i) =
n
X
j=1
A(i, j)x(j) = A(i, i)x(i) +
X
j̸=i
A(i, j)x(j) = Q(i, i)x(i) +
n
X
j=1
R(i, j)x(j).
Here, we write x(i) for the ith component of the vector x to avoid confusion with the
subscripts in the sequence of approximations x0, x1, x2, . . .. Solving the ith equation for
x(i) and using the resulting formula as the update rule for obtaining xk+1(i) from xk, we
get
xk+1(i) = Q(i, i)−1

b(i) −
n
X
j=1
R(i, j)xk(j)


for 1 ≤i ≤n. These formulas are equivalent to the matrix equation (10.2).
We analyze the convergence properties of Jacobi’s Algorithm in §10.12.
10.3
Gauss–Seidel Algorithm
The Gauss–Seidel Algorithm is a variation of Jacobi’s Algorithm that uses the updated
value of each unknown variable as soon as that value is computed. We illustrate the idea
with the same 3×3 system Ax = b considered in §10.2. The key modification to the previous
example is that the update rules in (10.1) are replaced by



rk+1
=
(4 −sk + tk)/5
sk+1
=
(−1 + rk+1 −2tk)/5
tk+1
=
(3 −2rk+1 −2sk+1)/4
for all k ≥0.
(10.3)
Iterative Algorithms in Numerical Linear Algebra
255
The update rule for rk+1 is the same as before. When we compute sk+1, we have already
computed the new value rk+1 but we have not yet computed tk+1. So we use rk+1 and tk
on the right side of the update rule for sk+1. When we compute tk+1, we use the newly
computed values rk+1 and sk+1 on the right side instead of the old values rk and sk.
Using the Gauss–Seidel formulas and starting with x0 = 0, we compute the following
sequence of approximate solutions:


0.8
−0.04
0.37

,


0.882
−0.172
0.395

,


0.913
−0.175
0.381

,


0.911
−0.170
0.379

,


0.910
−0.170
0.380

, . . . .
This sequence appears to be converging to the exact solution x = [0.91 −0.17 0.38]T even
faster than the sequence produced by the Jacobi method.
To solve a general system Ax = b using the Gauss–Seidel Algorithm, we require that
every diagonal entry of A be nonzero (which may require preprocessing A by permuting rows
and columns). Starting with any initial vector x0, we compute x1, x2, . . . , via the update
rules
xk+1(i) = A(i, i)−1

b(i) −
X
j<i
A(i, j)xk+1(j) −
X
j>i
A(i, j)xk(j)


for all k ≥0,
(10.4)
which are computed in order as i increases from 1 to n.
Next, we give a matrix formulation of the Gauss–Seidel iterative procedure. We first
rewrite (10.3) by moving all terms with subscript k + 1 to the left side, obtaining


5
0
0
−1
5
0
2
2
4




rk+1
sk+1
tk+1

=


4
−1
3

+


0
−1
1
0
0
−2
0
0
0




rk
sk
tk

.
The matrix on the left consists of the entries in the original matrix A on or below the main
diagonal. The matrix on the right is found by negating the entries of A above the main
diagonal. For the general system Ax = b, define Q and R in Mn(C) by setting Q(i, j) =
A(i, j) for all i ≥j, Q(i, j) = 0 for i < j, and R = A −Q. Multiplying (10.4) by A(i, i) and
adding P
j<i A(i, j)xk+1(j) to both sides, we see that the equations (10.4) (for i ranging
from 1 to n) are equivalent to the matrix update rule
Qxk+1 = b −Rxk
for all k ≥0.
When using this update rule, it should be kept in mind that we do not find xk+1 by explicitly
computing Q−1 and applying this matrix to b−Rxk. Rather, we compute b−Rxk and then
backsolve for xk+1 as indicated in (10.4). Note that (10.2) can also be written in the form
Qxk+1 = b −Rxk, but with a different choice of Q and R. In both cases, a key property of
Q is that Qxk+1 = y can be quickly solved for xk+1 in much less time than the n3 steps
required for a general matrix inversion.
We study the convergence properties of the Gauss–Seidel Algorithm in §10.14.
10.4
Vector Norms
To analyze the convergence of iterative algorithms, we need to define the distance between
two vectors in a vector space. To do so, we first introduce the concept of the length (or
256
Advanced Linear Algebra
norm) of a vector. Given a real or complex vector space V , a norm on V is a function
N : V →R, denoted N(x) = ||x|| for x ∈V , which must satisfy the following axioms.
• Nonnegativity: For all x ∈V , 0 ≤||x|| < ∞; and ||x|| = 0 iff x = 0.
• Scalar Rule: For all x ∈V and all scalars c, ||cx|| = |c| · ||x||.
• Triangle Inequality: For all x, y ∈V , ||x + y|| ≤||x|| + ||y||.
A vector space V together with a specific norm N on V is called a normed vector space.
For example, let V be the real vector space Rn. For x = (x1, x2, . . . , xn) ∈Rn, define
the sup norm by setting
||x||∞= max(|x1|, |x2|, . . . , |xn|).
(10.5)
Let us check the axioms for a normed vector space. Fix x, y ∈Rn and c ∈R. First, since
|xi| ≥0 for all i, the maximum of these n numbers is also nonnegative and finite; and the
maximum is zero iff all xi = 0 iff x = 0. So ||x||∞≥0 with equality iff x = 0. Second,
|cxi| = |c| · |xi| for each i, so
||cx||∞= max(|c| · |x1|, . . . , |c| · |xn|) = |c| max(|x1|, . . . , |xn|) = |c| · ||x||∞.
Third, the Triangle Inequality in R tells us that |xi + yi| ≤|xi| + |yi| for 1 ≤i ≤n. Now,
|xi| ≤max1≤j≤n |xj| = ||x||∞and |yi| ≤max1≤j≤n |yj| = ||y||∞, so
|xi + yi| ≤||x||∞+ ||y||∞.
This inequality holds for all i between 1 and n, so
||x + y||∞= max(|x1 + y1|, . . . , |xn + yn|) ≤||x||∞+ ||y||∞,
as needed. An identical proof shows that Cn is a complex normed vector space using the
sup norm; in this case, |xj| denotes the magnitude of the complex number xj.
Another norm on the real vector space Rn is the 1-norm, defined by
||x||1 = |x1| + |x2| + · · · + |xn|
(10.6)
for all x = (x1, . . . , xn) ∈Rn. The first two axioms are readily verified; let us check the
Triangle Inequality. Given x, y ∈Rn, we have |xi + yi| ≤|xi| + |yi| in R for 1 ≤i ≤n.
Adding these n inequalities, we obtain
||x + y||1 =
n
X
i=1
|xi + yi| ≤
n
X
i=1
(|xi| + |yi|) =
n
X
i=1
|xi| +
n
X
i=1
|yi| = ||x||1 + ||y||1.
A third example of a norm on Rn is the Euclidean norm or 2-norm, defined by
||x||2 =
p
|x1|2 + |x2|2 + · · · + |xn|2
(10.7)
for x = (x1, . . . , xn) ∈Rn. You can check that the norm axioms are satisfied; the proof
of the Triangle Inequality follows from the Cauchy–Schwarz Inequality (see Exercise 29).
Similarly, you can verify that formulas (10.6) and (10.7) define norms on the complex vector
space Cn.
A vector u in a normed vector space V is called a unit vector iff ||u|| = 1. Given
any nonzero v ∈V , ||v||−1v is a unit vector. To verify this, let c = ||v||−1, and note
||cv|| = |c| · ||v|| = ||v||−1||v|| = 1.
Iterative Algorithms in Numerical Linear Algebra
257
10.5
Metric Spaces
A metric space is a set X together with a distance function (or metric) d : X × X →R,
which must satisfy the following axioms.
• Nonnegativity: For all x, y ∈X, 0 ≤d(x, y) < ∞, and d(x, y) = 0 iff x = y.
• Symmetry: For all x, y ∈X, d(x, y) = d(y, x).
• Triangle Inequality: For all x, y, z ∈X, d(x, z) ≤d(x, y) + d(y, z).
The nonnegative real number d(x, y) is the distance from x to y.
Any normed vector space V becomes a metric space if we define d(x, y) = ||x −y|| for
x, y ∈V . To check this, fix x, y, z ∈V . We have 0 ≤||x −y|| < ∞, with ||x −y|| = 0 iff
x −y = 0 iff x = y, so the first axiom for a metric space holds. We have
d(y, x) = ||y −x|| = ||(−1)(x −y)|| = | −1| · ||x −y|| = ||x −y|| = d(x, y),
so the second axiom holds. We compute
d(x, z) = ||x −z|| = ||(x −y) + (y −z)|| ≤||x −y|| + ||y −z|| = d(x, y) + d(y, z),
so the third axiom holds. A metric defined from a norm in this way has two additional
properties related to the vector space structure of V .
• Translation Invariance: For all x, y, z ∈V , d(x + z, y + z) = d(x, y).
• Dilation Property: For all x, y ∈V and all scalars c, d(cx, cy) = |c|d(x, y).
Translation invariance holds since d(x+z, y+z) = ||(x + z) −(y + z)|| = ||x −y|| = d(x, y).
To check the dilation property, compute
d(cx, cy) = ||cx −cy|| = ||c(x −y)|| = |c| · ||x −y|| = |c|d(x, y).
10.6
Convergence of Sequences
In any metric space (X, d), we can define what it means for a sequence to converge. Given
x ∈X and given a sequence (x0, x1, x2, . . .) = (xk : k ≥0) of points in X, we say that
the sequence (xk) converges to x iff for every ϵ > 0, there exists k0 ∈Z≥0 such that for all
k ≥k0, d(xk, x) < ϵ. Informally, this definition says that the sequence gets arbitrarily close
to x (and stays close) for terms far enough out in the sequence. When this condition holds,
we write “limk→∞xk = x,” and we also write “xk →x as k →∞.”
Not all sequences in a metric space (X, d) converge. But if a sequence (xk : k ≥0)
converges to some limit x, then x is unique. To prove this, suppose xk →x and also xk →y
for some x, y ∈X. Given ϵ > 0, choose k1, k2 ∈Z≥0 so that d(xk, x) < ϵ/2 for all k ≥k1
and d(xk, y) < ϵ/2 for all k ≥k2. Take k = max(k1, k2) to see that d(x, y) ≤d(x, xk) +
d(xk, y) < ϵ. Since this holds for every positive ϵ, we must have d(x, y) = 0, so that x = y.
Now suppose V is a normed vector space; we always assume V has the metric d(x, y) =
||x −y|| defined using the norm. Given two sequences (vk : k ≥0) and (wk : k ≥0) in
V , suppose vk →v and wk →w for some v, w ∈V . We assert that vk + wk →v + w.
258
Advanced Linear Algebra
To prove this, fix ϵ > 0. Choose k1, k2 ∈Z≥0 such that d(vk, v) < ϵ/2 for all k ≥k1 and
d(wk, w) < ϵ/2 for all k ≥k2. For all k ≥max(k1, k2), the Triangle Inequality gives
d(vk + wk, v + w) =||(vk + wk) −(v + w)|| = ||(vk −v) + (wk −w)||
≤||vk −v|| + ||wk −w|| < ϵ/2 + ϵ/2 = ϵ.
So limk→∞(vk + wk) = v + w.
Next, assume c is a scalar and vk →v; we assert that cvk →cv. This result is evident
when c = 0, so assume c ̸= 0. Given ϵ > 0, choose k0 so that d(vk, v) < ϵ/|c| for all k ≥k0.
Then for all k ≥k0, we compute d(cvk, cv) = |c|d(vk, v) < ϵ. So limk→∞(cvk) = cv. More
generally, you can show that if ck →c in R or C and vk →v in V , then ckvk →cv in V
(Exercise 60). You can also show by induction that if (v(1)
k
: k ≥0), . . . , (v(m)
k
: k ≥0) are m
sequences converging respectively to v(1), . . . , v(m), and if c1, . . . , cm are fixed scalars, then
lim
k→∞(c1v(1)
k
+ · · · + cmv(m)
k
) = c1v(1) + · · · + cmv(m).
We say that limits preserve linear combinations of vectors.
10.7
Comparable Norms
As we have seen, a given vector space V may possess many different norms and hence many
different metrics. The definition of a convergent sequence depends on the metric used, so it
is possible that a given sequence (vk : k ≥0) might converge to some v relative to one norm,
but not converge to v relative to some other norm. However, when V is finite-dimensional,
we can show that this does not happen. In other words, for any two norms || · || and || · ||′
on V , we will prove that vk →v relative to || · || iff vk →v relative to || · ||′. So for studying
questions of convergence, we can use whichever norm is most convenient for calculations.
We say that two norms || · || and || · ||′ on a vector space V are comparable iff there
exist real constants C, D with 0 < C, D < ∞such that for all x ∈V , ||x|| ≤C||x||′ and
||x||′ ≤D||x||. For example, let us show that the sup norm and the 2-norm are comparable
on V = Rn. It suffices to prove that for all x = (x1, . . . , xn) ∈Rn,
||x||∞≤||x||2 ≤√n||x||∞
(here C = 1 and D = √n). To prove the first inequality, observe that |xi| =
p
|xi|2 ≤
qPn
j=1 |xj|2 = ||x||2 for all i. Taking the maximum over all i gives ||x||∞≤||x||2. To prove
the second inequality, fix an index k with |xk| = maxi |xi|. Then |xi|2 ≤|xk|2 for all i, so
||x||2 =
v
u
u
t
n
X
i=1
|xi|2 ≤
v
u
u
t
n
X
i=1
|xk|2 = √n|xk| = √n||x||∞.
Similarly, you may prove that the sup norm and the 1-norm on Rn are comparable by
showing that
||x||∞≤||x||1 ≤n||x||∞
for all x ∈Rn. Comparability of norms is an equivalence relation (Exercise 65), so it follows
that the 1-norm and the 2-norm are comparable as well.
Iterative Algorithms in Numerical Linear Algebra
259
We now sketch the proof that any vector norm || · || on Rn is comparable to the
2-norm. (A similar result holds for Cn, but these results do not extend to infinite-
dimensional spaces.) First we prove that there is a constant C ∈(0, ∞) such that for
all x = (x1, . . . , xn) ∈Rn, ||x|| ≤C||x||2. Let e1, . . . , en be the standard basis vectors in
Rn, so that x = x1e1 + · · · + xnen. Let C = ||e1|| + ||e2|| + · · · + ||en||, which is a positive
finite constant. Use the norm axioms to compute
||x||
=
||x1e1 + · · · + xnen|| ≤||x1e1|| + · · · + ||xnen||
=
|x1| · ||e1|| + · · · + |xn| · ||en||
≤
||x||∞||e1|| + · · · + ||x||∞||en|| = C||x||∞≤C||x||2.
Obtaining the inequality ||x||2 ≤D||x|| requires a compactness argument. Here, we cite
without proof some results from calculus on the metric space Rn with the Euclidean metric
d(x, y) = ||x −y||2. Let S = {x ∈Rn : ||x||2 = 1} be the unit sphere in this metric space.
The set S is closed and bounded in Rn, so S is a compact subset of Rn. Define f : S →R
by f(x) = ||x|| for x ∈S. We claim the function f is continuous. Given ϵ > 0, let δ = ϵ/C.
Then for x, y ∈S satisfying d(x, y) = ||x −y||2 < δ, we have
|f(x) −f(y)| = | ||x|| −||y|| | ≤||x −y|| ≤C||x −y||2 < ϵ.
(The first inequality follows from Exercise 38.) From calculus, we know that a continuous
real-valued function on a compact set attains its minimum value at some point in the set.
So, there exists x0 ∈S such that f(x0) ≤f(x) for all x ∈S. Note that x0 ̸= 0 (since
||x0||2 = 1), so f(x0) = ||x0|| > 0. Let D = ||x0||−1, which is a positive finite constant.
Given any x ∈Rn, we can now prove that ||x||2 ≤D||x||. This inequality certainly holds if
x = 0. If ||x||2 = 1, then x ∈S, so f(x0) ≤f(x), which means D−1 ≤||x||. Multiplying by
D, we get ||x||2 = 1 ≤D||x|| as needed. Next, consider an arbitrary nonzero y ∈Rn. Let
c = ||y||2 and x = c−1y ∈S. Multiplying both sides of the known inequality ||x||2 ≤D||x||
by c, we get ||cx||2 ≤D||cx||, so ||y||2 ≤D||y||. This completes the proof that ||·|| and ||·||2
are comparable. By Exercise 65, any two norms on Rn (or Cn) are comparable.
Given comparable norms || · || and || · ||′ on Rn or Cn, we now show that vk →v relative
to the first norm iff vk →v relative to the second norm. Choose constants C and D as in
the definition of comparable norms. Suppose vk →v relative to ||·||. Given ϵ > 0, choose k0
so that k ≥k0 implies ||vk −v|| < ϵ/D. Then k ≥k0 implies ||vk −v||′ ≤D||vk −v|| < ϵ. So
vk →v relative to || · ||′. Conversely, suppose vk →v relative to || · ||′. Given ϵ > 0, choose
k1 so that k ≥k1 implies ||vk −v||′ < ϵ/C. Then k ≥k1 implies ||vk −v|| ≤C||vk −v||′ < ϵ.
Combining this result with the preceding theorem, we conclude that in Rn or Cn, vk →v
relative to any given vector norm iff vk →v relative to the 2-norm (or the 1-norm or the
sup norm).
10.8
Matrix Norms
Recall that Mn(C) is the set of all n × n matrices with entries in C. Since Mn(C) is a
complex vector space, we can consider vector norms defined on Mn(C). All such norms
satisfy 0 ≤||A|| < ∞, ||A|| = 0 iff A = 0, ||cA|| = |c| · ||A||, and ||A + B|| ≤||A|| + ||B|| for
all A, B ∈Mn(C) and all c ∈C. A matrix norm is a vector norm || · || on Mn(C) satisfying
this additional axiom:
• Submultiplicativity: For all A, B ∈Mn(C), ||AB|| ≤||A|| · ||B||.
260
Advanced Linear Algebra
For example, define ||A|| = n · maxi,j |A(i, j)| for A ∈Mn(C). The axioms for a vector
norm can be checked as in §10.4. To prove submultiplicativity, fix A, B ∈Mn(C) and i, j
between 1 and n. We know (AB)(i, j) = Pn
k=1 A(i, k)B(k, j), so
n|(AB)(i, j)| ≤
n
X
k=1
n|A(i, k)| · |B(k, j)| ≤
n
X
k=1
n(n−1||A||)(n−1||B||) = ||A|| · ||B||.
This holds for all i, j, so taking the maximum gives ||AB|| ≤||A|| · ||B|| as needed. (The
inequality would not hold if we omitted the n from the definition of ||A||.)
Given any vector norm ||·|| on Cn, we now construct a matrix norm on Mn(C) called the
matrix norm induced by the given vector norm. In addition to the properties listed above,
this matrix norm satisfies ||In|| = 1 and ||Av|| ≤||A|| · ||v|| for all A ∈Mn(C) and v ∈Cn.
To define the matrix norm, let U = {v ∈Cn : ||v|| = 1} be the set of unit vectors in Cn
relative to the given vector norm. For A ∈Mn(C), let
||A|| = sup{||Av|| : v ∈U}.
(10.8)
So the norm of A is the least upper bound of the set of lengths ||Av|| as v ranges over
all unit vectors in Cn (we view elements of Cn as column vectors here). For example,
||0|| = sup{||0v|| : v ∈U} = sup{0} = 0 and ||In|| = sup{||Inv|| : v ∈U} = sup{1} = 1.
We now prove that our definition satisfies the requirements of a matrix norm. Fix A, B
in Mn(C) and c ∈C. For the nonnegativity axiom, note that 0 ≤||A|| ≤∞since ||A|| is the
least upper bound of a set of nonnegative real numbers. Proving that ||A|| is finite is a bit
tricky. We know || · || is comparable to || · ||∞, so there is a positive finite constant C with
||v||∞≤C||v|| for all v ∈Cn. Given any v = (v1, . . . , vn) ∈U, we have v = v1e1+· · ·+vnen,
so Av = A Pn
i=1 viei = Pn
i=1 vi(Aei). Taking norms gives
||Av||
≤
n
X
i=1
||vi(Aei)|| =
n
X
i=1
|vi| · ||Aei||
≤
n
X
i=1
||v||∞||Aei|| ≤
n
X
i=1
C||v|| · ||Aei|| = C
n
X
i=1
||Aei||.
Thus, C Pn
i=1 ||Aei|| is a finite upper bound for the set {||Av|| : v ∈U}, so the least upper
bound ||A|| of this set is indeed finite. We already saw that ||0|| = 0. On the other hand,
a nonzero matrix A must have some nonzero column A[j] = Aej. Taking v = ||ej||−1ej,
which is a unit vector, we have ||Av|| = ||ej||−1||Aej|| ̸= 0, so ||A|| > 0.
For the scalar axiom, note that ||(cA)v|| = |c| · ||Av|| for all v ∈U. So the least upper
bound of the numbers ||(cA)v|| over all v ∈U is |c| times the least upper bound of the
numbers ||Av||, giving ||cA|| = |c| · ||A||. To verify the Triangle Inequality, fix v ∈U and
compute ||(A + B)v|| = ||Av + Bv|| ≤||Av|| + ||Bv|| ≤||A|| + ||B||. Thus ||A|| + ||B|| is an
upper bound for the set {||(A + B)v|| : v ∈U}, and hence the least upper bound ||A + B||
of this set satisfies ||A + B|| ≤||A|| + ||B||.
Next, we check that ||Av|| ≤||A|| · ||v|| for A ∈Mn(C) and v ∈Cn. Both sides are zero
when v = 0. For nonzero v, write v = cu where c = ||v|| and u ∈U. Then compute
||Av|| = ||A(cu)|| = ||c(Au)|| = |c| · ||Au|| ≤|c| · ||A|| = ||A|| · ||v||.
To prove submultiplicativity, fix A, B ∈Mn(C). For any v ∈U, the fact just proved gives
||(AB)v|| = ||A(Bv)|| ≤||A|| · ||Bv|| ≤||A|| · ||B|| · ||v|| = ||A|| · ||B||.
Thus ||A|| · ||B|| is an upper bound for the set {||(AB)v|| : v ∈U}, and hence the least
upper bound ||AB|| of this set satisfies ||AB|| ≤||A|| · ||B||.
Iterative Algorithms in Numerical Linear Algebra
261
Not all matrix norms are induced by vector norms via (10.8). For example, the norm
||A|| = n · maxi,j |A(i, j)| cannot be induced by any vector norm since ||In|| = n ̸= 1. Let us
call a matrix norm || · || induced iff there exists a vector norm for which || · || is the matrix
norm induced by this vector norm.
The proof of the following lemma uses the matrix norm induced by a given vector norm.
Suppose (vk : k ≥0) is a sequence in Cn converging to v ∈Cn relative to a vector norm ||·||.
Then for all A ∈Mn(C), Avk →Av. The result holds when A = 0, so assume A ̸= 0. Fix
ϵ > 0, and choose k such that k ≥k0 implies ||vk −v|| < ϵ/||A||, where ||A|| is the matrix
norm induced by the given vector norm. Then k ≥k0 implies ||Avk −Av|| = ||A(vk −v)|| ≤
||A|| · ||vk −v|| < ϵ. For this proof to work, it is critical to know (as proved above) that ||A||
is finite.
10.9
Formulas for Matrix Norms
In this section, we develop some explicit formulas for computing the matrix norms induced
by the sup norm, the 1-norm, and the 2-norm on Cn. We start by showing that the matrix
norm ||A||∞induced by the vector norm ||v||∞satisfies
||A||∞= max
1≤i≤n
n
X
j=1
|A(i, j)| = max
1≤i≤n ||A[i]||1,
(10.9)
where A[i] = (A(i, 1), . . . , A(i, n)) is row i of A. Let U = {v ∈Cn : ||v||∞= 1}. For any
v ∈U, we compute
||Av||∞
=
max
1≤i≤n |(Av)i| = max
1≤i≤n

n
X
j=1
A(i, j)vj

≤
max
1≤i≤n
n
X
j=1
|A(i, j)| · |vj| ≤max
1≤i≤n
n
X
j=1
|A(i, j)| · ||v||∞= max
1≤i≤n
n
X
j=1
|A(i, j)|.
So the expression on the right side of (10.9) is an upper bound for {||Av||∞: v ∈U},
hence ||A||∞≤max1≤i≤n
Pn
j=1 |A(i, j)|. To establish the reverse inequality, fix an index
k between 1 and n for which Pn
j=1 |A(k, j)| attains its maximum. For each j between 1
and n, choose vj to be a complex number of modulus 1 such that A(k, j)vj = |A(k, j)|. (If
A(k, j) = reiθ in polar form, we can take vj = e−iθ.) Now observe that v = (v1, . . . , vn) ∈Cn
has ||v||∞= 1 since |vj| = 1 for all j. So v ∈U, and we conclude that
||A||∞≥||Av||∞= max
1≤i≤n |(Av)i| ≥|(Av)k| =

n
X
j=1
A(k, j)vj

=
n
X
j=1
|A(k, j)|
= max
1≤i≤n
n
X
j=1
|A(i, j)|.
Next, we prove that the matrix norm ||A||1 induced by the vector norm ||v||1 satisfies
||A||1 = max
1≤j≤n
n
X
i=1
|A(i, j)| = max
1≤j≤n ||A[j]||1,
(10.10)
262
Advanced Linear Algebra
where A[j] = (A(1, j), . . . , A(n, j))T is column j of A. Let U = {v ∈Cn : ||v||1 = 1}. For
any v ∈U,
||Av||1
=
n
X
i=1
|(Av)i| =
n
X
i=1

n
X
j=1
A(i, j)vj

≤
n
X
i=1
n
X
j=1
|A(i, j)| · |vj| =
n
X
j=1
|vj|
n
X
i=1
|A(i, j)|
≤
n
X
j=1
|vj|
"
max
1≤j≤n
n
X
i=1
|A(i, j)|
#
= ||v||1 max
1≤j≤n
n
X
i=1
|A(i, j)| = max
1≤j≤n
n
X
i=1
|A(i, j)|.
Since v ∈U is arbitrary, we conclude that ||A||1 ≤max1≤j≤n
Pn
i=1 |A(i, j)|. For the reverse
inequality, fix an index k for which Pn
i=1 |A(i, k)| is maximized. Since ||ek||1 = 1, ek ∈U,
and hence
||A||1 ≥||Aek||1 = ||A[k]||1 =
n
X
i=1
|A(i, k)| = max
1≤j≤n
n
X
i=1
|A(i, j)|.
Let ||A||2 be the matrix norm induced by the vector norm ||v||2. For any B ∈Mn(C),
define the spectral radius ρ(B) to be the maximum magnitude of all the complex eigenvalues
of B, i.e.,
ρ(B) = max{|c| : c ∈C is an eigenvalue of B}.
It can be shown (Exercise 72) that ||A||2 =
p
ρ(A∗A) for A ∈Mn(C). You can also show
that ||A||2 is the largest singular value of A (see Exercise 59 in Chapter 9).
10.10
Matrix Inversion via Geometric Series
The Geometric Series Formula
1
1 −r = 1 + r + r2 + r3 + · · · + rk + · · · =
∞
X
k=0
rk
is valid for all complex numbers r such that |r| < 1. In this section, we prove an analogous
formula in which r is replaced by an n × n complex matrix C. We assume that ||C|| < 1 for
some induced matrix norm on Cn; this is the analog of the hypothesis |r| < 1.
Under this assumption, we show that I −C is an invertible matrix, and
(I −C)−1 = I + C + C2 + · · · + Ck + · · · =
∞
X
k=0
Ck,
(10.11)
where the infinite series of matrices denotes the limit of the sequence of partial sums
(I + C + C2 + · · · + Ck : k ≥0). To get a contradiction, assume that I −C is not invertible.
Then there exists a nonzero vector x with (I −C)x = 0, so x = Cx. Taking norms gives
||x|| = ||Cx|| ≤||C|| · ||x||. Dividing by the positive real number ||x|| gives 1 ≤||C||,
contradicting our hypothesis on C. So B = (I −C)−1 does exist. B must be nonzero, so
||B|| > 0.
Iterative Algorithms in Numerical Linear Algebra
263
Now fix k ≥0, and compute
(I−C)(I+C+C2+· · ·+Ck) = (I+C+C2+· · ·+Ck)−(C+C2+· · ·+Ck+Ck+1) = I−Ck+1.
Multiplying both sides by B gives I + C + C2 + · · · + Ck = B −BCk+1. Given ϵ > 0, we
can choose k0 so that k ≥k0 implies ||C||k+1 < ϵ/||B||; this is possible since ||C|| < 1. For
any k ≥k0, it follows that
||(I + C + C2 + · · · + Ck) −B|| = || −BCk+1|| ≤||B|| · ||C||k+1 < ϵ.
This proves that the sequence of partial sums converges to B = (I −C)−1, as needed.
10.11
Affine Iteration and Richardson’s Algorithm
In this section, we study the convergence of the iterative algorithm obtained by repeatedly
applying an affine map to a given initial vector. The input to the algorithm consists of a
fixed matrix C ∈Mn(C), a vector d ∈Cn, and an initial vector x0 ∈Cn. We construct a
sequence (xk : k ≥0) in Cn by the recursive formula
xk+1 = Cxk + d
(k ≥0).
(10.12)
We seek conditions under which this sequence of vectors converges to some limit x ∈Cn.
If the limit x exists, then we can find it by letting k go to infinity on both sides of (10.12).
Assuming that the sequence (xk) converges to x, the sequence (xk+1) on the left side also
converges to x. On the right side, (Cxk) converges to Cx, so (Cxk +d) converges to Cx+d.
The two sides converge to the same limit, so x = Cx + d. This means that the limit x must
solve the linear system (I −C)x = d. If I −C is invertible, then the only possible limit of
the iterative algorithm is x = (I −C)−1d.
To prove convergence, we assume that ||C|| < 1 for some induced matrix norm on
Cn. As shown in the last section, this assumption ensures that I −C is invertible. We
prove that for any choice of initial vector x0 ∈Cn, the sequence (xk) defined by (10.12)
converges to x = (I −C)−1d, and we find an upper bound on the error ||xk −x||. We
know xk+1 = Cxk + d for k ≥0, and x = Cx + d. Subtracting these equations gives
xk+1 −x = (Cxk + d) −(Cx + d) = Cxk −Cx = C(xk −x). Taking norms, we get
||xk+1 −x|| = ||C(xk −x)|| ≤||C|| · ||xk −x||
for all k ≥0. If k ≥1, we can write ||xk −x|| ≤||C|| · ||xk−1 −x|| to bound the right side,
giving ||xk+1 −x|| ≤||C||2||xk−1 −x||. Continuing in this way, we eventually obtain the
bound
||xk+1 −x|| ≤||C||k+1||x0 −x||,
valid for all k ≥0. The right side goes to zero at an exponential rate as k goes to infinity,
since ||C|| < 1. Therefore, limk→∞||xk −x|| = 0, proving that xk →x as needed.
This iterative method finds the unique x solving (I −C)x = d. So, if we want to use this
method to solve a linear system Ax = b (for given A ∈Mn(C) and b ∈Cn), we can choose
C = I −A and d = b. As long as r = ||I −A|| < 1 for some induced matrix norm, the
iterative method converges to the solution x, with the error in the kth term bounded by rk
times the initial error ||x0 −x||. Since C = I −A, the iteration formula (10.12) becomes
xk+1 = xk + (b −Axk)
(k ≥0),
and we have recovered Richardson’s Algorithm for solving a linear system.
264
Advanced Linear Algebra
We can use the formulas for matrix norms in §10.9 to find explicit sufficient con-
ditions on A guaranteeing that Richardson’s Algorithm converges. On one hand, if
max1≤i≤n
Pn
j=1 |(I −A)(i, j)| < 1, then (10.9) shows that ||I −A||∞< 1, so the algorithm
converges. On the other hand, if max1≤j≤n
Pn
i=1 |(I −A)(i, j)| < 1, then (10.10) shows that
||I −A||1 < 1, so the algorithm converges. For example, the matrix A =

1.1
0.3
−0.4
0.8

considered in §10.1 satisfies both conditions just mentioned, so Richardson’s Algorithm for
solving Ax = b converges for any choice of b and x0. In fact, since ||I −A||1 = 1/2, we know
that ||xk −x|| ≤2−k||x0 −x|| for all k ≥1, so that the bound for the maximum error is cut
in half in each successive iteration.
10.12
Splitting Matrices and Jacobi’s Algorithm
We can apply the analysis in the preceding section to iterative algorithms that solve Ax = b
by the following procedure. The algorithm picks an invertible splitting matrix Q (depending
on the given A ∈Mn(C)) and computes (xk : k ≥0) using the iteration formula
Qxk+1 = b −(A −Q)xk
(k ≥0).
(10.13)
For example, Richardson’s Algorithm takes Q = I; Jacobi’s Algorithm takes Q to be the
diagonal part of A; and the Gauss–Seidel Algorithm takes Q to be the lower-triangular part
of A. Recall that an algorithm of this kind is only practical if Qxk+1 = y can be solved
quickly for xk+1 given y.
By multiplying both sides of (10.13) on the left by Q−1, we see that (10.13) is
algebraically (though not computationally) equivalent to
xk+1 = Q−1b −(Q−1A −Q−1Q)xk = (I −Q−1A)xk + Q−1b.
Therefore, we can invoke the results of §10.11 taking C = I −Q−1A and d = Q−1b. Note
that (I −C)x = d iff Q−1Ax = Q−1b iff Ax = b for this choice of C and d. We conclude that
if ||I −Q−1A|| < 1 for some induced matrix norm, then the sequence defined by (10.13)
converges to the correct solution for any starting vector x0. We also have the error bound
||xk −x|| ≤||I −Q−1A||k||x0 −x||
(10.14)
for all k ≥0.
Using this analysis, we can prove a sufficient condition on A guaranteeing the convergence
of the Jacobi Algorithm for solving Ax = b. Call A ∈Mn(C) diagonally dominant iff
|A(i, i)| >
n
X
j=1
j̸=i
|A(i, j)|
for all i between 1 and n.
(Such a matrix must have all diagonal entries nonzero.) We show that the Jacobi method
always converges given a diagonally dominant input matrix A. In this case, Q−1 is a diagonal
matrix with Q−1(i, i) = A(i, i)−1. Therefore, the i, j-entry of I −Q−1A is 0 if i = j, and
−A(i, i)−1A(i, j) otherwise. So for each i between 1 and n,
n
X
j=1
|(I −Q−1A)(i, j)| =
n
X
j=1
j̸=i
|A(i, j)|
|A(i, i)| < 1.
Iterative Algorithms in Numerical Linear Algebra
265
Taking the maximum over all i, it follows from (10.9) that ||I −Q−1A||∞< 1, proving
convergence.
10.13
Induced Matrix Norms and the Spectral Radius
In the previous sections, we proved that various iterative algorithms converge if a certain
matrix B ∈Mn(C) satisfies ||B|| < 1 for some induced matrix norm. It is possible that
||B||1 ≥1, ||B||∞≥1, and yet ||B|| < 1 for some other matrix norm induced by some
vector norm other than ||·||1 or ||·||∞. To obtain a sufficient condition for convergence that
is as powerful as possible, we would really like to know the quantity
f(B) = inf{||B|| : || · || is an induced matrix norm on Cn},
(10.15)
which is the greatest lower bound in R of the numbers ||B|| as the norm varies over all
possible induced matrix norms. Recall from §10.9 that the spectral radius of B is
ρ(B) = max{|c| : c ∈C is an eigenvalue of B}.
We prove that f(B) = ρ(B) for all B ∈Mn(C).
To begin, let c ∈C be an eigenvalue of B with |c| = ρ(B) and v ̸= 0 be an associated
eigenvector. Fix an arbitrary vector norm || · || on Cn, and note that u = ||v||−1v satisfies
||u|| = 1 and Bu = cu. Therefore, using the matrix norm induced by this vector norm, we
compute
||B|| ≥||Bu|| = ||cu|| = |c| · ||u|| = |c| = ρ(B).
So ρ(B) is a lower bound for the set of numbers on the right side of (10.15). As f(B) is the
greatest lower bound of this set, ρ(B) ≤f(B) follows.
Showing the opposite inequality f(B) ≤ρ(B) requires more work. It is enough to
prove that f(B) ≤ρ(B) + ϵ for each fixed ϵ > 0. Given ϵ > 0, we first show that
there is an invertible matrix S ∈Mn(C) such that S−1BS is upper-triangular and
||S−1BS||∞≤ρ(B) + ϵ. We give an argument using Jordan canonical forms (Chapter 8);
a different argument based on unitary triangularization of complex matrices is sketched
in Exercise 73. Define T : Cn →Cn by T(v) = Bv for v ∈Cn. We know there is an
ordered basis X = (x1, x2, . . . , xn) for Cn such that [T]X is a Jordan canonical form. This
means that for certain scalars c1, . . . , cn ∈C and d1, . . . , dn ∈{0, 1}, T(x1) = c1x1 and
T(xi) = cixi + dixi−1 for 2 ≤i ≤n. Replace the ordered basis X by the ordered basis
Y = (y1, y2, . . . , yn), where yi = ϵixi for 1 ≤i ≤n. We compute T(y1) = c1y1 and
T(yi) = T(ϵixi) = ϵiT(xi) = ϵi(cixi + dixi−1) = ciyi + (diϵ)yi−1
for 2 ≤i ≤n. Letting S = E[id]Y , where E = (e1, . . . , en) is the standard ordered basis
of Cn, we see that U = S−1BS = [T]Y is an upper-triangular matrix with main diagonal
entries c1, . . . , cn, entries equal to zero or ϵ on the next higher diagonal, and zeroes elsewhere.
Since U is triangular, the eigenvalues of U are c1, . . . , cn. Since B is similar to U, these are
also the eigenvalues of B. For 1 ≤i ≤n, Pn
j=1 |U(i, j)| is either |ci| or |ci| + ϵ. By (10.9),
we see that
||U||∞≤max
1≤i≤n(|ci| + ϵ) = ρ(B) + ϵ.
To continue, we need a clever choice of a vector norm and its induced matrix norm.
You can check (Exercise 39) that for any fixed invertible S ∈Mn(C), the formula ||v||S =
266
Advanced Linear Algebra
||S−1v||∞(for v ∈Cn) defines a vector norm on Cn. Also, the induced matrix norm satisfies
||A||S = ||S−1AS||∞for all A ∈Mn(C). Taking S to be the matrix found in the previous
paragraph, we find that
f(B) ≤||B||S = ||S−1BS||∞= ||U||∞≤ρ(B) + ϵ,
as needed.
We have now proved that f(B) = ρ(B) for all B ∈Mn(C). Using (10.15), we see that
ρ(B) < 1 iff there exists an induced matrix norm such that ||B|| < 1. Accordingly, we can
restate the convergence result in §10.12 as follows. An iterative algorithm based on (10.13)
converges if the spectral radius ρ(I −Q−1A) is less than 1.
10.14
Analysis of the Gauss–Seidel Algorithm
This section uses the convergence criterion ρ(I −Q−1A) < 1 to prove that the Gauss–Seidel
Algorithm converges if the input matrix A ∈Mn(C) is diagonally dominant. Recall that
diagonal dominance means |A(i, i)| > P
j̸=i |A(i, j)| for all i. Let c ∈C be any eigenvalue
of I −Q−1A with associated eigenvector x = (x1, . . . , xn) ̸= 0. It suffices to prove |c| < 1.
We know (I −Q−1A)x = cx. Left-multiplying both sides by Q gives (Q −A)x = c(Qx).
Recall that, in the Gauss–Seidel Algorithm, Q contains the entries of A on or below the
main diagonal, and Q −A involves the entries of A above the main diagonal. So, taking the
ith component of (Q −A)x = c(Qx) gives
−
X
j:j>i
A(i, j)xj = c
X
j:j≤i
A(i, j)xj
for all i. Isolating the term cA(i, i)xi and taking magnitudes gives
|c| |A(i, i)| |xi| =

−
X
j:j>i
A(i, j)xj −c
X
j:j<i
A(i, j)xj

≤
X
j:j>i
|A(i, j)| |xj|+|c|
X
j:j<i
|A(i, j)| |xj|.
Fix an index i such that |xi| = max1≤k≤n |xk| > 0. Then |xj|/|xi| ≤1 for all j ̸= i, so
dividing the preceding inequality by |xi| gives
|c| |A(i, i)| ≤
X
j:j>i
|A(i, j)| + |c|
X
j:j<i
|A(i, j)|.
(10.16)
Now, the diagonal dominance of A implies that |A(i, i)|−P
j:j<i |A(i, j)| > P
j:j>i |A(i, j)| ≥
0. Therefore, we can solve (10.16) for |c| to conclude that
|c| ≤
P
j:j>i |A(i, j)|
|A(i, i)| −P
j:j<i |A(i, j)| < 1,
as needed.
10.15
Power Method for Finding Eigenvalues
To compute the spectral radius of A ∈Mn(C) from the definition, we need to know the
largest complex eigenvalue of A. We now discuss an iterative algorithm called the power
Iterative Algorithms in Numerical Linear Algebra
267
method whose goal is to compute this largest eigenvalue and an associated eigenvector. The
algorithm takes as input the matrix A and an arbitrary nonzero initial vector x0 ∈Cn. The
algorithm iteratively computes yk+1 = Axk and xk+1 = yk+1/||yk+1||∞for all k ≥0. If we
get yk+1 = 0, then the algorithm fails, and we try again with a different x0. At stage k, xk
is the algorithm’s approximation for the required eigenvector. The associated eigenvalue is
estimated by choosing an index i and returning ck = (Axk)(i)/xk(i). Any index i between
1 and n can be used here, as long as |xk(i)| is not too close to zero.
For example, let A =


1
4
−1
0
3
2
1
−1
−3

and x0 =


1
0
0

. The vectors x1, x2, . . . , x12 are:


1
0
1

,


0
1
−1

,


1
0.2
0.4

,


1
1
−0.286

,


1
0.459
0.162

,


1
0.636
0.020

,


1
0.553
0.086

,


1
0.586
0.060

,


1
0.572
0.071

,


1
0.578
0.067

,


1
0.575
0.068

,


1
0.576
0.068

.
At
this
stage,
our
estimate
for
the
eigenvector
is
x12
=
(1, 0.576, 0.068),
and
Ax12 = (3.236, 1.864, 0.22). Dividing each entry of Ax12 by the corresponding entry of
x12 gives us three estimates for the largest eigenvalue of A: 3.236, 1.864/0.576 ≈3.236, and
0.22/0.068 ≈3.235. In fact, factoring the characteristic polynomial of A shows that the
exact eigenvalues of A are −1, 1 −
√
5, and 1 +
√
5. Thus, ρ(A) = 1 +
√
5 ≈3.23607, so that
we obtain quite good estimates for ρ(A) after twelve iterations of the algorithm.
We now give a sufficient condition on A guaranteeing that the approximations produced
by the power method converge to ρ(A). The condition we impose on A is that A is a
diagonalizable matrix having a unique eigenvalue c ∈C such that |c| = ρ(A). (Some of the
exercises explore what happens if these conditions are not met.) Given these assumptions,
we can choose an ordered basis Z = (z1, . . . , zn) of Cn consisting of n linearly independent
eigenvectors of A. We have Azi = cizi for the complex eigenvalues c1, . . . , cn of A. We can
reorder the basis Z so that
ρ(A) = |c1| > |c2| ≥|c3| ≥· · · ≥|cn|.
Given any x0 ∈Cn, we obtain xk by a sequence of k steps, each of which involves multiplying
by A and then rescaling to get a unit vector. Letting bk ∈R>0 be the product of the rescaling
factors used to reach xk, we see that xk = bkAkx0 for all k ≥1.
Write x0 = d1z1 + d2z2 + · · · + dnzn for unique scalars di ∈C. We assume d1 ̸= 0, which
happens almost surely if x0 is chosen at random in Cn. By replacing each zi in the basis Z
by dizi, we can assume without loss of generality that every di is 1. We have Akzi = ck
i zi
for all i and all k ≥1, so xk = bk(ck
1z1 +ck
2z2 +· · ·+ck
nzn). Letting b′
k = bkck
1 and ri = ci/c1
for 2 ≤i ≤n, we can rewrite this as
xk = b′
k(z1 + rk
2z2 + · · · + rk
nzn).
We have |ri| = |ci|/|c1| < 1 for 2 ≤i ≤n, so for each such i, the sequence rk
i zi goes to zero
as k goes to infinity. Letting x′
k = xk/b′
k, it follows that x′
k = z1 + rk
2z2 + · · · + rk
nzn →z1
as k →∞.
Next, let i be any index such that z1(i) ̸= 0. Since x′
k →z1 relative to || · ||∞, the
inequality 0 ≤|x′
k(i) −z1(i)| ≤||x′
k −z1||∞shows that x′
k(i) →z1(i) in C as k goes to
infinity. Similarly, since Ax′
k →Az1 = c1z1, we see that (Ax′
k)(i) →c1z1(i) as k goes to
268
Advanced Linear Algebra
infinity. Finally, since xk = b′
kx′
k,
lim
k→∞
(Axk)(i)
xk(i)
= lim
k→∞
(Ax′
k)(i)
x′
k(i)
= c1z1(i)
z1(i)
= c1,
which says that the approximations produced by the power method do converge to the
largest eigenvalue c1 of A.
Although x′
k converges to the eigenvector z1 associated with the eigenvalue c1, we cannot
conclude that the sequence xk itself converges to any fixed eigenvector of A. The trouble is
that the complex scaling constant b′
k = bkck
1 may cause the sequence xk to jump between
several eigenvectors that differ by a complex scaling factor of modulus 1. For example, if
A =
 0
−1
1
0

and x0 is the eigenvector
 i
1

associated with the eigenvalue i, the power
method produces the periodic sequence
 i
1

,
 −1
i

,
 −i
−1

,

1
−i

,
 i
1

,
 −1
i

,
 −i
−1

,

1
−i

, . . . .
This sequence does not converge, but each vector in the sequence is an eigenvector associated
with the eigenvalue i. On the other hand, you can adapt the analysis given above to show
that xk −b′
kz1 →0 as k →∞(Exercise 75). Informally, this means that for large enough k,
xk is guaranteed to come arbitrarily close to some eigenvector associated with the eigenvalue
c1, but this eigenvector may depend on k. Alternatively, since bk is known and c1 is being
estimated by the algorithm, we can use an estimate for c1 to compute xk/(bkck
1) as an
approximation of the eigenvector z1.
10.16
Shifted and Inverse Power Method
The power method provides an iterative algorithm for computing the largest eigenvalue of
A ∈Mn(C). What if we want to find one of the other eigenvalues of A? This section and
the next one discuss several approaches to this question.
Observe first that the largest eigenvalue of A is the eigenvalue farthest from the origin
in the complex plane. Suppose we replace the matrix A by a shifted matrix A + bI, where
b ∈C is a fixed constant. Let c ∈C be an eigenvalue of A with associated eigenvector
x ∈Cn. Since Ax = cx, we have (A + bI)x = Ax + bx = (c + b)x, so that c + b ∈C is
an eigenvalue of A + bI with associated eigenvector x. Conversely, if c′ is an eigenvalue of
A + bI, a similar computation shows that c′ −b is an eigenvalue of A. Geometrically, the
eigenvalues of A + bI are obtained by shifting (or translating) all eigenvalues of A by the
fixed scalar b. For any c ∈C, the distance from c to 0 in C is the same as the distance from
c + b to b in C. So the eigenvalue of A farthest from the origin gets shifted to the eigenvalue
of A + bI farthest from the point b.
Suppose C ∈Mn(C) is a given matrix, b ∈C is a given scalar, and we want to compute
the complex eigenvalue of C farthest from b. Taking A = C −bI in the previous paragraph,
we first compute the eigenvalue c of A farthest from the origin using the power method.
Then the required eigenvalue for C is c+b. This algorithm is called the shifted power method.
Iterative Algorithms in Numerical Linear Algebra
269
Next, suppose A ∈Mn(C) and we want to find the eigenvalue of A closest to the origin.
If A is not invertible, this eigenvalue is zero. If A−1 exists and c is any eigenvalue of A with
associated eigenvector x, then Ax = cx implies A−1Ax = A−1cx, hence x = cA−1x. As c
must be nonzero, we see A−1x = c−1x, so that c−1 is an eigenvalue of A−1 with associated
eigenvector x. The argument is reversible, so the eigenvalues of A−1 are the multiplicative
inverses in C of the eigenvalues of A. The inverse of a complex number written in polar
form is (reiθ)−1 = r−1e−iθ. It follows that for nonzero c, d ∈C, c is closer to the origin than
d iff c−1 is farther from the origin than d−1. Using the power method to find the eigenvalue
c of A−1 that is farthest from the origin, it follows that c−1 is the eigenvalue of A closest
to the origin. This algorithm is called the inverse power method.
We can use the shifted inverse power method to find the eigenvalue of a given C ∈Mn(C)
that is closest to a given b ∈Mn(C). We apply the original power method to the matrix
(C −bI)−1 to obtain the largest eigenvalue c, and then return c−1 + b.
When implementing the inverse power method (or its shifted version), we need to
compute yk+1 = A−1xk given xk. Equivalently, we must solve Ayk+1 = xk for the
unknown vector yk+1. In practice, it is often more efficient to obtain yk+1 by using Gaussian
elimination algorithms to solve Ayk+1 = xk rather than computing A−1 explicitly. This is
especially true when A has special structure such as sparsity.
10.17
Deflation
Suppose A ∈Mn(C) is a given matrix and we have found, by whatever method, one
particular eigenvalue c of A and an associated eigenvector x. This section describes a general
algorithm called deflation that produces a new matrix B ∈Mn−1(C) whose eigenvalues
(counted by their algebraic multiplicities as roots of the characteristic polynomial) are
precisely the eigenvalues of A with one copy of c removed. In particular, by repeatedly
applying the power method (or its variants) followed by deflation to a given matrix A, we
can eventually compute all the eigenvalues of A.
Recall Schur’s Theorem from §7.7: for any matrix A ∈Mn(C), there exists a unitary
matrix U ∈Mn(C) such that U −1AU = U ∗AU is upper-triangular. We obtain the deflation
algorithm by examining the computations implicit in the proof of this result. Assume c1 ∈C
is the known eigenvalue of A with associated known eigenvector x1 ∈Cn. Dividing x1 by
||x1||2, we can assume that ||x1||2 = 1. Let T : Cn →Cn be defined by T(v) = Av for
v ∈Cn. The key step in the inductive proof of Schur’s theorem was to extend the list (x1)
to an orthonormal basis X = (x1, x2, . . . , xn) of Cn. We can compute such an orthonormal
basis X explicitly using the Gram–Schmidt Orthonormalization Algorithm (see §9.2 and
Exercise 15 in Chapter 9).
Arrange the column vectors x1, x2, . . . , xn as the columns of a matrix U, so U = E[id]X
where E = (e1, . . . , en) is the standard ordered basis of Cn. Since the columns of
U are orthonormal, U is a unitary matrix (see §7.5), so U −1 = U ∗. We can com-
pute C = [T]X = U ∗AU explicitly. The first column of this matrix is (c1, 0, . . . , 0)T. Let
B ∈Mn−1(C) be obtained from C by deleting the first row and first column. Computing
the characteristic polynomial of C by expanding the determinant along the first column,
we see that χA(t) = χC(t) = (t −c1)χB(t). Therefore, the matrix B has the required
eigenvalues, and the deflation algorithm returns B as its output.
270
Advanced Linear Algebra
10.18
Summary
1.
Vector Norms. A vector norm || · || on a real or complex vector space V satisfies
0 ≤||x|| < ∞, ||x|| = 0 ⇔x = 0, ||cx|| = |c| · ||x||, ||x + y|| ≤||x|| + ||y||
for all x, y in V and all scalars c. The pair (V, || · ||) is called a normed vector
space. Examples of vector norms on Rn and Cn include
||x||∞= max
1≤i≤n |xi|,
||x||1 = |x1| + · · · + |xn|,
||x||2 =
p
|x1|2 + · · · + |xn|2.
2.
Metric Spaces. A metric space is a set X and a metric d satisfying
0 ≤d(x, y) < ∞, d(x, y) = 0 ⇔x = y, d(x, y) = d(y, x), d(x, z) ≤d(x, y)+d(y, z)
for all x, y, z ∈X. A normed vector space V has the metric d(x, y) = ||x −y||,
which also satisfies d(x + z, y + z) = d(x, y) and d(cx, cy) = |c|d(x, y) for all
x, y, z ∈V and all scalars c.
3.
Convergence Properties. In a metric space (X, d), a sequence (xk)k≥0 converges to
x ∈X iff for all ϵ > 0, there exists k0 ∈Z≥0 such that for all k ≥k0, d(xk, x) < ϵ.
The limit of a convergent sequence is unique when it exists. In Cn, if vk →v and
wk →w, then vk + wk →v + w, cvk →cv for any scalar c, and Avk →Av for
any matrix A.
4.
Comparable Norms. Vector norms ||·|| and ||·||′ on a vector space V are comparable
iff there exist real C, D with 0 < C, D < ∞such that for all x ∈V , ||x|| ≤C||x||′
and ||x||′ ≤D||x||. Any two vector norms on Rn or Cn are comparable. In
particular, ||x||∞≤||x||2 ≤√n||x||∞and ||x||∞≤||x||1 ≤n||x||∞.
5.
Matrix Norms. A matrix norm || · || : Mn(C) →R≥0 satisfies
||A|| = 0 ⇔A = 0, ||cA|| = |c|·||A||, ||A+B|| ≤||A|+||B||, ||AB|| ≤||A||·||B||
for all A, B ∈Mn(C) and all c ∈C. Any vector norm ||·|| on Cn has an associated
matrix norm ||A|| = sup{||Av|| : v ∈Cn, ||v|| = 1}, and ||Ax|| ≤||A|| · ||x|| for all
x ∈Cn. A matrix norm defined in this way from some vector norm is called an
induced matrix norm.
6.
Formulas for Matrix Norms. Fix A ∈Mn(C).
(a) The matrix norm induced by the sup norm is ||A||∞= max
1≤i≤n
n
X
j=1
|A(i, j)|.
(b) The matrix norm induced by the 1-norm is ||A||1 = max
1≤j≤n
n
X
i=1
|A(i, j)|.
(c) The spectral radius ρ(A) is the maximum of the numbers |c| as c ranges over
all complex eigenvalues of A. The spectral radius of A also equals inf{||A||}, where
|| · || varies over all induced matrix norms on Cn.
(d) The matrix norm induced by the 2-norm is ||A||2 =
p
ρ(A∗A).
7.
Geometric Series for Matrix Inverses. If C ∈Mn(C) satisfies ||C|| < 1 for some
induced matrix norm (or equivalently, if ρ(C) < 1), then I −C is invertible and
(I −C)−1 = P∞
k=0 Ck. The error of the partial sum ending at Ck is at most
||(I −C)−1|| · ||C||k+1.
Iterative Algorithms in Numerical Linear Algebra
271
8.
Affine Iteration. If C ∈Mn(C) satisfies ||C|| < 1 for some induced matrix norm
(or equivalently, if ρ(C) < 1), then for any d, x0 ∈Cn, the sequence defined by
xk+1 = Cxk +d (for k ≥0) converges to the unique x ∈Cn solving (I −C)x = d,
and ||xk −x|| ≤||C||k||x0 −x||.
9.
Richardson’s Algorithm. This iterative method solves Ax = b by the update rule
xk+1 = xk +(b−Axk). It converges if ||I −A|| < 1 for some induced matrix norm
(i.e., ρ(I −A) < 1), in which case ||xk −x|| ≤||I −A||k||x0 −x||. Convergence is
guaranteed if maxi
P
j |(I −A)(i, j)| < 1 or if maxj
P
i |(I −A)(i, j)| < 1.
10.
Jacobi’s Algorithm. This iterative method solves Ax = b by the update rule
xk+1 = Q−1(b −Rxk) where Q is the diagonal part of A and R = A −Q, i.e.,
xk+1(i) = A(i, i)−1[b(i) −
X
j:j̸=i
A(i, j)xk(j)].
If ρ(I −Q−1A) < 1, convergence occurs with ||xk −x|| ≤||I −Q−1A||k||x0 −x||.
Convergence is guaranteed if |A(i, i)| < P
j:j̸=i |A(i, j)| for all i.
11.
Gauss–Seidel Algorithm. This iterative method solves Ax = b by the update rule
Qxk+1 = b −Rxk where Q is the lower-triangular part of A and R = A −Q, i.e.,
xk+1(i) = A(i, i)−1[b(i) −
X
j:j<i
A(i, j)xk+1(j) −
X
j:j>i
A(i, j)xk(j)].
If ρ(I −Q−1A) < 1, convergence occurs with ||xk −x|| ≤||I −Q−1A||k||x0 −x||.
Convergence is guaranteed if |A(i, i)| < P
j:j̸=i |A(i, j)| for all i.
12.
Power Method. The power method approximates the largest eigenvalue c of A ∈
Mn(C) by starting with x0 ̸= 0, computing xk+1 = Axk/||Axk||∞for k ≥0,
and estimating c ≈(Axk)(i)/xk(i) where i is chosen so |xk(i)| is not too close to
zero. If A is diagonalizable with a unique eigenvalue c of magnitude ρ(A), then
for most choices of x0, the power method approximations converge to c, and the
xk approach associated eigenvectors (depending on k). The requirement on x0 for
convergence is that the expansion of x0 as a linear combination of eigenvectors
of A must involve the eigenvector associated with c.
13.
Variations of the Power Method. The shifted power method finds the eigenvalue
of A ∈Mn(C) farthest from b ∈C by applying the power method to A −bI
and adding b to the output. The inverse power method finds the eigenvalue of A
closest to zero by applying the power method to A−1 and taking the reciprocal
of the output. The shifted inverse power method finds the eigenvalue of A closest
to b by using the power method to get the largest eigenvalue c of (A −bI)−1, and
returning c−1 + b.
14.
Deflation. If we have found one eigenvalue c of A ∈Mn(C) and an associated
unit eigenvector x, we can compute a matrix B ∈Mn−1(C) whose eigenvalues
are the remaining eigenvalues of A as follows. Extend x to an orthonormal basis
of Cn by the Gram–Schmidt algorithm. Let U be the unitary matrix having these
basis vectors as columns, with x in column 1. Let B be the matrix U ∗AU with
the first row and column erased.
272
Advanced Linear Algebra
10.19
Exercises
1.
Given v = (2, −4, −1, 0, 3), compute ||v||∞, ||v||1, and ||v||2.
2.
Given v = (3 + 4i, −2 −i, 5i), compute ||v||∞, ||v||1, and ||v||2.
3.
Sketch the set of unit vectors in R2 for the norms || · ||∞, || · ||1, and || · ||2.
4.
Sketch the set of unit vectors in R3 for the norms || · ||∞, || · ||1, and || · ||2.
5.
For each matrix A, execute Richardson’s method (computing x1, . . . , x5 by hand)
to try to solve Ax = b using b = [1 2]T and x0 = 0. Compare x5 to the exact
solution x. Does the method appear to be converging?
(a)

1
1/2
0
1

(b)
 1
2
2
1

(c)

1
0.1
0.1
1

(d)
 1
−1
1
1

6.
For each matrix A, predict whether Richardson’s Algorithm converges. If so, give
an estimate on the error ||xk −x|| for an appropriate vector norm.
(a)


1.3
0.2
−0.3
0.1
0.9
−0.2
0.2
0.2
1.2


(b)


1
1
2
0
1
2
2
1
1


(c)
 3.5
−4.5
1.8
−2.2

(d) A ∈Mn(R) given by A(i, j) = 2i−j for i ≤j, and A(i, j) = 0 for i > j.
7.
Write a program in a computer algebra system to implement Richardson’s
Algorithm. Use the program to solve Ax = b, where
A =


1
0.1
0
0.2
−0.1
0.1
0.1
1
0
0.1
0
0.2
0
0.2
1
−0.1
0
0.1
−0.3
0
0
1
−0.1
0
−0.1
0.1
0
0
1
0
0
0.2
0.1
−0.1
0.3
1


,
b =


2
1.3
0
−1
0.6
1.7


.
Give an error estimate for your answer.
8.
For each matrix A and column vector b, execute Jacobi’s Algorithm (starting
with x0 = 0 and computing x1, . . . , x5) to try to solve Ax = b. Compare x5 to
the exact solution x. Does the method appear to be converging?
(a) A =

2
1
−1
2

, b =
 3
1

(b) A =

0
5
−2
1

, b =

2
−2

(c) A =

1
3
3
1

, b =

1
2

(d) A =


3
1
1
2
4
−1
0
−1
2

, b =


4
−1
0

.
9.
Repeat Exercise 8 using the Gauss–Seidel algorithm.
10.
For each square matrix A, predict whether Jacobi’s Algorithm converges. If so,
give an estimate on the error ||xk −x|| for an appropriate vector norm.
(a)


1
2
3
2
3
5
3
4
5


(b)


8
4
2
3
9
1
1
−2
4


(c) A =

2
−5
1
3

(d) A(i, i) = c > 2, A(i, j) = 1 for |i −j| = 1, and A(i, j) = 0 otherwise.
11.
Repeat Exercise 10 using the Gauss–Seidel algorithm.
12.
Write a program in a computer algebra system to implement Jacobi’s Algorithm.
Iterative Algorithms in Numerical Linear Algebra
273
Use the program to solve Ax = b, where
A =


3.2
1.1
0
0
−1.2
0
1.3
4.1
−1.2
1.5
0
0
0.8
0
−1.7
0
0.5
0
0.7
2.1
0
5.3
0
−1.3
0.3
0
−0.6
1.1
−2.4
0
1.4
−0.8
0
0
−1.0
4.9


,
b =


4.1
−2.2
1.3
−0.8
0
−3.2


.
Give an error estimate for your answer.
13.
Repeat Exercise 12 using the Gauss–Seidel algorithm.
14.
For each matrix A, execute several iterations of the power method to approximate
the largest eigenvalue of A and an associated eigenvector. Also find all eigenvalues
of A exactly and compare to the algorithm’s output.
(a)

4
−2
−3
−1

(b)


3
1
0
1
3
1
0
1
3


(c)


1
2
i
2
1
−1
−i
−1
1


15.
Repeat Exercise 14, but use the inverse power method to find the smallest
eigenvalue of A and an associated eigenvector.
16.
Repeat Exercise 14, but use the shifted power method to find the eigenvalue of
A farthest from 4 and an associated eigenvector.
17.
Let A ∈M4(R) have A(1, 4) = A(3, 2) = A(4, 2) = A(4, 4) = −1 and all other
entries equal to 1. Use the inverse shifted power method to find the eigenvalue of
A closest to −3 and an associated eigenvector.
18.
For each matrix A, find the exact eigenvalues of A (with multiplicities). Try
executing the power method on A and x0 for a few iterations, and explain what
goes wrong (and why) in each case.
(a) A =
 2
0
0
−2

, x0 =
 1
1

(b) A =


1
2
3
4
5
6
7
8
9

, x0 =


1
−2
1


(c) A =


3
2
1
0
2
1
0
0
1

, x0 =


0
−3
4


(d) A =

−1/2
√
3/2
−
√
3/2
−1/2

, x0 =

1
0

.
19.
Write a program to implement the power method for calculating eigenvalues. Test
your program on the matrices in Exercises 7 and 12.
20.
For each matrix A in Exercise 5, try to approximate A−1 using the geometric
series formula (10.11). For which matrices does the series converge?
21.
Use the geometric series formula (10.11) to estimate A−1, where A is the matrix
in Exercise 7.
22.
For fixed b ∈C, define A ∈Mn(C) by setting A(i, j) = 1 for i = j, A(i, j) = b for
j = i + 1, and A(i, j) = 0 for all other i, j. Use (10.11) to find A−1.
23.
Prove: if C is nilpotent (meaning Ck = 0 for some positive integer k), then I−C is
invertible and (I −C)−1 is given exactly by the finite sum I +C +C2+· · ·+Ck−1.
24.
Consider the affine iteration xk+1 = Cxk + d for fixed C ∈Mn(C) and d ∈Cn.
Express xk as a sum of terms involving powers of C, x0, and d. Prove that if C
is nilpotent with Cr = 0, then xr is the exact solution to (I −C)x = d.
274
Advanced Linear Algebra
25.
Suppose C ∈Mn(C) satisfies ρ(C) > 1 and I −C is invertible. Show that for
all d ∈Cn, there exists x0 ∈Cn such that the affine iteration (10.12) starting at
x0 converges. Show there exist d, x0 ∈Cn such that the affine iteration (10.12)
starting at x0 does not converge.
26.
Prove that the 1-norm on Cn satisfies the axioms for a vector norm.
27.
Define ||(x1, x2, x3)|| = |x1| + |x3|. Which axioms for a vector norm are satisfied?
28.
Let ||(x1, x2)|| = (
p
|x1|+
p
|x2|)2. Which axioms for a vector norm are satisfied?
29.
Cauchy–Schwarz Inequality. Prove: for x, y ∈Rn, |x • y| ≤||x||2||y||2.
(Compute (ax + by) • (ax + by) for a, b ∈R. For x ̸= 0 ̸= y, take a = ||x||−1
2
and b = ±||y||−1
2 .)
30.
Prove that the 2-norm on Rn satisfies the axioms for a vector norm. (For the
triangle inequality, compute ||x + y||2
2.)
31.
Let ||(x1, x2)|| = |x1|2 + |x2|2. Which axioms for a vector norm are satisfied?
32.
Let (V, || · ||V ) and (W, || · ||W ) be normed vector spaces. For (v, w) ∈V × W,
define ||(v, w)||1 = ||v||V + ||w||W . Prove this defines a vector norm on V × W.
33.
Let (V, ||·||V ) and (W, ||·||W ) be normed vector spaces. For (v, w) ∈V ×W, define
||(v, w)||∞= max(||v||V , ||w||W ). Prove this defines a vector norm on V × W.
34.
Explain why the 1-norm and sup norm on Rn are special cases of the norms
defined in the previous two exercises.
35.
For each real p ∈[1, ∞), the p-norm on Cn is defined on x = (x1, . . . , xn) ∈Cn
by ||x||p = (|x1|p + |x2|p + · · · + |xn|p)1/p.
(a) Show || · ||p satisfies the first two axioms for a vector norm.
(b) H¨older’s Inequality. Assume 1 < p, q < ∞and p−1 + q−1 = 1.
Prove: for all x, y ∈Cn,
n
X
i=1
|xiyi| ≤||x||p||y||q.
(First prove it assuming ||x||p = ||y||q = 1, using Exercise 111 of Chapter 11.)
(c) Minkowski’s Inequality. Prove: for all x, y ∈Cn, ||x + y||p ≤||x||p + ||y||p.
(Start with ||x + y||p
p, note |xi + yi|p ≤(|xi| + |yi|) · |xi + yi|p−1, and use (b).)
36.
Prove: for x ∈Rn, limp→∞||x||p = ||x||∞.
37.
For fixed real numbers p, r ≥1, find the minimum constant C = C(p, r) such that
for all x ∈Cn, ||x||p ≤C||x||r. (One approach is to use Lagrange multipliers.)
38.
Prove: for all x, y in a normed vector space, | ||x|| −||y|| | ≤||x −y||.
39.
For a fixed invertible S ∈Mn(C), define ||v||S = ||S−1v||∞for v ∈Cn.
Prove ||·||S is a vector norm on Cn. Prove that the matrix norm induced by ||·||S
satisfies ||A||S = ||S−1AS||∞for all A ∈Mn(C).
40.
Suppose T : V →W is a vector space isomorphism. Given a vector norm ||·||V on
V , let ||w||W = ||T −1(w)||V for w ∈W. Show ||·||W is a vector norm on W. Prove
that for the metrics associated with the norms on V and W, d(T(v), T(v′)) =
d(v, v′) for all v, v′ ∈V .
41.
Explain why the vector norm || · ||S in §10.13 is a special case of the construction
in the previous exercise.
42.
(a) Prove directly from the definitions that for all x = (x1, . . . , xn) ∈Cn and all
i, |xi| ≤||x|| where || · || is the 1-norm, the 2-norm, or the sup norm.
(b) Use a theorem from the text to show that for any vector norm on Cn and all
i, there is a constant K with |xi| ≤K||x||.
Iterative Algorithms in Numerical Linear Algebra
275
(c) Let V
be a finite-dimensional real or complex normed vector space
with ordered basis Z = (z1, . . . , zn). Each v ∈V has a unique expansion
v = p1(v)z1 + · · · + pn(v)zn for certain scalars pi(v). Prove there are constants
Ki such that for all v ∈V , |pi(v)| ≤Ki||v||.
43.
Consider the Q-vector space V = {a + b
√
2 : a, b ∈Q}.
(a) Define ||a + b
√
2|| to be the absolute value (in R) of a + b
√
2. Show that the
axioms for a vector norm (using rational scalars) hold.
(b) Show that there does not exist a constant K such that for all x = a+b
√
2 ∈V ,
|a| ≤K||x||. (Contrast to Exercise 42(c).)
44.
Let V be the real vector space of continuous functions f : [0, 1] →R. Show that
||f||∞= supx∈[0,1] |f(x)| is a vector norm on V .
45.
Let V be the real vector space of continuous functions f : [0, 1] →R. Show that
||f||I =
R 1
0 |f(x)| dx is a vector norm on V .
46.
Let V , || · ||∞, and || · ||I be defined as in Exercises 44 and 45.
(a) Show there is no constant C such that for all f ∈V , ||f||∞≤C||f||I.
(b) Is there a constant D such that for all f ∈V , ||f||I ≤D||f||∞? Explain.
47.
For A ∈Mn(C) with n > 1, define ||A|| = max1≤i,j≤n |A(i, j)|. Prove that this is
not a matrix norm.
48.
For A ∈Mn(C), define ||A|| = Pn
i,j=1 |A(i, j)|. Is this a matrix norm? If so, is
this an induced matrix norm?
49.
Frobenius Matrix Norm. For A ∈Mn(C), define ||A||F =
qPn
i,j=1 |A(i, j)|2.
Prove || · ||F is a matrix norm. Is || · ||F induced by any vector norm on Cn?
Explain. Prove ||A||2
F is the trace (sum of diagonal entries) of A∗A.
50.
Let U be a unitary matrix.
(a) Prove: for all v ∈Cn, ||Uv||2 = ||v||2.
(b) Must (a) hold for the 1-norm or the sup norm on Cn?
(c) Prove: for all A ∈Mn(C), ||UA||2 = ||A||2 = ||AU||2.
(d) Does (c) hold for the matrix norms induced by the 1-norm or the sup norm
on Cn? Does (c) hold for the Frobenius matrix norm (Exercise 49)?
51.
For any real or complex vector space V and x, y ∈V , let d(x, y) = 1 if x ̸= y, and
d(x, y) = 0 if x = y. Prove that (V, d) is a metric space. Is there a vector norm
on V such that d is the associated metric? Explain.
52.
Suppose V is a real or complex vector space and d is a metric on V invariant
under translations and respecting dilations. For x ∈V , define ||x|| = d(x, 0V ).
Prove || · || is a vector norm whose associated metric is d.
53.
Let (X, dX) and (Y, dY ) be metric spaces. Show X × Y is a metric space with
metric given by d((x1, y1), (x2, y2)) = dX(x1, x2) + dY (y1, y2) for x1, x2 ∈X and
y1, y2 ∈Y .
54.
Suppose (X, d) is a metric space and (xk)k≥0 is a sequence in X.
(a) Suppose xk = x ∈X for all k. Prove limk→∞xk = x.
(b) Assume xk →x ∈X. Prove: for all j ∈Z>0, limk→∞xk+j = x.
(c) Suppose (yk)k≥0 is a sequence such that yk = xjk for some indices j0 < j1 <
· · · < jk < · · · (so (yk) is a subsequence of (xk)). Prove: if xk →x, then yk →x.
(d) Give an example to show that the converse of the result in (c) can fail.
55.
Suppose (xk)k≥0 is a sequence in a normed vector space V .
(a) Prove: if xk →x in V , then ||xk|| →||x|| in R.
276
Advanced Linear Algebra
(b) Show that if ||xk|| →0 in R, then xk →0V in V .
(c) Give an example where ||xk|| →r > 0 in R but (xk) does not converge in V .
56.
Let (X, d) be a metric space with xk ∈X for k ≥0. We say (xk) is a Cauchy
sequence iff for all ϵ > 0, there exists k0 ∈Z≥0 such that for all j, k ≥k0,
d(xk, xj) < ϵ. Prove that every convergent sequence is a Cauchy sequence. Give
an example of a Cauchy sequence that does not converge.
57.
Prove that if (xk) is a Cauchy sequence in a metric space (X, d) and some
subsequence (yk) = (xjk) converges to x, then xk →x.
58.
We say a metric space (X, d) is complete iff every Cauchy sequence in X converges
to some point in X. Using the fact that R1 is complete, prove: for all n > 1, Rn
is complete using the metric d(x, y) = ||x −y||∞for x, y ∈Rn.
59.
For any vector norm || · || on Rn, use the previous exercise to prove that Rn is
complete using the metric d(x, y) = ||x −y|| for x, y ∈Rn.
60.
Prove: if ck →c (in R or C) and vk →v in a normed vector space V , then
ckvk →cv.
61.
Prove that ||x||∞≤||x||1 ≤n||x||∞for all x ∈Rn.
62.
Find a nonzero x ∈Rn such that ||x||1 = ||x||2 = ||x||∞.
63.
Find all y ∈Rn such that ||y||2 = √n||y||∞.
64.
Find all z ∈Rn such that ||z||1 = n||z||∞.
65.
Let S be the set of all vector norms on a (possibly infinite-dimensional) vector
space V . Show that comparability of norms defines an equivalence relation on S.
How many equivalence classes are there when V = Rn?
66.
Let ||·|| be any vector norm on Rn. Let S = {x ∈Rn : ||x|| = 1} be the set of unit
vectors relative to this norm. Show S is a closed and bounded (hence compact)
subset of the metric space (Rn, d), where d(y, z) = ||y −z||2 for y, z ∈Rn. For the
matrix norm ||A|| induced by || · ||, show there exists x ∈S with ||Ax|| = ||A||.
67.
Let || · || be any matrix norm. Prove: for all A ∈Mn(C) and all k ∈Z≥0,
||Ak|| ≤||A||k. If A is invertible, is there any relation between ||A−1|| and ||A||−1?
68.
Let ||A|| be the matrix norm induced by a vector norm || · || on Cn.
(a) Prove: ||A|| = sup{||Ax||/||x|| : x ∈Cn, x ̸= 0}.
(b) Prove: ||A|| = inf{C ∈R : ||Ax|| ≤C||x|| for all x ∈Cn}.
69.
For each matrix A in Exercise 6, compute ||A||∞, ||A||1, ||A||2, and ||A||F (see
Exercise 49). Take n = 4 when computing ||A||2 in part (d).
70.
(a) For A ∈Mn(C), show that ||A||∞= ||(||A[1]||1, . . . , ||A[n]||1)||∞.
(b) State a similar formula for ||A||1.
71.
Let S = {u ∈Rn : ||u||2 = 1}. For each matrix A below, define f : S →R by
f(u) = ||Au||2
2 for u ∈S. Find the maximum value of f on S and hence compute
||A||2. (Use Lagrange multipliers for (b) and (c).)
(a)

3
1
−1
3

(b)

1
1
1
1

(c)


1
1
0
0
1
1
1
0
1


72.
Prove: for all A ∈Mn(C), ||A||2 =
p
ρ(A∗A). (Note A∗A is positive semidefinite.)
73.
Use the theorem in §7.7 to show that given any ϵ > 0 and any B in Mn(C),
there is an invertible S in Mn(C) such that S−1BS is upper-triangular and
||S−1BS||∞≤ρ(B) + ϵ. (If T is triangular and D is diagonal with D(i, i) = δi
for some δ > 0, compare the entries of T and D−1TD.)
Iterative Algorithms in Numerical Linear Algebra
277
74.
Assume the setup in §10.15. (a) Suppose the largest eigenvalue c has several
associated eigenvectors, say c = c1 = · · · = cm and |cm| > |cm+1|. Prove the
power method still converges. (b) Suppose instead that |c1| = · · · = |cm| > |cm+1|
with m > 1. Must the power method converge? (c) Suppose we happen to pick
x0 with d1 = 0. Assuming |c2| > |c3|, what does the power method do?
75.
With the setup in §10.15, prove that xk −b′
kz1 →0 as k →∞.
76.
Execute the deflation algorithm on the matrix A =


1
4
−1
0
3
2
1
−1
−3

, which has
a known eigenvalue c = −1 and associated eigenvector x = [3 −1 2]T.
77.
Let A =


0
1
1
1
1
0
1
0
1

. Find the largest eigenvalue of A and an associated
eigenvector by the power method. Use deflation to recover the other two
eigenvalues of A.
78.
Let A =


0
0
0
1
1
0
0
0
0
1
0
0
0
0
1
0

. By inspection, find an eigenvector of A associated with
the eigenvalue 1. Use deflation to find B ∈M3(C) such that χB(t) = (t −1)χA(t).
79.
(a) Give an example of a matrix A and a scalar c ̸= 0 such that Richardson’s
Algorithm fails to converge when applied to Ax = b, but does converge when
applied to (cA)x = cb. (b) For any A ∈Mn(C) and nonzero c ∈C, how are the
eigenvalues of I −A related to the eigenvalues of I −cA? (c) Find a condition on
the eigenvalues of A guaranteeing the existence of c ∈C such that Richardson’s
Algorithm converges for the system (cA)x = cb. Describe how to choose c if we
know all the eigenvalues of A.
80.
Let A =
 a
b
c
d

, Q1 =
 a
0
0
d

, and Q2 =
 a
0
c
d

where a, b, c, d ∈C with
a ̸= 0 ̸= d. Find the eigenvalues of I −Q−1
1 A and I −Q−1
2 A. Deduce sufficient
conditions for the convergence of the Jacobi method and the Gauss–Seidel method
on the system Ax = y.
81.
Give a specific example of a matrix A ∈M3(R) and b ∈R3 such that the Jacobi
method converges when applied to Ax = b, but the Gauss–Seidel method does
not converge for this system.
82.
Give a specific example of a matrix A ∈M3(R) and b ∈R3 such that the Gauss–
Seidel method converges when applied to Ax = b, but the Jacobi method does
not converge for this system.
83.
Discuss how the shifted power method might be used to find the largest eigenvalue
of A ∈Mn(C) in each of these cases: (a) A is not invertible; (b) two eigenvalues
of A both have magnitude ρ(A).
84.
Suppose A ∈Mn(C) is lower-triangular with nonzero entries on the diagonal.
(a) What happens if we use the Gauss–Seidel algorithm to solve Ax = b?
(b) Must the Jacobi method converge for this system? Explain.
85.
Given A =
 a
b
c
d

∈M2(R), find ||A||2 and ρ(A) in terms of a, b, c, d.
86.
Define ||A|| = ρ(A) for A ∈Mn(C). Which axioms for a matrix norm are satisfied?
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
Part IV
The Interplay of Geometry and
Linear Algebra
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
11
Affine Geometry and Convexity
A central concept in linear algebra is the idea of a subspace of a vector space. Subspaces
provide an algebraic abstraction of uncurved geometric objects such as lines and planes
through the origin in three-dimensional space. However, there are many other uncurved
geometric figures that are not subspaces, including lines and planes not passing through
the origin, individual points, line segments, triangles, quadrilaterals, polygons in the plane,
tetrahedra, cubes, and solid polyhedra. This chapter discusses affine sets and convex sets,
which provide an algebraic setting for studying such figures and their higher-dimensional
analogs.
Geometrically, an affine subset of a vector space is a subspace that has been translated
away from the origin by adding some fixed vector. Affine sets can also be described as
intersections of hyperplanes, as solution sets of systems of linear equations, or as the set
of affine combinations of a given set of vectors. A notion of affine independence, analogous
to linear independence, leads to affine versions of bases, dimension, and coordinate systems
for affine sets. Affine maps preserve the affine structure of affine sets, in the same way that
linear maps preserve the linear structure of vector spaces and subspaces.
A convex set in Rn contains the line segment joining any two of its points. Given a set
S of points in Rn, there is a smallest convex set containing S, called the convex hull of S.
We can build the convex hull from the generating set S, by taking all convex combinations
of the generators, or we can obtain the convex hull by intersecting all convex sets that
contain S. This fact is one instance of a family of theorems asserting the equivalence of a
generative description of a class of sets and an intersectional description of that same class.
We shall encounter several such theorems in this chapter, including the fundamental result
that convex hulls of finite sets coincide with bounded intersections of finitely many closed
half-spaces.
The chapter concludes with a discussion of convex real-valued functions, which are closely
related to convex sets. Convex functions play a prominent role in analysis, linear algebra, and
optimization theory, especially linear programming. We prove Jensen’s Inequality for convex
functions and describe how to test convexity by checking the first and second derivatives of
a function.
11.1
Linear Subspaces
Before beginning our study of affine geometry, we discuss some facts about subspaces of
vector spaces that are used in the development of the affine theory. Let F be a field, n be a
positive integer, and V be an n-dimensional vector space over F. We know V is isomorphic
DOI: 10.1201/9781003484561-11
281
282
Advanced Linear Algebra
to the vector space F n consisting of n-tuples v = (v1, . . . , vn) with each vi in F (see §6.3).
We often identify an n-tuple v with the n × 1 column vector


v1
...
vn

.
A subspace of V is a subset W of V satisfying these three closure conditions:
(i) 0V belongs to W (closure under identity);
(ii) for all v, w ∈W, v + w is in W (closure under vector sum);
(iii) for all v ∈W and c ∈F, cv is in W (closure under scalar multiplication).
In this chapter, we may refer to subspaces of V as linear subspaces to emphasize the
distinction between subspaces and affine sets (defined later). Given a subspace W, an integer
k ≥0, vectors v1, . . . , vk in W, and scalars c1, . . . , ck in F, it follows by induction on k that
c1v1 + · · · + ckvk must belong to W. The expression c1v1 + · · · + ckvk is called a linear
combination of v1, . . . , vk. Thus, subspaces are closed under taking linear combinations of
their elements. Conversely, any subset of V that is closed under linear combinations must
be a linear subspace of V . Here, we use the convention that a linear combination of k = 0
terms is interpreted as 0V .
Geometrically, the condition that 0V ∈W means that all linear subspaces are required
to pass through the origin. Every subspace W of V is itself an F-vector space, and the
dimension of W is between 0 and n. One-dimensional subspaces of V are called lines through
the origin; two-dimensional subspaces of V are called planes through the origin; and (n−1)-
dimensional subspaces of the n-dimensional space V are called linear hyperplanes. Lines and
planes in R3 that do not pass through (0, 0, 0) are not linear subspaces; they are examples
of the affine sets to be studied later.
11.2
Examples of Linear Subspaces
This section describes four ways of constructing linear subspaces, each of which leads to a
different description of the points in the subspace.
First, suppose that V is F n (viewed as a set of column vectors), and A ∈Mm,n(F) is
an m × n matrix with entries in F. The null space of A is W = {x ∈V : Ax = 0}. We
check that W is a subspace of V as follows. W is a subset of V by definition. To check (i),
note A0 = 0, so 0 ∈W. To check (ii), fix v, w ∈W and show that v + w ∈W. We know
Av = 0 = Aw, so A(v + w) = Av + Aw = 0 + 0 = 0, so v + w is in W. To check (iii), fix
v ∈W and c ∈F and show that cv ∈W. We know Av = 0, so A(cv) = c(Av) = c0 = 0,
so cv ∈W. We can think of the null space of A more concretely as the solution set of the
system of m homogeneous linear equations in n unknowns







A(1, 1)x1 + A(1, 2)x2 + · · · + A(1, n)xn
=
0
A(2, 1)x1 + A(2, 2)x2 + · · · + A(2, n)xn
=
0
. . .
A(m, 1)x1 + A(m, 2)x2 + · · · + A(m, n)xn
=
0.
(11.1)
The equations are called homogeneous because their right-hand sides are zero.
Second, suppose that V is F m (viewed as a set of column vectors), and A ∈Mm,n(F)
is a given matrix. The range of A is W = {v ∈V : ∃x ∈F n, Ax = v}. It is routine to
verify that W is a subspace of F m. We can give a more abstract version of these first two
constructions by considering an arbitrary F-linear map T : V →Z between two F-vector
spaces V and Z. The kernel of T, defined to be ker(T) = {v ∈V : T(v) = 0Z}, is a subspace
Affine Geometry and Convexity
283
of V . The image of T, defined to be img(T) = {z ∈Z : ∃v ∈V, T(v) = z}, is a subspace
of Z. To recover the earlier examples involving the matrix A, take T : F n →F m to be
T(v) = Av for v ∈F n. This is a linear map whose kernel is the null space of A and whose
image is the range of A.
Third, suppose S = {v1, . . . , vk} is a set of k vectors in V . Let W consist of all linear
combinations of elements of S, so that a vector w is in W iff there exist scalars c1, . . . , ck ∈F
with w = c1v1 + · · · + ckvk. It is routine to check that W satisfies the closure conditions (i),
(ii), and (iii), so that W is a subspace of V . We call W the subspace spanned by S or the
linear span of S, writing W = Sp(S) or W = SpF (S). Observe that S ⊆W, because each
vi can be written in the form 0v1 + · · · + 1vi + · · · + 0vn. For infinite S ⊆V , let SpF (S)
consist of all linear combinations of finitely many elements of S. Again, we may check that
this is a linear subspace of V containing S. For all linear subspaces Z of V , if S ⊆Z then
SpF (S) ⊆Z since Z is closed under linear combinations. So, SpF (S) is the smallest linear
subspace of V containing S. The fact that the range of A ∈Mm,n(F) is a subspace is a
special case of the spanning construction. This follows from the fact that the elements Ax
in the range of A are precisely the linear combinations x1A[1] + x2A[2] + · · · + xnA[n] of the
n columns of A (see §4.7).
Fourth, suppose {Wi : i ∈I} is any collection of linear subspaces of V . The intersection
W = T
i∈I Wi is readily seen to be a subspace of V . For example, any intersection of
linear hyperplanes in V is a subspace of V . We claim that the null space of a matrix
A ∈Mm,n(F) can be viewed as a special case of this construction. To see why, note that
the solution set of the system of equations (11.1) is the intersection of the solution sets of
the m individual equations considered separately. Consider the solution set Z of a particular
equation c1x1 + · · · + cnxn = 0, where every ci ∈F. If all ci are zero, then Z is all of F n,
and we can discard this equation from the system without changing the final solution set
of the full system. If some ci ̸= 0, we solve the equation for xi to get
xi = c−1
i (−c1x1 −· · · −ci−1xi−1 −ci+1xi+1 −· · · −cnxn).
We obtain all solutions by choosing any scalar value for each xj with j ̸= i and then using
the displayed relation to compute xi. For example, if k ̸= i is fixed and we choose xk = 1
and xj = 0 for all j ̸= k, i, then xi = −c−1
i ck. From this description, we can verify that
the set {ek −c−1
i ckei : 1 ≤k ≤n, k ̸= i} is a basis for Z of size n −1, where ei and ek
are standard basis vectors in F n. This means that Z is a linear hyperplane in F n. The null
space of A is the intersection of the linear hyperplanes associated with each nonzero row of
A. This result also holds when A is a zero matrix using the convention that the intersection
of an empty collection of hyperplanes is all of F n.
11.3
Characterizations of Linear Subspaces
We now show that every linear subspace can be described in one of the ways discussed in
the last section.
Theorem Characterizing Linear Subspaces.
Let W be any subspace of an n-
dimensional F-vector space V .
(a) If dim(W) = k, then W is the span of a list of k linearly independent vectors.
(b) If V = F n and dim(W) = n −m, then for some A ∈Mm,n(F), W is the solution set of
the system Ax = 0 of m homogeneous nonzero linear equations (W is the null space of A).
(c) If dim(W) = n −m, then W is an intersection of m linear hyperplanes in V .
284
Advanced Linear Algebra
Proof. Part (a) follows from the fact that the subspace W is itself a vector space, so it has a
basis S of size k (see §1.8 and §17.14). Since S spans W, W consists of all linear combinations
of elements of S. In fact, the linear independence of S ensures that each w ∈W can be
written in exactly one way as c1v1 + · · · + ckvk with ci ∈F and vi ∈S.
To prove (b), let W be an (n −m)-dimensional subspace of F n with ordered basis B =
(vm+1, vm+2, . . . , vn). By adding appropriate vectors to the beginning of the list B, we can
extend B to an ordered basis C = (v1, . . . , vm, vm+1, . . . , vn) for F n. Define T : F n →F m
by setting
T(c1v1 + · · · + cmvm + cm+1vm+1 + · · · + cnvn) =


c1
...
cm


for all c1, . . . , cn ∈F.
Let A ∈Mm,n(F) be the matrix of T with respect to the standard ordered bases of F n and
F m, so T(x) = Ax for all x ∈F n (see Chapter 6). On one hand, the kernel of T consists
precisely of the linear combinations of vectors in B, which is the given subspace W. On the
other hand, since T(x) = Ax, we see that the kernel of T is exactly the null space of A,
which is the solution set of the system of equations Ax = 0. If some row i of A were zero,
then the ith entry of Ax would be zero for every x. But the range of A equals the image
of T, which is all of F m. So none of the rows of A can be zero. We have now expressed
W as the solution set of m homogeneous nonzero linear equations. As seen in the previous
section, it follows that W is the intersection of the m linear hyperplanes determined by the
m rows of A.
The previous paragraph establishes the special case of (c) where W is a subspace of F n.
For a general n-dimensional vector space V with (n−m)-dimensional subspace W, we know
there is a vector space isomorphism g : V →F n. (For example, as seen in Chapter 6, we
can obtain g by selecting an ordered basis of V and mapping each v ∈V to the coordinates
of v relative to this basis.) Applying the isomorphism g to W gives a subspace W ′ = g[W],
which is an (n −m)-dimensional subspace of F n. So we can find m linear hyperplanes
H′
1, . . . , H′
m in F n with W ′ = H′
1 ∩· · · ∩H′
m. Let Hi = g−1[H′
i] for 1 ≤i ≤m, which is an
(n −1)-dimensional subspace (linear hyperplane) in W since g−1 is an isomorphism. Then
W = g−1[W ′] = g−1[H′
1 ∩· · · ∩H′
m] = g−1[H′
1] ∩· · · ∩g−1[H′
m] = H1 ∩· · · ∩Hm,
so W is an intersection of m linear hyperplanes in V .
11.4
Affine Combinations and Affine Sets
Let V be an n-dimensional vector space over a field F. We have seen that the linear subspaces
of V are the subsets of V closed under linear combinations. Given v1, . . . , vk ∈V , define
an affine combination of v1, . . . , vk to be a linear combination c1v1 + · · · + ckvk where each
ci ∈F and c1 + c2 + · · · + ck = 1F . An affine set in V is a subset W of V that is closed
under affine combinations. In other words, W ⊆V is affine iff for all positive integers k, all
w1, . . . , wk ∈W, and all c1, . . . , ck ∈F, if c1 + · · · + ck = 1, then c1w1 + · · · + ckwk ∈W.
For the field F = R, it can be shown that a set W is affine iff for all w, z ∈W and all c ∈R,
cw+(1−c)z is in W. Thus, in real vector spaces, it suffices to check the affine combinations
with k = 2 terms. But this fact does not hold in every field; see Exercise 38.
Affine Geometry and Convexity
285
P
0
0
v
w
u
v
w
L
w−v
u+s(v−u)+t(w−u)
v+t(w−v)
v−u
w−u
FIGURE 11.1
Vector description of lines and planes in R3.
To motivate the definition of affine combinations, we recall the vector equations of lines
and planes in R3 that do not necessarily pass through the origin. Consider the line L in R3
through the distinct points v, w ∈R3. Geometrically, we can reach each point on this line
by starting at the origin, traveling to the tip of the vector v (viewed as an arrow starting
at 0), then following some scalar multiple t(w −v) of the direction vector w −v for the line
(where t ∈R). See Figure 11.1. Algebraically, the line consists of all vectors v + t(w −v) as
t ranges over R. Restating this,
L = {tw + (1 −t)v : t ∈R} = {tw + sv : t, s ∈R and t + s = 1}.
In other words, L consists of all affine combinations of v and w. You can check that L is an
affine set in R3 (this is a special case of a general result proved in §11.6).
Similarly, consider the plane P in R3 through three non-collinear points u, v, w ∈R3.
As in the case of the line, we reach arbitrary points in P from the origin by going to the tip
of u, then traveling within the plane some amount in the direction v −u and some other
amount in the direction w −u; see Figure 11.1. Algebraically,
P = {u + s(v −u) + t(w −u) : s, t ∈R} = {(1 −s −t)u + sv + tw : s, t ∈R}
= {ru + sv + tw : r, s, t ∈R, r + s + t = 1}.
So P is the set of affine combinations of u, v, w. You may check that P is an affine set in
R3.
A plane in R3 can also be specified as the solution set of one linear (possibly non-
homogeneous) equation in three unknowns. A line in R3 is the solution set of two linear
equations in three unknowns. More generally, you may check that for any matrix A ∈
Mm,n(F) and b ∈F m, the set of all solutions x ∈F n to the non-homogeneous system of
linear equations Ax = b is an affine subset of F n. We shall see later (§11.9) that all affine
subsets of F n have this form.
286
Advanced Linear Algebra
11.5
Affine Sets and Linear Subspaces
We can use linear subspaces of V to build more examples of affine sets in V . Let W be a fixed
linear subspace of V . Since W is closed under all linear combinations of its elements, which
include affine combinations as a special case, W is an affine subset of V . More generally,
for any subset S of V and any vector u ∈V , define the translate of S by u to be the
set u + S = {u + x : x ∈S}. We now prove that each translate u + W is an affine set.
Fix v1, . . . , vk ∈u + W and c1, . . . , ck ∈F with c1 + · · · + ck = 1. We must check that
c1v1 + · · · + ckvk ∈u + W. Write each vi as u + wi for some wi ∈W. Then
k
X
i=1
civi =
k
X
i=1
ci(u + wi) =
k
X
i=1
ciu +
k
X
i=1
ciwi = u
k
X
i=1
ci +
k
X
i=1
ciwi.
We know Pk
i=1 ci = 1 and Pk
i=1 ciwi ∈W (because W is a linear subspace), so our
calculation shows that Pk
i=1 civi is in u + W, as needed. Exactly the same calculation
proves that any translate u + X of an affine set X is also an affine set.
Taking W to be the subspace V , we see that the whole space V is an affine set. Taking
W to be the subspace {0}, we see that every one-point set u + W = {u} is an affine set.
By definition, an affine line is a translate u + W where dim(W) = 1; an affine plane is a
translate u + W where dim(W) = 2; and an affine hyperplane is a translate u + W where
dim(W) = n −1 (meaning that W is a linear hyperplane).
We next prove that for all X ⊆V , X is a linear subspace iff X is an affine set and
0V ∈X. We already know the forward direction is true. Conversely, suppose X is an affine
set containing 0V . Given v, w ∈X, v + w is the affine combination −1 · 0V + 1v + 1w
of elements of X, so v + w ∈X. Given v ∈X and c ∈F, cv is the affine combination
(1 −c) · 0V + cv of elements of X, so cv ∈X. We also know 0V ∈X, so X is a linear
subspace.
Theorem on Affine Sets and Linear Subspaces. For every nonempty affine set X in
V , there exists a unique linear subspace W in V such that X = u + W for some u ∈V .
The vector u such that X = u + W is not unique if |X| > 1. In fact, the proof will show
that X = u+W and W = −u+X for any choice of u ∈X. We call W the direction subspace
of X. This theorem provides one geometric characterization of the nonempty affine sets in
V : they are precisely the translates of the linear subspaces of V .
Proof. Fix a nonempty affine set X in V , and fix u ∈X. Define W = −u + X. On one
hand, W is an affine set, being a translate of the affine set X. On the other hand, u ∈X
implies 0 = −u + u ∈W. By the result preceding the theorem, W is a linear subspace, and
evidently u + W = u + (−u + X) = X. We now prove uniqueness of the linear subspace
W. Say X = u + W = v + W ′ for some u, v ∈V and linear subspaces W and W ′. Since
0 ∈W and 0 ∈W ′, u and v belong to X. Then v = u + w for some w ∈W, hence
W ′ = −v + X = (−v + u) + W = −w + W = W. (The last step uses the fact that
x + W = W for all x in a subspace W.)
Since the direction subspace of an affine set X is uniquely determined by X, we can
define the affine dimension of X (written dim(X)) to be the vector-space dimension of its
direction subspace. For instance, points, affine lines, affine planes, and affine hyperplanes
have respective affine dimensions 0, 1, 2, and n −1. The affine dimension of ∅is undefined.
Affine Geometry and Convexity
287
11.6
The Affine Span of a Set
Let S = {v1, . . . , vk} be a finite subset of V . We know that the smallest linear subspace of
V containing S is the linear span Sp(S) consisting of all linear combinations of v1, . . . , vk.
By analogy, define the affine span or affine hull of S, denoted aff(S), to be the set of all
affine combinations of elements of S. In symbols,
aff({v1, . . . , vk}) =
(
c1v1 + · · · + ckvk : ci ∈F,
k
X
i=1
ci = 1
)
.
Similarly, for an infinite subset S of V , let aff(S) be the set of all affine combinations of
finitely many vectors belonging to S. For example, in V = R3, our discussion of Figure 11.1
shows that aff({v, w}) is the line through v and w, and aff({u, v, w}) is the plane through
u, v, and w. The next theorem shows that aff(S) is the smallest affine set containing S.
Theorem on Affine Spans. For all S ⊆V , aff(S) is an affine set containing S. If S ⊆T
and T is an affine set, then aff(S) ⊆T. The set aff(S) of all affine combinations of vectors
in S equals the intersection of all affine sets in V containing S.
Proof. To check that aff(S) is affine, fix w1, . . . , ws ∈aff(S) and c1, . . . , cs ∈F with
Ps
j=1 cj = 1. We show w = Ps
j=1 cjwj belongs to aff(S). There are finitely many
elements v1, . . . , vk ∈S such that each wj is some affine combination of these elements,
say wj = Pk
i=1 aijvi with aij ∈F and Pk
i=1 aij = 1 for 1 ≤j ≤s. Compute
w =
s
X
j=1
cjwj =
s
X
j=1
cj
 k
X
i=1
aijvi
!
=
s
X
j=1
k
X
i=1
cjaijvi =
k
X
i=1


s
X
j=1
cjaij

vi.
So w is a linear combination of the vi, and the coefficients in this combination have sum
k
X
i=1
s
X
j=1
cjaij =
s
X
j=1
cj
k
X
i=1
aij =
s
X
j=1
cj · 1 = 1.
Hence w is an affine combination of v1, . . . , vk, which proves that w ∈aff(S). For each
x ∈S, x = 1x is an affine combination of elements of S, so S ⊆aff(S). Finally, suppose
T is any affine subset of V with S ⊆T. Since T is closed under affine combinations of any
of its elements, affine combinations of elements taken from the subset S must be in T. So
aff(S) ⊆T. This inclusion holds for all affine sets T containing S, so aff(S) is a subset of
the intersection of all such sets. On the other hand, aff(S) is an affine set containing S, so
aff(S) is one of the sets T involved in this intersection. Thus, the intersection is a subset of
aff(S).
11.7
Affine Independence
In three-dimensional geometry, any two distinct points determine a line, and any three
non-collinear points determine a plane. We would like similar descriptions of general affine
sets A of the form A = aff(S), where the affine spanning set S is as small as possible. The
288
Advanced Linear Algebra
examples of lines and planes indicate that the size of S exceeds the affine dimension of A
by 1. To make this precise, we introduce the notion of affinely independent sets.
Recall that a list of vectors L = (v1, . . . , vk) in V is linearly independent iff the only
linear combination of the vi that gives 0V is the combination with all zero coefficients. In
other words, L is linearly independent iff for all c1, . . . , ck ∈F, if c1v1+· · ·+ckvk = 0V , then
ci = 0F for 1 ≤i ≤k. We now define L to be affinely independent iff for all c1, . . . , ck ∈F
such that c1 + c2 + · · · + ck = 0F , c1v1 + · · · + ckvk = 0V implies ci = 0F for 1 ≤i ≤k. The
list L is affinely dependent iff there exist c1, . . . , ck ∈F with some ci ̸= 0, Pk
j=1 cj = 0, and
Pk
j=1 cjvj = 0. A set S of vectors (finite or not) is affinely independent iff every finite list of
distinct vectors in S is affinely independent. The following result relates affine independence
to linear independence.
Theorem Relating Affine Independence and Linear Independence. The list
L = (v0, v1, v2, . . . , vk) is affinely independent iff the list L′ = (v1 −v0, v2 −v0, . . . , vk −v0)
is linearly independent.
Proof. We prove the contrapositive in both directions. Assuming L is affinely dependent,
we have scalars c0, . . . , ck ∈F summing to zero and not all zero, such that Pk
i=0 civi = 0.
We can rewrite the given combination of the vi as
c1(v1 −v0) + c2(v2 −v0) + · · · + ck(vk −v0) + (c0 + c1 + c2 + · · · + ck)v0 = 0.
But the sum of all the ci is zero, so we have expressed zero as a linear combination of
the vectors in L′ with coefficients c1, . . . , ck. These coefficients cannot all be zero, since
otherwise Pk
i=0 ci = 0 would give c0 = 0 as well. This proves the linear dependence of L′.
Conversely, assuming L′ is linearly dependent, we have scalars d1, . . . , dk ∈F (not all zero)
with d1(v1 −v0) + · · · + dk(vk −v0) = 0. Defining d0 = −d1 −d2 −· · · −dk, we then have
d0v0 + d1v1 + · · · + dkvk = 0 where not all di are zero, but the sum of all di is zero. So L is
affinely dependent.
When using this result to detect affine independence of a finite set of vectors, we
can list the elements of the set in any order. So, given a finite set S ⊆V , we can test
the affine independence of S by picking any convenient x ∈S and checking if the set
T = {y −x : y ∈S, y ̸= x} is linearly independent. For example, if S is any set of linearly
independent vectors in V , then S ∪{0V } is affinely independent.
11.8
Affine Bases and Barycentric Coordinates
Let X be a nonempty affine subset of V . A list L = (v0, . . . , vk) is an ordered affine basis
of X iff L is affinely independent and X = aff({v0, . . . , vk}). Similarly, a set S is an affine
basis of X iff S is affinely independent and X = aff(S).
We can use the direction subspace of X to find an affine basis for X. Specifically, write
the given affine set X as X = u + W, where u ∈X and W is a linear subspace of V .
Suppose dim(W) = k, and let (w1, . . . , wk) be an ordered vector-space basis of W. We
claim L = (u, u + w1, . . . , u + wk) is an ordered affine basis of X. Using the criterion
from §11.7, we see that L is affinely independent, since subtracting u from every other
vector on the list produces the linearly independent list (w1, . . . , wk). Next, u ∈X and each
u + wi ∈X, so the set S = {u, u + w1, . . . , u + wk} is contained in the affine set X. Hence
Affine Geometry and Convexity
289
aff(S) ⊆X. To prove the reverse inclusion, fix any z ∈X. Then z −u ∈W, so we can write
z −u = c1w1 + · · · + ckwk for some ci ∈F. Hence, z itself can be written
z = u +
k
X
i=1
ciwi =
 
1 −
k
X
i=1
ci
!
u +
k
X
i=1
ci(u + wi),
which is an affine combination of the elements of L. We now see that X, which has affine
dimension k = dim(W), has an ordered affine basis consisting of k + 1 vectors.
By a similar argument, we now show that any affine basis of X must have size k +1. We
can rule out the possibility of infinite affine bases using the fact that dim(V ) = n is finite.
Now, let L = (y0, y1, . . . , ys) be any ordered affine basis of X; we show s = k (so the list L
has size k +1). We know L′ = (y1 −y0, . . . , ys −y0) is a linearly independent list. Moreover,
X = y0 + W, so each vector in L′ belongs to W. Let us check that W is the linear span of
L′. Given w ∈W, we have y0 + w ∈X, so that y0 + w = c0y0 + c1y1 + · · · + csys for some
c0, c1, . . . , cs ∈F such that c0 + c1 + · · · + cs = 1. Solving for w gives
w = (−1 + c0 + c1 + · · · + cs)y0 + c1(y1 −y0) + · · · + cs(ys −y0).
The coefficient of y0 is zero, so w is a linear combination of the vectors in L′. Thus, L′ is
an ordered basis of the k-dimensional space W, forcing s = k.
These proofs illustrate the technique of establishing facts about affine concepts involving
X by looking at the corresponding linear concepts involving the direction subspace of X.
As further examples of this method, you can prove that any affinely independent subset
of X can be extended to an affine basis of X, and any affine spanning set for X contains
a subset that is an affine basis of X (Exercise 28). Furthermore, the maximum size of an
affinely independent set in an n-dimensional space V is n + 1.
Now let X be an affine set with ordered affine basis L = (v0, v1, . . . , vk). We claim that for
each z ∈X, there exist unique scalars c0, c1, . . . , ck ∈F with Pk
i=0 ci = 1 and Pk
i=0 civi = z.
These scalars are called the barycentric coordinates of z relative to L. Existence of the ci
follows since X = aff({v0, . . . , vk}). To see that the ci are unique, suppose we also had
d0, d1, . . . , dk ∈F with Pk
i=0 di = 1 and Pk
i=0 divi = z. Subtracting the two expressions for
z gives Pk
i=0(ci −di)vi = 0, where the sum of the coefficients ci −di is 1 −1 = 0. By affine
independence of L, ci −di = 0 for all i, so ci = di for all i.
Let us work out an example of barycentric coordinates for an affine plane in R3. Let P be
the set of points (x, y, z) ∈R3 satisfying x+2y −3z = 5. By choosing values for y and z and
calculating x, we obtain the three vectors u = (5, 0, 0), v = (3, 1, 0), and w = (2, 0, −1) in
P. You can check that (u, v, w) is an ordered affine basis for P. Consider a point (−8, 2, −3)
in P. We can express this point as the affine combination
(−8, 2, −3) = −4(5, 0, 0) + 2(3, 1, 0) + 3(2, 0, −1) = −4u + 2v + 3w.
So, the point in P with Cartesian coordinates (−8, 2, −3) has barycentric coordinates
(−4, 2, 3) relative to the ordered affine basis (u, v, w).
11.9
Characterizations of Affine Sets
We defined affine sets to be those subsets of a vector space V that are closed under affine
combinations. As in the case of linear subspaces, we can give several other characterizations
of which nonempty subsets of V are affine.
290
Advanced Linear Algebra
Theorem Characterizing Affine Sets.
Let X be a nonempty affine subset of an n-
dimensional vector space V .
(a) X is a translate of a unique linear subspace of V (the direction subspace of X).
(b) X is the affine span of an ordered list of k affinely independent vectors, for some k
between 1 and n + 1.
(c) If V = F n and dim(X) = n −m, then X is the solution set of a system of m (possibly
non-homogeneous) nonzero linear equations Ax = b for some A ∈Mm,n(F) and b ∈F m.
(d) If dim(X) = n −m, then X is an intersection of m affine hyperplanes in V .
Proof. We proved part (a) in §11.5 and part (b) in §11.8. For (c), let X be an (n −m)-
dimensional affine set in F n. Using (a), write X = u+W for some (n−m)-dimensional linear
subspace of F n and some u ∈F n. By (b) of §11.3, there is a matrix A ∈Mm,n(F) such that
W = {w ∈F n : Aw = 0}. We claim that for b = Au, we have X = {x ∈F n : Ax = b}. On
one hand, if x ∈X, then x = u+w for some w ∈W, so Ax = A(u+w) = Au+Aw = b+0 = b.
On the other hand, if x ∈F n satisfies Ax = b, then A(x −u) = Ax −Au = b −b = 0, so
x −u ∈W and x ∈u + W = X. We prove (d) similarly: write X = u + W as in (c), and
invoke (c) of §11.3 to find linear hyperplanes H1, . . . , Hm in V with W = H1 ∩· · · ∩Hm.
Each translate u + Hi is an affine hyperplane in V , and you may check that
X = u + W = u + (H1 ∩· · · ∩Hm) = (u + H1) ∩· · · ∩(u + Hm).
11.10
Affine Maps
Let V and W be two vector spaces over the field F. A linear map or linear transformation is
a function T : V →W such that T(x+y) = T(x)+T(y) and T(cx) = cT(x) for all x, y ∈V
and all c ∈F. From this definition and induction, we see that a linear map T satisfies
T(c1x1 + · · · + ckxk) = c1T(x1) + · · · + ckT(xk) for all k ∈Z≥0, all ci ∈F, and all xi ∈V .
In other words, linear maps preserve linear combinations. By analogy, we define a map
U : V →W to be an affine map or affine transformation iff U preserves affine combinations.
In other words, U is an affine map iff U(c1x1 + · · · + ckxk) = c1U(x1) + · · · + ckU(xk) for
all k ∈Z≥0, all xi ∈V , and all ci ∈F satisfying c1 + · · · + ck = 1. When F = R, it suffices
to check the condition for k = 2; see Exercise 51. More generally, we allow the domain (or
codomain) of an affine map to be an affine subset of V (or W). An affine isomorphism is a
bijective affine map.
Every linear map is an affine map, but the converse is not true. For example, given a
fixed u ∈V , the translation map Tu : V →V defined by Tu(v) = u + v for v ∈V is an
affine map. To check this, fix k ≥0, xi ∈V , and ci ∈F with Pk
i=1 ci = 1, and compute
Tu(c1x1 + · · · + ckxk) = u + c1x1 + · · · + ckxk = (c1 + · · · + ck)u + c1x1 + · · · + ckxk
= c1(u + x1) + · · · + ck(u + xk) = c1Tu(x1) + · · · + ckTu(xk).
However, since linear maps must send 0V to 0V , Tu is a linear map only when u = 0. More
generally, you can check that for A ∈Mm,n(F) and b ∈F m, the map U : F n →F m given
by U(x) = Ax + b for x ∈F n is always affine, but is linear only when b = 0.
Affine Geometry and Convexity
291
In fact, a map U : V →W is a linear map iff U is an affine map and U(0V ) = 0W . The
forward implication is immediate from the definitions. Conversely, assume U is an affine
map with U(0) = 0. Given x, y ∈V and c ∈F, compute:
U(x + y) = U(−1 · 0 + 1x + 1y) = −1U(0) + 1U(x) + 1U(y) = U(x) + U(y);
U(cx) = U((1 −c) · 0 + cx) = (1 −c)U(0) + cU(x) = cU(x).
So U is linear.
The following facts are routinely verified: the identity map on any affine set is an
affine isomorphism; the composition of two affine maps (resp. affine isomorphisms) is an
affine map (resp. affine isomorphism); and the inverse of an affine isomorphism is also
an affine isomorphism. For example, every translation map Tu is an affine isomorphism
with inverse T −1
u
= T−u, which is also affine. You can also check that the direct image or
preimage of an affine set under an affine map is an affine set.
We know that every linear map T : F n →F m has the form T(x) = Ax for a unique
matrix A ∈Mm,n(F) (namely, A is the matrix whose jth column is T(ej) for 1 ≤j ≤n).
We use this to deduce a corresponding fact for affine maps.
Theorem on Affine Maps.
(a) Every affine map U : F n →F m has the form U(x) = Ax + b for a unique A ∈Mm,n(F)
and a unique b ∈F m.
(b) Let V and W be finite-dimensional vector spaces. Every affine map U : V →W has
the form U = Tb ◦S for a uniquely determined linear map S : V →W and a uniquely
determined b ∈W (here, Tb is translation by b on W).
Proof. To prove (a), let U : F n →F m be an affine map. Choose b = U(0) ∈F m, and
consider the map T −1
b
◦U : F n →F m, where T −1
b
is the inverse of translation by b in the
space F m. The composition T −1
b
◦U is an affine map sending zero to zero, so it is a linear
map from F n to F m. Thus, there is a matrix A with T −1
b
◦U(x) = Ax for all x ∈F n.
Solving for U, we see that U(x) = Tb(Ax) = Ax + b for all x ∈F n. Now, b is uniquely
determined by U since U(x) = Ax + b forces b = A0 + b = U(0) no matter what A is.
Knowing this, A is uniquely determined because it is the unique matrix representing the
linear map T −1
b
◦U relative to the standard bases of F n and F m. Part (b) can be proved
by choosing ordered bases for V and W and invoking part (a).
The next theorem lets us build affine maps by specifying how the map should act on an
affine basis.
Universal Mapping Property (UMP) for Affine Bases.
Let V and W be finite-
dimensional vector spaces. Suppose X ⊆V is an affine set with affine basis S = {v0, . . . , vk}.
For any y0, . . . , yk ∈W, there exists a unique affine map U : X →W with U(vi) = yi for
0 ≤i ≤k.
Proof. We can give an explicit formula for U by noting that each x ∈X has a unique
expression as an affine combination x = Pk
i=0 civi with ci ∈F and Pk
i=0 ci = 1. So if U
exists with the required properties, U must be given by the formula U(x) = Pk
i=0 ciU(vi) =
Pk
i=0 ciyi. A routine calculation confirms that this formula really does define an affine map
sending each vi to yi. Thus, U does exist and is unique.
You can also prove the UMP by reducing to the direction subspace of X and appealing
to the universal mapping property for the linear basis {v1 −v0, . . . , vk −v0} of this subspace
(Exercise 50).
292
Advanced Linear Algebra
11.11
Convex Sets
For our study of convexity in the rest of this chapter, we shall consider only real vector
spaces, taking V to be Rn. To motivate the definition of a convex set, consider the line
L shown in Figure 11.1. The entire line L through v and w is obtained by taking affine
combinations (1 −t)v + tw as t ranges over the entire real line R. In contrast, if we only
wanted to get the points of L on the line segment joining v to w, we would only take the
affine combinations (1 −t)v + tw = v + t(w −v) with t ranging through real numbers in the
closed interval I = [0, 1]. (Taking t = 0 gives v, taking t = 1 gives w, and intermediate t’s
give precisely the points in the interior of the line segment.) In general, for any v, w ∈Rn,
we define the line segment joining v and w to be the set {(1 −t)v + tw : t ∈R, 0 ≤t ≤1}.
A subset C of Rn is convex iff for all v, w ∈C and all t ∈R with 0 ≤t ≤1, (1−t)v +tw
belongs to C. Geometrically, a set C is convex iff the line segment joining any two points
of C is always contained in C. All affine sets are convex, but the converse is not true. For
example, any finite closed interval [a, b] in R1 is readily seen to be convex but not affine.
Given finitely many points v1, . . . , vk ∈Rn, a convex combination of these points is
a linear combination c1v1 + · · · + ckvk with c1 + · · · + ck = 1 and ci ≥0 for all i (and
hence ci ∈[0, 1] for all i). We now show that a convex set C ⊆Rn is closed under convex
combinations: for all positive integers k and all v1, . . . , vk ∈C, every convex combination of
v1, . . . , vk belongs to C. The proof uses induction on k. When k = 1, we must have c1 = 1.
Given that v1 ∈C, we certainly have c1v1 = 1v1 = v1 ∈C. If k = 2, then we must have
c1 = 1 −c2, so c1v1 + c2v2 ∈C by definition of a convex set. For the induction step, fix
k ≥3, and assume that convex combinations of k−1 or fewer points of C are already known
to be in C. Fix v1, . . . , vk ∈C and c1, . . . , ck ∈[0, 1] with Pk
i=1 ci = 1. If c1 = 1, then the
conditions on the ci force c2 = · · · = ck = 0, so that c1v1 + · · · + ckvk = v1 ∈C. Otherwise,
let di = ci/(1 −c1) for 2 ≤i ≤k, and note that
c1v1 + c2v2 + · · · + ckvk = c1v1 + (1 −c1)(d2v2 + · · · + dkvk).
Since c2, . . . , ck are nonnegative real numbers with sum 1 −c1, it follows that d2, . . . , dk
are nonnegative real numbers with sum 1. Then w = d2v2 + · · · + dkvk is a convex
combination of k −1 elements of C, which is in C by the induction hypothesis. So
Pk
i=1 civi = (1 −c1)w + c1v1 ∈C by definition of a convex set. This completes the induction
step.
11.12
Convex Hulls
Given S ⊆Rn, we have studied the linear span of S, which consists of all linear combinations
of elements of S, and the affine span of S, which consists of all affine combinations of
elements of S. By analogy, we define the convex span or convex hull of S ⊆Rn, denoted
conv(S), to be the set of all convex combinations of finitely many elements of S.
Theorem on Convex Hulls. For all S ⊆Rn, conv(S) is a convex set containing S. If
S ⊆T and T is a convex set, then conv(S) ⊆T. The set conv(S) of all convex combinations
of vectors in S equals the intersection of all convex sets in Rn containing S.
Proof. To check the convexity of conv(S), fix x, y ∈conv(S) and t ∈[0, 1]; we must check
that z = tx + (1 −t)y ∈conv(S). By definition, there are v1, . . . , vk ∈S, c1, . . . , ck ∈[0, 1]
Affine Geometry and Convexity
293
with sum 1, and d1, . . . , dk ∈[0, 1] with sum 1, such that x = Pk
i=1 civi and y = Pk
i=1 divi.
(By taking some ci and di to be zero, we can assume that the same elements v1, . . . , vk of
S are used in the expressions for x and y.) We compute
z = tx + (1 −t)y =
k
X
i=1
(tci + (1 −t)di)vi.
Each of ci, di, t, 1 −t is a nonnegative real number, so each coefficient tci + (1 −t)di is a
nonnegative real number. The sum of these coefficients is
k
X
i=1
(tci + (1 −t)di) = t
k
X
i=1
ci + (1 −t)
k
X
i=1
di = t · 1 + (1 −t) · 1 = 1.
So z is a convex combination of v1, . . . , vk ∈S, hence z ∈conv(S) as required. Since any
v ∈S is the convex combination v = 1v, we have S ⊆conv(S). If T is any convex subset
of Rn containing S, then conv(S) ⊆T since T is closed under convex combinations of its
elements. It follows, as in the proof of the Theorem on Affine Spans, that conv(S) is the
intersection of all convex sets T in Rn containing S.
The convex hulls of some finite sets of points are illustrated in Figure 11.2. In general, the
convex hull of a one-point set {v} is {v}; the convex hull of {v, w} is the line segment joining
v and w; the convex hull of three non-collinear points {v, w, x} is the triangle (including
interior) with vertices v, w, x; and the convex hull of four non-coplanar points {v, w, x, y}
is the tetrahedron (including interior) with vertices v, w, x, y. For k ≥0, we define a k-
dimensional simplex to be a set conv({v0, v1, . . . , vk}), where {v0, v1, . . . , vk} is an affinely
independent subset of Rn. Geometrically, simplices are k-dimensional analogs of triangles
and tetrahedra. A specific example of a k-dimensional simplex is ∆k = conv({0, e1, . . . , ek}),
where the ei are standard basis vectors in Rk. Expanding the definition, we see that ∆k is
the set of vectors (c1, . . . , ck) ∈Rk with ci ≥0 for all i and Pk
i=1 ci ≤1. This sum can be
less than 1, since the scalar c0 disappears in the convex combination c00+c1e1 +· · ·+ckek.
11.13
Carath´eodory’s Theorem on Convex Hulls
Consider the six-point set S = {u, v, w, x, y, z} ⊆R2 shown in the lower-right part of
Figure 11.2. The convex hull conv(S) consists of all convex combinations of the six points
of S. If we tried to create this convex hull by drawing line segments between every pair
of points in S, the resulting set would not be convex (it would consist of the sides and
diagonals of the shaded hexagonal region). On the other hand, suppose we took all convex
combinations of three-element subsets of S. For each fixed subset of size 3, the set of convex
combinations using these three elements is a triangle with interior. We see from the figure
that the union of these triangles is all of conv(S). Thus, to obtain the convex hull in this
example, it suffices to use only convex combinations of three or fewer points from S. The
next theorem generalizes this remark to convex hulls in higher dimensions.
Carath´eodory’s Theorem on Convex Hulls. For any subset S of Rn, conv(S) consists
of all convex combinations of n + 1 or fewer points of S.
Proof. Fix S ⊆Rn, and pick any point w ∈conv(S). By definition, we know there exist
an integer m ≥0, points v0, v1, . . . , vm ∈S, and nonnegative real scalars c0, c1, . . . , cm such
294
Advanced Linear Algebra
six coplanar points
v
w
v
w
x
v
w
x
y
u
v
w
x
y
z
four non−coplanar points
FIGURE 11.2
Examples of convex hulls.
that c0 + c1 + · · · + cm = 1 and w = c0v0 + c1v1 + · · · + cmvm. If m ≤n, the claimed result
holds for the point w. So assume m > n. We show how to find another expression for w as a
convex combination of fewer than m + 1 points of S. Continuing this reduction process for
at most m −n steps, we eventually arrive at an expression for w as a convex combination
of at most n + 1 points of S.
We can assume all vi are distinct, since otherwise we can reduce m by using
cv + dv = (c + d)v to group together two terms in the convex combination. We can assume
all ci > 0, since otherwise we can reduce m by deleting the term civi = 0 from the sum.
Since m + 1 > n + 1, the set {v0, . . . , vm} in Rn must be affinely dependent. So there exist
d0, . . . , dm ∈R with some di ̸= 0, d0 +· · ·+dm = 0, and d0v0 +· · ·+dmvm = 0. At least one
di is strictly positive, and we can reorder terms to ensure that dm > 0 and cm/dm ≤ci/di
for all i < m such that di > 0. Define bi = ci −(cm/dm)di for 0 ≤i < m. We claim that
each bi ≥0. If di > 0, the claim follows by multiplying cm/dm ≤ci/di by the positive
quantity di and rearranging. If di ≤0, the claim follows since ci > 0, cm/dm > 0, hence
−(cm/dm)di ≥0 and bi ≥0. Next, b0 + b1 + · · · + bm−1 = 1, because
m−1
X
i=0
bi =
m−1
X
i=0
ci −(cm/dm)
m−1
X
i=0
di = (1 −cm) −(cm/dm)(−dm) = 1.
Finally, w = Pm−1
i=0 bivi, because
m−1
X
i=0
bivi =
m−1
X
i=0
civi −(cm/dm)
m−1
X
i=0
divi = (w −cmvm) −(cm/dm)(−dmvm) = w.
We have now expressed w as a convex combination of m points of S.
Affine Geometry and Convexity
295
Exercise 77 uses Carath´eodory’s Theorem to prove that the convex hull of a closed and
bounded (compact) subset of Rn is closed and bounded.
11.14
Hyperplanes and Half-Spaces in Rn
For every affine hyperplane H in Rn, there exist a1, . . . , an, b ∈R such that
H = {(x1, . . . , xn) ∈Rn : a1x1 + · · · + anxn = b};
this follows from part (c) of the Theorem Characterizing Affine Sets. Recall that the dot
product or inner product of two vectors w = (w1, . . . , wn) and x = (x1, . . . , xn) in Rn is
defined by
w • x = ⟨w, x⟩= w1x1 + w2x2 + · · · + wnxn.
Letting a = (a1, . . . , an) ̸= 0, we can write H = {x ∈Rn : a • x = b}. Recall that the
Euclidean length of the vector a is ||a|| = √a • a = (a2
1 + a2
2 + · · · + a2
n)1/2. By multiplying
both sides of the equation a • x = b by the nonzero scalar 1/||a||, we obtain an equivalent
equation u • x = c where u is a unit vector (i.e., ||u|| = 1) and c = ||a||−1b ∈R. By
multiplying by −1 if needed, we can arrange that c ≥0. You can check that when c ̸= 0,
the unit vector u and positive scalar c are uniquely determined by H. If c = 0, then there
are exactly two possible unit vectors u that can be used in the equation defining H.
Given an affine hyperplane H, write H = {x ∈Rn : u • x = c} with ||u|| = 1 and c ≥0.
If c = 0, then H consists of all vectors x ∈Rn perpendicular to the unit vector u. If c > 0
and x0 is any given point on H, then x ∈H iff u • x = u • x0 iff u • (x −x0) = 0 iff x −x0
is perpendicular to the unit vector u. Accordingly, u and its nonzero scalar multiples are
called normal vectors for H.
The two closed half-spaces determined by H are the sets {x ∈Rn : u • x ≥c} and
{x ∈Rn : u • x ≤c}. Geometrically, the first set consists of points of H together with
points outside of H on the same side of H as u (where u is a vector with tail drawn at
some point x0 ∈H), and the second set consists of H together with the points on the
opposite side of H from u. Similarly, the two open half-spaces determined by H are the sets
{x ∈Rn : u • x > c} and {x ∈Rn : u • x < c}. All four half-spaces determined by H,
along with H itself, are convex. For example, suppose x, y ∈S = {z ∈Rn : u • z < c} and
t ∈[0, 1]. We know u • x < c and u • y < c. Since the inner product on Rn is linear in each
variable and 0 ≤t ≤1, we have
u • (tx + (1 −t)y) = t(u • x) + (1 −t)(u • y) < tc + (1 −t)c = c,
so tx + (1 −t)y ∈S. A closed half-space is the solution set of a linear inequality
u1x1 + · · · + unxn ≤c, while an open half-space is the solution set of a strict linear
inequality. You can check that the intersection of any family of convex subsets of Rn is
also convex. Taking each convex set in the family to be a hyperplane or a closed half-space
or an open half-space, we conclude: the solution set of any family (possibly infinite) of
linear equations, inequalities, and strict inequalities in n variables is a convex subset of Rn.
Observe that each linear equation u • x = c appearing in the family could be replaced by
the two linear inequalities u • x ≤c and (−u) • x ≤c.
296
Advanced Linear Algebra
11.15
Closed Convex Sets
We might expect a description of convex sets as intersections of closed half-spaces, by
analogy with our description of affine sets as intersections of affine hyperplanes. However,
not all convex sets can be written in this form. To see why, recall that a subset C of Rn is
closed iff for every sequence of vectors v1, . . . , vk, . . . in C that converge to a vector v ∈Rn
(meaning limk→∞||vk −v|| = 0), the limit vector v belongs to C. In other words, C is
closed (in the topological sense) iff C is closed under taking limits of convergent sequences
of vectors in C. Using this definition, you can check that the intersection of any family of
closed subsets of Rn is also a closed set. Furthermore, every closed half-space is a closed
set (as the name suggests). Combining these facts, we see that any intersection of closed
half-spaces must be a closed and convex set. In this section, we prove the converse: every
closed and convex subset of Rn is the intersection of all closed half-spaces containing it.
To obtain this result, we need a theorem that lets us separate certain subsets of Rn using
an affine hyperplane. Recall that a set D ⊆Rn is bounded iff there exists M ∈R such that
for all x ∈D, ||x|| ≤M.
Hyperplane Separation Theorem. For all disjoint, closed, convex sets C and D in Rn
with C bounded, there is an affine hyperplane H in Rn (with associated open half-spaces
H+ and H−) such that C ⊆H+ and D ⊆H−or vice versa.
Proof. Let C and D be sets satisfying the hypotheses of the theorem. For points x, y ∈Rn,
let d(x, y) = ||x −y|| =
p
(x −y) • (x −y) be the distance between x and y in Rn. For
a point x ∈Rn and a nonempty set B ⊆Rn, let d(x, B) = infy∈B d(x, y) be the distance
between x and B. For two nonempty sets A and B in Rn, let d(A, B) = infx∈A d(x, B) =
infx∈A,y∈B d(x, y) be the distance between A and B. A theorem of real analysis states that
a closed bounded subset C of Rn is sequentially compact, meaning that every sequence of
points of C has a convergent subsequence. Using compactness of C, another theorem states
that every continuous function f : C →R attains its minimum value, meaning that there
is x ∈C with f(x) ≤f(w) for all w ∈C. With these definitions and facts in hand, you
can check that there exist x ∈C and y ∈D with d(x, y) = d(C, D) (see Exercise 70). Set
r = d(x, y) = ||y −x||; since C and D are disjoint, x ̸= y and r > 0.
Let u be the unit vector r−1(y −x) parallel to the line segment from x to y in Rn. Let
c1 = u • x and c2 = u • y. Let Hx = {v ∈Rn : u • v = c1} be the affine hyperplane through
x perpendicular to u and Hy = {v ∈Rn : u • v = c2} be the affine hyperplane through
y perpendicular to u. We cannot have c1 = c2, since otherwise u • (y −x) = c2 −c1 = 0
would give r = r(u • u) = u • (ru) = 0. We consider the case c1 < c2 below; the case c1 > c2
is handled similarly. We claim that C ⊆{v ∈Rn : u • v ≤c1} and that D ⊆{v ∈Rn :
u • v ≥c2}. The claim is illustrated by Figure 11.3, in which C is contained in the closed
half-space weakly left of Hx, and D is contained in the closed half-space weakly right of Hy.
The separation theorem follows from the claim by taking H = {z ∈Rn : u • z = c3} for any
choice of c3 ∈R with c1 < c3 < c2.
We now prove the claim. Fix a point z ∈Rn with u • z = c > c1 = u • x; we show
z ̸∈C. Suppose z ∈C to obtain a contradiction. Since x, z ∈C and C is convex, the entire
line segment joining x and z is contained in C. Figure 11.3 suggests that we can drop an
altitude from y to this line segment to find a point w ∈C with d(w, y) < d(x, y) = d(C, D),
giving a contradiction. To make this pictorial argument precise, we consider the function
g : [0, 1] →R such that g(t) = d(y, x + t(z −x))2 for t ∈[0, 1]. By definition of distance and
Affine Geometry and Convexity
297
D
x
z
w
Hy
C
x
y
H
FIGURE 11.3
Finding a hyperplane to separate C and D.
the linearity of the dot product in each argument, we know
g(t) = [(y −x) −t(z −x)] • [(y −x) −t(z −x)]
= (y −x) • (y −x) −2t(y −x) • (z −x) + t2(z −x) • (z −x).
Recalling r = d(x, y), y −x = ru, and setting s = d(z, x), this becomes
g(t) = r2 −2t((ru) • z −(ru) • x) + s2t2 = s2t2 −2r(c −c1)t + r2.
The graph of g in R2 is a parabola with slope g′(0) = −2r(c −c1) < 0 at t = 0. So, for a
sufficiently small positive t ∈[0, 1], w = x + t(z −x) satisfies d(w, y) =
p
g(t) <
p
g(0) =
d(y, x). This contradiction shows z ̸∈C. A similar argument proves that no point z ∈Rn
with u • z < c2 can belong to D.
Theorem on Closed Convex Sets and Half-Spaces. Every closed convex set D in Rn
is the intersection of all closed half-spaces S with D ⊆S. D is also the intersection of all
open half-spaces containing D.
Proof. Let D ⊆Rn be closed and convex. Let E be the intersection of all closed half-spaces
containing D. The inclusion D ⊆E is evident from the definition of E. For the reverse
inclusion, suppose x ∈Rn and x ̸∈D. Apply the Hyperplane Separation Theorem to the
one-point set C = {x} and the closed convex set D; note that C is closed, convex, bounded,
and disjoint from D. We obtain a hyperplane H with C ⊆H+ (one of the open half-spaces
for H) and D ⊆H−(the other open half-space for H). Then S = H−∪H is a closed
half-space that contains D but not x. This is one of the half-spaces we intersect to obtain
E, so x ̸∈E as needed. The proof for open half-spaces is similar, taking S = H−this
time.
298
Advanced Linear Algebra
11.16
Cones and Convex Cones
We would like to understand the structure of subsets of Rn formed by intersecting a finite
number of closed half-spaces. Our ultimate goal is to prove that a subset S of Rn is the
convex hull of a finite set of points iff S is bounded and S = H1 ∩· · · ∩Hk for finitely many
closed half-spaces Hi. This theorem is geometrically very plausible for subsets of R2 and R3
(cf. Figures 11.2 and 11.3), but it is quite tricky to prove in general dimensions. To prove
this result, and to describe unbounded intersections of finitely many closed half-spaces, we
must first develop some machinery involving cones.
A subset C of Rn is a cone iff 0 ∈C and for all x ∈C and all b > 0 in R, bx ∈C. In
other words, cones are sets containing 0 that are closed under multiplication by positive real
scalars. Equivalently, cones are nonempty sets closed under multiplication by nonnegative
scalars. We claim a cone C is convex iff C is closed under addition. On one hand, suppose C
is a convex cone and x, y ∈C. Then z = (1/2)x+(1/2)y ∈C by convexity, so x+y = 2z ∈C
because C is a cone, so C is closed under addition. On the other hand, suppose a cone C is
closed under addition. For x, y ∈C and t ∈[0, 1], we know tx ∈C and (1 −t)y ∈C, since
t ≥0 and 1 −t ≥0. Then tx + (1 −t)y ∈C, so that C is convex.
A positive combination (or conical combination) of vectors v1, . . . , vk
∈Rn is a
linear combination c1v1 + · · · + ckvk, where each ci is in R≥0. By the remarks in the
previous paragraph and induction on k, we see that convex cones are closed under
positive combinations of their elements. Conversely, any subset of Rn closed under positive
combinations is a convex cone (taking k = 0 shows that such a subset must contain zero).
Given S ⊆Rn, the convex cone generated by S is the set cone(S) consisting of all
positive combinations of vectors v1, . . . , vk ∈S. Let us show that cone(S) is a convex cone
containing S. Taking k = 0, we see that 0 ∈cone(S). Fix x, y ∈cone(S) and b > 0
in R. We can write x = c1v1 + · · · + ckvk and y = d1v1 + · · · + dkvk for some real scalars
ci, di ≥0 and some v1, . . . , vk ∈S. Then bx = Pk
i=1(bci)vi ∈cone(S) since each bci ≥0, and
x + y = Pk
i=1(ci + di)vi ∈cone(S) since each ci + di ≥0. So cone(S) is a convex cone. This
cone contains S since each v ∈S is the positive combination v = 1v. Moreover, if T is any
convex cone containing S, then cone(S) ⊆T since T is closed under positive combinations
of its elements. We conclude that cone(S) can also be described as the intersection of all
convex cones in Rn containing S.
All linear subspaces W of Rn are convex cones, as they are closed under all linear
combinations of their elements (hence are closed under positive combinations). Define a
linear half-space of Rn to be any closed half-space determined by a linear hyperplane in
Rn. Explicitly, linear half-spaces are sets of the form {x ∈Rn : u • x ≤0} for some nonzero
u ∈Rn. You can check that linear half-spaces are convex cones. Moreover, any intersection
of convex cones is also a convex cone. In particular, intersections of linear half-spaces are
convex cones.
By imposing finiteness conditions on the constructions in the two preceding paragraphs,
we obtain two special classes of convex cones. First, a V-cone is a convex cone of the form
cone(S), where S is a finite subset of Rn. Explicitly, C ⊆Rn is a V-cone iff there exist a
positive integer k and v1, . . . , vk ∈Rn such that
C = {c1v1 + · · · + ckvk : ci ∈R, ci ≥0 for 1 ≤i ≤k}.
We call v1, . . . , vk generators of the cone C. Second, an H-cone is an intersection of finitely
many linear half-spaces in Rn. Explicitly, D ⊆Rn is an H-cone iff there exist a positive
integer k and u1, . . . , uk ∈Rn such that
D = {x ∈Rn : ui • x ≤0 for 1 ≤i ≤k}.
Affine Geometry and Convexity
299
The next four sections are devoted to proving that a subset of Rn is an H-cone iff it is a
V-cone. This theorem is the key to understanding the structure of finite intersections of
general closed half-spaces.
11.17
Intersection Lemma for V-Cones
You can check that the intersection of any H-cone in Rn with any linear subspace W is also
an H-cone. To help prove that H-cones are the same as V-cones, we first need to prove that
V-cones satisfy a special case of this intersection property. Specifically, given a V-cone C
in Rn, let W be the linear subspace Rn−1 × {0}, which is the solution set of the equation
xn = 0. We prove that C ∩W is also a V-cone.
Since C is a V-cone, we have C = cone({v1, . . . , vm}) for some positive integer m and
vi ∈Rn. Here and below, we write vi(n) for the nth coordinate of the vector vi. We divide
the generators vi of C into classes based on the sign of their last coordinates. To do this,
let I = {1, 2, . . . , m}, and define
I0 = {i ∈I : vi(n) = 0};
I+ = {i ∈I : vi(n) > 0};
I−= {i ∈I : vi(n) < 0}.
Next, define a V-cone
D = cone({vi : i ∈I0} ∪{vi(n)vj −vj(n)vi : i ∈I+, j ∈I−}).
It suffices to show that C ∩W = D.
First, we check that each generator of D has nth coordinate zero, hence is in W. This
is true by definition for the vi with i ∈I0. Given i ∈I+ and j ∈I−, the nth coordinate
of vi(n)vj −vj(n)vi is vi(n)vj(n) −vj(n)vi(n) = 0. Since all generators of D belong to the
subspace W, any positive combination of those generators is also in W. Hence, D ⊆W.
Next, note that each generator of D is a positive combination of generators of the convex
cone C (e.g., for i ∈I+ and j ∈I−, vi(n) ≥0 and −vj(n) ≥0). Since all generators of D
belong to the convex cone C, the convex cone D is contained in C. In summary, D ⊆C ∩W.
For the reverse inclusion, fix v ∈C ∩W. Since v ∈C, we can write v = c1v1 +· · ·+cmvm
for some real scalars ci ≥0. Since v ∈W, 0 = v(n) = c1v1(n) + · · · + cmvm(n). Consider
two cases. Case 1: for all i ∈I, civi(n) = 0. Then for all i ∈I, ci = 0 or vi(n) = 0, so that
the only vi appearing with nonzero coefficients in the expression for v are vi with i ∈I0. So
v is a positive combination of some of the generators of D, hence v ∈D. Case 2: for some
i ∈I, civi(n) ̸= 0. In the equation P
i∈I civi(n) = 0, drop terms indexed by i ∈I0, and
move terms indexed by i ∈I−to the right side. We obtain a strictly positive quantity
b =
X
i∈I+
civi(n) =
X
j∈I−
cj(−vj(n)) > 0.
Now, using both formulas for b, compute:
v
=
X
i∈I
civi =
X
i∈I0
civi + 1
b
X
i∈I+
bcivi + 1
b
X
j∈I−
bcjvj
=
X
i∈I0
civi + 1
b
X
i∈I+
X
j∈I−
cj(−vj(n))civi + 1
b
X
j∈I−
X
i∈I+
civi(n)cjvj
=
X
i∈I0
civi +
X
i∈I+
X
j∈I−
cicj
b (vi(n)vj −vj(n)vi).
300
Advanced Linear Algebra
Note the coefficients ci and cicj/b are all nonnegative, so this expression is a positive
combination of the generators for D. Therefore, v ∈D in Case 2, completing the proof
that C ∩W ⊆D.
We have now proved that for a V-cone C in Rn, the intersection C ∩(Rn−1 × {0}) is
a V-cone in Rn−1. Iterating this result, we deduce the Intersection Lemma for V-Cones:
for any V-cone C in Rn and any k ≤n, C ∩(Rn−k × {0}k) is a V-cone in Rn−k, where we
identify Rn−k with Rn−k × {0}k ⊆Rn.
11.18
All H-Cones Are V-Cones
We now use the Intersection Lemma for V-Cones to prove that every H-cone is a V-cone.
Given an H-cone C ⊆Rn, choose a positive integer k and u1, . . . , uk ∈Rn with
C = {x ∈Rn : ui • x ≤0 for 1 ≤i ≤k}.
The first step is to convert this H-cone into a V-cone in the higher-dimensional space Rn+k.
To do this, define
D = {(x, y) ∈Rn × Rk : ui • x ≤y(i) for 1 ≤i ≤k},
(11.2)
where y(i) denotes the ith component of the vector y. You can check that 0 ∈D, and D is
closed under addition and positive scalar multiples, so D is a convex cone. For 1 ≤j ≤n,
let ej be the jth standard basis vector in Rn. Define wj = (u1(j), u2(j), . . . , uk(j)) ∈Rk, so
wj(i) = ui(j) for 1 ≤i ≤k. For 1 ≤i ≤k, let e′
i be the ith standard basis vector in Rk.
We show that D is a V-cone by confirming that
D = cone({(ej, wj) : 1 ≤j ≤n} ∪{(−ej, −wj) : 1 ≤j ≤n} ∪{(0, e′
i) : 1 ≤i ≤k}).
Call the convex cone on the right side D′. For 1 ≤i ≤k and 1 ≤j ≤n, ui • ej = ui(j) =
wj(i) ≤wj(i) and ui • (−ej) = −ui(j) = −wj(i) ≤−wj(i), so that ±(ej, wj) ∈D. For
1 ≤i, r ≤k, ui • 0 = 0 ≤e′
r(i) since e′
r(i) is 0 or 1. These inequalities prove that all
generators of D′ are in the convex cone D, so D′ ⊆D.
For the reverse inclusion, fix (x, y) ∈D satisfying the conditions in (11.2). For any real z,
let sgn(z) = 1 if z ≥0, sgn(z) = −1 if z < 0, so that z = |z| sgn(z) in all cases. Exercise 84,
proves the identity
(x, y) =
n
X
j=1
|x(j)|(sgn(x(j))ej, sgn(x(j))wj) +
k
X
i=1
(y(i) −ui • x)(0, e′
i).
(11.3)
This identity expresses (x, y) as a linear combination of generators of D′, where all the
scalars |x(j)| and y(i) −ui • x are nonnegative by the assumed conditions on (x, y). It
follows that (x, y) ∈D′, completing the proof that D = D′ and D is a V-cone.
Comparing the original definitions of C and D, we see that C ×{0}k = D ∩(Rn ×{0}k).
By invoking the Intersection Lemma for V-Cones, we see that C × {0}k is a V-cone. Then
C itself is also a V-cone.
Affine Geometry and Convexity
301
11.19
Projection Lemma for H-Cones
You can check that the image of any V-cone in Rn under any linear map is also a V-cone.
To help prove that V-cones are the same as H-cones, we first need to prove that H-cones
satisfy a special case of this projection property. Specifically, consider the linear projection
map p : Rn →Rn−1 given by p(x) = (x(1), . . . , x(n −1)) for x ∈Rn. Given any H-cone C
in Rn, we prove that p[C] = {p(x) : x ∈C} is also an H-cone. Geometrically, p[C] is the
projection of C onto Rn−1 (identified with Rn−1 × {0} in Rn).
By definition, there exist a positive integer k and u1, . . . , uk ∈Rn with
C = {x ∈Rn : ui • x ≤0 for 1 ≤i ≤k}.
(11.4)
Also, z ∈Rn−1 belongs to p[C] iff there exists r ∈R with (z, r) ∈C. We need to pass from
the system of linear inequalities (11.4) defining C to a new system of linear inequalities
(involving only the first n −1 variables) with solution set p[C]. The idea is to take carefully
chosen positive combinations of the original linear inequalities to eliminate the variable xn.
To implement this elimination process, write the index set I = {1, 2, . . . , k} for the
original list of inequalities as the disjoint union of subsets
I0 = {i ∈I : ui(n) = 0};
I+ = {i ∈I : ui(n) > 0};
I−= {i ∈I : ui(n) < 0}.
For i ∈I0, let u′
i = (ui(1), ui(2), . . . , ui(n −1)) ∈Rn−1. For i ∈I+ and j ∈I−, let
vij = ui(n)uj −uj(n)ui. Each vij has nth component ui(n)uj(n) −uj(n)ui(n) = 0; let
v′
ij ∈Rn−1 be vij with this zero deleted. Finally, let
D = {z ∈Rn−1 : u′
i • z ≤0 for all i ∈I0, and v′
ij • z ≤0 for all i ∈I+, j ∈I−}.
By definition, D is an H-cone in Rn−1, so it suffices to prove that p[C] = D.
To prove p[C] ⊆D, we fix z ∈p[C] and check that z ∈D. There is r ∈R with
x = (z, r) ∈C. On one hand, for i ∈I0, we know ui • x ≤0. Since ui(n) = 0, the left side
of this inequality does not involve r, and we get u′
i • z ≤0. On the other hand, fix i ∈I+
and j ∈I−. Multiply the known inequality uj • x ≤0 by the scalar ui(n) ≥0, multiply the
known inequality ui • x ≤0 by the scalar −uj(n) ≥0, and add the resulting inequalities.
We thereby see that vij •x ≤0. As noted above, vij(n) = 0 by construction, so we also have
v′
ij • z ≤0. This means that z satisfies all the inequalities defining D, so that z ∈D.
To prove D ⊆p[C], we fix z ∈D and must prove the existence of r ∈R with
x = (z, r) ∈C. On one hand, for i ∈I0, the known inequality u′
i•z ≤0 guarantees the needed
inequality ui • x ≤0 for any choice of r, since ui(n) = 0. On the other hand, for any fixed
i ∈I+, the needed inequality ui • x ≤0 holds for x = (z, r) iff Pn−1
s=1 ui(s)z(s) + ui(n)r ≤0
iff
r ≤ui(n)−1
n−1
X
s=1
(−ui(s)z(s))
(recall ui(n) > 0). Similarly, for any fixed j ∈I−, uj • x ≤0 holds iff
r ≥uj(n)−1
n−1
X
s=1
(−uj(s)z(s))
(note that the inequality is reversed since uj(n) < 0). Combining these observations, we see
that the remaining inequalities needed to ensure x ∈C hold for x = (z, r) iff
uj(n)−1
n−1
X
s=1
(−uj(s)z(s)) ≤r ≤ui(n)−1
n−1
X
s=1
(−ui(s)z(s))
302
Advanced Linear Algebra
for all i ∈I+ and all j ∈I−. This collection of constraints on r is equivalent to the single
condition
max
j∈I−
"
uj(n)−1
n−1
X
s=1
(−uj(s)z(s))
#
≤r ≤min
i∈I+
"
ui(n)−1
n−1
X
s=1
(−ui(s)z(s))
#
.
There exists an r ∈R satisfying this condition iff
max
j∈I−
"
uj(n)−1
n−1
X
s=1
(−uj(s)z(s))
#
≤min
i∈I+
"
ui(n)−1
n−1
X
s=1
(−ui(s)z(s))
#
,
which holds iff
uj(n)−1
n−1
X
s=1
(−uj(s)z(s)) ≤ui(n)−1
n−1
X
s=1
(−ui(s)z(s))
for all i ∈I+ and all j ∈I−. Multiplying by the negative number ui(n)uj(n), the inequality
just written is equivalent to
ui(n)
n−1
X
s=1
(−uj(s)z(s)) ≥uj(n)
n−1
X
s=1
(−ui(s)z(s)),
which holds iff
n−1
X
s=1
(ui(n)uj(s)z(s) −uj(n)ui(s)z(s)) ≤0
iff (ui(n)uj −uj(n)ui) • (z, r) ≤0 iff v′
ij • z ≤0. All of these last inequalities are true, since
z ∈D, and we see therefore that z ∈p[C]. So D ⊆p[C].
We have now proved that for an H-cone C in Rn, the projection p[C] onto Rn−1 is an
H-cone in Rn−1. Iterating this result, we deduce the Projection Lemma for H-Cones: for an
H-cone C in Rn and any k ≤n, the projection
pk[C] = {z ∈Rn−k : ∃r1, . . . , rk ∈R, (z, r1, . . . , rk) ∈C} = {z ∈Rn−k : ∃y ∈Rk, (z, y) ∈C}
is an H-cone in Rn−k.
11.20
All V-Cones Are H-Cones
We now use the Projection Lemma for H-Cones to prove that every V-cone is an H-cone.
Given a V-cone C ⊆Rn, there exist a positive integer k and v1, . . . , vk ∈Rn with
C = {c1v1 + · · · + ckvk : ci ∈R, ci ≥0 for 1 ≤i ≤k}.
The key observation is that we can convert this V-cone into an H-cone in the higher-
dimensional space Rn+k. To do this, define
D =
(
(x, y) ∈Rn+k : x =
k
X
i=1
y(i)vi and y(i) ≥0 for 1 ≤i ≤k
)
.
(11.5)
To see why D is an H-cone, let ej and e′
i denote standard basis vectors in Rn and Rk,
respectively. Define ui = (0, −e′
i) for 1 ≤i ≤k and wj = (ej, (−v1(j), . . . , −vk(j))) for
Affine Geometry and Convexity
303
1 ≤j ≤n. You can check that (x, y) satisfies the conditions in the definition of D iff
ui • (x, y) ≤0 for 1 ≤i ≤k and wj • (x, y) ≤0 for 1 ≤j ≤n and (−wj) • (x, y) ≤0 for
1 ≤j ≤n. So D is the solution set of a finite system of homogeneous linear inequalities,
hence D is an H-cone.
To finish the proof, we need only observe that x ∈Rn belongs to C iff there exists
y ∈Rk with (x, y) ∈D. By invoking the Projection Lemma for H-Cones, we see that C is
an H-cone.
11.21
Finite Intersections of Closed Half-Spaces
Now that we know V-cones are the same as H-cones, we can prove the following result
characterizing finite intersections of closed half-spaces.
Theorem on Finite Intersections of Closed Half-Spaces A subset C of Rn has the
form C = H1 ∩H2 ∩· · · ∩Hs for finitely many closed half-spaces H1, . . . , Hs iff there
exist finitely many vectors v1, . . . , vk, w1, . . . , wm ∈Rn with C = conv({v1, . . . , vk}) +
cone({w1, . . . , wm}).
The plus symbol denotes the sum of sets in Rn, namely A + B = {a + b : a ∈A, b ∈B}.
Proof. First assume that there are finitely many vectors vi and wj in Rn with
C = conv({v1, . . . , vk}) + cone({w1, . . . , wm}).
(11.6)
A typical point v ∈C looks like
v = c1v1 + · · · + ckvk + d1w1 + · · · + dmwm
(11.7)
where ci, dj ≥0 are scalars such that Pk
i=1 ci = 1. Identifying Rn with Rn × {0}, we can
regard each vi and wj as a vector in Rn+1. In Rn+1, define a V-cone
D = cone(v1 + en+1, . . . , vk + en+1, w1, . . . , wm).
D consists of all points v ∈Rn+1 that can be written in the form
v = c1v1 + · · · + ckvk + d1w1 + · · · + dmwm +
 k
X
i=1
ci
!
en+1,
(11.8)
for some scalars ci, dj ≥0. Let H0 denote the affine hyperplane Rn × {1} in Rn+1. We
can obtain exactly those points in the intersection D ∩H0 by choosing scalars ci, dj ≥0
in (11.8) such that Pk
i=1 ci = 1. Since these are precisely the conditions imposed on the
scalars in (11.7), we see that D ∩H0 is the translate C + en+1.
As shown in §11.20, D is an H-cone, so D is a finite intersection of certain linear half-
spaces H′
1, . . . , H′
s in Rn+1. It follows that
C + en+1 = D ∩H0 = (H′
1 ∩H0) ∩· · · ∩(H′
s ∩H0).
Translating back to Rn × {0}, we see that C = Ts
i=1((H′
i ∩H0) −en+1), where each set in
the intersection is readily seen to be a closed half-space in Rn. So C has been expressed as
a finite intersection of closed half-spaces.
304
Advanced Linear Algebra
Conversely, assume C is the intersection of finitely many closed half-spaces in Rn, say
C = {x ∈Rn : ui • x ≤bi for 1 ≤i ≤p}
for some integer p ≥0 and ui ∈Rn ⊆Rn+1. In Rn+1, define an H-cone
D = {z ∈Rn+1 : (ui −bien+1) • z ≤0 for 1 ≤i ≤p and (−en+1) • z ≤0}.
The last linear inequality in this definition amounts to requiring that the last coordinate of
each point in D be nonnegative. Observe that for points z ∈Rn+1 of the form (x, 1), the
other linear inequalities in the definition of D hold iff ui • x ≤bi for each i. It follows from
these remarks that D ∩H0 = C + en+1, where (as before) H0 denotes the affine hyperplane
Rn × {1} in Rn+1.
As shown in §11.18, we can write D = cone({z1, . . . , zs}) for some s ∈Z≥0 and zi ∈
Rn+1. By reordering the zi and rescaling some zi by positive scalars if needed, we can
assume that zi(n + 1) = 1 for 1 ≤i ≤k and zi(n + 1) = 0 for k < i ≤s. Then a typical
element of D is a positive combination
k
X
i=1
cizi +
s
X
i=k+1
dizi
for some ci, di ≥0.
To find the points of D ∩H0, we need to restrict the ci and di to those nonnegative scalars
that make the last coordinate equal to 1. By choice of the zi, the restriction needed is
precisely Pk
i=1 ci = 1. Since D ∩H0 = C + en+1, we see that the points in C are exactly
those points that can be written in the form
k
X
i=1
ci(zi −en+1) +
s
X
i=k+1
dizi
where ci, di ≥0 and
k
X
i=1
ci = 1.
This means that
C = conv({z1 −en+1, . . . , zk −en+1}) + cone({zk+1, . . . , zs}),
where we view all generators as elements of Rn. So C has been expressed in the form (11.6).
Theorem on Convex Hulls of Finite Sets. A set C ⊆Rn is the convex hull of finitely
many points iff C is a bounded intersection of finitely many closed half-spaces.
Proof. Assume C = conv({v1, . . . , vk}). This is the special case of (11.6) with m = 0, so
C is an intersection of finitely many closed half-spaces. Moreover, any convex combination
w = Pk
i=1 civi with ci ≥0 and Pk
i=1 ci = 1 satisfies
||w|| ≤
k
X
i=1
|ci| · ||vi|| ≤
k
X
i=1
||vi||,
so C is bounded.
Conversely, assume C is a bounded set that is the intersection of finitely many closed
half-spaces. We have proved that C can be written in the form (11.6), and we may assume
no wi is zero. If m > 0, then {dw1 : d ≥0} would be an unbounded subset of C. So m = 0,
and C = conv({v1, . . . , vk}) as needed.
Affine Geometry and Convexity
305
11.22
Convex Functions
We conclude the chapter with a brief introduction to convex functions. Let C be a convex
subset of Rn. A function f : C →R is called convex iff f(cx+(1−c)y) ≤cf(x)+(1−c)f(y)
for all x, y ∈C and all c ∈[0, 1]. (Compare to the definition of an affine map, where equality
was required to hold for all c ∈R.) To see how this definition is related to the idea of convex-
ity for sets, define the epigraph of f to be the set epi(f) = {(x, z) ∈Rn+1 : x ∈C, z ≥f(x)},
which consists of all points in Rn+1 above the graph of f. We claim that f is a convex
function iff epi(f) is a convex set.
To verify this, first assume epi(f) is a convex set. Given x, y ∈C and c ∈[0, 1], the
points (x, f(x)) and (y, f(y)) are in epi(f). By convexity of the epigraph, we know
c(x, f(x)) + (1 −c)(y, f(y)) = (cx + (1 −c)y, cf(x) + (1 −c)f(y)) ∈epi(f).
The definition of the epigraph now gives f(cx + (1 −c)y) ≤cf(x) + (1 −c)f(y), so f is a
convex function. Conversely, assume f is a convex function. Fix c ∈[0, 1] and two points
(x, z), (y, w) ∈epi(f), where x, y ∈Rn and z, w ∈R. We know z ≥f(x) and w ≥f(y). By
convexity of f,
cz + (1 −c)w ≥cf(x) + (1 −c)f(y) ≥f(cx + (1 −c)y),
and hence c(x, z) + (1 −c)(y, w) = (cx + (1 −c)y, cz + (1 −c)w) ∈epi(f). So epi(f) is a
convex set. For C ⊆R, the convexity of f (or epi(f)) means that for any real a < b in C,
the graph of f on the interval [a, b] is always weakly below the line segment joining (a, f(a))
to (b, f(b)).
Convexity is a rather strong condition to impose on a function. For instance, for all
real c < d, a convex function f : [c, d] →R must be continuous on the open interval (c, d).
Figure 11.4 illustrates the proof. We verify continuity of f at an arbitrary point x ∈(c, d).
Having fixed x, choose y < x in (c, d) and z > x in (c, d). Draw the line L1 of slope m1
through (y, f(y)) and (x, f(x)) and the line L2 of slope m2 through (x, f(x)) and (z, f(z)).
Since (x, f(x)) is below the line L of slope m joining (y, f(y)) and (z, f(z)), it is geometrically
evident that m1 ≤m ≤m2. So, in the part of the plane right of x, L1 is below L2. Now
consider any sequence of points xn in the open interval (x, z) such that (xn) converges to
x. To see that f is right-continuous at x, we must show limn→∞f(xn) = f(x). By using
L
1
x n
c
x
y
z
L2
d
graph of f
L
FIGURE 11.4
A convex function must be continuous on an open interval.
306
Advanced Linear Algebra
the remark at the end of the last paragraph with a = x and b = z, we see that every point
(xn, f(xn)) is below the line L2. On the other hand, using the same remark with a = y and
b = xn, we see that (x, f(x)) is below the line joining (y, f(y)) to (xn, f(xn)), and hence
(xn, f(xn)) is above the line L1. Each of the lines L1 and L2 is the graph of a continuous
(in fact, affine) function whose limit as xn approaches x is f(x). By the Sandwich Theorem
for Limits, (xn) must also converge to x as n goes to infinity. Similar remarks prove the
left-continuity of f at x. However, f need not be continuous at the endpoints c and d of its
domain.
We have proved (§11.11) that convex sets are closed under convex combinations of
their elements. Applying this remark to epi(f) for a convex function f : C →R, we
obtain a result called Jensen’s Inequality. Let x1, . . . , xk ∈C and c1, . . . , ck ∈R with
each ci ≥0 and Pk
i=1 ci = 1. Since (xi, f(xi)) ∈epi(f) for all i, the convex combination
(Pk
i=1 cixi, Pk
i=1 cif(xi)) is in epi(f). Hence,
f
 k
X
i=1
cixi
!
≤
k
X
i=1
cif(xi).
(11.9)
This is the discrete form of Jensen’s Inequality. By applying this inequality to Riemann
sums approximating a Riemann integral, we can obtain a version of Jensen’s Inequality for
Integrals. Specifically, suppose f : (c, d) →R is convex and g : [0, 1] →R is an integrable
function taking values in (c, d). Then
f
Z 1
0
g(x) dx

≤
Z 1
0
f(g(x)) dx.
This follows from (11.9) by setting ci = 1/k and xi = g(i/k) for 1 ≤i ≤k, which causes
the two sums to approximate the two integrals just written, and then taking the limit of
the inequality as k goes to infinity. By continuity of f, the limit of the left side of (11.9) is
f(
R 1
0 g(x) dx). Similarly, the right side approaches
R 1
0 f(g(x)) dx, where the integrability of
f ◦g follows since f is continuous. In the context of probability theory, sums involving convex
combinations and integrals on [0, 1] are special cases of expectations of random variables on
a probability space. In this setting, Jensen’s Inequality becomes f(E[X]) ≤E[f(X)], where
f is convex, X is a random variable, and E denotes expected value.
11.23
Derivative Tests for Convex Functions
We have not yet given any examples of convex functions. Every affine function f : Rn →R
is certainly convex; when n = 1, the graph of such a function is a line. The absolute value
function (f(x) = |x| for x ∈R) is convex, since it is visually apparent that the epigraph
of f is convex. More generally, suppose x0 < x1 < x2 < · · · < xn and y0, y1, . . . , yn are
given real numbers. Drawing the line segments from (xi−1, yi−1) to (xi, yi) for 1 ≤i ≤n
gives the graph of a function f : [x0, xn] →R. This graph consists of a sequence of line
segments having slopes mi = (yi −yi−1)/(xi −xi−1). You can check that f is convex iff
m1 ≤m2 ≤· · · ≤mn, i.e., the slopes of successive line segments are weakly increasing.
This suggests that a differentiable function f : (c, d) →R is convex iff the first derivative
f ′ : (c, d) →R is an increasing function. To explain this geometrically, assume f is convex,
and fix y < x in (c, d) as shown in Figure 11.4. The secant line L1 joining (y, f(y)) to
(x, f(x)) has some slope m1. Convexity tells us that the graph of f between y and x is
Affine Geometry and Convexity
307
below this secant line, so it is geometrically evident that the tangent line to the graph of f
at y has slope weakly less than m1. In symbols, f ′(y) ≤m1. But, as we argued earlier, the
graph of f to the right of x must be above the secant line L1, so that the tangent line to
the graph of f at x has slope weakly greater than m1. Thus, m1 ≤f ′(x), so f ′(y) ≤f ′(x)
and f ′ is increasing.
Conversely, assume f ′ is increasing. To verify convexity of f, it suffices to show that
for all y < x < z in (c, d), the point (x, f(x)) is weakly below the line segment L joining
(y, f(y)) and (z, f(z)) (see Figure 11.4). Let the secant lines L1 and L2 shown in the figure
have respective slopes m1 and m2. Then the point (x, f(x)) is weakly below L iff m1 ≤m2.
By the Mean Value Theorem, there is a point y1 with y < y1 < x and f ′(y1) = m1, and
there is z1 with x < z1 < z and f ′(z1) = m2. Since y1 < z1 and f ′ is increasing, m1 ≤m2
as needed.
By a theorem from calculus, we deduce that a twice-differentiable function f : (c, d) →R
is convex iff the second derivative f ′′ : (c, d) →R is nonnegative; i.e., f ′′(x) ≥0 for all
x ∈(c, d). This provides a convenient criterion for testing convexity of many functions. For
example, f(x) = ex for x ∈R, g(x) = x2 for x ∈R, and h(x) = −ln x for x > 0, are all
convex functions by the second derivative test. In elementary calculus, these functions are
often described as being concave up. For convex C ⊆Rn, we say that a function f : C →R
is concave iff −f is convex iff f(cx + (1 −c)y) ≥cf(x) + (1 −c)f(y) for all x, y ∈C and all
c ∈[0, 1]. When n = 1, concave functions are called concave down in elementary calculus.
The second derivative test extends to multivariable functions as follows. Suppose C is an
open convex subset of Rn, and f : C →R has continuous second-order partial derivatives.
For each z ∈C, define the Hessian matrix Hz ∈Mn(R) by letting Hz(i, j) = fxi,xj(z) =
∂2f
∂xi∂xj (z) for 1 ≤i, j ≤n. Then f is convex on C iff for all z ∈C, Hz is a positive
semidefinite matrix (which means v • Hzv = P
i,j fxi,xj(z)vivj ≥0 for all v ∈Rn). The
proof is outlined in Exercise 104; the idea is to reduce to the one-variable case by composing
f with affine maps g : (c, d) →C and using the chain rule to compute (f ◦g)′′.
11.24
Summary
Let F be a field in which 1F + 1F ̸= 0F and V be an n-dimensional F-vector space.
1.
Types of Linear Combinations. A linear combination of v1, . . . , vk ∈V is a vector
of the form c1v1 + · · · + ckvk with all ci ∈F. This vector is:
(a) an affine combination iff Pk
i=1 ci = 1F ;
(b) a convex combination iff F = R, all ci ≥0, and Pk
i=1 ci = 1;
(c) a positive combination iff F = R and all ci ≥0.
2.
Definitions of Special Sets and Maps. Table 11.1 summarizes the definitions of
the structured sets and maps studied in this chapter.
3.
Characterizations of Linear Subspaces. Every k-dimensional linear subspace of
F n is: (a) SpF (S) for some linearly independent set S of size k;
(b) the null space of some (n −k) × n matrix;
(c) the range of some n × k matrix;
(d) the intersection of n −k linear hyperplanes in F n;
(e) the solution set of a system of n −k homogeneous linear equations in n
unknowns.
308
Advanced Linear Algebra
TABLE 11.1
Definitions of affine and convex concepts.
Concept
Definition
Additional Properties
linear subspace W
0V ∈W;
W is closed under linear combinations
x, y ∈W ⇒x + y ∈W;
x ∈W, c ∈F ⇒cx ∈W.
affine set X
x, y ∈X, c ∈F ⇒
X is closed under affine combinations;
cx + (1 −c)y ∈X.
X = u + W for dir. subspace W
convex set C
x, y ∈C, t ∈[0, 1] ⇒
C is closed under convex combinations
tx + (1 −t)y ∈C.
cone C
0 ∈C;
C is convex iff C is closed under +
x ∈C, t ≥0 ⇒tx ∈C
convex cone C
0 ∈C,
C is closed under positive combinations
x, y ∈C ⇒x + y ∈C,
x ∈C, t ≥0 ⇒tx ∈C.
V-cone C
C = cone(S) for
same as H-cone
some finite S ⊆Rn
H-cone C
C = {x ∈Rn : ui • x ≤0
same as V-cone
for u1, . . . , uk ∈Rn}
linear span SpF (S)
{lin. combs. of vectors in S}
intersection of all subspaces W ⊇S
affine span aff(S)
{aff. combs. of vectors in S}
intersection of all affine sets X ⊇S
convex hull conv(S)
{conv. combs. of points in S}
intersection of all convex sets C ⊇S
convex cone cone(S)
{pos. combs. of points in S}
intersection of all convex cones C ⊇S
linear hyperplane
{x ∈Rn : u • x = 0}
2 choices for unit vector u
affine hyperplane
{x ∈Rn : u • x = c}
unit vector u is unique when c > 0
closed half-space
{x ∈Rn : u • x ≤c}
other half-space: {x ∈Rn : u • x ≥c}
open half-space
{x ∈Rn : u • x < c}
other half-space: {x ∈Rn : u • x > c}
linear map T
x, y ∈V, c ∈F ⇒
T preserves linear combinations
T(x + y) = T(x) + T(y),
T(cx) = cT(x)
affine map U
x, y ∈V, c ∈F ⇒
U preserves affine combinations,
U(cx + (1 −c)y) =
U(x) = T(x) + b (T linear)
cU(x) + (1 −c)U(y)
convex function f
x, y ∈C, t ∈[0, 1] ⇒
f is continuous on open interval
(convex domain C)
f(tx + (1 −t)y) ≤
Jensen’s Inequality holds
tf(x) + (1 −t)f(y)
concave function g
x, y ∈C, t ∈[0, 1] ⇒
f is continuous on open interval
(convex domain C)
g(tx + (1 −t)y) ≥
−g is convex
tg(x) + (1 −t)g(y)
4.
Characterizations of Affine Sets. Every nonempty affine subset X of F n of affine
dimension k is: (a) aff(S) for some affinely independent set S of size k + 1;
(b) u + W for a unique k-dimensional linear subspace W (the direction subspace
of X) and all u ∈X;
(c) the solution set of a system of n −k (possibly non-homogeneous) linear
equations in n unknowns;
(d) the intersection of n −k affine hyperplanes in F n.
5.
Affine Independence, Bases, and Dimension. A list L = (v0, v1, . . . , vk) of vectors
in V
is affinely independent iff for all ci ∈F such that Pk
i=0 ci = 0, if
Pk
i=0 civi = 0 then all ci = 0. L is affinely independent iff (v1 −v0, . . . , vk −v0) is
linearly independent. An affine basis of an affine set X is an affinely independent
set whose affine span is X. The dimension of X is the dimension of its direction
subspace, which is one less than the size of any affine basis of X. Affinely
Affine Geometry and Convexity
309
independent subsets of X can be extended to an affine basis; affine spanning sets
for X contain an affine basis; no affinely independent set is larger than an affine
basis for X; and any function defined on an affine basis of X extends uniquely
to an affine map with domain X. Each point v in X has unique barycentric
coordinates expressing v as an affine combination of an ordered affine basis of X.
6.
Carath´eodory’s Theorem on Convex Hulls. For all S ⊆Rn, every element of
C = conv(S) is a convex combination of at most n + 1 elements of S. One
consequence is that convex hulls of closed and bounded (compact) sets are also
closed and bounded.
7.
Separation by Hyperplanes. If C is a closed, bounded, convex subset of Rn and D
is a closed, convex subset of Rn disjoint from C, there is an affine hyperplane H
in Rn such that C and D are in opposite open half-spaces of H. So, closed convex
sets are the intersection of all open (or all closed) half-spaces containing them.
8.
Theorems on Generation vs. Intersection. For all subsets C of Rn:
(a) C has the form Sp(S) for some set S iff C is an intersection of linear
hyperplanes.
(b) C has the form aff(S) for some set S iff C is an intersection of affine
hyperplanes.
(c) C is closed and has the form conv(S) for some set S iff C is an intersection
of closed half-spaces.
(d) C has the form conv(S) for some finite set S iff C is a bounded intersection
of finitely many closed half-spaces.
(e) C has the form conv(S) + cone(T) for some finite sets S and T iff C is an
intersection of finitely many closed half-spaces.
(f) C has the form cone(T) for some finite set T iff C is an intersection of finitely
many linear half-spaces (this says V-cones are the same as H-cones).
9.
Intersection Lemma for V-Cones. Given a V-cone C = cone({v1, . . . , vm}) ⊆Rn,
the intersection C ∩(Rn−1 × {0}) is the V-cone
D = cone({vi : vi(n) = 0} ∪{vi(n)vj −vj(n)vi : vi(n) > 0, vj(n) < 0}).
10.
Projection Lemma for H-Cones. Given an H-cone
C = {x ∈Rn : ui • x ≤0 for 1 ≤i ≤k},
the projection p[C] = {z ∈Rn−1 : ∃r ∈R, (z, r) ∈C} is the H-cone
D = {z ∈Rn−1 : ui • (z, 0) ≤0 for all i with ui(n) = 0 and
(ui(n)uj −uj(n)ui) • (z, 0) ≤0 for all i, j with ui(n) > 0, uj(n) < 0}.
11.
Theorems on Convex Functions. A function f with convex domain is convex iff its
epigraph epi(f) = {(x, y) : x ∈C, y ≥f(x)} is a convex set. For convex functions
f and scalars ci ≥0 summing to 1, f(P
i cixi) ≤P
i cif(xi) and f(
R 1
0 g(x) dx) ≤
R 1
0 f(g(x)) dx whenever these expressions are defined (Jensen’s Inequality). A
convex function whose domain is an open interval of R must be continuous. For
one-variable functions f such that f ′ exists, f is convex iff f ′ is increasing. When
f ′′ exists, f is convex iff f ′′ ≥0. If C ⊆Rn is open and convex and f : C →R
has continuous second partial derivatives on C, f is convex iff the Hessian matrix
Hz = (fxi,xj(z))1≤i,j≤n is positive semidefinite for all z in C.
310
Advanced Linear Algebra
11.25
Exercises
In these exercises, assume V and W are vector spaces over a field F with dim(V ) = n unless
otherwise stated.
1.
Prove that a linear subspace W of V is closed under linear combinations.
2.
Prove: If W ⊆V is closed under linear combinations, then W is a subspace of V .
3.
Let A ∈Mm,n(F).
(a) Prove the range of A is a subspace of F m.
(b) Prove that T : F n →F m, given by T(x) = Ax for x ∈F n, is F-linear.
(c) Confirm that ker(T) is the null space of A, and img(T) is the range of A.
4.
Prove that the intersection of any collection of subspaces of V is a subspace.
5.
Prove that the intersection of any collection of affine subsets of V is affine.
6.
Prove that the intersection of any collection of convex subsets of Rn is convex.
7.
Prove that the intersection of any collection of convex cones in Rn is a convex
cone.
8.
Given subsets W1, . . . , Wk of V , the sum of these subsets is
W1 + · · · + Wk = {x1 + · · · + xk : x1 ∈W1, . . . , xk ∈Wk}.
Prove that the sum of linear subspaces is a linear subspace.
9.
Prove that the sum of affine sets is affine.
10.
Prove that the sum of convex sets is convex.
11.
Prove that the sum of convex cones is a convex cone.
12.
Prove or disprove: if X, Y , and X ∪Y are all linear subspaces of V , then X ⊆Y
or Y ⊆X.
13.
Prove or disprove: if X, Y , and X ∪Y are all affine subsets of V , then X ⊆Y or
Y ⊆X.
14.
Prove or disprove: if X, Y , and X ∪Y are all convex subsets of Rn, then X ⊆Y
or Y ⊆X.
15.
Given vector spaces V1, . . . , Vk and subsets Si ⊆Vi, the product of these subsets
is S1 × · · · × Sk = {(x1, . . . , xk) : xi ∈Si}. Prove that the product of linear
subspaces is a linear subspace.
16.
Prove that the product of affine sets is an affine set.
17.
Prove that the product of convex sets is a convex set.
18.
Prove that the product of convex cones is a convex cone.
19.
Let S = {v1, . . . , vk} ⊆V . Prove that Sp(S) is a linear subspace of V . Prove that
Sp(S) is the intersection of all subspaces of V that contain S.
20.
Prove every m-dimensional subspace of F n is the range of some A ∈Mn,m(F).
21.
Prove every k-dimensional subspace of V is the kernel of a linear map T : V →
F n−k.
22.
Prove every k-dimensional subspace of V is the image of a linear map S : F k →V .
Affine Geometry and Convexity
311
23.
Prove: for all A ∈Mm,n(F) and b ∈F m, the set {x ∈F n : Ax = b} is an affine
subset of F n.
24.
Prove: a nonempty S ⊆V is a linear subspace of V iff S + S = S and cS = S for
all nonzero c ∈F.
25.
Prove: if S ̸= ∅is an affine set, then the direction subspace of S is S + (−1)S.
26.
Prove: for all affine sets S and all c ∈F, cS is an affine set.
27.
Say that two nonempty affine sets S, T ⊆V are parallel iff S and T have the same
direction subspace. (a) Show that two affine lines in R2 are parallel iff the lines
are both vertical or both lines have the same slope. (b) Show that parallelism is
an equivalence relation on the set of nonempty affine subsets of V , such that the
equivalence class of S consists of all translates u + S for u ∈V .
28.
Let X be an affine subset of V . (a) Prove: for all affinely independent S ⊆X,
there exists an affine basis of X containing S. (b) Prove: for all sets T ⊆X with
aff(T) = X, there exists an affine basis of X contained in T.
29.
Show that any subset of an affinely independent set is also affinely independent.
30.
Show that the maximum size of an affinely independent subset of the n-
dimensional space V is n + 1.
31.
Show that {v0, v1, . . . , vk} is affinely dependent iff some vi is an affine combination
of the vj with j ̸= i.
32.
Given S = {v0, v1, . . . , vk} ⊆Rn, let v′
k be an affine combination of v0, . . . , vk
in which vk appears with nonzero coefficient. Let S′ be S with vk replaced
by v′
k. Show aff(S′) = aff(S). Show S′ is affinely independent iff S is affinely
independent.
33.
Given A ∈Mm,n(F), let B be obtained from A by appending a column of 1s to
the right end of A. Show that the rows of A are affinely independent in F n iff the
rows of B are linearly independent in F n+1.
34.
Points in General Position. Say that points v1, . . . , vk ∈Rn are in general
position iff every subset of {v1, . . . , vk} of size n+1 or less is affinely independent.
For n = 2, show that distinct points v1, . . . , vk ∈R2 are in general position iff no
three vi are collinear. State and prove an analogous result n = 3.
35.
Let c1, . . . , ck be distinct real numbers. Set vi = (cn
i , cn−1
i
, . . . , c2
i , ci) for 1 ≤i ≤
k. Prove v1, . . . , vk ∈Rn are in general position. [Hint: Use the fact that the
Vandermonde matrix (see Exercise 40 in Chapter 5) has nonzero determinant.]
36.
Prove: for each positive integer n, it is impossible to write Rn as a union of finitely
many affine hyperplanes.
37.
Use the previous exercise and induction on k to prove that for all positive integers
k and n, there exist k points in general position in Rn (see Exercise 34).
38.
Assume 1F + 1F ̸= 0F in the field F. Prove S ⊆V is affine iff cx + (1 −c)y ∈S
for all x, y ∈S and all c ∈F. Give an example to show that this result may fail
if 1F + 1F = 0F .
39.
Prove: for any field F, S ⊆V is affine iff ax + by + cz ∈S for all x, y, z ∈S and
all a, b, c ∈F with a + b + c = 1F .
40.
Prove: for all A ∈Mm,n(F) and all b ∈F m, the map U : F n →F m given by
U(x) = Ax + b is an affine map. Show that U is linear iff b = 0.
41.
Prove the identity map on any affine set is an affine isomorphism.
312
Advanced Linear Algebra
42.
Prove the composition of two affine maps (resp. affine isomorphisms) is an affine
map (resp. affine isomorphism).
43.
Prove the inverse of an affine isomorphism is also an affine isomorphism.
44.
Let U : X →Y be an affine map between affine sets. Prove U[S] is affine for all
affine S ⊆X, and U −1[T] is affine for all affine T ⊆Y .
45.
Let U : Rn →Rm be an affine map. Prove U[S] is convex for all convex S ⊆X,
and U −1[T] is convex for all convex T ⊆Y .
46.
Let U : V →W be an affine map. Prove: for all S ⊆V , U[aff(S)] = aff(U[S]).
Deduce that u + aff(S) = aff(u + S) for all u ∈V and S ⊆V .
47.
Prove: for every affine map U : V →W between vector spaces V and W, there
exists a unique linear map S : V →W and a unique b ∈W with U = Tb ◦S,
where Tb : W →W is translation by b.
48.
Prove or disprove: for every affine map U : V →W between any vector spaces V
and W, there exists a unique linear map S : V →W and a unique c ∈V with
U = S ◦T −1
c
.
49.
Check the map U defined at the end of §11.10 is affine and sends each vi to yi.
50.
Prove the UMP for affine bases of X (see §11.10) by invoking the analogous UMP
for a linear basis of the direction subspace of X (cf. (6.2)).
51.
Assume 1F + 1F ̸= 0F in the field F. Prove U : V →W is an affine map iff
U(cx + (1 −c)y) = cU(x) + (1 −c)U(y)
for all x, y ∈V and all c ∈F. Give an example to show that this result may fail
if 1F + 1F = 0F .
52.
Prove: for any field F, a map U : V →W is an affine map iff U(ax + by + cz) =
aU(x) + bU(y) + cU(z) for all x, y, z ∈V and all a, b, c ∈F with a + b + c = 1F .
53.
Let X and Y be affine sets, C = (v0, . . . , vk) be a list in X, U : X →Y be an
affine map, and C′ = (U(v0), . . . , U(vk)). Prove:
(a) If C is affinely independent and U is injective, then C′ is affinely independent.
(b) If X = aff(C) and U is surjective, then Y = aff(C′).
(c) If C is an affine basis of X and U is bijective, then C′ is an affine basis of Y .
54.
Let B = (v0, v1, v2) = ((1, 0, 1), (2, 3, 1), (1, 1, 2)) and X = aff(B).
(a) Prove B is an affinely independent ordered list.
(b) Find the Cartesian coordinates of the point that has barycentric coordinates
(1/3, 1/2, 1/6) relative to B.
(c) Find the barycentric coordinates relative to B of the point with Cartesian
coordinates (0, 0, 4).
55.
Let B = ((2, 0, 0), (1, 1, 0), (0, 1, 3), (−1, −1, 1)), which is an affine basis of R3.
Given v = (x, y, z) ∈R3, find the barycentric coordinates of v relative to B.
56.
Let ∆= conv({e1, e2, . . . , ek+1}) ⊆Rk+1. Show that ∆is a k-dimensional
simplex such that Cartesian coordinates of points in ∆coincide with barycentric
coordinates of points in ∆relative to the ordered affine basis (e1, . . . , ek+1).
Describe ∆as the solution set of a system of linear inequalities.
57.
Show that for any two k-dimensional simplexes ∆1 and ∆2 in Rn, there exists an
affine isomorphism of Rn mapping ∆1 onto ∆2.
Affine Geometry and Convexity
313
58.
Barycenter of a Simplex. Let {v0, . . . , vk} be affinely independent in Rn. The
barycenter of the simplex ∆= conv({v0, . . . , vk}) is the point in ∆all of whose
barycentric coordinates (relative to the vi) are 1/(k + 1). (a) Show that when
k = 1, the barycenter of ∆is the midpoint of the line segment with endpoints v0
and v1. (b) Find the barycenter of the triangle conv({(1, 2), (3, 5), (2, −4)}), and
illustrate in a sketch. (c) Find the barycenter of conv({0, e1, . . . , en}) in Rn.
59.
Let {v0, . . . , vk} be affinely independent in Rn. Let w be the barycenter of ∆=
conv({v0, . . . , vk}). For 0 ≤i ≤k, let ∆i = conv(({v0, . . . , vk} \ {vi}) ∪{w}).
Prove that ∆= Sk
i=0 ∆i.
60.
The graph of a function T : V →W is the set
G(T) = {(x, T(x)) : x ∈V } ⊆V × W.
(a) Show that the graph of a linear map is a linear subspace of V × W.
(b) Show that the graph of an affine map is an affine subset of V × W.
(c) Show that for every affine set S in F n, there exist m ≤n, an affine map
U : F n−m →F m, and a linear isomorphism P : F n →F n that acts by permuting
standard basis vectors, such that P[S] is the graph of U.
61.
(a) Show that for all linear hyperplanes H in Rn, there exist exactly two unit
vectors u ∈Rn with H = {x ∈Rn : u • x = 0}.
(b) Show that for all affine hyperplanes H in Rn not containing 0, there is exactly
one unit vector u ∈Rn and one c > 0 with H = {x ∈Rn : u • x = c}.
62.
In R1, explicitly describe all: (a) linear subspaces; (b) affine sets; (c) convex sets;
(d) cones. Prove that the sets you describe, and no others, have each property.
63.
Prove that C = {(x, y, z) ∈R3 : z2 = x2 + y2} is a cone. Is C a convex cone?
64.
Prove that D = {(x, y, z) ∈R3 : z ≥
p
x2 + y2} is a convex cone.
65.
Is E = {(x, y, z) ∈R3 : z2 ≥x2 + y2} a cone? Is E convex? Explain.
66.
Fix x0 ∈Rn and r ≥0.
(a) Prove the open ball B = {x ∈Rn : d(x, x0) < r} is convex.
(b) Is the closed ball B′ = {x ∈Rn : d(x, x0) ≤r} convex? Explain.
(c) Is the sphere S = B′ \ B convex? Explain.
67.
Let S and T be convex subsets of Rn.
(a) Prove: for all c ∈R, cS is a convex set.
(b) Prove: if S and T are convex and c ∈[0, 1], then cS + (1 −c)T is convex.
68.
Let S = conv({(cos(2πk/6), sin(2πk/6)) : 0 ≤k < 6})
and T = conv({(cos(π/2 + 2πk/3), sin(π/2 + 2πk/3)) : 0 ≤k < 3}).
Sketch the sets cS + (1 −c)T for c ∈{0, 1/4, 1/2, 2/3, 1}.
69.
Let S ⊆Rn be a convex set.
(a) Prove the closure of S (the set of x ∈Rn with d(x, S) = 0) is convex.
(b) Prove the interior of S (the union of all open balls of Rn contained in S) is
convex.
70.
Let C be a nonempty compact (closed and bounded) subset of Rn. Given a subset
D ̸= ∅in Rn, define f : C →R by f(z) = d(z, D) for z ∈C. Prove:
(a) f is continuous, and there is x ∈C with d(x, D) = d(C, D).
(b) If D is compact, then there are x ∈C and y ∈D with d(x, y) = d(C, D).
(c) The conclusion of (b) holds for all closed D, even if D is unbounded.
71.
Give an example of two closed, unbounded sets C, D ⊆R2 such that d(C, D) is
not of the form d(v, w) for any v ∈C, w ∈D.
314
Advanced Linear Algebra
72.
The Hyperplane Separation Theorem (§11.15) assumes that C is convex, closed,
and bounded and that D is convex and closed. Give examples to show that the
omission of any one of these five assumptions may cause the conclusion of the
theorem to fail.
73.
Complete the proof of the Hyperplane Separation Theorem by showing that
D ⊆{v ∈Rn : u • v ≥c2} and indicating what adjustments are needed in the
case c1 > c2.
74.
Give an example of a convex subset of R2 that cannot be written as an intersection
of any family of half-spaces (even allowing a mixture of open and closed half-
spaces).
75.
(a) Prove that the intersection of any family of closed subsets of Rn is closed.
(b) Prove that every closed half-space is a closed set.
(c) Prove that for fixed x0 ∈Rn and r > 0, the closed ball {x ∈Rn : d(x, x0) ≤r}
is a closed set.
76.
(a) Prove that every affine hyperplane is a closed set.
(b) Prove that every affine set is a closed set.
(c) Prove that every simplex is a closed set.
77.
Prove that if S ⊆Rn is closed and bounded, then conv(S) is a closed convex
subset of Rn. (Use Carath´eodory’s Theorem and sequential compactness of S
and [0, 1].)
78.
Give an example of a countably infinite set S such that conv(S) is not closed.
79.
If S ⊆Rn is bounded but not closed, must conv(S) be bounded? Prove or give a
counterexample.
80.
Radon’s Theorem. Suppose S = {v0, v1, . . . , vn+1} is a set of n + 2 vectors in
Rn. Prove there exist disjoint subsets T and U of S with conv(T) ∩conv(U) ̸= ∅.
81.
Fix n, k ≥0. For any convex sets C, D ⊆Rn+k, let C +n,k D be the set
{(x, y) ∈Rn × Rk : ∃u ∈Rk, ∃v ∈Rk, (x, u) ∈C, (x, v) ∈D, and y = u + v}.
Prove C +n,k D is convex. Prove +n,k is an associative, commutative binary
operation on the set of convex subsets of Rn+k.
82.
Let C, D ⊆Rn be convex sets. Define C#D = S
t∈[0,1][(1−t)C ∩tD]. Prove C#D
is convex. [Hint: Study cone(C + en+1) +n,1 cone(D + en+1).]
83.
(a) Prove: if C and D are convex cones, then C#D = C ∩D.
(b) Prove: if C and D are convex cones, then C + D = conv(C ∪D).
(c) Give examples to show that (a) and (b) can fail if C and D are convex sets
that are not cones.
84.
This exercise checks some details of the proof in §11.18.
(a) Show that D defined in (11.2) is a convex cone.
(b) Confirm identity (11.3).
(c) Confirm that C × {0}k = D ∩(Rn × {0}k).
(d) Show that since C × {0}k is a V-cone, C is a V-cone.
85.
For the set D defined in (11.5), confirm that (x, y) ∈D iff (x, y) satisfies the
linear inequalities given in §11.20.
86.
Let S = {(0, 1, 2), (0, −1, 1), (−1, 0, 2), (3, 0, 0)} ⊆R3. Sketch S, Sp(S), aff(S),
conv(S), and cone(S). Give a linear inequality defining the closed half-space of
aff(S) containing the origin. Give a description of cone(S) as an H-cone.
Affine Geometry and Convexity
315
87.
Prove: for all S ⊆Rn, conv(S) ⊆aff(S) ∩cone(S). Does equality always hold?
88.
(a) Prove the intersection of an H-cone in Rn with any linear subspace of Rn is
an H-cone. (b) Prove the image of a V-cone in Rn under a linear map is a V-cone.
(c) Can you give a direct proof of (a) for V-cones, or (b) for H-cones, without
using the theorem that V-cones and H-cones are the same?
89.
Say P ⊆Rn is a V-polyhedron iff P = conv(S) for a finite set S ⊆Rn. Prove:
(a) The intersection of finitely many V-polyhedra is a V-polyhedron.
(b) The intersection of a V-polyhedron with an affine set in Rn is a V-polyhedron.
90.
Say P ⊆Rn an H-polyhedron iff P = H1 ∩· · · ∩Hs for finitely many closed
half-spaces Hi. Prove:
(a) The image of an H-polyhedron under an affine map is an H-polyhedron.
(b) The sum of finitely many H-polyhedra is an H-polyhedron.
91.
Let S = {(−2, 1), (−1, 4), (0, −1), (1, 1), (2, 3), (3, 0)} ⊆R2. Sketch conv(S) in R2
and cone(S + e3) in R3. Express cone(S + e3) as a specific intersection of linear
half-spaces. Express conv(S) as a specific intersection of closed half-planes.
92.
Let S be the solution set in R2 of the system of linear inequalities
y −x ≤3, x ≤2, x + y ≤3, x −2y ≤4, −y ≤2, 2x + y ≤−2, −3x + y ≤3.
Find a finite set T with S = conv(T).
93.
Let S be the solution set in R3 of the linear inequalities 0 ≤z ≤2, 0 ≤y ≤z,
y ≤x ≤(y+6)/3, x−y+z ≤3. Sketch S and find a finite set T with S = conv(T).
94.
Let S = {(±3, 0, 0), (1, ±1, 0), (−1, ±1, 0), (±1/2, 0, 1), (0, ±1/2, 1)}. Draw T =
conv(S) and find a system of linear inequalities with solution set T.
95.
Let C be the V-cone cone(v1, . . . , v5) in R4, where v1 = (3, 1, 1, 1), v2 = (0, 1, 2, 2),
v3 = (1, 0, 2, 0), v4 = (−1, −1, 1, −1), v5 = (2, 2, 0, 3). Follow the proof in §11.17
to find a finite set S with C ∩(R3 × {0}) = cone(S). Find a finite set T with
C ∩(R2 × {0}2) = cone(T).
96.
Let C be the H-cone in R4 = {(w, x, y, z) : w, x, y, z ∈R} defined by the system
of inequalities
2w −x + y ≤0, w + x + 2y + 2z ≤0, x −3z ≤0, w + y −z ≤0, 2x + y + z ≤0.
Follow the proof in §11.19 to find a system of inequalities whose solution set is
the projection of C onto R3 × {0}. Find a system of inequalities defining the
projection of C onto R2 × {0}2. What is this projection?
97.
Let C be the V-cone cone(v1, . . . , v5) in R3, where
v1 = (1, 2, 1), v2 = (1, 4, 0), v3 = (4, 1, 0), v4 = (4, 3, −1), v5 = (3, 4, −1).
Sketch C and find an explicit description of C as an H-cone.
98.
Let C be the H-cone defined by the inequalities
x −3y −z ≤0, x + y −z ≤0, −11x + 7y −z ≤0, −x −3y −z ≤0.
Sketch C and find an explicit description of C as a V-cone.
99.
In Figure 11.4, let the lines L, L1, and L2 have respective slopes m, m1, and m2.
Carefully prove that (x, f(x)) is weakly below L iff m1 ≤m ≤m2. Use definitions
to prove that for convex C ⊆R, f : C →R is convex iff for all y < x < z in C,
(x, f(x)) is weakly below the line segment joining (y, f(y)) and (z, f(z)).
316
Advanced Linear Algebra
100.
Determine whether each function below is convex, concave, or neither.
(a) f(x) = −2x + 5 for x ∈R
(b) f(x) = x3 −x for x ∈R
(c) f(x) = sin2 x for x ∈R
(d) f(x) = cos x for x ∈[π/2, 3π/2]
(e) f(x) = arctan(x) for x ≥0
(f) f(x) =
√
1 −x2 for x ∈[−1, 1]
(g) f : [0, ∞) →R given by f(x) = xr, for fixed r ≥1
(h) f : [0, ∞) →R given by f(x) = xr, for fixed r ∈[0, 1)
(i) f : (0, ∞) →R given by f(x) = xr, for fixed r < 0
101.
Draw a diagram like Figure 11.4 and use it to prove that the convex function f
is left-continuous at x ∈(c, d).
102.
Give an example to show that a convex function f : [c, d] →R may not be
right-continuous at c or left-continuous at d.
103.
Suppose a < b in R, f : (c, d) →R is convex, and g : [a, b] →(c, d) is integrable.
Prove (b −a)f(
R b
a g(x) dx) ≤
R b
a f((b −a)g(x)) dx. Give an example to show that
the inequality may fail if we omit (b −a) from both sides.
104.
Let C be an open convex subset of Rn and f : C →R be a function with
continuous second-order partial derivatives. Show f is convex on C iff for all
c < d in R and all affine maps g : (c, d) →C, f ◦g : (c, d) →R is a convex
function. Let g : (c, d) →C have the formula g(t) = x + tv for t ∈(c, d), where
x, v ∈Rn are fixed. Show that f ◦g is convex iff v •Hyv ≥0 for all y in the image
of g. Using this, prove f is convex iff Hy is positive semidefinite for all y ∈C.
105.
Prove g : Rn →R, given by g(x) = ||x|| = √x • x for x ∈Rn, is convex.
106.
Let C = {x ∈Rn : x(i) > 0 for 1 ≤i ≤n}. Prove f : C →Rn, given by
f(x) = −(x(1)x(2) · · · x(n))1/n for x ∈C, is convex.
107.
Let f : C →R be convex. Prove: for all r ∈R, {x ∈C : f(x) < r} and
{x ∈C : f(x) ≤r} are convex sets. Must the sets {x ∈C : f(x) > r} and
{x ∈C : f(x) ≥r} be convex? Prove or give a counterexample.
108.
Prove: for all k ≥1 and all real z1, . . . , zk > 0, (z1+z2+· · ·+zk)/k ≥
k√z1z2 · · · zk.
(Apply Jensen’s Inequality to a certain convex function.)
109.
Let B and C be convex sets, f : C →R and g : R →R be convex functions, and
T : B →C be a linear map. (a) Prove: if g is increasing, then g ◦f is convex. (b)
Give an example to show that g ◦f may not be convex if g is not increasing. (c)
Prove: f ◦T is convex. (d) If T is only an affine map, must (c) be true? Explain.
110.
(a) Suppose f1, . . . , fk : C →R are convex functions. Prove every positive linear
combination of these functions is convex. (b) Suppose {fi : i ∈I} is a family
of convex functions with domain C such that g(x) = supi∈I fi(x) is finite for all
x ∈C. Prove g : C →R is convex.
111.
Prove: for all real p, q > 1 and r, s > 0, if p−1+q−1 = 1, then r1/ps1/q ≤r/p + s/q.
12
Ruler and Compass Constructions
The ancient Greeks investigated and solved many construction problems in plane geometry.
Here are some examples of geometric construction problems.
• Altitudes: Given a line ℓand a point P, construct the line through P that meets ℓat a
right angle.
• Parallels: Given a line ℓand a point P not on ℓ, construct the line through P that is
parallel to ℓ.
• Polygons: Given a circle with center O and a point P on the circle, construct a regular
n-sided polygon inscribed in the circle and having P as one vertex. For example, when
n = 3, 4, 5, 6, 7, the goal is to construct equilateral triangles, squares, regular pentagons,
regular hexagons, and regular heptagons.
• Angle Bisection: Given two lines ℓ1, ℓ2 that meet at a point P at an angle θ, draw a line
through P that makes an angle θ/2 with ℓ1.
• Angle Trisection: Given two lines ℓ1, ℓ2 that meet at a point P at an angle θ, draw a line
through P that makes an angle θ/3 with ℓ1.
• Cube Duplication: Given a line segment PQ, construct a line segment AB such that a
cube with AB as one side has twice the volume of a cube with PQ as one side.
• Squaring the Circle: Given a line segment PQ, construct a line segment AB such that the
square with AB as one side has the same area as the circle with PQ as radius.
To solve these construction problems, the Greeks only allowed themselves to use two
basic tools: a ruler, which can draw the line passing through any two points; and a compass,
which can draw the circle with a given center passing through a given point. The ruler cannot
be used to measure distances, and the compass cannot be used to draw several circles with
the same radius in different parts of the diagram. To emphasize these restrictions, the ruler is
sometimes called a straightedge, and the compass is sometimes called a collapsing compass.
A great variety of construction problems can be solved using only a ruler and compass.
For example, the constructions of altitudes, parallels, angle bisectors, and regular n-gons
for n = 3, 4, 5, 6, 8 are all possible. On the other hand, some of the other problems on the
preceding list are impossible to solve using only a ruler and compass. The most famous of
these problems are squaring the circle, trisecting an arbitrary angle, and duplicating the
cube. It is also impossible to construct a regular heptagon (7-sided polygon) and many
other regular polygons. One of the amazing achievements of Gauss was the discovery that
a regular 17-sided polygon can be constructed using a ruler and compass.
This chapter develops the mathematical tools needed to prove the unsolvability or
solvability of constructions with ruler and compass. Remarkably, deciding the solvability of
a given geometric construction can ultimately be translated into a linear algebra question
involving the dimensions of certain vector spaces. To see how this occurs, we must first link
the geometric operations occurring in ruler and compass constructions to various arithmetic
DOI: 10.1201/9781003484561-12
317
318
Advanced Linear Algebra
operations, including the extraction of square roots. We then use field theory to characterize
the numbers obtainable by means of these arithmetic operations. Any field K with subfield
F can be regarded as a vector space with scalars coming from F. The fact that each such
vector space has a unique dimension is the key to proving the impossibility or possibility of
various geometric constructions.
To implement this agenda, we begin by giving rigorous definitions of three different
kinds of constructible numbers — one based on geometry, another based on arithmetic, and
a third based on field theory. The core theorem of this subject asserts that the three notions
of constructibility are all equivalent. Once we prove this theorem, we use it to analyze some
of the famous construction problems mentioned above.
The main prerequisites for reading this chapter are the definitions of fields and
vector spaces (see Chapter 1), facts about the dimension of vector spaces (see §1.8), an
acquaintance with Euclidean geometry and analytic geometry, and some knowledge of
irreducible polynomials (Chapter 3).
12.1
Geometric Constructibility
We first give an informal description of ruler and compass constructions, which we then
translate into a rigorous definition. Our geometric constructions occur in a two-dimensional
Euclidean plane. We use an x, y-coordinate system to identify this plane with R2, the set
of ordered pairs (a, b) with a, b ∈R. Alternatively, we can identify the plane with the set
C of complex numbers by letting the point (a, b) correspond to the complex number a + ib.
The initial data for our geometric constructions consist of two points A and B in the plane
corresponding to the complex numbers 0 and 1. Thus, A has coordinates (0, 0), and B has
coordinates (1, 0).
We are allowed to construct new points, lines, and circles in R2 by applying a finite
sequence of actions from the following list.
• If two different points P and Q have already been constructed, we can draw the unique
line through P and Q, which we denote by L(P, Q).
• If two different points P and Q have already been constructed, we can draw the unique
circle with center P passing through Q, which we denote by C(P; Q).
• If two unequal, non-parallel lines ℓ1 and ℓ2 have been constructed, we can locate the
unique point where these two lines intersect.
• If a line ℓand a circle C have been constructed, we can locate all intersection points of
this line and this circle (if any).
• If two unequal circles C1 and C2 have been constructed, we can locate all intersection
points of these two circles (if any).
For our proofs, we need a more formal definition of geometrically constructible points.
We recursively define a set GC of complex numbers, called the geometrically constructible
numbers, by the following rules.
G0.
0 ∈GC and 1 ∈GC.
G1.
If P, Q, R, S ∈GC, L(P, Q) and L(R, S) are unequal lines, and L(P, Q) ∩L(R, S) =
{T}, then T ∈GC.
Ruler and Compass Constructions
319
G2.
If P, Q, R, S ∈GC and T ∈L(P, Q) ∩C(R; S), then T ∈GC.
G3.
If P, Q, R, S ∈GC, C(P; Q) and C(R; S) are unequal circles, and T ∈C(P; Q) ∩
C(R; S), then T ∈GC.
G4.
The only numbers in GC are those that can be obtained by applying rules G0,
G1, G2, and G3 a finite number of times.
We can also rephrase this recursive definition in the following iterative fashion. A
complex number Q is in GC iff there is a finite sequence of points
P0 = 0, P1 = 1, P2, P3, . . . , Pk = Q,
where k ∈Z≥0, and for all i with 2 ≤i ≤k, there exist r, s, t, u < i such that Pi is in
L(Pr, Ps) ∩L(Pt, Pu) or in L(Pr, Ps) ∩C(Pt; Pu) or in C(Pr; Ps) ∩C(Pt; Pu). (In the first
and third alternatives, we require that the two lines or circles do not coincide.) We call the
sequence P0, P1, . . . , Pk a geometric construction sequence for Q.
For example, the sequence 0, 1, −1, i
√
3,
√
3 is a geometric construction sequence for
√
3,
so
√
3 ∈GC. To verify this, first note that −1 is a point in the intersection of the line L(0, 1)
(the x-axis) and the circle C(0; 1) (the unit circle). Next, i
√
3 is one of the two intersection
points of C(1; −1) and C(−1; 1), since these circles have radius 2 and the point (0,
√
3) has
distance 2 from both (−1, 0) and (1, 0). Finally,
√
3 is in C(0; i
√
3) ∩L(0, 1).
12.2
Arithmetic Constructibility
Next, we define a notion of constructibility involving arithmetic operations. We consider
complex numbers that can be built up from the integers by performing the standard
arithmetic operations (addition, subtraction, multiplication, and division in C) together
with the operation of extracting square roots. Such numbers appear naturally, for example,
in connection with the quadratic formula. Recall that for a, b, c ∈C with a nonzero, the
roots of the equation ax2 + bx + c in C are
r1 = −b +
√
b2 −4ac
2a
,
r2 = −b −
√
b2 −4ac
2a
.
We see that these roots can be built from the initial data a, b, c by performing arithmetic
operations and taking square roots.
To formalize this idea, we recursively define a set AC of complex numbers, called the
arithmetically constructible numbers, by the following rules.
A0.
0 ∈AC and 1 ∈AC.
A1.
If a, b ∈AC, then a + b ∈AC.
A2.
If a ∈AC, then −a ∈AC.
A3.
If a, b ∈AC, then a · b ∈AC.
A4.
If a ∈AC is nonzero, then a−1 ∈AC.
A5.
If a ∈AC and b ∈C and b2 = a, then b ∈AC.
A6.
The only numbers in AC are those that can be obtained by applying rules A0
through A5 a finite number of times.
320
Advanced Linear Algebra
A less formal phrasing of rule A5 says that if a ∈AC then √a ∈AC. Since every nonzero
complex number has two complex square roots, the notation √a used here is ambiguous.
However, this is not a serious difficulty, since rule A2 guarantees that both square roots of
a must belong to AC.
We can give an iterative formulation of the recursive definition of AC, as follows. A
complex number z is in AC iff there is a finite sequence of numbers
x0 = 0, x1 = 1, x2, x3, . . . , xk = z,
where k ∈Z≥0, and for all i with 2 ≤i ≤k, either there exists r < i with xi = −xr or
xi = x−1
r
(xr ̸= 0) or xi = √xr, or there exist r, s < i with xi = xr + xs or xi = xr · xs. The
sequence x0, x1, . . . , xk is called an arithmetic construction sequence for z.
For example, an arithmetic construction sequence for i
√
3 is 0, 1, 2, 3, −3, i
√
3, hence
i
√
3 ∈AC. To build this sequence, we use rule A0 twice, then rule A1 twice, then rule A2,
then rule A5. For a more elaborate illustration of arithmetic construction sequences, suppose
a, b, c ∈AC with a ̸= 0. Then the roots r1, r2 of the quadratic equation ax2 + bx + c are
also in AC. For instance, we prove that r1 ∈AC by producing an arithmetic construction
sequence as follows. Begin the sequence with 0, 1, 2, 3, 4, followed by the concatenation of
arithmetic construction sequences for a, b, and c. Continue this sequence as follows:
b2, ac, 4ac, −4ac, b2 −4ac,
p
b2 −4ac, −b, −b +
p
b2 −4ac, 2a, r1.
You can check that each new term listed here is produced from one or two previous terms
by invoking some rule A0 through A5.
12.3
Preliminaries on Field Extensions
To continue, we need some facts about field extensions. Let K be a field, as defined in §1.2.
Recall (§1.4) that a subfield of K is a subset F of K such that 0K ∈F, 1K ∈F, and
for all a, b ∈F, we have a + b ∈F, −a ∈F, a · b ∈F, and if a ̸= 0K, then a−1 ∈F.
(Note the similarity of these closure conditions to rules A0 through A4.) A subfield F of
K becomes a field by restricting the sum and product operations on K to the subset F.
We say K is an extension field of F iff F is a subfield of K. A chain of fields is a sequence
F0 ⊆F1 ⊆F2 ⊆· · · ⊆Fn in which Fn is a field, and each Fi (for 0 ≤i < n) is a subfield of
Fn. In this chapter, the fields under consideration are subfields of C, so the operations in
these fields are ordinary addition and multiplication of complex numbers.
Suppose K is any field with subfield F. A key observation is that we can regard K
as a vector space over the field F, by taking vector addition to be the field addition
+ : K × K →K, and taking scalar multiplication s : F × K →K to be the restriction
of the field multiplication · : K ×K →K to the domain F ×K. In other words, we multiply
a scalar c ∈F by a vector v ∈K by forming the product c · v in the field K. The vector
space axioms follow by comparing the axioms for the field K (and subfield F) to the axioms
for an F-vector space. Like any other F-vector space, K has a unique dimension viewed as
a vector space over F. This dimension is written [K : F] and is called the degree of K over
F. For example, K = C is a two-dimensional vector space over its subfield F = R (since
(1, i) is an ordered basis of the real vector space C), so the degree [C : R] is 2. On the other
hand, [R : Q] = ∞, since R is uncountable but every finite-dimensional Q-vector space is
countable. (More precisely, [R : Q] is a certain infinite cardinal giving the Q-dimension of R.
However, for our purposes in this chapter, we do not need this extra precision. The notation
Ruler and Compass Constructions
321
[K : F] = ∞means that K is an infinite-dimensional F-vector space.) The following formula
plays a critical role in all subsequent developments.
Degree Formula for Field Extensions. If F ⊆K ⊆E is a chain of three fields, then
[E : F] = [E : K][K : F].
Proof. We assume in this proof that [E : K] = m and [K : F] = n for some positive
integers m and n. It can be shown (Exercise 20) that if [E : K] = ∞or [K : F] = ∞, then
[E : F] = ∞. Since [E : K] = m, there exists an ordered basis B = (y1, y2, . . . , ym) for the
K-vector space E. We call B a K-basis of E to emphasize that scalars for this space come
from K. Similarly, since [K : F] = n, there is an ordered F-basis C = (z1, . . . , zn) for the
F-vector space K. Consider the list of all products zjyi:
D = (z1y1, z1y2, . . . , z1ym, z2y1, . . . , z2ym, . . . , zny1, . . . , znym).
D is a list of mn elements, which are not yet known to be distinct. We claim D is an ordered
F-basis of E; the needed conclusion [E : F] = mn follows from this claim.
First, we prove that D spans E as an F-vector space. Given any w ∈E, we can write
w = Pm
i=1 kiyi for certain scalars ki ∈K, since B is a K-basis of E. Since each ki ∈K, and
since C is an F-basis of K, we can find further scalars fij ∈F such that ki = Pn
j=1 fijzj
for 1 ≤i ≤m. Inserting these expressions into the formula for w and simplifying, we obtain
w =
m
X
i=1


n
X
j=1
fijzj

yi =
m
X
i=1
n
X
j=1
fij(zjyi).
We have expressed an arbitrary vector w ∈E as an F-linear combination of the elements
of the list D, so this list does span the F-vector space E.
Next, we prove that the list D is F-linearly independent. Assume cij ∈F are scalars for
which Pm
i=1
Pn
j=1 cij(zjyi) = 0. We must prove all cij are 0. Using the distributive laws to
regroup terms, our assumption can be written
0 =
m
X
i=1


n
X
j=1
cijzj

yi.
Each term in parentheses belongs to the field K, since zj ∈K and cij ∈F ⊆K. The list
B = (y1, . . . , ym) is known to be K-linearly independent, so we deduce that Pn
j=1 cijzj = 0
for all i with 1 ≤i ≤m. Now we can invoke the known F-linear independence of the list
C = (z1, . . . , zn) to see that cij = 0 for all i, j with 1 ≤i ≤m and 1 ≤j ≤n.
Consider a chain of finitely many fields, say
F0 ⊆F1 ⊆F2 ⊆F3 ⊆· · · ⊆Fn.
Iteration of the previous result gives the General Degree Formula
[Fn : F0] = [Fn : Fn−1] · · · [F3 : F2][F2 : F1][F1 : F0] =
n
Y
i=1
[Fi : Fi−1].
The last preliminary concept we need is the notion of a field extension generated by
one element. Suppose K is a field with subfield F, and let z ∈K. Define F(z), the field
extension of F obtained by adjoining z, to be the intersection of all subfields L of K such
322
Advanced Linear Algebra
that F ⊆L and z ∈L. You can check that F(z) is a subfield of K with F ⊆F(z) and
z ∈F(z); and for any subfield M of K with F ⊆M and z ∈M, F(z) ⊆M. You can also
verify that F(z) consists of all elements of K that can be written in the form f(z)g(z)−1,
for some polynomials f, g with coefficients in F such that g(z) ̸= 0. In this chapter, we are
mainly interested in the case where K = C and F = Q, so Q(z) is the smallest subfield of
the complex numbers containing the given complex number z.
12.4
Field-Theoretic Constructibility
Our third notion of constructibility involves field extensions of degree 2. We say that a
complex number z has a square root tower iff there is a chain of fields
Q = F0 ⊆F1 ⊆· · · ⊆Fn ⊆C
such that z ∈Fn and [Fk : Fk−1] = 2 for 1 ≤k ≤n. Let SQC denote the set of all z ∈C
such that z has a square root tower. We claim:
If z ∈SQC, then [Q(z) : Q] = 2e for some e ≥0.
(12.1)
To prove this, first use the Degree Formula to see that [Fn : Q] = 2n. Since z ∈Fn and
Q ⊆Fn, we have a chain of fields Q ⊆Q(z) ⊆Fn. It follows from the Degree Formula that
[Q(z) : Q] divides [Fn : Q] = 2n, so that [Q(z) : Q] must be a power of 2. It can be shown
that the converse of (12.1) is not true in general. However, the contrapositive of (12.1) is
certainly true:
If [Q(z) : Q] is not a power of 2, then z ̸∈SQC.
(12.2)
We can now precisely state the main goal of this chapter.
Constructibility Theorem. GC = AC = SQC. In other words, a complex number z is
geometrically constructible iff z is arithmetically constructible iff z has a square root tower.
Once this theorem is known, we can use the criterion (12.2) to prove the unsolvability
of various geometric construction problems. In fact, for such applications, it suffices to
know the weaker result GC ⊆AC ⊆SQC. Similarly, we can demonstrate the solvability of
certain geometric constructions by building appropriate square root towers. We illustrate
this technique in §12.9 and §12.10, after we prove the Constructibility Theorem GC = AC =
SQC in the intervening sections.
12.5
Proof that GC ⊆AC
In this section, we give the formal proof that GC ⊆AC. The details are somewhat lengthy,
but the intuition behind the proof can be concisely summarized as follows. When computing
intersections of lines and circles via analytic geometry, the coordinates of newly located
points can be computed from coordinates of previous points by arithmetic operations and
extractions of square roots. This is the key conceptual feature of the formulas derived here.
Our proof requires facts from plane analytic geometry, labeled AG1 through AG5.
Ruler and Compass Constructions
323
AG1.
For every line ℓin the plane, there exist real numbers a, b, s such that
ℓ= {(x, y) ∈R2 : ax + by = s},
where a and b are not both 0. If ℓ= L(P, Q) where P = (x0, y0) and Q = (x1, y1),
then we may take a = y1 −y0, b = x0 −x1, and s = ax0 + by0 = ax1 + by1. So if
x0, y0, x1, y1 ∈AC, then we can choose a, b, s so that a, b, s ∈AC.
AG2.
If C is a circle in R2 with center (c, d) and radius r > 0, then
C = {(x, y) ∈R2 : (x −c)2 + (y −d)2 = r2}.
If (e, f) is any point on C, then r2 = (e−c)2 +(f −d)2. So if c, d, e, f ∈AC, then
r2 and r belong to AC.
AG3.
Consider lines ℓ1 and ℓ2 with equations ax + by = r and cx + dy = s. These lines
are not parallel iff ad −bc ̸= 0. In this case, the unique point (x0, y0) where ℓ1
and ℓ2 intersect is given by Cramer’s Rule (see §5.12):
x0 = rd −bs
ad −bc,
y0 = as −rc
ad −bc.
So if a, b, c, d, r, s ∈AC, then x0, y0 ∈AC.
AG4.
Suppose a line ℓhas equation ax+by = s, and a circle C has equation (x−c)2+(y−
d)2 = r2. A point (x, y) is on both ℓand C iff both equations hold simultaneously.
Suppose that b is nonzero. Substituting y = (s −ax)/b into the equation of the
circle, we see that x must satisfy the equation (x −c)2 + ((s −ax)/b −d)2 = r2.
This equation can be rewritten as Ax2 + Bx + D = 0, where
A = 1 + a2
b2 ,
B = −2c + 2ad
b
−2as
b2 ,
D = c2 + d2 −r2 −2ds
b
+ s2
b2 .
So x is a root of a linear or quadratic equation with the indicated coefficients,
and y = (s −ax)/b is determined by x. If a, b, s, c, d, r ∈AC, then we see from
these formulas that A, B, D ∈AC and x, y ∈AC. A similar analysis holds when
a is nonzero.
AG5.
Suppose C1 and C2 are circles with equations (x −a)2 + (y −b)2 = r2 and
(x −c)2 + (y −d)2 = s2. A point (x, y) is on both circles iff both equations hold
simultaneously. Subtracting the second equation from the first, we obtain a third
equation
(2c −2a)x + (2d −2b)y = c2 + d2 + r2 −a2 −b2 −s2.
Assuming that the circles are non-concentric, this is the equation of a line ℓ. The
first and second equations have the same solution set as the second and third
equations, as you can check. It follows that the intersection points of C1 and C2
are the same as the intersection points of ℓand C2. So we can find the coordinates
of these points using the formulas from the preceding item. If a, b, c, d, r, s ∈AC,
then the coefficients of the third equation are all in AC, so the coordinates x and
y of the intersection point(s) also belong to AC.
We now prove that GC ⊆AC. Given z = (x, y) = x + iy ∈GC, we know there is a
geometric construction sequence z0, z1, . . . , zk with last term z. Write zr = xr + iyr with
xr, yr ∈R for 0 ≤r ≤k. We show that x = xk and y = yk are in AC by strong induction
on k. Since i ∈AC, it follows that z = x + iy is in AC. If k = 0, then z0 = 0 = 0 + 0i, and 0
is in AC. If k = 1, then z1 = 1 = 1 + 0i, and 0, 1 ∈AC. Fix k > 1. We make the induction
hypothesis that for all r < k, xr ∈AC and yr ∈AC. Consider the three possible cases for
zk.
324
Advanced Linear Algebra
• Case G1: There exist r, s, t, u < k such that zk = (xk, yk) is the unique point of intersection
of the line ℓ1 through (xr, yr) and (xs, ys) and the line ℓ2 through (xt, yt) and (xu, yu).
Since xr, yr, xs, ys, xt, yt, xu, yu are all known to be in AC by the induction hypothesis, we
see that xk and yk are in AC by AG1 and AG3.
• Case G2: There exist r, s, t, u < k such that zk = (xk, yk) belongs to both the line ℓthrough
(xr, yr) and (xs, ys) and the circle C with center (xt, yt) passing through (xu, yu). Since
xr, yr, xs, ys, xt, yt, xu, yu are known to be in AC by the induction hypothesis, we see that
xk and yk are in AC by AG1, AG2, and AG4.
• Case G3: There exist r, s, t, u < k such that zk = (xk, yk) belongs to both the circle C1
through (xs, ys) with center (xr, yr) and the circle C2 through (xu, yu) with center (xt, yt).
Since xr, yr, xs, ys, xt, yt, xu, yu are known to be in AC by the induction hypothesis, we
see that xk and yk are in AC by AG2 and AG5.
12.6
Proof that AC ⊆GC
We proved that GC is contained in AC by showing that the coordinates of the intersections
of lines and circles can always be found from previous coordinates by arithmetic operations
and square root extractions. To prove the reverse containment, we must show the converse:
arithmetic operations and square root extractions can be performed geometrically with
a ruler and compass. We give the necessary geometric constructions (facts CR1 through
CR12) followed by the proof that AC ⊆GC. Each construction can be justified by the
axioms and theorems of Euclidean geometry, but we omit the detailed justifications here.
CR1.
Given distinct points A and B, we can construct an equilateral triangle with side
AB, the midpoint of AB, and the perpendicular bisector of AB. First draw the
line connecting A and B, then draw the circle centered at A passing through B,
then draw the circle centered at B passing through A. The two circles must meet1
in two points, say C and D. Then ∆ABC and ∆ABD are equilateral triangles,
←→
CD is the perpendicular bisector of AB, and the intersection of
←→
CD and
←→
AB is
the midpoint of AB.
E
A
B
C
D
1A careful axiomatic development of Euclidean geometry would include a proof that the claimed
intersection points really do exist. We omit this.
Ruler and Compass Constructions
325
CR2.
Given three points A, B, C with A ̸= B, we can construct the unique altitude
through C perpendicular to
←→
AB. Choose notation so that C ̸= A. Draw
←→
AB and
the circle with center C passing through A. If this circle meets
←→
AB at a second
point A′ ̸= A, then the required altitude is the perpendicular bisector of AA′,
which can be built by CR1. If, instead, the circle is tangent to
←→
AB at A, the
required altitude is
←→
AC. The construction works whether or not C is on the line
←→
AB.
B
C
A
A’
CR3.
Given three points A, B, C with A ̸= B, we can construct the unique line through
C parallel to
←→
AB. Apply CR2 once to get a line
←→
CD perpendicular to
←→
AB. Apply
CR2 again to get a line
←→
CE perpendicular to
←→
CD, hence parallel to
←→
AB.
E
C
A
B
D
CR4.
Given the initial points 0 and 1, we can draw the x-axis and the y-axis. The x-axis
is the line through 0 and 1. Now use CR2 to draw the y-axis.
x
0
1
y
CR5.
For all real x, y, x + iy = (x, y) is in GC iff x = (x, 0) is in GC and y = (y, 0) is
in GC. If x + iy = (x, y) ∈GC, we can use CR2 to drop altitudes to the x-axis
and the y-axis to find the points (x, 0) and (0, y). Drawing the circle centered at
326
Advanced Linear Algebra
0 and passing through (0, y) allows us to locate (y, 0). The converse statement is
proved similarly, by reversing these steps.
x
0
(x,y)
iy
y
The next constructions show how to implement real arithmetic using a ruler and
compass. We can then handle complex arithmetic by using CR5 to reduce to the real case.
CR6.
For all real x, y ∈GC, x + y ∈GC and x −y ∈GC. Given x, y, locate the point
(x, y) = x + iy as in CR5. The circle with center x passing through x + iy meets
the real axis at the points x + y and x −y. Hence x + y and x −y are in GC.
(x,y)
0
iy
y
x
x + y
x − y
CR7.
For all real y ∈GC, −y ∈GC. Take x = 0 in CR6.
CR8.
For all real x, y ∈GC, xy ∈GC. Drawing circles with center 0 through 1 and
x, we can locate i and ix on the imaginary axis. Draw the line through i and y.
Then use CR3 to draw the line through ix parallel to this line. Let z be the point
where this line meets the real axis. By considering the two similar right triangles
in the figure, we see that y/1 = z/x, so that z = xy. Our figure assumes that x
and y are positive. The reader can adapt this construction to the case of negative
variables (alternatively, this can be deduced from the positive case using CR7).
z = xy
0
y
x
ix
i
Ruler and Compass Constructions
327
CR9.
For all nonzero real x ∈GC, x−1 ∈GC. Draw i and the line through x and i.
Then draw the line through 1 parallel to this line, which meets the imaginary axis
in some point iz. Use a circle centered at 0 to find the real point z. Comparing
similar right triangles again, we see that z/1 = 1/x, so z = x−1 is in GC.
iz
0
1
x
i
z
CR10.
For all positive real x ∈GC, √x ∈GC. Draw −1, and then construct the midpoint
P of the line segment joining −1 and x. Draw the circle through x with center P,
and let iy be the point where this circle meets the imaginary axis. We can then
construct y, which we claim is the positive square root of x. To see this, draw line
segments from iy to −1 and from iy to x. The angle formed by these segments
at iy is a right angle, since it is inscribed in a semicircle. It readily follows that
the right triangle with vertices 0, −1, and iy is similar to the right triangle with
vertices 0, iy, and x. So 1/y = y/x, which shows that y2 = x and y = √x.
iy
0
x
P
−1
CR11.
For all complex u, v ∈GC, we have u + v ∈GC, −u ∈GC, uv ∈GC, and (if
u ̸= 0) u−1 ∈GC. Write u = a + ib and v = c + id where a, b, c, d ∈R. By CR5,
the real numbers a, b, c, d are all in GC. Now,
u + v = (a + c) + i(b + d),
−u = (−a) + i(−b),
uv = (ac −bd) + i(ad + bc),
u−1 =

a
a2 + b2

+ i

−b
a2 + b2

.
Using CR6, CR7, CR8, and CR9, we see that the real and imaginary parts of all
these expressions are in GC. Hence, another application of CR5 shows that u+v,
−u, uv, and u−1 are all in GC.
CR12.
For all complex z ∈GC, √z ∈GC. Write z = reiθ in polar form, where r ≥0
and θ are real numbers. Drawing the circle through z with center 0, we see that
r = |z| ∈GC. By CR10, we can construct the real square root of r. By CR1,
we can construct the perpendicular bisector of the line segment joining z and
r. Then the points w where this bisector intersects the circle with center 0 and
328
Advanced Linear Algebra
radius √r are in GC. You can check that w = ±√reiθ/2 in polar form, and these
are the two complex square roots of z. Thus both square roots are in GC.
1/2
0
|z|
z
z1/2
|z|
With these tools in hand, we prove AC ⊆GC as follows. Given z ∈AC, we know there
is an arithmetic construction sequence z0, z1, . . . , zk with last term z. We proceed by strong
induction on k. If k = 0, then z = 0 is in GC. If k = 1, then z = 1 is in GC. Given k > 1, we
may assume the induction hypothesis that z0, . . . , zk−1 are all in GC. Now if zk = zr + zs
for some r, s < k, then zk ∈GC follows by CR11. The same conclusion holds if zk = −zr or
zk = zrzs or zk = 1/zr where r, s < k. Finally, if zk = √zr for some r < k, then zk ∈GC
by CR12. Thus, in all cases, z = zk is in GC.
12.7
Algebraic Elements and Minimal Polynomials
Before proving that AC = SQC, we need some more facts about field extensions of the form
F(z). Suppose K is a field, F is a subfield of K, and z ∈K. We say that z is algebraic over
F iff there exists a nonzero polynomial g ∈F[x] satisfying g(z) = 0K.
Theorem on Minimal Polynomials. Suppose F is a subfield of K and z ∈K is algebraic
over F. There exists a unique monic, irreducible polynomial m ∈F[x] such that m(z) = 0K.
No polynomial in F[x] of smaller degree than m has z as a root.
The polynomial m is called the minimal polynomial of z over F.
Proof. Because z ∈K is algebraic over F, there exist nonzero polynomials in F[x] having z
as a root. Pick a nonzero polynomial m ∈F[x] of least degree such that m(z) = 0. Dividing
by the leading coefficient if needed, we can assume that m is monic. We claim m must be an
irreducible polynomial in F[x], meaning there is no factorization m = fh with f, h ∈F[x]
each having lower degree than m. If such a factorization did exist, then evaluating both
sides at z would give 0K = m(z) = f(z)h(z). Since K is a field, this forces f(z) = 0K or
h(z) = 0K. Either possibility contradicts minimality of the degree of m.
Now we prove the uniqueness of m. To get a contradiction, assume p ̸= m is another
monic, irreducible polynomial in F[x] with p(z) = 0K. Since p and m are non-associate
irreducible polynomials, gcd(p, m) = 1. It follows (see Chapter 3) that pr+ms = 1 for some
r, s ∈F[x]. Evaluating both sides at z and recalling that p(z) = 0K = m(z), we obtain
0K = 1K. This contradicts the definition of a field.
We can use minimal polynomials to give an explicit basis for the field extension F(z),
viewed as an F-vector space, in the case where z is algebraic over F.
Ruler and Compass Constructions
329
Theorem on a Basis for F(z). Suppose K is a field with subfield F, and z ∈K is algebraic
over F with minimal polynomial m ∈F[x] of degree d. Then B = (1, z, z2, . . . , zd−1) is an
ordered F-basis for F(z), and [F(z) : F] = d = deg(m).
Proof. Let W be the subspace of the F-vector space K spanned by the vectors in the list
B. W can also be described as the set of elements of the form g(z), where g ∈F[x] is
either 0 or has degree less than d. We first show that B is an F-linearly independent list.
Suppose c0, c1, . . . , cd−1 ∈F satisfy c01 + c1z + · · · + cd−1zd−1 = 0K. Then the polynomial
g = c0 + c1x + · · · + cd−1xd−1 is in F[x] and has z as a root. Since no polynomial of
degree less than d = deg(m) has z as a root, it must be that g is the zero polynomial, so
c0 = c1 = · · · = cd−1 = 0. This shows that B is an ordered basis for the F-vector space W.
To finish the proof, we show that W = F(z). On one hand, since F(z) is a subfield of
K containing z and F, closure under sums and products shows that F(z) must contain all
powers of z and all F-linear combinations of these powers. In particular, W ⊆F(z). To
prove the reverse inclusion F(z) ⊆W, it suffices to show that W is a subfield of K with
F ⊆W and z ∈W. Keeping in mind that W is the set of F-linear combinations of the
powers of z in B, it is routine to check that W contains F (hence contains 0K and 1K), that
z ∈W, and that W is closed under addition, additive inverses, and scalar multiplication by
elements of F.
We check that W is closed under multiplication. Consider two arbitrary elements u, v ∈
W, which have the form u = g(z) and v = h(z) for certain polynomials g, h ∈F[x] that
are either 0 or have degree less than d. Note uv = g(z)h(z) = (gh)(z), where the product
polynomial gh ∈F[x] may have degree as high as 2d−2. We now show that for all p ∈F[x]
of any degree, p(z) ∈W; it follows that uv = (gh)(z) is indeed in W. First we use induction
on n to prove that zn ∈W for all n ≥0. This is true by definition of W for 0 ≤n < d.
Fix n ≥d, and assume as induction hypothesis that z0, z1, . . . , zn−1 belong to W. Let the
minimal polynomial of z over F be m = a0 + a1x + · · · + ad−1xd−1 + xd, where all ai ∈F.
Multiplying by xn−d and evaluating at x = z, we find that
zn = −a0zn−d −a1zn−d+1 −· · · −ad−1zn−1.
By induction hypothesis, each power of z on the right side is in W. So zn, which is an
F-linear combination of these powers, is also in W. It now follows that p(z), which is an
F-linear combination of powers of z, belongs to W.
To finish the proof that W is a subfield of K, we check that W is closed under
multiplicative inverses. Fix a nonzero element u ∈W, which has the form u = g(z) for
some nonzero g ∈F[x] of degree less than d. As m is irreducible and deg(g) < deg(m),
we must have gcd(g, m) = 1, and so 1 = gr + ms for some r, s ∈F[x]. Evaluating both
sides at z gives 1K = g(z)r(z) + m(z)s(z) = ur(z). Now, as seen in the previous paragraph,
r(z) ∈W; and ur(z) = 1 shows that u−1 = r(z) ∈W.
Theorem on Algebraic Elements and Finite Degree Extensions. Let F be a subfield
of K and z ∈K. If z is algebraic over F, then the degree [F(z) : F] is finite and equals the
degree of the minimal polynomial of z over F. Conversely, if d = [F(z) : F] is finite, then z
is algebraic over F with minimal polynomial having degree d.
Proof. The first statement follows from the previous theorem. For the converse, assume
d = [F(z) : F] is finite. Consider the list of d + 1 vectors (1, z, z2, . . . , zd) in the F-vector
space F(z). Since F(z) is d-dimensional, this list must be linearly dependent over F (see
§1.8). So there exist scalars c0, c1, . . . , cd ∈F, not all zero, with Pd
i=0 cizi = 0K. The
polynomial Pd
i=0 cixi ∈F[x] is nonzero and has z as a root. So z is algebraic over F. By
the first part of the theorem, the minimal polynomial of z over F has degree d.
330
Advanced Linear Algebra
Here is an example needed in our subsequent discussion of SQC. Suppose F is a subfield
of K, z is in K but not in F, and y = z2 is in F. Then [F(z) : F] = 2 and (1, z) is an
ordered F-basis of the F-vector space F(z). Indeed, since x2 −z2 = x2 −y is a nonzero
polynomial in F[x] having z as a root, we see that z is algebraic over F. The polynomial
x2 −y = (x −z)(x + z) does not factor into linear factors in F[x], since z is not in F. It
follows that x2 −y is irreducible in F[x]. This polynomial is also monic and has z as a root,
so x2 −y is the minimal polynomial of z over F. The stated conclusions now follow from
the Theorem on a Basis for F(z), since the minimal polynomial has degree 2. For instance,
taking F = Q and z =
√
2, we see that Q(
√
2) is a 2-dimensional Q-vector space with
ordered basis (1,
√
2). So, every element of the field Q(
√
2) has a unique representation of
the form a + b
√
2 for some a, b ∈Q.
12.8
Proof that AC = SQC
We now prove that AC ⊆SQC, meaning that every arithmetically constructible complex
number has a square root tower. Let z ∈AC have an arithmetic construction sequence
z0, z1, . . . , zk. We use this sequence to build a square root tower for z, as follows. Start with
Q = K0, which contains z0 = 0 and z1 = 1. Suppose that, for a fixed i ≤k, we already have
a chain of fields
Q = K0 ⊆K1 ⊆K2 ⊆· · · ⊆Ks ⊆C
such that each [Kj : Kj−1] = 2 and all the elements z0, z1, . . . , zi−1 belong to Ks. Consider
the possible cases for zi. If zi is the sum or product of two preceding elements in the
sequence, or if zi is the additive inverse or multiplicative inverse of a preceding element,
then zi is in Ks because Ks is a subfield of C. The only other possibility is that z2
i = zr for
some r < i. If zi happens to belong to Ks already, we can keep the current chain of fields.
Otherwise, the example at the end of the last section applies to show that Ks+1 = Ks(zi)
is a field extension of Ks that contains zi and satisfies [Ks+1 : Ks] = 2. Thus, the recursive
construction of the chain of fields can continue. Proceeding in this way taing i = 2, 3, . . . , k,
we eventually obtain a square root tower for z, so that z ∈SQC.
To prove SQC ⊆AC, we need one final lemma about field extensions. Suppose F ⊆K
are subfields of C such that [K : F] = 2. We assert that there is w ∈K such that w ̸∈F,
w2 ∈F, K = F(w), and (1, w) is an ordered basis for the F-vector space K. Start by picking
any v ∈K with v ̸∈F. We have a chain of fields F ⊆F(v) ⊆K, so the Degree Formula
gives 2 = [K : F] = [K : F(v)][F(v) : F]. Since v ̸∈F, the field F(v) properly contains F,
which forces [F(v) : F] = 2, [K : F(v)] = 1, and hence K = F(v). The minimal polynomial
of v exists and has degree 2 = [F(v) : F]. Let this minimal polynomial be x2 + bx + c with
b, c ∈F, so v2 + bv + c = 0K. Completing the square gives (v + b/2)2 = b2/4 −c ∈F. Let
w = v + b/2. We have w ∈K, w2 ∈F, and w ̸∈F, since otherwise v = w −b/2 would
be in F. The same reasoning used for v shows that K = F(w) and [F(w) : F] = 2. By
the Theorem on a Basis of F(z), (1, w) is an F-basis of K, and every element of K can be
written uniquely in the form a + dw for some a, d ∈F.
Now suppose z ∈SQC, and let
Q = K0 ⊆K1 ⊆K2 ⊆· · · ⊆Kt ⊆C
be a square root tower for z. We show by induction on j that Kj ⊆AC for 0 ≤j ≤t,
which implies z ∈AC since z ∈Kt. First consider K0 = Q. Since 0 ∈AC, 1 ∈AC, and
AC is closed under addition, every positive integer n is in AC. So all integers are in AC by
Ruler and Compass Constructions
331
A2, hence all rational numbers are in AC by A3 and A4. For the induction step, assume
0 ≤j < t and Kj ⊆AC. Use the lemma to get w ∈Kj+1 such that w2 ∈Kj and every
element of Kj+1 has the form a + dw for some a, d ∈Kj. Now w2 ∈Kj ⊆AC, so A5 shows
that w ∈AC. Then the induction hypothesis, A1, and A3 show that a + dw ∈AC for all
a, d ∈Kj. So Kj+1 ⊆AC, completing the induction.
12.9
Impossibility of Geometric Construction Problems
In this section, we apply (12.2) to prove that the problems of duplicating a cube, squaring
a circle, inscribing a regular heptagon in a circle, and trisecting a 60◦angle are unsolvable
with ruler and compass. To use (12.2), we compute [Q(z) : Q] by finding the degree of
the minimal polynomial of z over Q. To check that a given polynomial m ∈Q[x] is the
minimal polynomial of a given z ∈C, it suffices to verify that m is monic, m(z) = 0, and
m is irreducible in Q[x]. The following results (proved in Chapter 3) can help establish the
irreducibility of m.
First, given m ∈Q[x], there is a nonzero c ∈Q with cm ∈Z[x], and m is irreducible
in Q[x] iff cm is irreducible in Q[x]. So we need only test irreducibility of polynomials with
integer coefficients. If the new polynomial is cm = anxn+· · ·+a1x+a0 (where each ai ∈Z),
the Rational Root Theorem states that any rational root of cm (or of m) must have the
form p/q, where p divides a0 and q divides an in Z. Thus, we can find all rational roots
of the polynomial by checking finitely many possibilities. A polynomial of degree 2 or 3 in
Q[x] with no rational roots is automatically irreducible in Q[x].
Another criterion for irreducibility in Q[x] involves reduction of the coefficients modulo
a prime p. Suppose cm = anxn + · · · + a1x + a0 ∈Z[x] where an is not divisible by
p. Consider the polynomial q = (an mod p)xn + · · · + (a1 mod p)x + (a0 mod p) ∈Zp[x]
obtained by reducing each integer coefficient modulo p. If the polynomial q is irreducible in
Zp[x] for some prime p, then the original polynomial cm is irreducible in Q[x]. Irreducibility
in the polynomial ring Zp[x] can be checked algorithmically since there are only finitely
many possible factors of q. But this sufficient condition for irreducibility is not a necessary
condition: there exist irreducible polynomials in Q[x] whose reductions modulo every prime
are reducible in Zp[x]. There is an algorithm (due to Kronecker) for testing in finitely many
steps whether an arbitrary polynomial in Z[x] is irreducible in Q[x]; see §3.19.
We can now analyze the construction problems mentioned at the beginning of this
section. First consider duplication of the cube. Given a cube with side length 1, the cube
with twice the volume has side length
3√
2. You can check that if there exist P, Q ∈GC such
that the line segment PQ has length
3√
2, then the real number
3√
2 must also belong to GC.
The minimal polynomial of
3√
2 is x3 −2 ∈Q[x]. This polynomial is irreducible in Q[x] since
it has degree 3 and has no rational roots (noting that ±1, ±2 are not roots). Observe that
[Q(
3√
2) : Q] = deg(x3 −2) = 3 is not a power of 2. By (12.2),
3√
2 is not in GC.
Next, consider the problem of trisecting an angle of measure θ = 60◦. We can assume
that the angle in question is formed by the x-axis and the line y = x
√
3 passing through
(cos 60◦, sin 60◦). If this angle could be trisected, then (cos 20◦, sin 20◦) ∈GC, so (by CR5)
the real number α = cos 20◦would be in GC. We compute the minimal polynomial of α
in Q[x] as follows. First, repeated use of the trigonometric addition formulas leads to the
identity cos(3t) = 4(cos t)3 −3 cos t. Letting t = 20◦, we see that 1/2 = 4α3 −3α. It follows
that α is a root of the monic polynomial x3 −(3/4)x −1/8 ∈Q[x], and α is a root of
the rescaled polynomial 8x3 −6x −1 with integer coefficients. The only possible rational
roots of these polynomials are ±1, ±1/2, ±1/4, and ±1/8, but none of these are roots.
332
Advanced Linear Algebra
Since these polynomials have degree 3 with no roots in Q, they are irreducible in Q[x].
So x3 −(3/4)x −1/8 is the minimal polynomial of α over Q, [Q(α) : Q] = 3, and hence
cos 20◦̸∈GC by (12.2).
Next, consider the problem of inscribing a regular heptagon (seven-sided polygon)
in the unit circle. If one vertex of the heptagon is (1, 0), an adjacent vertex is
(cos(2π/7), sin(2π/7)) = e2πi/7. So we must show that ω = e2πi/7 ̸∈GC. What is the
minimal polynomial of ω in Q[x]? Note that ω is a root of the polynomial x7 −1. But this
polynomial is reducible, with factorization
x7 −1 = (x −1)(x6 + x5 + x4 + x3 + x2 + x + 1).
Since ω is not a root of x−1, it must be a root of m = x6 +x5 +· · ·+x+1 ∈Z[x]. You can
check that the reduction of m in Z3[x] is irreducible, and hence m is irreducible in Q[x].
(More generally, for any prime p, it can be shown that 1 + x + x2 + · · · + xp−1 is irreducible
in Q[x]; see Exercise 83 in Chapter 3.) Thus, m is the minimal polynomial of ω. Since m
has degree 6, [Q(ω) : Q] = 6. This degree is not a power of 2, so ω is not geometrically
constructible.
Finally, consider the notorious problem of squaring the circle. A circle with radius 1 has
area π, so the square with the same area would have side length √π. If √π were in GC, we
would also have π ∈GC by CR8. But a hard theorem of Lindemann states that π is not
algebraic over Q. By the Theorem on Algebraic Elements and Finite Degree Extensions, we
see that [Q(π) : Q] = ∞. Since this degree is not a finite power of 2, (12.2) applies to show
that √π and π are not in GC. We do not prove Lindemann’s theorem here; the interested
reader may find a proof in [32, Vol. 1, Chapter 4] or [23, §1.7].
12.10
Constructibility of a Regular 17-Sided Polygon
In this section, we show that a regular 17-sided polygon can be constructed using a ruler
and compass. It suffices to show that ω = e2πi/17 is in GC, since once the line segment
from 1 to ω is available, the other sides of the polygon can be readily constructed. Let
m = x16 + x15 + · · · + x + 1 ∈Q[x]. By Kronecker’s Algorithm, or by reduction mod 3,
or by Exercise 83 in Chapter 3, we can verify that m is irreducible in Q[x]. Moreover,
x17 −1 = (x −1)m, from which it follows that m is the minimal polynomial of ω over Q.
Thus, [Q(ω) : Q] = deg(m) = 16 = 24.
Unfortunately, this information is not enough by itself to conclude that ω ∈GC, since
the converse of (12.1) is not always true. So we adopt a different approach. Let θ = 2π/17.
We develop an explicit formula for cos θ to show that cos θ ∈AC. Since AC = GC, we can
construct cos θ geometrically, and then we can construct e2πi/17 by drawing the altitude to
the real axis through the point (cos θ, 0).
We make repeated use of the following observation. Suppose r1 and r2 are complex
numbers with r1 + r2 = b and r1r2 = c. Then (x −r1)(x −r2) = x2 −bx + c, and by the
Quadratic Formula, r1, r2 = (b ±
√
b2 −4c)/2.
For each integer j ≥1, we have ωj = e2πij/17 = cos(jθ)+i sin(jθ) and ω−j = cos(−jθ)+
i sin(−jθ) = cos(jθ) −i sin(jθ), hence ωj + ω−j = 2 cos(jθ). Consider the real numbers
x1 = 2(cos θ + cos(2θ) + cos(4θ) + cos(8θ)),
x2 = 2(cos(3θ) + cos(5θ) + cos(6θ) + cos(7θ)).
The previous remark shows that the sum x1 + x2 is
ω + ω−1 + ω2 + ω−2 + ω4 + ω−4 + ω8 + ω−8 + ω3 + ω−3 + ω5 + ω−5 + ω6 + ω−6 + ω7 + ω−7.
Ruler and Compass Constructions
333
Since ω17 = 1, we have ω−j = ω17−j, so we can also write the sum as
x1 + x2 =
16
X
i=1
ωi = m(ω) −1 = −1.
Next, recall the trigonometric identities cos(u+v)+cos(u−v) = 2 cos u cos v and cos(−u) =
cos u. Also, for k > 8, cos(kθ) = cos(−kθ) = cos((17 −k)θ) since 17θ = 2π. Using these
identities along with the distributive law, we find that
x1x2 = 2(cos(4θ) + cos(2θ) + cos(6θ) + cos(4θ) + cos(7θ) + cos(5θ) + cos(8θ) + cos(6θ)
+ cos(5θ) + cos(1θ) + cos(7θ) + cos(3θ) + cos(8θ) + cos(4θ) + cos(8θ) + cos(5θ)
+ cos(7θ) + cos(1θ) + cos(8θ) + cos(1θ) + cos(7θ) + cos(2θ) + cos(6θ) + cos(3θ)
+ cos(6θ) + cos(5θ) + cos(4θ) + cos(3θ) + cos(3θ) + cos(2θ) + cos(2θ) + cos(1θ)).
Gathering terms, we discover that x1x2 = 4(x1 + x2) = −4. By our initial observation,
x1, x2 = (−1 ±
√
17)/2. Now, x1 ≈1.56155 > 0 and x2 ≈−2.56155 < 0, so we conclude:
x1 = −1 +
√
17
2
,
x2 = −1 −
√
17
2
.
The next step is to consider the real numbers
y1 = 2(cos θ + cos(4θ)),
y2 = 2(cos(2θ) + cos(8θ)),
y3 = 2(cos(3θ) + cos(5θ)),
y4 = 2(cos(6θ) + cos(7θ)).
We see that y1 + y2 = x1 and, by the same trigonometric identities used above,
y1y2 = 2(cos(3θ) + cos(1θ) + cos(8θ) + cos(7θ) + cos(6θ) + cos(2θ) + cos(5θ) + cos(4θ)).
So y1y2 = x1 + x2 = −1. The initial observation applies again to give
y1 = x1 +
p
x2
1 + 4
2
,
y2 = x1 −
p
x2
1 + 4
2
,
where the signs were found by comparison to decimal approximations of y1 and y2. Similarly,
y3 + y4 = x2 and
y3y4 = 2(cos(8θ) + cos(3θ) + cos(7θ) + cos(4θ) + cos(6θ) + cos(1θ) + cos(5θ) + cos(2θ)).
So y3y4 = x1 + x2 = −1, and hence
y3 = x2 +
p
x2
2 + 4
2
,
y4 = x2 −
p
x2
2 + 4
2
.
To finish, consider z1
= 2 cos θ and z2
= 2 cos(4θ). We have z1 + z2
= y1 and
z1z2 = 2 cos(5θ) + 2 cos(3θ) = y3. One final application of the initial observation gives
z1 = y1 +
p
y2
1 −4y3
2
,
where the sign may be determined by noting that z1 > z2 > 0. We see by inspection
of the formulas that x1, x2 ∈AC, hence each yi ∈AC, hence z1 ∈AC, hence finally
cos(2π/17) = z1/2 ∈AC = GC. More explicitly, if we substitute formulas for y1, etc., into
the formula for z1 and simplify, we find that cos(2π/17) is exactly equal to
−1 +
√
17 +
p
34 −2
√
17 +
r
−1 +
√
17 +
p
34 −2
√
17
2
+ 16

1 +
√
17 −
p
34 + 2
√
17

16
,
and this expression is obviously in AC!
334
Advanced Linear Algebra
12.11
Overview of Solvability by Radicals
The recursive definition of AC allows us to construct new numbers by using addition,
subtraction, multiplication, division, and the extraction of square roots. If we expand
this definition to allow the extraction of nth roots for any n ≥2, we can obtain even
more numbers. This leads to the following recursive definition of RC, the set of radically
constructible complex numbers.
R0.
0 ∈RC and 1 ∈RC.
R1.
If a, b ∈RC then a + b ∈RC.
R2.
If a ∈RC then −a ∈RC.
R3.
If a, b ∈RC then a · b ∈RC.
R4.
If a ∈RC is nonzero, then a−1 ∈RC.
R5.
If a ∈RC, b ∈C, n ≥2 is an integer, and bn = a, then b ∈RC.
R6.
The only numbers in RC are those that can be obtained by applying rules R0
through R5 a finite number of times.
Equivalently, z ∈RC iff there is a finite sequence
z0 = 0, z1 = 1, z2, . . . , zk = z,
where k ≥0 and for all i with 2 ≤i ≤k, either zi is the sum or product of two preceding
numbers in the sequence, or zi is the additive inverse, multiplicative inverse, or nth root
of some preceding number in the sequence. Such a sequence is called a radical construction
sequence for z.
For example, consider a quadratic polynomial ax2 + bx + c with a, b, c ∈RC and a ̸= 0.
The roots of this polynomial are (−b±
√
b2 −4ac)/(2a), which are also in RC. Next, consider
a reduced cubic polynomial y3 + qy + r where q, r ∈RC (“reduced” means the coefficient of
y2 is 0). The Cubic Formula states that the roots of this polynomial have the form u−q/(3u)
where u is any complex number satisfying u3 = (−r+
p
r2 + 4q3/27)/2. It follows that these
roots are also in RC. Similarly, the Quartic Formula can be used to show that the roots
of a fourth-degree polynomial with coefficients in RC are also in RC. However, there exist
fifth-degree polynomials in Z[x] whose roots are not in RC. This is the famous theorem that
quintic (degree 5) polynomials are not always solvable by radicals.
We only have space here to give an outline of the proof strategy. The first step is to
give a field-theoretic rephrasing of the definition of RC, which is analogous to the result
AC = SQC proved above. Specifically, for a prime p, let us say that a field extension F ⊆K
is pure of type p if there exists w ∈K with K = F(w) and wp ∈F. Informally, we obtain
K from F by adjoining a pth root of an element of F. It can be shown that z ∈RC iff there
is a chain of fields
Q = K0 ⊆K1 ⊆K2 ⊆· · · ⊆Kt ⊆C
where z ∈Kt and each Kj is a pure extension of Kj−1.
The second step is to use Galois theory to convert a chain of fields of the type just
described to a chain of finite groups
G = G0 ⊇G1 ⊇G2 ⊇· · · ⊇Gt = {e}
in which Gi is a normal subgroup of Gi−1 for each i ≥1, and Gi−1/Gi is a cyclic group of
size p. Not all finite groups G possess chains of subgroups satisfying these conditions; those
groups that do are called solvable. Finally, with each polynomial in Q[x] we can associate
Ruler and Compass Constructions
335
a field extension of Q (the splitting field of the polynomial) and a finite group (the Galois
group of the polynomial). We can construct degree 5 polynomials whose associated Galois
groups are not solvable, and this can be combined with the preceding results to show that
the roots of these polynomials cannot all belong to RC. For more information, consult the
texts on Galois theory listed in the section on suggested further reading.
12.12
Summary
1.
Geometrically Constructible Numbers. GC is the set of all points P in the complex
plane for which there is a sequence P0 = 0, P1 = 1, P2, . . . , Pk = P such that each
Pj (j ≥2) is an intersection point of two lines and/or circles determined by
preceding points in the sequence.
2.
Arithmetically Constructible Numbers. AC is the set of all complex numbers z
for which there is a sequence z0 = 0, z1 = 1, z2, . . . , zk = z such that each zj
(j ≥2) is the sum, additive inverse, product, multiplicative inverse, or square
root of preceding number(s) in the sequence.
3.
Field Extensions. Given a field K and subfield F, the degree [K : F] is the
dimension of K viewed as a vector space over F. For z ∈K, the subfield F(z) is
the intersection of all subfields of K containing F and z. Given a chain of fields
F0 ⊆F1 ⊆F2 ⊆· · · ⊆Fn, the Degree Formula says [Fn : F0] = Qn
i=1[Fi : Fi−1].
4.
Square Root Towers. SQC is the set of all complex numbers z for which there is
a chain of fields Q = K0 ⊆K1 ⊆· · · ⊆Kt ⊆C with z ∈Kt and [Ki : Ki−1] = 2
for all i ≥1. We have Ki = Ki−1(w) for some w ∈Ki such that w2 ∈Ki−1, and
Ki = {a + bw : a, b ∈Ki−1}. These facts are used to show SQC = AC.
5.
Constructibility Theorem. GC = AC = SQC. For a point z ∈C to be constructible
by ruler and compass, it is necessary (but not always sufficient) that [Q(z) : Q] be
a finite power of 2. In particular,
3√
2, π, cos 20◦, and e2πi/7 are not constructible,
but e2πi/17 is constructible.
6.
Algebraic Elements and Minimal Polynomials. Given a field K, a subfield F, and
z ∈K, z is algebraic over F iff z is the root of some nonzero polynomial in
F[x]; this holds iff [F(z) : F] is finite. In this case, there exists a unique monic
irreducible polynomial m ∈F[x] having z as a root, called the minimal polynomial
of z over F. Letting d = deg(m), (1, z, z2, . . . , zd−1) is an ordered F-basis of F(z),
so that [F(z) : F] = d, the degree of the minimal polynomial of z.
7.
Proof Idea for AC ⊆GC. AC is contained in GC because it is possible to simulate
each arithmetical operation +, −, ×, ÷, √
by geometrical constructions. For
example, if we draw the circle with center (x−1)/2 passing through x ∈R>0, this
circle hits the positive imaginary axis at the point i√x. Similar triangles can be
used to implement real multiplication and division. Altitudes and parallels can
be used to transfer distances and thereby perform real addition and subtraction.
Complex arithmetic can be reduced to real arithmetic on real and imaginary
parts, and complex square roots of reiθ can be found by taking the square root
of the real modulus r and bisecting the angle θ.
8.
Proof Idea for GC ⊆AC. GC is contained in AC because the coordinates of
the intersection points of two lines and/or circles can be calculated from the
coordinates of points determining those lines and circles by arithmetic operations
and square root extractions.
336
Advanced Linear Algebra
12.13
Exercises
1.
Give explicit arithmetic construction sequences for each complex number.
(a) 5
(b) −2 + i
(c)
√
7
(d) 2e5πi/6
2.
Give explicit geometric construction sequences for each complex number.
(a) 5
(b) −2 + i
(c)
√
7
(d) 2e5πi/6
3.
Give explicit square root towers for each complex number.
(a) 5
(b) −2 + i
(c)
√
7
(d) 2e5πi/6
4.
Find all z ∈C having geometric construction sequences of length at most 3.
5.
Find all z ∈C that have geometric construction sequences of length at most 4.
6.
Find all z ∈C having arithmetic construction sequences of length at most 4.
7.
Find all z ∈C that have arithmetic construction sequences of length at most 5.
8.
Use the definition of AC to prove in detail that Q ⊆AC.
9.
Use the definition of AC to prove that AC is a subfield of C.
10.
Let n be a positive integer. Prove 0, 1, 2, . . . , n is a geometric construction
sequence for n. Prove if P0 = 0, P1 = 1, P2, . . . , Pk = n is any geometric
construction sequence for n, then P0, . . . , Pk, 2n is a geometric construction
sequence for 2n. Prove if P0 = 0, P1 = 1, P2 = −1, . . . , Pk = n is a geometric
construction sequence for n, then P0, . . . , Pk, 2n + 1 is a geometric construction
sequence for 2n + 1.
11.
Find a geometric construction sequence for 99 of length 9.
12.
Prove: if n ∈Z>0 is k bits long when written in base 2, then n has a geometric
construction sequence of length k + 2.
13.
Prove: for all positive integers n that are k bits long when written in base 2, n
has an arithmetic construction sequence of length at most 2k.
14.
(a) Let a, b ∈AC have arithmetic construction sequences x0, . . . , xk
= a
and y0, . . . , ym = b. Prove that x0, . . . , xk, y0, . . . , ym, a + b is an arithmetic
construction sequence for a + b. (b) Prove that the recursive definition of AC
(using rules A0 through A6) is logically equivalent to the iterative definition of
AC (using arithmetic construction sequences).
15.
Let θ ∈R. (a) Give a geometric proof that cos θ ∈GC iff sin θ ∈GC iff eiθ ∈GC.
(b) Give an arithmetic proof that cos θ ∈AC iff sin θ ∈AC iff eiθ ∈AC.
(c) Prove or disprove: for all θ, if eiθ ∈AC then θ ∈AC.
16.
Let F be a subfield of K. Prove that K is an F-vector space by writing the field
axioms for K and comparing them to the axioms for a vector space over F.
17.
(a) Let K be a field with subfield F. Prove that F = K iff [K : F] = 1. (b) Give
an example of a chain of fields F ⊆K ⊆E where [E : F] = [K : F], yet K ̸= E.
18.
Let F = {a + b
√
3 + c
√
5 + d
√
15 : a, b, c, d ∈Q}. Prove F is a subfield of C.
Compute [F : Q], [F : Q(
√
3)], and [F : Q(
√
5)], and find ordered bases for each
of these vector spaces. Find five different subfields of F, and prove carefully that
they are all different.
19.
Let F ⊆E ⊆K be a chain of fields where [K : F] is prime. Prove E = F or
E = K.
Ruler and Compass Constructions
337
20.
(a) Given a chain of fields F ⊆K ⊆E, prove that [E : F] is finite iff [E : K] is
finite and [K : F] is finite. (b) Generalize (a) to a chain of more than 3 fields.
21.
Field Extension Generated by a Set. Given a field K, a subfield F, and a
subset S of K, define F(S) to be the intersection of all subfields of K containing
F ∪S.
(a) Prove F(S) is a subfield of K, F ⊆F(S), and S ⊆F(S).
(b) Prove: for any subfield M of K, F(S) ⊆M iff F ⊆M and S ⊆M.
(c) Prove: for all subsets S, T of K, F(S)(T) = F(T)(S) = F(S ∪T).
(d) Prove: for all subsets S, T of K, F(S) = F(T) iff S ⊆F(T) and T ⊆F(S).
22.
Give a specific example of a field K, a subfield F, and two disjoint nonempty
subsets S and T of K \ F with |S| ̸= |T| and yet F(S) = F(T).
23.
Let K be a field. (a) Prove that the set {n.1K : n ∈Z} of additive multiples of 1
in K is a subring of K that is isomorphic either to Z or to Zp for some prime p.
(b) Prove that the intersection of all subfields of K is a field that is isomorphic
either to Q or to Zp for some prime p.
24.
Suppose K is a field, F is a subfield of K, and z ∈K.
(a) Prove F(z) = {f(z)/g(z) : f, g ∈F[x] and g(z) ̸= 0}. (Call the right side
E. To prove F(z) ⊆E, show E is a subfield of K containing F and z. To prove
E ⊆F(z), show every subfield M of K containing F and z must also contain E.)
(b) Let R = {f(z) : f ∈F[x]}. Prove that R is a subring of K that contains F
and z. Prove that if z is algebraic over F, then R = F(z).
(c) Use the theorem that π is not algebraic over Q to prove that {f(π) : f ∈Q[x]}
is not a subfield of C.
25.
Describe a specific square root tower for e2πi/17.
26.
Suppose x0, y0, x1, y1 ∈AC satisfy x0 ̸= x1. Let the unique line through (x0, y0)
and (x1, y1) have slope m and y-intercept b for some m, b ∈R. Prove m, b ∈AC.
27.
Consider a line y = mx+d and a parabola y = ax2+bx+c where a, b, c, d, m ∈AC.
Prove that all intersection points of this line and this parabola are in AC.
28.
In the proof of CR1, use theorems about triangles to confirm the assertions that
E is the midpoint of AB and CD is perpendicular to AB.
29.
Use an actual ruler and compass to construct a regular hexagon inscribed in the
unit circle with one vertex at (1, 0).
30.
Use an actual ruler and compass to construct a square inscribed in the unit circle
with one vertex at eπi/4.
31.
Use an actual ruler and compass to construct: (a) the line y = 3x + 1; (b) the
altitude to the line in (a) passing through (2, 0); (c) the line parallel to the line
in (a) passing through (2, 0).
32.
Use an actual ruler and compass to construct: (a) the real numbers x =
√
6 and
y = −3/2; (b) x + y; (c) x −y; (d) xy; (e) 1/x.
33.
Use an actual ruler and compass to construct: (a) u = 2 + i and v = 3 −2i;
(b) u + v; (c) u −v; (d) uv; (e) the two square roots of u; (f) 1/v.
34.
Prove geometrically: if P and Q are in GC and r is the length of the line segment
PQ, then the real number r = (r, 0) is in GC. Conclude that the collapsing
compass and ruler can simulate a real compass’s ability to transfer a fixed distance
from one part of a diagram to another.
35.
Given an algebraic proof of the preceding exercise using the fact that GC = AC.
338
Advanced Linear Algebra
36.
Prove geometrically: for all α, β ∈R, if eiα ∈GC and eiβ ∈GC, then ei(β−α) ∈
GC. Conclude that when deciding the constructibility of regular n-gons inscribed
in the unit circle, it suffices to consider n-gons with one vertex at (1, 0).
37.
Given an algebraic proof of the preceding exercise using the fact that GC = AC.
38.
In CR11, we proved that for all complex u, v ∈GC, u+v ∈GC by looking at real
and imaginary parts. Give an alternate geometric proof of this result by using
the parallelogram law for finding the vector sum of u and v in R2.
39.
Prove geometrically that u, v ∈GC implies uv ∈GC by using the polar
multiplication formula (reiα) · (seiβ) = (rs)ei(α+β).
40.
Given two points P and Q, show how to use a ruler and compass to construct
points R and S that trisect the line segment PQ. Illustrate your construction
with an actual ruler and compass.
41.
Consider points A = eπi/3, B = 0, and C = 1, so ∠ABC is a 60◦angle. As shown
in the previous exercise, we can find points D and E that trisect the line segment
AC. Write D and E in the form a + bi with a, b ∈R. Use this to approximate the
measure of the angle ∠EBC in degrees. Has this construction trisected ∠ABC?
42.
Starting with the points 0 and 1, prove that a compass alone can be used to
construct all points in the set {a + be2πi/6 : a, b ∈Z}. (Remarkably, it can be
shown that every point in GC can be constructed with only a compass, starting
from the two points 0 and 1.)
43.
Find (with proof) the minimal polynomials over Q of each complex number.
(a) 22/7 (b) 2i (c)
√
11 (d)
√
3 +
√
7 (e)
5√
2 (f)
√
2 + i (g) e2πi/6 (h) e2πi/12
44.
Let F be a field and g be an irreducible polynomial in the polynomial ring F[x].
Let I = F[x]g be the principal ideal in F[x] generated by g. Let K be the quotient
ring F[x]/I. (See Chapter 1 for the relevant definitions.)
(a) Prove K is a field having a subfield F1 = {c + I : c ∈F} isomorphic to F.
(b) Use the isomorphism between F and F1 to convert g to a polynomial g1 ∈
F1[x]. Prove g1(x + I) = 0 + I, so the coset x + I is a root of g1 in the field K.
(c) Suppose F is a subfield of C, and g ∈F[x] is the minimal polynomial over F
of some z ∈C. Prove that the field K in part (a) is isomorphic to the subfield
F(z) of C. (Apply the Fundamental Homomorphism Theorem for Rings to an
appropriate map.)
45.
Construct a specific field K with 343 elements containing an element z with
z3 = 2 and containing a subfield isomorphic to Z7. (Use the previous exercise.)
46.
Consider the polynomial x4 + x3 + x2 + x + 1 = (x5 −1)/(x −1). (a) Find
the complex roots of this polynomial in terms of cosines, sines, and/or complex
exponentials. (b) Divide the polynomial by x2 and let y = x + x−1. Convert
the quartic polynomial in x to a quadratic polynomial in y, and then use the
Quadratic Formula to solve for y and then x algebraically. (c) Use (a) and (b) to
prove cos 72◦= −1+
√
5
4
and sin 72◦= 1
2
q
5+
√
5
2
, and deduce that e2πi/5 ∈GC.
47.
Use an actual ruler and compass to construct a regular pentagon inscribed in the
unit circle.
48.
Let z be the unique real root of the polynomial x3 + 2x + 1 ∈Q[x].
(a) Find an ordered Q-basis of Q(z) and compute [Q(z) : Q].
(b) For −1 ≤n ≤7, express zn as a Q-linear combination of the basis in (a).
(c) Compute (z2 −3z + 2) · (2z2 + 5z −1) in terms of the basis in (a).
(d) Find (z2 −z + 1)−1 in terms of the basis in (a).
Ruler and Compass Constructions
339
49.
Let z = e2πi/12.
(a) Find an ordered Q-basis of Q(z) and compute [Q(z) : Q].
(b) For all n ∈Z, express zn as a Q-linear combination of the basis in (a).
(c) Compute (z3 + 2z −5) · (z2 + 3z + 4) in terms of the basis in (a).
(d) Find (z3 + 2z2 + 3z + 4)−1 in terms of the basis in (a).
50.
Suppose z ∈C has a minimal polynomial over Q of odd degree. Prove that x2 −2
is the minimal polynomial of
√
2 over the subfield Q(z).
51.
Let β be an irrational real number, and assume that for all integers k ≥1 there
exists a rational number sk = mk/nk (for some mk, nk ∈Z with nk > 0) satisfying
|β −sk| < (knk
k)−1. Prove β is not algebraic over Q, hence β ̸∈GC. Deduce that
P
n≥1 10−n! is not algebraic over Q.
52.
(a) Prove: for all z, w ∈C, if z and w are algebraic over Q, then z +w and zw are
algebraic over Q. (Consider the chain of fields Q ⊆Q(z) ⊆Q(z)(w) and look at
degrees.) (b) Let K be the set of all z ∈C that are algebraic over Q. Prove that
K is a subfield of C containing Q. (c) Is [K : Q] finite or infinite? Is K countable
or uncountable?
53.
Suppose K is a field with subfield F such that [K : F] = 2 and 1K + 1K ̸= 0K.
Prove there exists w ∈K with K = F(w) and w2 ∈F.
54.
Let K = {0, 1, 2, 3}, and define addition and multiplication in K as follows:
+
0
1
2
3
0
0
1
2
3
1
1
0
3
2
2
2
3
0
1
3
3
2
1
0
·
0
1
2
3
0
0
0
0
0
1
0
1
2
3
2
0
2
3
1
3
0
3
1
2
Prove that K is a field, F = {0, 1} is a subfield of K, and [K : F] = 2. Prove
there is no w ∈K with K = F(w) and w2 ∈F.
55.
For each n with 3 ≤n ≤18, decide (with proof) whether a regular n-sided
polygon can be constructed with ruler and compass.
56.
Prove: for all n ≥3, a regular n-sided polygon is constructible with ruler and
compass iff a regular 2n-sided polygon is constructible.
57.
Prove: if regular polygons with n sides and m sides are both constructible with
ruler and compass and k = lcm(m, n), then a regular k-sided polygon is also
constructible.
58.
(a) Use the trigonometric addition formula for cos(α + β) and other identities
to derive the formula cos(3t) = 4(cos t)3 −3 cos t. (b) Similarly, prove that
cos(u + v) + cos(u −v) = 2 cos u cos v. (c) Reprove the formulas in (a) and (b)
using cos t = (eit + e−it)/2 and facts about the exponential function.
59.
Let θ ∈(0, π/2) be the unique real number with cos θ = 3/5. Find the minimal
polynomial of θ/3. Explain how to construct eiθ with a ruler and compass. Prove
or disprove: the angle between the x-axis and the ray through eiθ can be trisected
with ruler and compass.
60.
Let θ ∈(0, π/2) be the unique real number with cos θ = 11/16. Find the minimal
polynomial of θ/3. Prove or disprove: the angle between the x-axis and the ray
through eiθ can be trisected with ruler and compass.
61.
In §12.10, verify that the signs in the formulas for y1, y2, y3, y4 are correct by
calculating decimal approximations for these quantities.
340
Advanced Linear Algebra
62.
Given a cubic polynomial x3+bx2+cx+d, show that the substitution y = x+b/3
leads to a new reduced cubic y3 + qy + r where the coefficient of y2 is 0.
63.
Find a substitution of variables changing a monic degree n polynomial in x into
a degree n polynomial in y where the coefficient of yn−1 is 0.
64.
Cubic Formula. Given q, r ∈C, let u ∈C̸=0 satisfy u3 = (−r +
p
r2 + 4q3/27)/2.
Verify by algebraic manipulations that y = u−q/(3u) is a solution of the reduced
cubic y3 + qy + r = 0. What happens if u = 0?
65.
Quartic Formula. Find a formula for the roots of a reduced quartic poly-
nomial y4 + py2 + qy + r as follows. Hypothesize a factorization of the
form (y2 + ky + ℓ)(y2 −ky + m). Obtain equations for the unknown coefficients
k, ℓ, m. Eliminate ℓand m. Obtain a cubic equation in the variable k2. Solve this
with the Cubic Formula and deduce formulas for the roots of the original quartic.
Conclude that if p, q, r ∈RC, then all complex roots of the quartic polynomial
are in RC.
66.
Prove, as asserted in §12.11, that for all complex z, z ∈RC iff there is a chain of
fields Q = K0 ⊆K1 ⊆K2 ⊆· · · ⊆Kt ⊆C, where z ∈Kt and each Kj is a pure
extension of Kj−1.
67.
Prove that cos(2π/7) ∈RC by finding an explicit algebraic formula for this
number. (Use a substitution like the one in Exercise 46 to find the roots of
(x7 −1)/(x −1).)
68.
Find and justify an exact formula for cos 1◦that is built up from integers using
only arithmetical operations, square roots, and cube roots. (By the trigonometric
addition formulas, this shows there are exact formulas for cos k◦for any integer
k. Curiously, grade school students are only made to memorize these formulas for
a few select choices of k.)
69.
True or false? Explain each answer.
(a) The minimal polynomial of e2πi/3 over Q is x3 −1.
(b) GC is a subfield of C.
(c) AC = RC.
(d) e6πi/17 ∈GC.
(e) For all integers n ≥1, there is a chain of fields Q ⊆K ⊆C with [K : Q] = n.
(f) For all w, z ∈C, Q(w) = Q(z) iff w = z.
(g) eπi/17 ∈SQC.
(h) For all fields K and subfields F with [K : F] = 4, there is at most one subfield
E with F ⊊E ⊊K.
(i) For every k ≥1, there exists a sequence of distinct points P0, P1, . . . , Pk that is
both an arithmetic construction sequence and a geometric construction sequence.
(j) For any field K and any subfield F and any z ∈K, if [K : F] is finite then z
is algebraic over F.
(k) For all z in a field K with Q ⊆K ⊆C, if [K : Q] = 2, then z2 ∈Q.
(l) For all real θ such that 0 < θ < π/2, if eiθ ∈GC then eiθ/3 ̸∈GC.
13
Dual Vector Spaces
A recurring theme in mathematics is the relationship between geometric spaces and
structure-preserving functions defined on these spaces. Galois theory for field extensions,
elementary algebraic geometry, and the theory of dual vector spaces can all be viewed as
instances of this common theme. Given a space V and a set R of functions on V , the idea is
to study the zero-set of a collection of functions in R, which is the set of points in V where
all the functions in the collection are zero. Similarly, given a subset of V , we can consider
the set of all functions in R that evaluate to zero at all points of the given subset. These
two maps — one sending subsets of R to subsets of V , and the other sending subsets of
V to subsets of R — may restrict to give a bijection (one-to-one correspondence) between
certain distinguished subsets of V and certain distinguished subsets of R. In many cases,
key aspects of the geometric structure of V are mirrored by the algebraic structure of R
through this correspondence. This interplay between V and R allows us to obtain structural
information about the properties of both V and R.
This chapter studies a fundamental and ubiquitous instance of this correspondence
between spaces and functions, namely the theory of dual vector spaces. Here, the geometric
space is a finite-dimensional vector space V over a field F. The collection R consists of
all linear functions mapping V into F. We will see that R is also a vector space over F,
called the dual space for V and denoted by V ∗. The maps mentioned above restrict to
inclusion-reversing bijections between the set of subspaces of V and the set of subspaces
of V ∗. Later, we discuss the connection between these results and the theory of bilinear
pairings, real inner product spaces, and complex inner product spaces. After an interlude
on Banach spaces, the chapter closes with a sketch of the ideal–variety correspondence,
which is a cornerstone of algebraic geometry.
13.1
Vector Spaces of Linear Maps
Before beginning our study of dual spaces, we first give a general construction for
manufacturing vector spaces. Let W be a vector space over a field F and S be any set.
Let Z be the set of all functions g : S →W. We introduce an F-vector space structure on
Z as follows. Let g and h be two functions in Z. We define a new function g + h : S →W
by letting g + h send each x in S to g(x) + h(x) in W. In symbols, (g + h)(x) = g(x) + h(x)
for all x ∈S. Note that the plus symbol on the right side is the addition in the given vector
space W, while the plus symbol on the left side is the new addition being defined on the set
Z. It is routine to check that this new addition on Z, called pointwise addition of functions,
satisfies the additive axioms for a vector space listeed in Table 1.2. The zero element of Z
is the zero function that sends every x ∈S to 0W , and the additive inverse of f ∈Z is the
function that sends x to −f(x) for all x ∈S.
Given g ∈Z and c ∈F, we define the scalar multiple c · g : S →W by letting
(c·g)(x) = c·(g(x)) for all x ∈S. Here, the product on the right side is scalar multiplication in
DOI: 10.1201/9781003484561-13
341
342
Advanced Linear Algebra
the F-vector space W, and the product on the left side is the pointwise scalar multiplication
being defined on Z. You can quickly verify the remaining vector space axioms for Z from
Table 1.4 (Exercise 1).
Here are some special cases of this construction. If S = {1, 2, . . . , n} and W is the field
F viewed as an F-vector space, then Z is the vector space F n of all n-tuples of elements
of F. If S = {1, 2, . . . , m} × {1, 2, . . . , n} and W = F, then Z is the vector space Mm,n(F)
of all m × n matrices with entries in F. See Chapter 4 for more details on these special
cases. If the set S is itself an F-vector space V , then Z is the vector space Fun(V, W) of all
functions from V to W, using pointwise sum and scalar product operations.
Given F-vector spaces V and W, define HomF (V, W) to be the set of all F-linear
transformations (vector space homomorphisms) from V to W. A function f : V →W
belongs to HomF (V, W) iff f(x + y) = f(x) + f(y) and f(cx) = cf(x) for all x, y ∈V and
all c ∈F. We claim that HomF (V, W) is an F-vector space under the pointwise operations
on functions defined above. It suffices to show that HomF (V, W) is a subspace of the vector
space Fun(V, W) of all functions from V to W. This follows since the zero function is
F-linear, and sums and scalar multiples of F-linear functions are also F-linear (Exercise 2).
We define the dual space of an F-vector space V to be the vector space V ∗= HomF (V, F)
of all F-linear transformations from V to F. Here, we are viewing the field F as an F-
vector space where vector addition is addition in F, and scalar multiplication is the same
as multiplication in the field F. We have dim(F) = 1, and (1F ) is an ordered basis of F.
Functions in V ∗are called linear functionals on V .
13.2
Dual Bases
We often define linear maps on a vector space V by specifying where the map sends vectors
in a given basis for V . The next theorem makes this process precise.
Universal Mapping Property (UMP) for Vector Spaces.
Suppose V and W are
F-vector spaces and B = (x1, . . . , xn) is an ordered basis for V . For any F-vector space
W and any ordered list (y1, . . . , yn) of elements of W, there exists a unique F-linear map
T : V →W such that T(xi) = yi for all i between 1 and n.
The UMP also holds for infinite-dimensional vector spaces, with a similar proof, but we
focus on the finite-dimensional case here.
Proof. Every v ∈V has a unique expansion as a linear combination of elements of B, say
v = c1x1 + c2x2 + · · · + cnxn
for some ci ∈F.
If T : V →W is any F-linear map sending xi to yi for all i, then by linearity, T must
send v to c1y1 + · · · + cnyn. This observation proves that T is unique if it exists. To show
existence of T, define T(Pn
i=1 cixi) = Pn
i=1 ciyi for any choice of ci ∈F, and check that T
really is F-linear and sends each xi to yi (Exercise 6). Linear independence of B is needed
to see that T is a well-defined function. This follows since v ∈V can be written in the form
Pn
i=1 cixi in exactly one way.
The universal mapping property can be expressed informally as follows. To define a
linear map from V to W, we are free to send each basis element xi to any element of W.
After choosing the images of the basis elements, the entire map T : V →W is uniquely
determined by linearity. Another way to say this is that each function g : {x1, . . . , xn} →W
“extends by linearity” to a unique linear map Tg : V →W.
Dual Vector Spaces
343
Define δij be 1 for i = j and 0 for i ̸= j. The next theorem shows how to convert an
ordered basis B for V to a related ordered basis Bd for V ∗, called the dual basis of B.
Theorem on Dual Bases. Let V be a finite-dimensional F-vector space with ordered
basis B = (x1, . . . , xn). There is a unique ordered basis Bd = (f1, . . . , fn) of V ∗such that
fi(xj) = δij for all i, j between 1 and n. In particular, dim(V ∗) = dim(V ) = n.
Proof. For each fixed i between 1 and n, consider the function gi : {x1, . . . , xn} →F such
that gi(xi) = 1F and gi(xj) = 0F for all j ̸= i. This function gi extends by linearity to give
a linear map fi : V →F satisfying
fi(c1x1 + · · · + cixi + · · · + cnxn) = ci
for all c1, . . . , cn ∈F.
Each fi is an element of V ∗= HomF (V, F). The UMP shows that Bd = (f1, . . . , fn) is the
unique ordered list in V ∗such that fi(xj) = δij for all i, j between 1 and n. To see that Bd
is an ordered basis of V ∗, we prove that Bd is a linearly independent list spanning V ∗. To
check linear independence, assume d1, . . . , dn ∈F satisfy
d1f1 + d2f2 + · · · + dnfn = 0.
(13.1)
Here, 0 is the zero function from V to F, and the operations are pointwise operations on
functions. We need to prove that every di = 0F . To do this, fix an index i, and evaluate
both sides of (13.1) at the element xi. We obtain
d1f1(xi) + · · · + difi(xi) + · · · + dnfn(xi) = 0(xi).
By definition of fj, the left side evaluates to di, while the right side is zero. Thus di = 0F
for each i, so Bd is a linearly independent list.
Next, we check that Bd spans V ∗. Given an arbitrary h ∈V ∗, we must express h as an
F-linear combination of elements of Bd. We claim that, in the space V ∗, we have
h = h(x1)f1 + h(x2)f2 + · · · + h(xn)fn.
(13.2)
Here, each h(xi) is a scalar in F, since h maps V to F. Both sides of (13.2) are linear maps
from V to F. Two linear functions with domain V are equal iff they have the same value
at each basis element xi in B. (This fact is a restatement of the uniqueness assertion in the
UMP.) So it suffices to check whether the two sides of (13.2) agree at a fixed basis element
xi. The left side sends xi to h(xi), and the right side sends xi to
h(x1)f1(xi) + · · · + h(xi)fi(xi) + · · · + h(xn)fn(xi) = h(xi).
Thus (13.2) does hold, and Bd does span V .
Since the list Bd has the same length n as the list B, we get dim(V ∗) = n = dim(V ).
Every vector space V has a basis, and any two bases of V have the same cardinality,
namely the dimension dim(V ). The Theorem on Dual Bases shows that in the finite-
dimensional case, dim(V ∗) = dim(V ). This dimension fact does not extend to infinite-
dimensional vector spaces (see §13.12 and Exercise 12).
13.3
The Zero-Set Operator
Let V be an n-dimensional vector space over the field F. Our next task is to set up the
correspondences between subsets of V ∗and subsets of V mentioned in the introduction to
344
Advanced Linear Algebra
this chapter. First we describe the map from subsets of V ∗to subsets of V . Let S be any
subset of V ∗, so S is some collection of linear functionals mapping V into F. Define the
zero-set of S to be
Z(S) = {x ∈V : f(x) = 0F for all f ∈S}.
When V = F n, we can think of Z(S) as the solution set of the system of homogeneous
linear equations f(x) = 0 as f ranges over S (see Exercise 20).
Here are some properties of the zero-set operator Z:
• For all subsets S of V ∗, Z(S) is a subspace of V . Proof: First, 0V ∈Z(S) because all
maps f ∈S (being linear) must send 0V to 0F . Second, assume x, y ∈Z(S). For any
f ∈S, f(x + y) = f(x) + f(y) = 0 + 0 = 0, so x + y ∈Z(S). Third, assume x ∈Z(S) and
c ∈F. For any f ∈S, f(cx) = cf(x) = c0 = 0, so cx ∈Z(S).
• Z is inclusion-reversing: S ⊆T in V ∗implies Z(T) ⊆Z(S) in V . Proof: Assume S ⊆T
and x ∈Z(T). For all f ∈S, f is also in T, and so f(x) = 0. This proves that x ∈Z(S).
• Z(∅) = V . Proof: By definition, Z(∅) is a subset of V . If it were a proper subset, then
there would exist x ∈V with x ̸∈Z(∅). This would imply the existence of f ∈∅with
f(x) ̸= 0. This is impossible, since ∅has no members.
• Z(V ∗) = {0V }. Proof: First, 0V ∈Z(V ∗), since Z(V ∗) is a subspace of V . Given v ̸= 0
in V , we show that v ̸∈Z(V ∗). We must find an f ∈V ∗with f(v) ̸= 0. For this, extend
the one-element list (v) to an ordered basis B = (v, v2, . . . , vn) of V . The first function in
the dual basis Bd is an element of V ∗sending v to 1F ̸= 0F .
• If S is any subset of V ∗and W = ⟨S⟩is the subspace of V ∗generated by S, then
Z(S) = Z(W). Proof: Since S ⊆W, we already know Z(W) ⊆Z(S). Next, fix x ∈Z(S).
To prove x ∈Z(W), fix f ∈W. We can write f = d1g1 + · · · + dkgk for some di ∈F and
gi ∈S. Evaluating at x gives f(x) = d1g1(x) + · · · + dkgk(x) = d10 + · · · + dk0 = 0. So
x ∈Z(W).
13.4
The Annihilator Operator
Next, we describe the map from subsets of V to subsets of V ∗. Let T be any subset of V ,
so T is a collection of vectors. Define the annihilator of T to be
A(T) = {g ∈V ∗: g(x) = 0F for all x ∈T}.
Here are some properties of the annihilator operator A:
• For all subsets T of V , A(T) is a subspace of V ∗. Proof: First, 0V ∗sends every element
of V to 0F . In particular, 0V ∗sends every element of T to 0F , so 0V ∗∈A(T). Second,
assume g, h ∈A(T). For any x ∈T, (g +h)(x) = g(x)+h(x) = 0+0 = 0, so g +h ∈A(T).
Third, assume g ∈A(T) and c ∈F. For any x ∈T, (cg)(x) = c(g(x)) = c0 = 0, so
cg ∈A(T).
• A is inclusion-reversing: S ⊆T in V implies A(T) ⊆A(S) in V ∗. Proof: Assume S ⊆T
and g ∈A(T). For all x ∈S, x is also in T, and so g(x) = 0. This proves that g ∈A(S).
• A(∅) = V ∗. The proof is analogous to the proof that Z(∅) = V (Exercise 23).
Dual Vector Spaces
345
• A(V ) = {0V ∗}. Proof: A linear functional f : V →F belongs to A(V ) iff f(x) = 0 for all
x in the domain V iff f is the zero function on V iff f is the zero functional in V ∗.
• If T is any subset of V and W = ⟨T⟩is the subspace of V generated by T, then
A(T) = A(W). Proof: Since T ⊆W, we already know A(W) ⊆A(T). Next, fix f ∈A(T).
To prove f ∈A(W), fix x ∈W. We can write x = c1x1 + · · · + ckxk for some ci ∈F and
xi ∈T. Linearity of f gives f(x) = Pk
i=1 cif(xi) = Pk
i=1 ci0 = 0. So f ∈A(W).
13.5
The Double Dual V ∗∗
There is a striking similarity between the properties of Z and the properties of A. This
similarity is not a coincidence. To explain it, we introduce the double dual space for V . The
double dual of V is the dual space of the vector space V ∗, namely
V ∗∗= (V ∗)∗= HomF (V ∗, F) = HomF (HomF (V, F), F).
For finite-dimensional V , we know dim(V ) = dim(V ∗) = dim(V ∗∗).
Let us spell out the definition of V ∗∗in more detail. An element of V ∗∗is an F-linear
function E : V ∗→F. The function E takes as input another F-linear function g : V →F
and produces as output a scalar E(g) ∈F. The F-linearity of E means that E(g + h) =
E(g) + E(h) and E(dg) = dE(g) for all g, h ∈V ∗and d ∈F.
Elements of V ∗∗may seem very abstract and difficult to visualize, since these elements
are functions that operate on other functions. However, we can construct some concrete
examples of elements of V ∗∗as follows. Suppose x is a fixed vector in V . Define a map
Ex : V ∗→F by letting Ex(g) = g(x) for all g ∈V ∗. In other words, Ex takes as input a
linear functional g : V →F and returns as output the value of g at the particular point
x ∈V , namely g(x). For this reason, the function Ex is called evaluation at x. To check
that Ex really is F-linear, use the definitions of the pointwise operations in V ∗to compute
Ex(g + h) = (g + h)(x) = g(x) + h(x) = Ex(g) + Ex(h) for all g, h ∈V ∗;
Ex(dg) = (dg)(x) = d(g(x)) = d(Ex(g)) for all g ∈V ∗, d ∈F.
So every Ex really is an element of V ∗∗.
We define a map ev : V →V ∗∗by setting ev(x) = Ex for all x ∈V . So ev sends each
vector x in V to the function “evaluation at x,” which is in the double dual of V .
Theorem on Evaluation Maps. For any F-vector space V , ev : V →V ∗∗is a one-to-one
F-linear map. If dim(V ) is finite, then ev is a vector space isomorphism.
Proof. We first check that ev : V →V ∗∗is an F-linear map. (This is not the same as
checking that ev(x) = Ex is F-linear for each x ∈V , which we have already done. Do not
confuse the function ev from V to V ∗∗with one of its outputs ev(x), which is a function
from V ∗to F.) We first fix x, y ∈V and check that ev(x + y) = ev(x) + ev(y). In other
words, we must prove that Ex+y = Ex + Ey in V ∗∗. These two functions from V ∗to F are
indeed equal, since for any g ∈V ∗,
Ex+y(g) = g(x + y) = g(x) + g(y) = Ex(g) + Ey(g) = (Ex + Ey)(g).
Next, fix x ∈V and c ∈F. We must prove ev(cx) = c ev(x), or equivalently Ecx = cEx.
For any g ∈V ∗,
Ecx(g) = g(cx) = cg(x) = c(Ex(g)) = (cEx)(g).
This proves linearity of ev.
346
Advanced Linear Algebra
We show that the linear map ev is one-to-one by checking that for all x ∈V , ev(x) = 0V ∗∗
implies x = 0V . We prove the contrapositive. Fix a nonzero x ∈V . By extending {x} to a
basis of V and using the UMP, we can find f ∈V ∗with f(x) ̸= 0 (cf. Exercise 12). Then
Ex(f) ̸= 0, so Ex = ev(x) is not the zero function from V ∗to F. Hence, ev(x) ̸= 0.
So far, we know ev : V
→V ∗∗is always F-linear and one-to-one. If V is finite-
dimensional, then dim(V ∗∗) = dim(V ) as remarked earlier. So the injective linear map ev
must also be surjective since the domain and codomain have the same finite dimension.
The theorem shows that for each finite-dimensional F-vector space V , we have a natural
vector space isomorphism ev : V →V ∗∗. The word natural is a technical mathematical
adjective explained more fully later. Informally, the isomorphism is natural because it does
not depend on any choices such as the chocie of an ordered basis. The fact that ev is a
bijection means that for every E ∈V ∗∗, there exists a unique x ∈V such that E = Ex
(evaluation at x). Linearity of ev means Ex+y = Ex + Ey and Ecx = cEx for all x, y ∈V
and c ∈F, as we confirmed earlier.
For the rest of this section, assume V is finite-dimensional. We often use the isomorphism
ev to identify V ∗∗with V , thereby blurring the distinction between an element x ∈V and
the associated evaluation map Ex ∈V ∗∗. This identification may seem confusing at first, but
it can clarify certain issues and make certain proofs less redundant. For instance, consider
the relation between Z and A mentioned at the beginning of this subsection. Let A∗be the
annihilator operator for the vector space V ∗, which sends subsets of V ∗to subsets of V ∗∗.
Given a subset S of V ∗, the definition of A∗gives
A∗(S) = {E ∈V ∗∗: E(h) = 0 for all h ∈S}.
But every E ∈V ∗∗has the form Ex for a unique x ∈V . Then ev(x) = Ex ∈A∗(S) iff
Ex(h) = 0 for all h ∈S iff h(x) = 0 for all h ∈S iff x ∈Z(S). This proves that A∗(S) is
the image of Z(S) under the isomorphism ev. If we use ev to identify V with V ∗∗, then the
previous statement says that A∗(S) = Z(S).
Similarly, letting Z∗be the zero-set operator for V ∗(which maps subsets of V ∗∗to
subsets of V ∗), we claim that Z∗(T) = A(T) for all subsets T of V , under the identification
of V ∗∗with V . More precisely, we are asserting that Z∗(ev[T]) = A(T) for all T ⊆V . The
reason is that g ∈A(T) iff g(x) = 0 for all x ∈T iff Ex(g) = 0 for all x ∈T iff g is sent to
zero by all functions in ev[T] iff g ∈Z∗(ev[T]).
With these facts in hand, we can see why the properties of the operators Z and A exactly
mirror each other. For example, given that the annihilator operator has already been shown
to be inclusion-reversing, it follows that the zero-set operator is also inclusion-reversing,
because S ⊆T ⊆V ∗implies Z(T) = A∗(T) ⊆A∗(S) = Z(S).
As another example of this process, we prove:
• For all T ⊆V , T ⊆Z(A(T));
• For all S ⊆V ∗, S ⊆A(Z(S)).
For the first result, assume T is a subset of V , fix x ∈T, and prove x ∈Z(A(T)). We must
show g(x) = 0 for all g ∈A(T). But this follows from the very definition of A(T), since
x ∈T. The first result therefore holds for all vector spaces V . In particular, applying this
result to the vector space V ∗instead of V , we see that S ⊆Z∗(A∗(S)) for all S ⊆V ∗. As
seen above, this means S ⊆A(Z(S)) for all S ⊆V ∗.
As a final example of working with the double dual, we show that the dual basis
construction works in reverse. Suppose B∗= (f1, . . . , fn) is any ordered basis of V ∗. We
claim there exists a unique dual basis (B∗)D = (x1, . . . , xn) of V such that fi(xj) = δij
Dual Vector Spaces
347
for all i, j between 1 and n. To get this basis, first form the dual basis (in the old sense)
(B∗)d = (E1, . . . , En), which is a basis of V ∗∗. Then write Ej = Exj = ev(xj) for a unique
xj ∈V . Since ev is an isomorphism, (x1, . . . , xn) is an ordered basis for V . For each i, j,
compute fi(xj) = Exj(fi) = Ej(fi) = δij. The uniqueness of the basis (x1, . . . , xn) follows
from the uniqueness of (E1, . . . , En) and the bijectivity of ev. We can also show that, for
any ordered basis B of V , BdD = B. To see why, write B = (x1, . . . , xn), Bd = (f1, . . . , fn),
and BdD = (z1, . . . , zn). We must have xi = zi for each i, because fi(zj) = δij, and we
know B is the unique ordered basis of V satisfying this condition.
13.6
Correspondence between Subspaces of V and V ∗
Let V be a finite-dimensional vector space over a field F. Let P(V ) be the set of all subsets
of V and S(V ) be the set of all subspaces of V . Define P(V ∗) and S(V ∗) similarly. In the
previous subsections, we introduced two inclusion-reversing maps
Z : P(V ∗) →S(V )
and
A : P(V ) →S(V ∗).
These maps are not one-to-one. For instance, if W is a subspace of V that is generated by
a proper subset S, then S ̸= W but A(S) = A(W). To get one-to-one correspondences, we
restrict the operators Z and A to act only on subspaces of V ∗and V , giving two inclusion-
reversing maps
Z : S(V ∗) →S(V )
and
A : S(V ) →S(V ∗).
Theorem on Correspondence Between Subspaces of V and V ∗. Assume V is a
finite-dimensional F-vector space with dim(V ) = n. The operators Z and A are mutually
inverse inclusion-reversing bijections between S(V ∗) and S(V ) satisfying
dim(W) + dim(Z(W)) = dim(V ) = dim(Y ) + dim(A(Y ))
for all W ∈S(V ∗), Y ∈S(V ).
(13.3)
The unrestricted Z and A operators satisfy
A(Z(S)) = ⟨S⟩and Z(A(T)) = ⟨T⟩
for all S ∈P(V ∗), T ∈P(V ).
Proof. We first prove the dimension formula n = dim(V ) = dim(Y ) + dim(A(Y )), where
Y is a fixed subspace of V . Let (y1, . . . , yk) be an ordered basis for Y . We extend this to
an ordered basis B = (y1, . . . , yk, yk+1, . . . , yn) for V . Let Bd = (f1, . . . , fk, fk+1, . . . , fn) be
the dual basis for V ∗. We prove that (fk+1, . . . , fn) is an ordered basis for A(Y ), which leads
to the required conclusion dim(A(Y )) = n −k = dim(V ) −dim(Y ). First, (fk+1, . . . , fn) is
a linearly independent list in V ∗, being a sublist of the ordered basis Bd. A generic element
f ∈V ∗can be written uniquely as
f = d1f1 + · · · + dkfk + dk+1fk+1 + · · · + dnfn
for some dj ∈F.
We claim f ∈A(Y ) iff d1 = · · · = dk = 0. On one hand, f ∈A(Y ) iff f ∈A({y1, . . . , yk}),
since these vectors generate the subspace Y . On the other hand, f(yi) = Pn
j=1 djfj(yi) = di
for all i. Therefore, f ∈A(Y ) iff f(y1) = · · · = f(yk) = 0 iff d1 = · · · = dk = 0. We now
see that A(Y ) consists precisely of all F-linear combinations of the linearly independent
vectors fk+1, . . . , fn. Thus (fk+1, . . . , fn) is an ordered basis for A(Y ).
348
Advanced Linear Algebra
To complete the proof of (13.3), apply the result of the last paragraph to V ∗instead of
V . Using the identification of V ∗∗with V , we get
dim(V ) = dim(V ∗) = dim(W)+dim(A∗(W)) = dim(W)+dim(Z(W))
for all W ∈S(V ∗).
Next, we show that the restricted Z and A operators are two-sided inverses of each other, so
that Z and A are bijections. Suppose Y is any subspace of V . We have seen that Z(A(Y ))
is a subspace of V containing Y . On the other hand, the dimension formulas in (13.3) show
that
dim(Z(A(Y ))) = n −dim(A(Y )) = n −(n −dim(Y )) = dim(Y ).
Since the dimensions are equal and finite, Z(A(Y )) cannot properly contain Y . So
Z(A(Y )) = Y holds. The same reasoning shows that A(Z(W)) = W for all W ∈S(V ∗).
Finally, for any subset S of V ∗, we have
A(Z(S)) = A(Z(⟨S⟩)) = ⟨S⟩.
Similarly, Z(A(T)) = ⟨T⟩for all T ⊆V .
To continue, we recall some definitions from the theory of posets (see the Appendix for
more details). A poset (partially ordered set) consists of a set Z and a relation ≤on Z that
is reflexive, antisymmetric, and transitive. This means that for all x, y, z ∈Z, x ≤x; if
x ≤y and y ≤x, then x = y; and if x ≤y and y ≤z, then x ≤z. Given S ⊆Z, an upper
bound for S is an element z ∈Z such that for all x ∈S, x ≤z. A least upper bound for S
is an upper bound w for S such that w ≤z for all upper bounds z for S. Lower bounds for
S and the greatest lower bound for S are defined similarly. A subset S of a poset may or
may not have a least upper bound. If the least upper bound exists, then it is unique and is
written sup S. Similar comments apply to the greatest lower bound of S, which is written
inf S when it exists. The poset Z is called a lattice iff every two-element subset of Z has a
least upper bound and a greatest lower bound. Z is a complete lattice iff every nonempty
subset of Z has a least upper bound and a greatest lower bound.
Observe that S(V ) and S(V ∗) are both posets ordered by set inclusion: X ≤Y means
X ⊆Y . You can check that if X and Y are subspaces of V , then in the poset S(V ),
inf({X, Y }) = X ∩Y and sup({X, Y }) = X + Y = {x + y : x ∈X, y ∈Y }. So S(V )
is a lattice. More generally, if {Xi : i ∈I} is any indexed collection of subspaces of V ,
then infi∈I Xi = T
i∈I Xi and supi∈I Xi = P
i∈I Xi, where P
i∈I Xi is the set of all finite
sums xi1 + · · · + xik with xij ∈Xij. So S(V ) is a complete lattice; similarly, S(V ∗) is a
complete lattice. We have shown that the maps Z : S(V ∗) →S(V ) and A : S(V ) →S(V ∗)
are bijections that reverse inclusions. In poset terminology, we can say that these maps are
poset anti-isomorphisms. It follows from this fact and the definitions of sup and inf that Z
and A interchange least upper bounds and greatest lower bounds (Exercise 43). In other
words,
Z(X + Y ) = Z(X) ∩Z(Y ),
Z(X ∩Y ) = Z(X) + Z(Y )
for all X, Y ∈S(V ∗); (13.4)
A(X + Y ) = A(X) ∩A(Y ),
A(X ∩Y ) = A(X) + A(Y )
for all X, Y ∈S(V );
(13.5)
and similar formulas hold for sums and intersections of indexed collections of subspaces.
13.7
Dual Maps
Suppose V and W are vector spaces over a field F, and T : V →W is a linear transformation.
If f : W →F is any element of W ∗, then the composition f ◦T is a linear map from V to
Dual Vector Spaces
349
F, so f ◦T is an element of V ∗. Thus, we can define a function T ∗: W ∗→V ∗by setting
T ∗(f) = f ◦T for all f ∈W ∗. The function T ∗is itself a linear map, since
T ∗(f + g) = (f + g) ◦T = (f ◦T) + (g ◦T) = T ∗(f) + T ∗(g)
for all f, g ∈W ∗;
T ∗(cf) = (cf) ◦T = c(f ◦T) = cT ∗(f)
for all c ∈F, f ∈W ∗.
To verify the equality (f +g)◦T = (f ◦T)+(g ◦T), note that both sides are functions from
V to F sending each x ∈V to f(T(x))+g(T(x)). A similar calculation justifies the equality
(cf) ◦T = c(f ◦T). We call the linear map T ∗: W ∗→V ∗the dual map to T : V →W.
Here are some algebraic properties of dual maps. First, the function sending T to T ∗
is an F-linear map from HomF (V, W) into HomF (W ∗, V ∗). This means that for any linear
maps T, U : V →W and scalar c ∈F, we have (T + U)∗= T ∗+ U ∗and (cT)∗= c(T ∗). For
example, the first equality holds because both sides are functions from W ∗to V ∗sending
g ∈W ∗to g ◦(T + U) = (g ◦T) + (g ◦U). Similarly, both (cT)∗and c(T ∗) send g to
c(g ◦T). Also, the mapping of T to T ∗is functorial. This means that for any linear maps
T : V →W and S : W →Z, (S ◦T)∗= T ∗◦S∗; and for any identity map id : V →V , the
dual map id∗: V ∗→V ∗is the identity map on V ∗. To check the first assertion, fix h ∈Z∗
and compute
(S ◦T)∗(h) = h ◦(S ◦T) = (h ◦S) ◦T = (S∗(h)) ◦T = T ∗(S∗(h)) = (T ∗◦S∗)(h).
The second assertion follows since id∗(f) = f ◦id = f for f ∈V ∗.
Suppose B = (v1, . . . , vn) is an ordered basis for V , and C = (w1, . . . , wm) is an ordered
basis for W. The matrix of T relative to input basis B and output basis C is the unique
m × n matrix of scalars A = [aij] such that T(vj) = Pm
i=1 aijwi for all j between 1 and n.
We have seen that the dual basis Bd = (f1, . . . , fn) is an ordered basis for V ∗, and the dual
basis Cd = (g1, . . . , gm) is an ordered basis for W ∗. Let us find the matrix of the linear map
T ∗relative to the bases Cd and Bd. By definition, this is the unique n×m matrix of scalars
A′ = [a′
rs] such that T ∗(gs) = Pn
r=1 a′
rsfr for all s between 1 and m. We claim that these
equations hold if we take a′
rs = asr, meaning that A′ is the transpose of the matrix A. To
see this, fix s with 1 ≤s ≤m, and check that the two functions T ∗(gs) and Pn
r=1 asrfr are
equal. Both functions are linear maps from V to F, so it suffices to see that they take the
same value at each basis element vk, where 1 ≤k ≤n. On one hand,
[T ∗(gs)](vk) = (gs ◦T)(vk) = gs(T(vk)) = gs
 m
X
i=1
aikwi
!
= ask.
The last equality uses the fact that Cd is the dual basis for C. On the other hand,
" n
X
r=1
asrfr
#
(vk) =
n
X
r=1
asrfr(vk) = ask
since Bd is the dual basis for B. To summarize, we have shown that if the matrix A represents
a linear map T relative to certain bases, then the transpose of A represents the dual map
T ∗relative to the dual bases. For this reason, the map T ∗is sometimes called the transpose
of the map T.
Given a linear map T : V →W, we have the dual map T ∗: W ∗→V ∗. Iterating
the construction produces the double dual map T ∗∗: V ∗∗→W ∗∗, which is defined by
T ∗∗(E) = E ◦T ∗for E ∈V ∗∗. Expanding this definition further, we see that T ∗∗(E) maps
g ∈W ∗to E(T ∗(g)) = E(g ◦T).
350
Advanced Linear Algebra
Now we can explain precisely what it means to say that the map ev = evV : V →V ∗∗
is natural. Naturality means that for any vector spaces V and W and any linear map
T : V →W, the following diagram commutes:
V
T
/
evV

W
evW

V ∗∗
T ∗∗/ W ∗∗
(that is, evW ◦T = T ∗∗◦evV ). To check this, fix x ∈V , and ask whether the two functions
evW (T(x)) = ET (x) and T ∗∗(evV (x)) = T ∗∗(Ex) in W ∗∗are equal. We fix g ∈W ∗and apply
each function to g. The first function produces ET (x)(g) = g(T(x)). The second function
gives
[T ∗∗(Ex)](g) = (Ex ◦T ∗)(g) = Ex(T ∗(g)) = Ex(g ◦T) = (g ◦T)(x) = g(T(x)).
Thus, the functions are equal, and the diagram does commute.
In the finite-dimensional case, we say that V ∗∗is naturally isomorphic to V under the
map evV , since we can transfer linear maps T : V →W to linear maps T ∗∗: V ∗∗→W ∗∗.
On the other hand, even though V and V ∗are isomorphic, it can be shown that there is no
natural isomorphism between V and V ∗when dim(V ) > 1 (Exercise 46). As we will soon
see, this situation can be partially remedied if V has appropriate additional structure (e.g.,
if V is an inner product space).
13.8
Bilinear Pairings of Vector Spaces
The rest of this chapter studies some variations and generalizations of the dual space
construction for finite-dimensional vector spaces. Let V and W be finite-dimensional vector
spaces over a field F. A function B : W × V →F is called a bilinear pairing of W and V
iff for all x, x′ ∈W, y, y′ ∈V , and c ∈F,
(a) [linearity in first input] B(x + x′, y) = B(x, y) + B(x′, y) and B(cx, y) = cB(x, y);
(b) [linearity in second input] B(x, y + y′) = B(x, y) + B(x, y′) and B(x, cy) = cB(x, y).
Here are two examples of bilinear pairings. Given any finite-dimensional space V , let
W = V ∗and B(f, y) = f(y) for f ∈V ∗and y ∈V . Condition (a) holds by definition of
the vector space operations in V ∗. For instance, for f, g ∈V ∗and y ∈V , B(f + g, y) =
(f + g)(y) = f(y) + g(y) = B(f, y) + B(g, y). Condition (b) holds since each f ∈V ∗is an
F-linear map. For the second example, let V = W = Rn and B(w, v) = w•v = w1v1+· · ·+
wnvn, the dot product on Rn. Conditions (a) and (b) are readily checked. When W = V ,
a bilinear pairing B is also called a bilinear form on V . We study bilinear forms in more
detail in Chapter 14.
Return to the case of a general bilinear pairing B : W × V →F. For each fixed y ∈V ,
define Ry : W →F by Ry(x) = B(x, y) for x ∈W. Condition (a) says that each Ry is a
linear map from W to F, so Ry belongs to the dual space W ∗. Similarly, for x ∈V , define
Lx : V →F by Lx(y) = B(x, y) for y ∈V . Condition (b) says that each Lx is a linear map
from V to F, so Lx ∈V ∗. We call Ry right-multiplication by y and Lx left-multiplication by
x (relative to the pairing B).
We can use these observations to define maps L : W →V ∗and R : V →W ∗determined
by the pairing B. For x ∈W and y ∈V , define L(x) = Lx ∈V ∗and R(y) = Ry ∈W ∗. We
already noted that each output L(x) and R(y) is an F-linear map. In fact, the functions L
Dual Vector Spaces
351
and R are also F-linear maps. For example, given x, x′ ∈W, we have L(x+x′) = L(x)+L(x′)
because both sides are functions from V to F sending y ∈V to B(x + x′, y) = B(x, y) +
B(x′, y). For x ∈W and c ∈F, L(cx) = cL(x) because both sides are functions from V to
F sending y ∈V to B(cx, y) = cB(x, y). We similarly verify the linearity of R.
Next, we introduce analogs of the operators Z and A for bilinear pairings. Given w ∈W
and v ∈V , say that w is orthogonal to v and write w ⊥v to mean B(w, v) = 0F . Given
S ⊆W and T ⊆V , say that S is orthogonal to T and write S ⊥T to mean B(s, t) = 0F for
all s ∈S and all t ∈T. When S = {s} has a single element, we write s ⊥T to mean {s} ⊥T;
and S ⊥t means S ⊥{t}. For any subset S of W, define the orthogonal complement of S
in V to be
S⊥= {v ∈V : S ⊥v} = {v ∈V : B(s, v) = 0 for all s ∈S}.
By linearity of B in the second input, S⊥is always a subspace of V (Exercise 62). Similarly,
for each subset T of V , define the orthogonal complement of T in W to be
⊥T = {w ∈W : w ⊥T} = {w ∈W : B(w, t) = 0 for all t ∈T}.
By linearity of B in the first input, T ⊥is a subspace of W. Using these definitions, we
readily check the following properties: for all S1, S2, S ⊆W and T1, T2, T ⊆V , S1 ⊆S2
implies S⊥
2 ⊆S⊥
1 ; T1 ⊆T2 implies ⊥T2 ⊆⊥T1; S ⊆⊥(S⊥); and T ⊆(⊥T)⊥.
We can now compute the kernels of the linear maps L : W →V ∗and R : V →W ∗. For
w ∈W, the following conditions are equivalent: w ∈ker(L); L(w) = 0, the zero map from
V to F; L(w)(v) = 0 for all v ∈V ; B(w, v) = 0 for all v ∈V ; w ∈⊥V . So ker(L) = ⊥V ;
we call this subspace of W the left kernel of B. Similarly, ker(R) = W ⊥is a subspace of
V called the right kernel of B. We say that B is left-nondegenerate iff ker(L) = {0}, in
which case L is a one-to-one map of W into V ∗. We say that B is right-nondegenerate iff
ker(R) = {0}, in which case R is a one-to-one map of V into W ∗.
13.9
Theorems on Bilinear Pairings
Under appropriate nondegeneracy assumptions, bilinear pairings lead to isomorphisms
involving dual vector spaces.
Theorem on Pairings Nondegenerate on Both Sides. Suppose W and V are finite-
dimensional F-vector spaces and B : W × V →F is a bilinear pairing that is both left-
nondegenerate and right-nondegenerate. Then L : W →V ∗and R : V →W ∗are vector
space isomorphisms, and dim(V ) = dim(W).
Proof. The nondegeneracy assumptions imply that L and R are both one-to-one, so
dim(W) ≤dim(V ∗) = dim(V ) ≤dim(W ∗) = dim(W).
Thus, the four spaces V , V ∗, W, and W ∗all have the same finite dimension n. Since L is
a one-to-one map from W to V ∗, where both spaces have dimension n, L must be onto.
Similarly R is onto. So L and R are isomorphisms.
Bijectivity of L can be rephrased as follows: for every linear map g : V →F, there exists
a unique vector w ∈W such that g(v) = B(w, v) for all v ∈V . This w exists because L is
onto, and w is unique because L is one-to-one. This gives a convenient concrete description
of the functions in V ∗: each such function is left-multiplication by w (relative to B) for
352
Advanced Linear Algebra
some uniquely determined w ∈W. Now W ∼= V ∗implies W ∗∼= V ∗∗. Combining this
isomorphism with R, we see that each function in V ∗∗can be viewed as the element of
W ∗given by right-multiplication by v for some unique v ∈V . Compare this to our earlier
description of V ∗∗, where every element of V ∗∗was given by evaluation at some uniquely
determined v ∈V . For the pairing of V ∗and V given by B(f, x) = f(x), you can checkk
that these two descriptions of elements of V ∗∗coincide.
Theorem on Left-Nondegenerate Pairings. Suppose W and V are finite-dimensional
vector spaces and B : W ×V →F is a left-nondegenerate bilinear pairing. For any subspace
S of W, there are isomorphisms V/S⊥∼= S∗and S ∼= (V/S⊥)∗induced by the maps L and
R. We have dim(V ) = dim(S⊥) + dim(S) and ⊥(S⊥) = S.
Taking S = W, we conclude in particular that V/W ⊥∼= W ∗and W ∼= (V/W ⊥)∗.
Analogous results hold for a right-nondegenerate pairing.
Proof. Let S be any subspace of W. Define R′ : V →S∗by R′(v)(s) = B(s, v) for v ∈V ,
s ∈S. Reasoning as we did earlier for R, the bilinearity of B shows that R′ does map V
into S∗, R′ is F-linear, and ker(R′) = S⊥. Passing to the quotient of V by this kernel, we
get a well-defined one-to-one linear map R′ : V/S⊥→S∗.
On the other hand, let T be any subspace of V . Define a map L′ : ⊥T →(V/T)∗by
L′(w)(v + T) = B(w, v) for w ∈⊥T, v ∈V . To see that L′ is well-defined, fix w ∈⊥T and
v, v′ ∈V with v + T = v′ + T. Then v −v′ ∈T, so B(w, v −v′) = 0 because w ∈⊥T. This
gives B(w, v) −B(w, v′) = 0 and B(w, v) = B(w, v′). Bilinearity of B implies that L′(w)
does belong to (V/T)∗and that L′ is an F-linear map. Observe that for w ∈ker(L′), we
have B(w, v) = 0 for all v ∈V , so w ∈⊥V = {0} because B is left-nondegenerate. So L′ is a
one-to-one linear map from ⊥T into (V/T)∗. Apply this result to the subspace T = S⊥of V .
Then L′ : ⊥(S⊥) →(V/S⊥)∗is one-to-one and linear. We also have S ⊆⊥(S⊥). Combining
all of this information gives
dim(S) ≤dim(⊥(S⊥)) ≤dim((V/S⊥)∗) = dim(V/S⊥) ≤dim(S∗) = dim(S).
So all the dimensions appearing here are equal and finite. It follows that S = ⊥(S⊥) and that
we have isomorphisms R′ : V/S⊥∼= S∗and L′ : S ∼= (V/S⊥)∗. From dim(V/S⊥) = dim(S),
we deduce that dim(V ) = dim(S⊥) + dim(S).
For any vector space V , recall S(V ) is the set of all subspaces of V , which is a
lattice ordered by set inclusion. The next theorem shows how bilinear pairings induce
correspondences between subspace lattices.
Theorem on Pairings and Subspace Lattices. Suppose W and V are finite-dimensional
vector spaces and B : W × V →F is a bilinear pairing. Define F : S(W) →S(V ) by
F(S) = S⊥for S ∈S(W). Define G : S(V ) →S(W) by G(T) = ⊥T for T ∈S(V ).
The maps F and G are inclusion-reversing. If B is left-nondegenerate, then G(F(S)) = S
and dim(V ) = dim(F(S)) + dim(S) for all S ∈S(W). If B is right-nondegenerate, then
F(G(T)) = T and dim(W) = dim(G(T)) + dim(T) for all T ∈S(V ). If B is nondegenerate
on the left and right, then F and G are two-sided inverses of each other and are lattice
anti-isomorphisms between S(W) and S(V ).
Proof. All assertions follow from the previous theorem and its analog for right-
nondegenerate pairings.
Theorem on General Bilinear Pairings. Suppose W and V are finite-dimensional
vector spaces and B : W × V →F is a bilinear pairing. Each space W/⊥V and V/W ⊥
is isomorphic to the dual of the other, so dim(W) −dim(⊥V ) = dim(V ) −dim(W ⊥). In
Dual Vector Spaces
353
particular, dim(V ) = dim(W) implies dim(⊥V ) = dim(W ⊥). So when V and W have the
same dimension, B is left-nondegenerate iff B is right-nondegenerate.
Proof. Define B′ : W/⊥V × V/W ⊥→F by B′(w + ⊥V, v + W ⊥) = B(w, v) for w ∈W,
v ∈V . It is routine to check that B′ is well-defined, bilinear, left-nondegenerate, and right-
nondegenerate (Exercise 64). By the first theorem in this section, we get isomorphisms
W/⊥V ∼= (V/W ⊥)∗and V/W ⊥∼= (W/⊥V )∗. The remaining assertions of the theorem
follow from this.
We summarize our conclusions in the case where dim(V ) = dim(W) = n and the
pairing B : W × V →F is left-nondegenerate or right-nondegenerate. In this case, B is
nondegenerate on both sides; L : W →V ∗and R : V →W ∗are isomorphisms; for any
subspace S of W, dim(S) + dim(S⊥) = n and ⊥(S⊥) = S; and for any subspace T of V ,
dim(T) + dim(⊥T) = n and (⊥T)⊥= T.
Theorem on Adjoints. Suppose W and V
are n-dimensional F-vector spaces and
B : W × V →F is a nondegenerate bilinear pairing. For each linear operator T : W →W,
there exists a unique linear operator T ′ : V →V satisfying
B(T(x), y) = B(x, T ′(y))
for all x ∈W, y ∈V .
(13.6)
T ′ is called the right adjoint of T relative to B. A similar result holds for left adjoints.
Proof. Fix y ∈V . The function sending x ∈W to B(T(x), y) ∈F is a linear map from W
to F, as is readily checked. Since R : V →W ∗is an isomorphism by our assumptions on
B, there is a unique element of V , call it T ′(y), such that B(T(x), y) = B(x, T ′(y)) holds
for all x ∈W. Thus there exists a unique function T ′ : V →V satisfying (13.6). It suffices
to check that T ′ is linear. Given y, z ∈V and any x ∈W, compute
B(T(x), y+z) = B(T(x), y)+B(T(x), z) = B(x, T ′(y))+B(x, T ′(z)) = B(x, T ′(y)+T ′(z)).
On the other hand, T ′(y + z) is the unique vector in V satisfying B(T(x), y + z) =
B(x, T ′(y + z)) for all x ∈W. So T ′(y +z) = T ′(y)+T ′(z). The proof that T ′(cy) = cT ′(y)
for c ∈F and y ∈V is similar.
Exercise 66 asks you to prove this theorem using the dual map T ∗: W ∗→W ∗.
Specifically, you can check that T ′ = R−1 ◦T ∗◦R. Informally, using the isomorphism
R : V ∼= W ∗, the right adjoint map T ′ : V →V can be identified with the dual map
T ∗: W ∗→W ∗.
13.10
Real Inner Product Spaces
We can use the theory of dual spaces and bilinear pairings to derive some fundamental
properties of inner product spaces. A real inner product space is a real vector space V and a
bilinear form (pairing) B : V × V →R such that for all x, y ∈V , B(x, y) = B(y, x), and for
all nonzero v ∈V , B(v, v) is a strictly positive real number. We often write B(x, y) = ⟨x, y⟩
and call B a symmetric, positive definite inner product on V . Such an inner product is
automatically nondegenerate: if x ∈V satisfies B(x, y) = 0 for all y ∈V , then B(x, x) = 0,
so x = 0. Because B is symmetric, we need not distinguish left and right when discussing
nondegeneracy or orthogonal complements. In particular, for any set S ⊆V , we write S⊥⊥
354
Advanced Linear Algebra
instead of ⊥(S⊥). The most familiar example of a real inner product space is the vector
space Rn with the dot product.
Given a finite-dimensional real inner product space V , we can restate some results from
the last section as follows. For each linear functional f : V →R, there is a unique x ∈V
such that f(y) = ⟨y, x⟩for all y ∈V . For each linear map T : V →V , there is a unique
adjoint map T ′ : V →V characterized by ⟨T(x), y⟩= ⟨x, T ′(y)⟩for all x, y ∈V . For any
ordered basis C = (x1, . . . , xn) of V , there exists a unique dual basis C′ = (y1, . . . , yn) for
V such that ⟨xi, yj⟩is 1 for i = j and 0 otherwise. To see this, let Cd = (f1, . . . , fn) be the
unique dual basis of C in V ∗. Then let y1, . . . , yn be the unique elements of V such that fj
is right-multiplication by yj. Note that (y1, . . . , yn) is an ordered basis of V , since it is the
image of the ordered basis Cd under the isomorphism R−1 : V ∗→V . If T has matrix A
relative to the ordered basis C of V , then the adjoint map T ′ has matrix AT (the transpose
of A) relative to the dual basis Cd (Exercise 67). For Rn with the dot product, the standard
basis C = (e1, . . . , en) is self-dual, meaning C′ = C.
For any subspace U of V , the orthogonal complement of U relative to B is
U ⊥= {y ∈V : ⟨u, y⟩= 0 for all u ∈U}.
The map F sending U to U ⊥is an inclusion-reversing bijection on the lattice S(V ) of
subspaces of V , and F is its own inverse: U ⊥⊥= U for all U ∈S(V ). Moreover, dim(U) +
dim(U ⊥) = dim(V ). Positive-definiteness shows that U ∩U ⊥= {0V }, since any x belonging
to both U and U ⊥satisfies B(x, x) = 0, hence x = 0. Now U + U ⊥is a subspace of V
of dimension dim(U) + dim(U ⊥) −dim(U ∩U ⊥) = dim(V ), so U + U ⊥must be all of V .
We summarize the two conditions U + U ⊥= V and U ∩U ⊥= {0} by saying that V is
the direct sum U ⊕U ⊥of any subspace U and its orthogonal complement. Beware: if we
replace positive definiteness by the weaker hypothesis that B is nondegenerate, then the
conclusions U ∩U ⊥= {0} and V = U + U ⊥are not guaranteed to hold.
Orthogonal complements are closely related to the operators A and Z from our discussion
of dual spaces. Given a finite-dimensional real inner product space V , nondegeneracy of the
inner product means there is an isomorphism R : V →V ∗. You can check that for all S ⊆V ,
A(S) = R[S⊥]; and for all T ⊆V ∗, Z(T) = R−1[T]⊥(Exercise 68). If we identify V with
V ∗using R, then these formulas can be written A(S) = S⊥and Z(T) = T ⊥. Informally,
for a real inner product space, the A and Z operators coalesce into a single orthogonality
operator acting on subsets of V .
13.11
Complex Inner Product Spaces
Recall that C = {x+iy : x, y ∈R} is the field of complex numbers, and complex conjugation
is defined by x + iy = x−iy for x, y ∈R. A complex inner product space consists of a complex
vector space V and a function B : V ×V →C satisfying these conditions for all v, v′, w ∈V
and c ∈C:
(a) [linearity in first input] B(v + v′, w) = B(v, w) + B(v′, w) and B(cv, w) = cB(v, w);
(b) [conjugate symmetry] B(w, v) = B(v, w);
(c) [positive definiteness] if v ̸= 0, then B(v, v) ∈R>0.
These conditions readily imply a fourth condition:
(d) [conjugate-linearity in second input] B(w, v + v′) = B(w, v) + B(w, v′) and B(w, cv) =
cB(w, v).
We often write B(v, w) = ⟨v, w⟩and call B a complex inner product. As before, v ⊥w
means B(v, w) = 0, which is equivalent to w ⊥v since 0 = 0. For S ⊆V , the orthogonal
Dual Vector Spaces
355
complement of S is S⊥= {v ∈V : S ⊥v}. Some texts use the convention that a complex
inner product is conjugate-linear in the first input and linear in the second input.
An example of a complex inner product space is Cn with the complex dot product,
defined on inputs v = (v1, . . . , vn) and w = (w1, . . . , wn) in Cn by ⟨v, w⟩= v1w1 + v2w2 +
· · · + vnwn. In particular, positive definiteness holds because ⟨v, v⟩= |v1|2 + · · · + |vn|2,
which is a positive real number for v ̸= 0. This fact would not hold without the conjugates
in the complex dot product, which is the main reason why we require conjugate-symmetry
rather than ordinary symmetry of B.
The main facts for a finite-dimensional complex inner product space V are similar to
the real case. The map sending U to U ⊥is a self-inverse, inclusion-reversing bijection on
the lattice of subspaces of V ; U ⊥⊥= U; dim(U) + dim(U ⊥) = dim(V ); and V = U ⊕U ⊥.
To prove these facts using the general theory of bilinear pairings, we need a technical
device to handle the conjugate-linearity in the second position. Define σ : C →C by σ(z) = z
for z ∈C. The map σ is a ring isomorphism that is equal to its own inverse. For any complex
vector space V , we define a new complex vector space V σ that has the same underlying set
and addition operation as V . Writing · for the scalar multiplication on V , define the scalar
multiplication • on V σ by c • v = c · v for all c ∈C, v ∈V . It is routine to check that V σ
really is a complex vector space; a subset U of V is a subspace of V iff U is a subspace of
V σ; and a list of vectors in the space V spans U, or is linearly independent, or is an ordered
basis of U, iff the list has the corresponding property in the space V σ. A C-linear map g
from V σ into another complex vector space Z is sometimes called a semi-linear map from
V to Z. Such a map satisfies g(v + w) = g(v) + g(w) and g(c · v) = g(c • v) = cg(v) for all
v, w ∈V and all c ∈C.
Suppose B is a complex inner product on V . The axioms for B show that B : V × V σ →
C is a bilinear pairing of C-vector spaces. In particular, for w ∈V , v ∈V σ, and c ∈C,
axiom (d) gives B(w, c • v) = B(w, c · v) = cB(w, v) = cB(w, v). So B is C-linear in both its
first and second inputs. The pairing B is nondegenerate since B(v, v) ̸= 0 for all nonzero
v ∈V . The facts claimed about orthogonal complements in the previous paragraph now
follow from the general results in §13.9. We also see that V and V σ are duals of each other
via the isomorphisms L : V →(V σ)∗and R : V σ →V ∗. In particular, every C-linear map
f : V →C has the form f(x) = ⟨x, y⟩for a unique y ∈V . Every semi-linear map g : V →C
has the form g(y) = ⟨x, y⟩for a unique x ∈V . For each ordered basis C = (x1, . . . , xn) of
V , we obtain a unique dual basis C′ = (y1, . . . , yn) of V σ (which is also a basis of V ), where
⟨xi, yj⟩is 1 for i = j and 0 for i ̸= j. For each C-linear map T : V →V , there is a unique
C-linear adjoint map T ′ : V σ →V σ such that ⟨T(x), y⟩= ⟨x, T ′(y)⟩for all x, y ∈V . In
fact, T ′ is also a C-linear (not semi-linear!) map from V to V , since
T ′(c · v) = T ′(c • v) = c • T ′(v) = c · T ′(v)
for all c ∈C, v ∈V .
Suppose A = [T]C is the matrix of T relative to the ordered basis C. Viewing T ′ as a
linear map on V σ, the matrix of T relative to C′ is the transpose of A. But viewing T ′ as a
linear map on V , the matrix of T relative to C′ is the conjugate-transpose of A, which has
i, j-entry A(j, i) (Exercise 70). For Cn with the complex dot product, the standard basis
C = (e1, . . . , en) is self-dual, meaning C′ = C.
13.12
Duality for Infinite-Dimensional Spaces
Although many of the results in this chapter apply to arbitrary vector spaces V , some
theorems require the hypothesis that V
be finite-dimensional. For example, we used
356
Advanced Linear Algebra
finite-dimensionality in the proofs that dim(V ) = dim(V ∗), that ev : V →V ∗∗is an
isomorphism, and that A and Z are bijections between the subspace lattices of V and
V ∗. If we start with an infinite-dimensional space V , it can be shown that the dimension
of the dual space V ∗is a larger infinite cardinal than dim(V ) (compare to Exercises 12
and 15). Then dim(V ∗∗) is larger still, so that V and V ∗∗are never isomorphic in the
infinite-dimensional case.
In order to obtain more satisfactory results for infinite-dimensional spaces, we can add
extra structure to the vector space V and modify the definition of the dual space V ∗to take
into account this extra structure. We sketch how this is done for Banach spaces. In many
real or complex vector spaces V , it is possible to define the norm or length of a vector,
denoted ||x||, satisfying these conditions: ||0|| = 0; for all nonzero x ∈V , ||x|| ∈R>0;
||cx|| = |c| · ||x|| for all scalars c and x ∈V ; and ||x + y|| ≤||x|| + ||y|| for all x, y ∈V .
The space V together with the norm function on V is called a normed vector space. Using
the norm, we can define the distance between two vectors by setting d(x, y) = ||x −y|| for
x, y ∈V , and this turns V into a metric space. If this metric space is complete (meaning
that every Cauchy sequence of vectors in V converges to a vector in V ; see Chapter 15 for
more details), then V is called a Banach space.
Given two Banach spaces V and W, we could study the vector space HomF (V, W)
of all linear maps from V to W. However, it is more fruitful to restrict attention to the
subspace of continuous linear maps from V to W. It can be shown that a linear map
T : V →W is continuous iff T is continuous at 0V iff there is a real constant C > 0 such
that ||T(x)|| ≤C||x|| for all x ∈V . For this reason, continuous linear maps are also called
bounded linear operators. In this setting, we adjust the definition of the dual space so that
V ∗consists of all continuous linear maps from V to the field of scalars (R or C).
With this adjusted definition, various Banach spaces that occur in integration theory
turn out to be duals of each other. For instance, for each fixed p with 1 < p < ∞, there
is a real Banach space Lp consisting of Lebesgue-measurable functions f : R →R such
that
R
R |f(x)|p dx is finite. Let q ∈R>1 satisfy 1/p + 1/q = 1. In advanced analysis, it is
shown that Lq is isomorphic to L∗
p as a Banach space. The isomorphism sends g ∈Lq to
the continuous linear functional that maps f ∈Lp to
R
R f(x)g(x) dx. This is an instance
of right-multiplication by g relative to a bilinear pairing; a key technical point is proving
that the integral of the product fg is finite. Since Lq ∼= L∗
p and L∗
q ∼= Lp, we deduce that
L∗∗
p ∼= L∗
q ∼= Lp, as we might have expected from the finite-dimensional case. However, not
all Banach spaces are isomorphic to their double duals. A Banach space V that is isomorphic
to V ∗∗is called reflexive.
Another subtlety that occurs in Banach spaces involves the Z and A operators. Recall
that for S ⊆V ∗, Z(S) consists of all x ∈V such that f(x) = 0 for all f ∈S. We can express
this in symbols by writing Z(S) = T
f∈S f −1[{0}]. In the setting of Banach spaces, each
f ∈S must be continuous, so that the preimage f −1[{0}] of the closed set {0} is a closed
subset of the metric space V . Then Z(S), being an intersection of closed sets, is always a
closed set (as well as a subspace) in V . Similarly, it can be shown that V ∗is a Banach space
and A(T) is closed in V ∗for all T ⊆V . Thus, in order to turn the operators Z and A into
bijective correspondences, it becomes necessary to restrict attention to the lattices of closed
subspaces of V and V ∗.
As the above discussion indicates, we often need to add topological ingredients to obtain
an effective theory for infinite-dimensional vector spaces. In particular, restricting attention
to continuous linear maps and closed linear subspaces is often essential. We shall see how
this works in more detail in Chapter 15, which covers metric spaces and the basic elements
of Hilbert space theory.
Dual Vector Spaces
357
13.13
A Preview of Affine Algebraic Geometry
We end this chapter with a brief introduction to another correspondence between spaces
and functions that appears in elementary algebraic geometry. We start with an algebraically
closed field F such as the field of complex numbers. Let F n = {(c1, . . . , cn) : ci ∈F} be
the set of all n-tuples of elements of F. In this setting, F n is called affine n-space over F.
Next, let R be the polynomial ring F[x1, . . . , xn]. Each formal polynomial p ∈R determines
a polynomial function (also denoted p) from F n to F. For instance, when n = 3 and
p = x3
1 + 2x2x2
3, the polynomial function p : F 3 →F is given by p(c1, c2, c3) = c3
1 + 2c2c2
3
for c1, c2, c3 ∈F.
As in the case of linear functionals, we can define a zero-set operator Z that maps subsets
of R to subsets of F n. Specifically, for all S ⊆R, let
Z(S) = {(c1, . . . , cn) ∈F n : p(c1, . . . , cn) = 0 for all p ∈S}.
We can regard Z(S) as the solution set of the (possibly infinite) system of polynomial
equations p(v) = 0 for p ∈S. Any subset of F n that has the form Z(S) for some S ⊆R
is called an affine variety in F n. Many texts write V(S) for Z(S) and call Z the variety
operator.
We list without proof some properties of Z. First, Z reverses inclusions: if S ⊆T ⊆R,
then Z(T) ⊆Z(S). Each subset S ⊆R generates an ideal
I = ⟨S⟩= {r1s1 + · · · + rmsm : m ∈Z≥0, ri ∈R, si ∈S}
consisting of all finite R-linear combinations of polynomials in S. Define the radical of any
ideal I of R to be
√
I = {p ∈R : pk ∈I for some k ∈Z>0}. The second property of Z
is Z(S) = Z(I) = Z(
√
I) where I = ⟨S⟩. Third, Z({0}) = F n and Z({1}) = Z(R) = ∅.
Fourth, for a family of subsets {Sj : j ∈J} of R, Z(S
j∈J Sj) = T
j∈J Z(Sj). Fifth, for a
family of ideals {Ij : j ∈J} of R, Z(P
j∈J Ij) = T
j∈J Z(Ij). Sixth, for ideals I1, I2, . . . , Ik
in R, Z(I1I2 · · · Ik) = Z(Tk
j=1 Ij) = Sk
j=1 Z(Ij). The Hilbert Basis Theorem asserts that
any ideal I in R = F[x1, . . . , xn] can be generated by a finite set S. Combining this theorem
with the second property, we see that any affine variety can be defined as the solution set
of a finite system of polynomial equations.
Next, we define an analog of the annihilator operator A, which maps subsets of F n to
subsets of R. Given S ⊆F n, let
A(S) = {p ∈R : p(c1, . . . , cn) = 0 for all (c1, . . . , cn) ∈S}.
It can be checked that A(S) is always a radical ideal of R; this is an ideal I such that
I =
√
I. Some texts write I(S) for A(S).
We state some properties of the annihilator operator and its relation to the zero-set
operator. First, A(∅) = R and A(F n) = {0R}. Second, A reverses inclusions: if S ⊆T ⊆F n,
then A(T) ⊆A(S). Third, for any family of subsets {Sj : j ∈J} of F n, A(S
j∈J Sj) =
T
j∈J A(Sj). Fourth, for any S ⊆F n, Z(A(S)) is the intersection of all varieties in F n that
contain S; this intersection is the smallest variety containing S. Fifth, for any subset S of
R, A(Z(S)) =
p
⟨S⟩. The fifth property, which requires algebraic closure of the field F, is
one version of a difficult theorem of algebraic geometry called Hilbert’s Nullstellensatz.
It follows from the results stated above that Z and A restrict to give poset anti-
isomorphisms between the lattice of affine varieties in F n and the lattice of radical ideals
of R = F[x1, . . . , xn]. This ideal–variety correspondence has many additional structural
properties. For instance, maximal ideals of R correspond to individual points in F n, prime
358
Advanced Linear Algebra
ideals of R correspond to irreducible varieties in F n, and so on. For more details, we refer
the reader to the excellent text by Cox, Little, and O’Shea [12].
13.14
Summary
Let F be a field and V and W be finite-dimensional vector spaces over F. Table 13.1
summarizes definitions concerning dual spaces. We also recall the following results, some of
which extend to infinite-dimensional spaces.
TABLE 13.1
Definitions related to dual spaces.
Concept
Definition
HomF (V, W)
set of all F-linear maps T : V →W
(a vector space under pointwise operations)
dual space V ∗
V ∗= HomF (V, F)
dual basis of ordered basis
unique ordered basis Xd = (f1, . . . , fn) of V ∗
X = (x1, . . . , xn) of V
with fi(xj) = 1 for i = j, fi(xj) = 0 for i ̸= j
zero set Z(S) of S ⊆V ∗
subspace {x ∈V : f(x) = 0 for all f ∈S}
annihilator A(T) of T ⊆V
subspace {g ∈V ∗: g(y) = 0 for all y ∈T}
double dual V ∗∗
V ∗∗= HomF (V ∗, F) = HomF (HomF (V, F), F)
evaluation map Ex for x ∈V
Ex ∈V ∗∗sends g ∈V ∗to g(x) ∈F
ev : V →V ∗∗
ev(x) = Ex for x ∈V (one-to-one linear map)
dual map of T : V →W
T ∗: W ∗→V ∗given by T ∗(g) = g ◦T for g ∈W ∗
bilinear pairing B : W × V →F
B is F-linear in first and second position
L : W →V ∗
L(w) sends v ∈V to B(w, v)
R : V →W ∗
R(v) sends w ∈W to B(w, v)
S ⊥T
B(s, t) = 0 for all s ∈S, t ∈T
S⊥for S ⊆W
subspace {v ∈V : S ⊥v}
⊥T for T ⊆V
subspace {w ∈W : w ⊥T}
left-nondegenerate pairing
⊥V = {0W }, so L is one-to-one
right-nondegenerate pairing
W ⊥= {0V }, so R is one-to-one
right adjoint of T : W →W
unique linear map T ′ : V →V such that
B(T(x), y) = B(x, T ′(y)) for x ∈W, y ∈V
real inner product
symmetric bilinear form on R-vector space V
with ⟨x, x⟩> 0 for all x ̸= 0 in V
complex inner product
form on C-vector space that is linear in input 1,
⟨v, w⟩= ⟨w, v⟩, and 0 ̸= v implies ⟨v, v⟩∈R>0
1.
Universal Mapping Property for Basis of a Vector Space. For every ordered basis
B = (x1, . . . , xn) of V and every list (y1, . . . , yn) of vectors in W, there exists a
unique F-linear map T : V →W with T(xi) = yi for 1 ≤i ≤n.
2.
Dual Bases. For every ordered basis B = (x1, . . . , xn) of V , there exists a unique
dual basis Bd = (f1, . . . , fn) of V ∗with fi(xj) = δij for all i, j between 1 and n.
For every ordered basis C = (g1, . . . , gn) of V ∗, there is a unique ordered basis
Dual Vector Spaces
359
CD = (z1, . . . , zn) of V with gi(zj) = δij for all i, j between 1 and n Since dim(V )
is finite, dim(V ) = dim(V ∗) and V ∼= V ∗. But this isomorphism is not natural;
it depends on a choice of basis for V .
3.
Zero-Set Operator and Annihilator Operator.
(a) For S ⊆V ∗, Z(S) is the set of x ∈V such that f(x) = 0 for all f ∈S.
(b) For T ⊆V , A(T) is the set of g ∈V ∗such that g(y) = 0 for all y ∈T.
(c) The operators Z and A reverse inclusions, map subsets to subspaces, and
satisfy Z(S) = Z(⟨S⟩), A(T) = A(⟨T⟩), A(Z(S)) = ⟨S⟩, and Z(A(T)) = ⟨T⟩.
(d) Since dim(V ) is finite, Z and A restrict to give poset anti-isomorphisms
between the lattice of subspaces of V and the lattice of subspaces of V ∗.
(e) For subspaces U and W of V ∗, dim(U) + dim(Z(U)) = dim(V ), Z(U + W) =
Z(U) ∩Z(W), and Z(U ∩W) = Z(U) + Z(W); similar formulas hold for A.
4.
Double Duals and Evaluation Maps.
(a) For x ∈V , evaluation at x is the linear map Ex : V ∗→F given by Ex(g) =
g(x) for g ∈V ∗. Ex belongs to the double dual space V ∗∗.
(b) The function ev : V →V ∗∗sending each x ∈V to Ex is linear and one-to-one.
(c) Since dim(V ) is finite, ev gives a natural vector-space isomorphism V ∼= V ∗∗.
Naturality means that T ∗∗= evW ◦T ◦ev−1
V
for all linear maps T : V →W.
(d) Using ev, the zero-set and annihilator operators linking V ∗and V ∗∗can be
identified with the annihilator and zero-set operators linking V and V ∗.
5.
Dual Maps.
(a) For each linear map T : V →W, there is a linear dual map T ∗: W ∗→V ∗
given by T ∗(g) = g ◦T for g ∈W ∗.
(b) If A is the matrix of T relative to the ordered bases B for V and C for W,
then the matrix of T ∗relative to the dual bases Cd and Bd is the transpose of A.
(c) For linear maps T and U and r ∈F, the formulas (T + U)∗= T ∗+ U ∗,
(rT)∗= r(T ∗), (idV )∗= idV ∗, and (T ◦U)∗= U ∗◦T ∗hold when defined.
6.
Bilinear Pairings.
(a) A bilinear pairing B : W × V →F induces linear maps L : W →V ∗and
R : V →W ∗with ker(L) = ⊥V and ker(R) = W ⊥.
(b) If B is left-nondegenerate and right-nondegenerate, then L and R are
isomorphisms and dim(V ) = dim(W).
(c) If B is left-nondegenerate and S is any subspace of W, then V/S⊥∼= S∗,
S ∼= (V/S⊥)∗, dim(V ) = dim(S⊥) + dim(S), and ⊥(S⊥) = S.
(d) For general B, each space W/⊥V and V/W ⊥is isomorphic to the dual of the
other, and dim(V ) = dim(W) implies dim(⊥V ) = dim(W ⊥).
(e) When dim(V ) = dim(W), B is left-nondegenerate iff B is right-nondegenerate.
7.
Pairings and Subspace Lattices. Suppose dim(V ) = dim(W) = n and the
bilinear pairing B : W × V →F is nondegenerate. The orthogonal complement
operators (relative to B) define mutually inverse, inclusion-reversing poset anti-
isomorphisms between the subspace lattice of W and the subspace lattice of V .
For S ∈S(W) and T ∈S(V ), dim(S) + dim(S⊥) = n = dim(T) + dim(⊥T).
8.
Adjoint Maps. Suppose dim(V ) = dim(W) = n and the bilinear pairing
B : W × V →F is nondegenerate. For each linear map T : W →W, there is
a unique linear right adjoint T ′ : V →V such that B(T(x), y) = B(x, T ′(y)) for
all x ∈W and y ∈V . A similar result holds for left adjoints.
9.
Real Inner Product Spaces. For a real vector space V , an inner product is a
symmetric bilinear form on V satisfying ⟨x, x⟩> 0 for all x ̸= 0, which guarantees
360
Advanced Linear Algebra
nondegeneracy of the form. The map sending U to U ⊥is a self-inverse, inclusion-
reversing bijection on the lattice of subspaces of V . For all subspaces U of V ,
U ⊥⊥= U, dim(U) + dim(U ⊥) = dim(V ), U ∩U ⊥= {0}, and V = U ⊕U ⊥.
10.
Complex Inner Product Spaces. For a complex vector space V , a complex inner
product is a map from V ×V to C that is linear in the first input, conjugate-linear
in the second input, conjugate symmetric, and positive definite. The subspace
facts from the previous item hold. The matrix of an adjoint map (relative to a
dual basis) is the conjugate-transpose of the matrix of the original map.
11.
Infinite-Dimensional Spaces. For an infinite-dimensional F-vector space with
topological structure (such as a Banach space where each vector has a norm),
we redefine V ∗to be the set of continuous linear maps from V to F. Here, V ∗∗
may or may not be isomorphic to V , and we must restrict Z and A to the lattices
of closed linear subspaces to get lattice isomorphisms.
13.15
Exercises
In these exercises, assume F is a field and V, W are finite-dimensional F-vector spaces unless
otherwise stated.
1.
Let S be a set and Z be the set of functions f : S →W.
(a) Verify that Z is a commutative group under pointwise addition of functions.
(b) Verify that Z is an F-vector space under pointwise operations.
2.
Let V and W be F-vector spaces. Prove that HomF (V, W) is a subspace of the
vector space of all functions from V to W (under pointwise operations).
3.
Explain why the set V of all differentiable functions f : (0, 3) →R is a subspace
of the set of all functions from (0, 3) to R (under pointwise operations).
4.
Let V be the vector space of differentiable functions f : (0, 3) →R under pointwise
operations. Decide whether each operator below is a linear functional in V ∗.
(a) D : V →V given by D(f) = f ′
(b) E : V →R given by E(f) = f(1)
(c) F : V →R given by F(f) = f ′(2)
(d) G : V →R given by G(f) = f(1)f(2)
(e) H : V →R given by H(f) =
R 2
1 f(x) dx
(f) I : V →R given by I(f) =
R 1
1/2 xf(x2) dx
5.
For n > 1, decide which functions below (with domain Mn(F)) are in Mn(F)∗.
(a) the map sending A to A(1, 2)
(b) the trace map given by tr(A) = Pn
i=1 A(i, i)
(c) the determinant function sending A to det(A)
(d) the transpose map sending A to AT
(e) the map sending A to the sum of all entries of A
(f) for fixed v ∈Mn,1(F), the map sending A to vTAv
(g) for F = R, the map sending A to the number of zero entries of A
6.
Check that the map T defined in the proof of the UMP (page 342) is F-linear
and sends each xi to yi.
Dual Vector Spaces
361
7.
Let B = {Ei,j : 1 ≤i ≤m, 1 ≤j ≤n} be the standard basis for Mm,n(F), where
Ei,j is the matrix with 1 in the i, j-position and 0 elsewhere. Explicitly describe
how each element in the dual basis Bd acts on an arbitrary matrix A ∈Mm,n(F).
8.
Define a map T : M1,n(F) →Mn,1(F)∗as follows. For a row vector w, let T(w)
be the map sending each column vector v to the 1 × 1 matrix wv, regarded as
an element of F. Prove that T is a vector space isomorphism. We can use this
isomorphism to identify the dual space of the vector space of n×1 column vectors
with the vector space of 1 × n row vectors.
9.
Let (E(1, 1), E(2, 1), . . . , E(n, 1)) be the standard ordered basis of Mn,1(F) and
(g1, g2, . . . , gn) be the dual basis of Mn,1(F)∗. Compute T −1(gj) for each j, where
T is the isomorphism in the previous exercise.
10.
Find a dual basis for the ordered basis
B =




2
1
1

,


1
0
3

,


−3
4
−1




of R3 = M3,1(R), using Exercise 8 to describe the answer as a list of row vectors.
11.
Given an ordered basis (v1, . . . , vn) of F n (viewed as column vectors), show how to
use matrix inversion to compute the dual basis of (F n)∗(viewed as row vectors).
12.
Formulate and prove a version of the universal mapping property in §13.2 that
applies to an infinite-dimensional vector space V with infinite basis B. Use the
UMP to prove that the dual space V ∗is isomorphic to the vector space of all
functions from B to F with pointwise operations.
13.
This exercise shows what can go wrong with the dual basis construction in the case
of infinite-dimensional vector spaces. Recall (Chapter 3) that F[x] is the vector
space of formal polynomials with coefficients in F. The set {xj : j ∈Z≥0} is a
basis for F[x]. For each k ∈Z≥0, define Ck : F[x] →F by Ck(P
i≥0 aixi) = ak.
Define h : F[x] →F by h(Pn
i=0 aixi) = Pn
i=0 ai.
(a) Prove Ck ∈F[x]∗, and compute Ck(xj) for all j, k ∈Z≥0.
(b) Prove {Ck : k ∈Z≥0} is F-linearly independent.
(c) Prove h ∈F[x]∗, but h is outside the span of {Ck : k ∈Z≥0}.
14.
Let F[[x]] be the vector space of formal power series with coefficients in F
(see §3.3). Define Ck : F[[x]] →F by Ck(P∞
i=0 aixi) = ak. Is {Ck : k ∈Z≥0} a
basis of F[[x]]∗?
15.
Prove that F[x]∗and F[[x]] are isomorphic as F-vector spaces.
16.
Elementary Operations and Dual Bases. Let B = (x1, . . . , xn) be an ordered
basis of V with dual basis Bd = (g1, . . . , gn).
(a) Let C be the ordered basis obtained from B by switching the positions of xi
and xj. Describe (with proof) how Cd is related to Bd.
(b) Let C = (x1, . . . , dxi, . . . , xn) where d ∈F is not 0. How is Cd related to Bd?
(c) Let C be obtained from B by replacing xi by xi + bxj for some i ̸= j and
some b ∈F. How is Cd related to Bd?
17.
Let Re and Im be the functions from C to R given by Re(a + ib) = a and
Im(a + ib) = b for a, b ∈R. Find an ordered basis B = (z1, z2) of C (viewed as a
real vector space) such that Bd = (Im, Re).
18.
Define f, g ∈(R2)∗by f((x, y)) = x+y and g((x, y)) = 2x−3y for x, y ∈R. Find
an ordered basis B = (v, w) of R2 such that Bd = (f, g).
362
Advanced Linear Algebra
19.
Given an ordered basis C = (f1, . . . , fn) of (F n)∗(viewed as row vectors), describe
a matrix computation that finds an ordered basis B = (v1, . . . , vn) of F n (viewed
as column vectors) such that C = Bd.
20.
Define functionals in (R4)∗by f((x1, x2, x3, x4)) = x1 + 2x2 −3x3 + x4,
g((x1, x2, x3, x4)) = 4x1 −x2 −x4, and h((x1, x2, x3, x4)) = 3x2 + 5x3 −x4.
(a) Find a basis for Z({f, g, h}).
(b) Find a basis for Z({f, g}).
(c) Find a basis for Z({f}).
21.
Let V be the vector space of differentiable functions f : (0, 3) →R. Define D, I ∈
V ∗by D(f) = f ′(1/2) and I(f) =
R 2
1 f(x) dx for f ∈V . Describe geometrically
which functions f belong to Z({D, I}).
22.
Let B = (x1, . . . , xn) be an ordered basis of V with dual basis Bd = (f1, . . . , fn).
(a) Prove: for all v ∈V , v = Pn
i=1 fi(v)xi.
(b) For 1 ≤i ≤n, explicitly describe Z({f1, . . . , fi}) and A({x1, . . . , xi}).
23.
Suppose T ⊆V and g ∈V ∗. Negate the definition to give a useful completion of
this sentence: “g ̸∈A(T) means...” Then prove that A(∅) = V ∗.
24.
For each subset of Rn, describe A(T) as explicitly as possible.
(a) {(2, 3)}
(b) {(s, s, s) : s ∈R}
(c) {(w, x, y, z) : w + x + y + z = 0 and w + z = x + y}
(d) {(x, y) : x2 + y2 = 1}
(e) {(x, y, z) : x2 + y2 = 1, z = 0}
25.
For each subset T in Exercise 24, describe Z(A(T)).
26.
For each subset T of Mn(R), describe A(T) in Mn(R)∗as explicitly as possible.
(a) {A ∈Mn(R) : tr(A) = 0}
(b) {A ∈Mn(R) : det(A) = 0}
(c) {upper-triangular A ∈Mn(R)}
(d) {diagonal A ∈Mn(R)}
(e) {cIn : c ∈R}
27.
Prove: for an n-dimensional vector space V and all T ⊆V ∗, there exists a subset
S of V ∗of size at most n with Z(T) = Z(S). Formulate and prove an analogous
statement for annihilators.
28.
Which of the five properties of zero-sets listed in §13.3 remain true for infinite-
dimensional vector spaces V ? Which of the five properties of annihilators listed
in §13.4 remain true for infinite-dimensional vector spaces V ?
29.
Show directly from the definitions that A(Z(S)) ⊇S for all S ⊆V ∗. Use this
and the identification of V and V ∗∗to deduce Z(A(T)) ⊇T for all T ⊆V .
30.
Prove: for any family of subsets {Si : i ∈I} of V ∗, Z(S
i∈I Si) = T
i∈I Z(Si).
Formulate and prove an analogous statement for the A operator.
31.
For general subsets S, T ⊆V ∗, must it be true that Z(S ∩T) = Z(S) + Z(T)?
32.
Use dual spaces to prove that every k-dimensional subspace W of F n is the
solution set of a system of n −k homogeneous linear equations. Also prove that
W is the intersection of n −k linear hyperplanes (subspaces of dimension n −1).
33.
Let F = Z2 and V = F 2. Explicitly describe all elements of V ∗by giving the
domain, codomain, and values of each f ∈V ∗. Explicitly describe all elements
E ∈V ∗∗. For each such E, find an x ∈V such that E = Ex (evaluation at x).
List all ordered bases of V . For each basis, find the associated dual basis of V ∗.
Dual Vector Spaces
363
34.
Let v1 = (1, 2, 3), v2 = (0, 1, −1), and v3 = (0, 0, 1), so B = (v1, v2, v3) is
an ordered basis of R3. Let Bd = (f1, f2, f3) be the dual basis for B. Compute
the matrix of each fi relative to the standard ordered bases of R3 and R. For
w = (2, 3, −1) and i = 1, 2, 3, find Ew(fi). Use the UMP to obtain E ∈(R3)∗∗
that sends f1 to 4, f2 to −1, and f3 to 0. Find x ∈R3 such that E = Ex.
35.
Let B = (x1, . . . , xn) be an ordered basis of V and Bd = (f1, . . . , fn) be the dual
basis of V ∗. Show that (Ex1, . . . , Exn) is the dual basis Bdd of V ∗∗.
36.
Prove: if V is infinite-dimensional, then the map ev : V →V ∗∗is not surjective.
(Use Exercise 12.)
37.
(a) Give a direct proof (using bases but not facts about A) that dim(W) +
dim(Z(W)) = dim(V ) for all subspaces W of V ∗.
(b) Use (a) to deduce dim(Y ) + dim(A(Y )) = dim(V ) for all subspaces Y of V .
38.
(a) In §13.6, give the details of the proof that A(Z(W)) = W for all W ∈S(V ∗).
(b) Explain why Z(A(T)) = ⟨T⟩for all T ⊆V .
39.
Let (e1, e2, e3) be the standard ordered basis of R3 and (f1, f2, f3) be the dual
basis of (R3)∗. Let X = ⟨f1, f2⟩and Y = ⟨f2, f3⟩. Verify (13.4) by explicitly
computing X ∩Y , X + Y , Z(X), Z(Y ), Z(X) + Z(Y ), Z(X) ∩Z(Y ), Z(X ∩Y ),
and Z(X + Y ).
40.
Let X = ⟨(1, 2, 1)⟩and Y
= ⟨(2, −1, 0)⟩in R3. Verify (13.5) by explicitly
computing X ∩Y , X + Y , A(X), A(Y ), A(X) + A(Y ), A(X) ∩A(Y ), A(X ∩Y ),
and A(X + Y ).
41.
Let T = {(1, 0, 1), (2, 2, 2)} ⊆R3. Find Z(A(T)) first by using a theorem, then
again by using the definitions of A and Z.
42.
Let S(V ) be the poset of subspaces of V , ordered by set inclusion.
(a) Prove inf(X, Y ) = X ∩Y for all X, Y ∈S(V ).
(b) Prove sup(X, Y ) = X + Y for all X, Y ∈S(V ).
(c) Prove inf(Xi : i ∈I) = T
i∈I Xi for all Xi ∈S(V ).
(d) Prove sup(Xi : i ∈I) = P
i∈I Xi for all Xi ∈S(V ).
43.
Suppose U and V are lattices and f : U →V is a bijection such that for all x, y ∈
U, x ≤y iff f(y) ≤f(x). Prove f(inf(x, y)) = sup(f(x), f(y)) and f(sup(x, y)) =
inf(f(x), f(y)) for all x, y ∈U. Explain how (13.4) and (13.5) follow from this.
44.
State and prove analogs of (13.4) and (13.5) that involve the greatest lower bound
and least upper bound of an indexed collection of subspaces.
45.
Let F be a finite field. Show that, for 0 ≤k ≤n, the number of k-dimensional
subspaces of an n-dimensional F-vector space V is the same as the number of
(n −k)-dimensional subspaces of V .
46.
Assume dim(V ) = n > 1. Prove there does not exist any isomorphism f : V →V ∗
such that f ◦T = T ∗◦f for all linear maps T : V →V .
47.
Prove (V × W)∗∼= V ∗× W ∗. Show that A(V × {0}) maps to {0} × W ∗under
this isomorphism. Generalize to a direct product of k vector spaces.
48.
If a linear map T : V →W has matrix A relative to an ordered basis B of V and
an ordered basis C of W, what is the matrix of T ∗∗using bases Bdd and Cdd?
49.
Let D : F[x] →F[x] be the map given by D(P
k≥0 akxk) = P
k≥1 kakxk−1.
Describe how D∗acts on the vector space F[x]∗∼= F[[x]] (see Exercise 15).
50.
Prove: for all linear T : V →V and all integers k ≥0, (T k)∗= (T ∗)k. Prove this
holds for all k ∈Z if T is invertible.
364
Advanced Linear Algebra
51.
Prove: if linear maps T, U : V →V are similar, then T ∗and U ∗are similar.
52.
Let T : V →W be a linear map. Prove: for all S ⊆W, T ∗[A(S)] ⊆A(T −1[S]).
Give an example where equality does not hold.
53.
Let T : V →W be a linear map. Prove: for all S ⊆V , A(T[S]) = (T ∗)−1[A(S)].
54.
Let T : V →W be a linear map. Prove ker(T ∗) = A(img(T)). Deduce that T ∗is
one-to-one iff T is onto.
55.
Let T : V →W be a linear map. Prove img(T ∗) = A(ker(T)). Deduce that T ∗is
onto iff T is one-to-one.
56.
Let V
be any F-vector space, possibly infinite-dimensional. Prove: for any
subspace U of V , Z(A(U)) = U, A(U) ∼= (V/U)∗, dim(U) = dim(V ∗/A(U)),
and dim(A(U)) = dim(V/U).
57.
Let V be any F-vector space, possibly infinite-dimensional. Let Sf(V ∗) be the
poset of finite-dimensional subspaces of V ∗. Let Sf(V ) be the poset of subspaces
U of V such that dim(V/U) is finite. Prove Z and A restrict to give poset anti-
isomorphisms between the lattices Sf(V ∗) and Sf(V ).
58.
Let R : V →W ∗be the map induced by a bilinear pairing B : W ×V →F. Show
R is F-linear with kernel W ⊥.
59.
Define B : M1,n(F) × Mn,1(F) →F by letting B(w, v) be the sole entry of wv.
Show B is a bilinear pairing that is nondegenerate on both sides.
60.
Suppose W has ordered basis (w1, . . . , wm) and V has ordered basis (v1, . . . , vn).
Prove: for each A ∈Mm,n(F), there exists a unique bilinear pairing B : W ×V →
F such that B(wi, vj) = A(i, j) for 1 ≤i ≤m and 1 ≤j ≤n.
61.
Let B be related to A ∈Mm,n(F) as in the previous exercise. Prove B is left-
degenerate iff wA = 0 for some nonzero row vector w ∈M1,m(F). State and
prove a similar condition for B to be right-degenerate. What can you say in the
special case m = n?
62.
Suppose B : W × V →F is a bilinear pairing, S ⊆W, and T ⊆V . Prove S⊥is
a subspace of V and ⊥T is a subspace of W. Prove the orthogonal complement
operators reverse inclusions, S ⊆⊥(S⊥), and T ⊆(⊥T)⊥.
63.
State and prove an analog of the Theorem on Left-Nondegenerate Pairings for
right-nondegenerate pairings B : W × V →F.
64.
Check the assertions about B′ in the proof of the Theorem on General Bilinear
Pairings.
65.
In the proof of the Theorem on Adjoints, check that T ′(cy) = cT ′(y) for c ∈F
and y ∈V .
66.
Prove the Theorem on Adjoints by checking that T ′ = R−1 ◦T ∗◦R.
67.
Suppose V is a real inner product space, T : V →V is a linear map, T ′ is
the adjoint of T, C is an ordered basis of V , and C′ is the dual basis. Prove: if
[T]C = A, then [T ′]C′ = AT.
68.
Let V be a real inner product space and R : V →V ∗the isomorphism sending
y ∈V to right-multiplication by y. Prove: for all S ⊆V , A(S) = R[S⊥]; and for
all T ⊆V ∗, Z(T) = R−1[T]⊥.
69.
Check the facts stated about V σ in §13.11.
Dual Vector Spaces
365
70.
Suppose V is a complex inner product space, T : V →V is a linear map, T ′ is
the adjoint of T, C is an ordered basis of V , C′ is the dual basis, and [T]C = A.
Prove: if we view T ′ as a C-linear map on V σ, then [T ′]C′ = AT. If we view T ′
as a C-linear map on V , then [T ′]C′ = AT.
71.
Prove results similar to Exercises 54 and 55 for the adjoint of a linear operator
on a real or complex inner product space.
72.
Give an example of a degenerate symmetric bilinear form B on R3 with
B(ei, ej) ̸= 0 for all i, j ∈{1, 2, 3}. Find all v ∈R3 with L(v) = 0.
73.
Give an example of a nondegenerate bilinear form B on R4 with B(v, v) = 0 for
all v ∈R4.
74.
Suppose B is a symmetric bilinear form on a real vector space V such that
B(v, v) = 0 for all v ∈V . Prove B = 0.
75.
Consider the following bilinear forms on R3:
B1(x, y) = x1y1 + x2y2 + x3y3;
B2(x, y) = x1y2 −x2y1 + 2x3y3;
B3(x, y) = x1y1 + x1y2 + x1y3 + x2y2 + x2y3 + x3y3;
B4(x, y) = x1y1 + x1y3 + 2x2y2 + x3y1 + x3y3.
Which of these bilinear forms are symmetric? Which are nondegenerate? Let
f : R3 →R be the linear functional f(x) = 3x1 −2x2 −x3 for x ∈R3. For
1 ≤i ≤4, find y ∈R3 such that f(x) = Bi(x, y) for all x ∈R3, or explain why
this cannot be done. For 1 ≤i ≤4, find z ∈R3 such that f(x) = Bi(z, x) for all
x ∈R3, or explain why this cannot be done.
76.
Let V have ordered basis X = (x1, . . . , xn), let the dual basis of V ∗be Xd =
(f1, . . . , fn), let A ∈Mn(F), and let B be the unique bilinear form on V such
that B(xi, xj) = A(i, j) for i, j between 1 and n. What is the matrix of the linear
map R : V →V ∗relative to the basis X of V and the basis Xd of V ∗? What is
the matrix of the linear map L : V →V ∗relative to the basis X of V and the
basis Xd of V ∗? Assume B is symmetric and nondegenerate, and T : V →V is a
linear map with matrix C relative to X. What is the matrix of the adjoint map
T ′ relative to X?
77.
Let T : R4 →R4 be the map T(x) = Ax for x ∈R4, where
A =


2
0
−1
3
1
1
−2
−1
0
1
1
2
3
5
−6
2

.
Find
the
adjoint
of T
relative
to
the
standard
inner
product
on R4.
Find the adjoint of T relative to the symmetric bilinear form B(x, y) =
x1y1 + x2y2 + x3y3 −x4y4. Find the adjoint of T relative to the symmetric
bilinear form B(x, y) = x1y2 + x2y1 + 2x3y4 + 2x4y3.
78.
Let U = {(t, 3t, t) : t ∈R} and W = {(x, y, z) ∈R3 : 5x −2y + 3z = 0}. Compute
U ⊥and W ⊥relative to the standard inner product on R3.
79.
For each linear operator T on V , let T ′ denote the adjoint operator relative to a
fixed nondegenerate symmetric bilinear form B. Use the defining formula (13.6)
to prove: for all S, T ∈HomF (V, V ) and c ∈F, (S + T)′ = S′ + T ′, (cS)′ = c(S′),
and (S ◦T)′ = T ′ ◦S′. Prove these formulas again using the relation between T ′
and T ∗.
366
Advanced Linear Algebra
80.
Give an example of a vector space V , a symmetric nondegenerate bilinear form
B on V , and a subspace W of V such that W ∩W ⊥̸= {0} and V ̸= W ⊕W ⊥.
For your choice of W, verify that W ⊥⊥= W and dim(W)+dim(W ⊥) = dim(V ).
81.
Show that for any ideal I in a commutative ring R,
√
I (defined on page 357) is
a radical ideal.
82.
Prove the first five properties of the operator Z listed in §13.13.
83.
Let I and J be ideals of a commutative ring R. Let IJ be the set of finite sums
i1j1 + · · · + imjm where i1, . . . , im ∈I and j1, . . . , jm ∈J. Prove that IJ is an
ideal of R, and IJ ⊆I ∩J. Prove that Z(IJ) = Z(I ∩J) = Z(I) ∪Z(J).
84.
Prove the first three properties of the operator A listed in §13.13.
85.
True or false? Explain each answer.
(a) For all x ∈V , there exists f ∈V ∗with f(x) ̸= 0.
(b) For all vector spaces Z, dim(Z) = dim(Z∗).
(c) Given dim(V ) = n, every nonzero f ∈V ∗satisfies dim(ker(f)) = n −1.
(d) For all vector spaces Z, ev : Z →Z∗∗is an isomorphism.
(e) For all S, T ⊆V ∗, if Z(S) = Z(T) then S = T.
(f) For all x ̸= y in V , there exists f ∈V ∗with f(x) ̸= f(y).
(g) For all linearly independent lists (f1, . . . , fk) in V ∗, there exist x1, . . . , xk ∈V
with fi(xj) = δij for 1 ≤i, j ≤k.
(h) For all subspaces U, Z of a real inner product space, if U ⊥= Z⊥then U = Z.
(i) The map D : HomF (V, W) →HomF (W ∗, V ∗) given by D(T) = T ∗is F-linear.
(j) For all linear maps S, T : V →V , (S ◦T)∗= S∗◦T ∗.
(k) For all T ⊆V ∗, Z(A(Z(T))) = Z(T).
14
Bilinear Forms
The dot product on Rn is defined by v • w = v1w1 + v2w2 + · · · + vnwn for all
v = (v1, v2, . . . , vn) and w = (w1, w2, . . . , wn) in Rn. The dot product supplies extra
algebraic structure to the vector space Rn, which allows us to define key geometric
ideas: the length of one vector, the distance between two vectors, the angle between
vectors, and perpendicularity of vectors. In more detail, the Euclidean norm of v ∈Rn
is ||v|| = √v • v =
p
v2
1 + v2
2 + · · · + v2n. We regard ||v|| as the length of the arrow
(directed line segment) from the origin of Rn to the point (v1, v2, . . . , vn). More generally,
the Euclidean distance from v to w is d2(v, w) = ||v −w|| =
p
(v −w) • (v −w). This
distance is the length of the line segment joining the points (v1, . . . , vn) and (w1, . . . , wn).
A unit vector is a vector u ∈Rn such that ||u|| = 1. The angle between nonzero vectors
v, w ∈Rn is the unique real θ in the range 0 ≤θ ≤π such that cos(θ) =
v•w
||v||·||w||. Vectors
v, w ∈Rn are called orthogonal iff v • w = 0. Nonzero vectors v and w are orthogonal iff
the angle between them is π/2 (a right angle), which means that the arrows representing v
and w are perpendicular.
Using the Gram–Schmidt algorithm, you can show that every subspace W of Rn has
an orthonormal basis, which is a list of unit vectors (w1, . . . , wk) spanning the subspace W
such that wi is orthogonal to wj for all i ̸= j. In other words, every w ∈W has the form
w = Pk
i=1 ciwi for some ci ∈R, and wi • wj is 1 if i = j and 0 if i ̸= j. These conditions
imply that (w1, . . . , wk) is linearly independent, so an orthonormal basis really is a basis of
W. Every orthonormal list in W can be extended to an orthonormal basis of W.
Given any subspace W of Rn, define the orthogonal subspace W ⊥to be the set of v ∈Rn
such that w • v = 0 for all w ∈W. Using an orthonormal basis for W, you can check that
Rn is the direct sum W ⊕W ⊥, meaning that every z ∈Rn can be written uniquely in the
form z = w + v, where w ∈W and v ∈W ⊥. Among other facts, we have W ∩W ⊥= {0},
(W ⊥)⊥= W, and dim(W) + dim(W ⊥) = n = dim(Rn). The map sending W to W ⊥is an
inclusion-reversing bijection on the lattice of all subspaces of Rn.
Our goal in this chapter is to study bilinear forms on general vector spaces V ; these
forms generalize the dot product on Rn. General bilinear forms share some, but not all,
of the properties of the dot product reviewed above. For example, the Lorentz form on R4
sends a pair of input vectors (t, x, y, z), (t′, x′, y′, z′) to the output tt′ −xx′ −yy′ −zz′. Using
this form, the product of a nonzero vector with itself can be negative or zero. The Lorentz
form is important in the study of spacetime and the special theory of relativity.
We begin the chapter with the definition of bilinear forms and some basic examples.
We show how to represent a bilinear form using a matrix by choosing an ordered basis for
the underlying vector space V . We define congruent matrices, which represent the same
bilinear form relative to different choices of bases. Next, we examine orthogonality, dual
spaces, orthogonal complements, and the radical of a bilinear form. Using these concepts,
we show that symmetric bilinear forms can be represented by diagonal matrices, while
alternate bilinear forms have matrices with a simple block-diagonal structure. The chapter
ends with more advanced structural results such as Witt’s Decomposition Theorem, Witt’s
Cancellation Theorem, and the generation of orthogonal groups by reflections.
DOI: 10.1201/9781003484561-14
367
368
Advanced Linear Algebra
14.1
Definition of Bilinear Forms
Let V be a vector space over a field F. A bilinear form on V is a function B : V × V →F
such that, for all u, v, w ∈V and all c ∈F:
• (linearity in first input) B(u + v, w) = B(u, w) + B(v, w) and B(cu, w) = cB(u, w);
• (linearity in second input) B(w, u + v) = B(w, u) + B(w, v) and B(w, cu) = cB(w, u).
In other words, for each fixed choice of w ∈V , the function sending u ∈V to B(u, w) is
a linear map from V to F, and the function sending u to B(w, u) is a linear map from V
to F. Since linear maps preserve linear combinations, we deduce the following additional
properties. For all v1, . . . , vk, w ∈V and all c1, . . . , ck ∈F,
B
 k
X
i=1
civi, w
!
=
k
X
i=1
ciB(vi, w)
and
B
 
w,
k
X
i=1
civi
!
=
k
X
i=1
ciB(w, vi).
Combining these formulas, we get the general bilinearity formula
B


k
X
i=1
civi,
m
X
j=1
djwj

=
k
X
i=1
m
X
j=1
cidjB(vi, wj)
(14.1)
for all choices of vi, wj in V and ci, dj in F. Since linear maps send zero to zero, we have
B(v, 0V ) = 0F and B(0V , w) = 0F for all v, w ∈V .
Here is terminology for some special types of bilinear forms. Let B be a bilinear form
on V .
• B is symmetric iff B(v, w) = B(w, v) for all v, w ∈V .
• B is antisymmetric iff B(v, w) = −B(w, v) for all v, w ∈V .
• B is alternate iff B(v, v) = 0 for all v ∈V .
• B is degenerate iff there is a nonzero w ∈V such that for all v ∈V , B(v, w) = 0.
• B is nondegenerate iff for each nonzero w ∈V , there is v ∈V (depending on w) with
B(v, w) ̸= 0. This means that 0V is the only vector w ∈V satisfying B(v, w) = 0 for all
v ∈V .
• A bilinear space is a vector space V together with a specific bilinear form B on V . We
may say that a bilinear space is symmetric, nondegenerate, etc., if the bilinear form B
has these properties.
Antisymmetric bilinear forms and alternate bilinear forms are closely related. For any
field F, define 2F = 1F + 1F . Every alternate bilinear form is antisymmetric, and the
converse holds if 2F ̸= 0F . To prove this, first suppose B is an alternate bilinear form. For
any v, w in V , we compute
0 = B(v + w, v + w) = B(v, v) + B(v, w) + B(w, v) + B(w, w) = 0 + B(v, w) + B(w, v) + 0,
so B(v, w) = −B(w, v). Conversely, suppose B is an antisymmetric bilinear form and
2F ̸= 0F . For any v in V , we know B(v, v) = −B(v, v), so B(v, v) + B(v, v) = 0F , so
(1F + 1F )B(v, v) = 0F . Because F is a field, we can multiply both sides by the inverse of
the nonzero element 2F = 1F + 1F to conclude that B(v, v) = 0F .
Bilinear Forms
369
14.2
Examples of Bilinear Forms
The dot product on the real vector space Rn is the motivating example for the concept of a
bilinear form. We can generalize the dot product to any field F as follows. Given n-tuples
v = (v1, . . . , vn) and w = (w1, . . . , wn) in the F-vector space F n, define B(v, w) = v1w1 +
v2w2+· · ·+vnwn. You can check from the field axioms that B is a symmetric, nondegenerate
bilinear form on F n. For example, given v, w ∈F n and c ∈F,
B(v, cw) = v1(cw1) + v2(cw2) + · · · + vn(cwn)
= c(v1w1) + c(v2w2) + · · · + c(vnwn)
= c(v1w1 + · · · + vnwn) = cB(v, w)
using associativity and commutativity of multiplication in F and the distributive law. To see
that B is nondegenerate, suppose w = (w1, . . . , wn) ∈F n is nonzero. Then some coordinate
wi is not 0F . Letting ei be the vector in F n with 1F in position i and 0F elsewhere, we
have B(ei, w) = wi ̸= 0F .
Based on your experience with the dot product in Rn, you might think that B(w, w) =
w2
1 + w2
2 + · · · + w2
n must also be nonzero. But this need not be true for a general field
F. For instance, if F = Z3 (the integers mod 3), n = 3, and w = (1, 1, 1) ∈F 3, then
B(w, w) = 1F + 1F + 1F = 3F = 0F . Even for the field F = R, it is possible for a
nondegenerate symmetric bilinear space to have vectors v with B(v, v) = 0, as we see in
later examples. A bilinear form on a real vector space V is called positive definite iff for
all nonzero v ∈V , B(v, v) is a strictly positive real number. The dot product on Rn is a
positive definite form. Positive definiteness implies nondegeneracy, but the converse does
not always hold.
Next, consider the F-vector space F 2. For v = (v1, v2) and w = (w1, w2) in F 2, let
B1(v, w) = v1w1 −v2w2,
B2(v, w) = v1w2 + v2w1,
B3(v, w) = v1w2 −v2w1.
(14.2)
You can check that these three functions are all bilinear and nondegenerate, even though
B2 and B3 satisfy B(e1, e1) = 0 = B(e2, e2), and B1 is not positive definite in the real case.
Forms B1 and B2 are symmetric, while B3 is alternate and antisymmetric. The bilinear
space (F 2, B2) is called a hyperbolic plane over the field F. Hyperbolic planes are important
building blocks for analyzing the structure of symmetric bilinear spaces.
For any F-vector space V , the zero function B0 : V × V →F, defined by B0(v, w) = 0F
for all v, w ∈V , is a symmetric and alternate bilinear form. B0 is degenerate except when V
itself is the zero space. Next, let V be any 1-dimensional F-vector space, which is isomorphic
to F = F 1. Given a nonzero v ∈V , there is a unique bilinear form Bv on V such that
Bv(v, v) = 1F , given by Bv(cv, dv) = cd for c, d ∈F. The form Bv is symmetric and
nondegenerate.
We can obtain new bilinear spaces by taking products of given bilinear spaces, as follows.
Suppose (V1, B1), (V2, B2), . . . , (Vk, Bk) are bilinear spaces over the same field F. Let V =
V1 × V2 × · · · × Vk be the direct product of the vector spaces V1, V2, . . . , Vk. By definition,
elements of V are k-tuples (v1, v2, . . . , vk) with each vi ∈Vi, and the vector space operations
on V are defined componentwise. We define B : V × V →F by setting
B((v1, v2, . . . , vk), (w1, w2, . . . , wk)) = B1(v1, w1)+B2(v2, w2)+· · ·+Bk(vk, wk). (14.3)
You can check that: B is a bilinear form on V ; B is symmetric iff all Bi are symmetric; B
is alternate iff all Bi are alternate; B is nondegenerate iff all Bi are nondegenerate. We call
(V, B) the direct product or the external direct sum of the bilinear spaces (Vi, Bi).
370
Advanced Linear Algebra
For example, for each s ∈R, let (R, Bs) be the real vector space R with the unique
bilinear form B such that Bs(1, 1) = s. Taking the direct product of n copies of (R, B1)
gives the space Rn with the dot product. For integers a, b, c ≥0, taking the direct product
of a copies of (R, B0), b copies of (R, B1), and c copies of (R, B−1) gives a real symmetric
bilinear space of dimension a+b+c, which is nondegenerate iff a = 0. As we prove in §14.9,
this is the most general example of a finite-dimensional real symmetric bilinear space. The
Witt Decomposition Theorem generalizes this structural result to symmetric bilinear spaces
over other fields.
As a final trio of examples, consider the following forms on the F-vector space F 2n. For
v = (v1, . . . , v2n) and w = (w1, . . . , w2n) in F 2n, define
B1n(v, w)
= v1w1 + · · · + vnwn −vn+1wn+1 −· · · −v2nw2n;
B2n(v, w)
=
n
X
i=1
viwn+i +
n
X
i=1
vn+iwi;
B3n(v, w)
=
n
X
i=1
viwn+i −
n
X
i=1
vn+iwi.
(14.4)
You can check that these are nondegenerate bilinear forms on F 2n, where the first two are
symmetric and the third is alternate. For j = 1, 2, 3, we can think of the bilinear spaces
(F 2n, Bjn) as direct sums of n copies of the spaces (F 2, Bj) considered in (14.2). Specifically,
consider the subspaces Zi = Fei +Fen+i of F 2n for i = 1, 2, . . . , n. Each Zi is a copy of F 2,
and Bjn acts on Zi in the same way that Bj acts on F 2. For example, for all a, b, c, d ∈F,
B2((a, b), (c, d)) = ad + bc, while B2n(aei + ben+i, cei + den+i) = ad + bc. Since F 2n is the
(internal) direct sum of the subspaces Z1, . . . , Zn, (F 2n, Bjn) is essentially the same as the
(external) direct sum of n copies of (F 2, Bj).
To make this last comment more precise, we introduce the idea of isomorphic bilinear
spaces. Given bilinear spaces (V, B) and (V ′, B′) over the same field F, a bilinear
homomorphism is an F-linear map T : V →V ′ such that B′(T(x), T(y)) = B(x, y) for
all x, y ∈V . By linearity of T and bilinearity of B and B′, it suffices to check the last
condition for all x, y in any given basis for V (Exercise 10). A bilinear isomorphism is an
invertible bilinear homomorphism. Continuing the previous example, take (V, B) to be the
direct product of n copies of (F 2, Bj), and let (V ′, B′) = (F 2n, Bjn). You can check that
T(((v1, x1), (v2, x2), . . . , (vn, xn))) = (v1, v2, . . . , vn, x1, x2, . . . , xn)
is an isomorphism between these bilinear spaces for j = 1, 2, 3.
The apparently different bilinear spaces (R2, B1) and (R2, B2) defined in (14.2) are
actually isomorphic. You can check that T : R2 →R2 defined by T(v1, v2) =
1
√
2(v1+v2, v1−
v2) is an invertible linear map; in fact, T −1 = T. We verify that B2(T(x), T(y)) = B1(x, y)
by checking on the standard basis:
B2(T(e1), T(e1)) = 1
2B2((1, 1), (1, 1)) = 1 = B1((1, 0), (1, 0)) = B1(e1, e1);
B2(T(e1), T(e2)) = 1
2B2((1, 1), (1, −1)) = 0 = B1((1, 0), (0, 1)) = B1(e1, e2);
B2(T(e2), T(e1)) = 1
2B2((1, −1), (1, 1)) = 0 = B1((0, 1), (1, 0)) = B1(e2, e1);
B2(T(e2), T(e2)) = 1
2B2((1, −1), (1, −1)) = −1 = B1((0, 1), (0, 1)) = B1(e2, e2).
In fact, (F 2, B1) and (F 2, B2) are isomorphic for any field F such that 2F ̸= 0F , but a
different proof is needed if 2 has no square root in F (Exercise 11).
Bilinear Forms
371
14.3
Matrix of a Bilinear Form
Let V be an n-dimensional vector space over a field F. To define a general function B
mapping the domain V × V into the codomain F, we must specify the scalar B(v, w) in
F for each ordered pair (v, w) of vectors v, w in V . When the function B happens to be a
bilinear form, we can uniquely determine B using less information. Let X = (x1, x2, . . . , xn)
be any ordered basis for V . Suppose we know the values B(xi, xj) for each ordered pair
(xi, xj) of basis elements xi, xj in X. Then we can find B(v, w) for any v, w in V as follows.
We have expansions v = Pn
i=1 cixi and w = Pn
j=1 djxj for certain unique scalars ci and dj
in F, since X is a basis of V . By the general bilinearity formula (14.1), we know
B(v, w) =
n
X
i=1
n
X
j=1
cidjB(xi, xj).
Introduce notation [B]X for the n × n matrix with entry B(xi, xj) in row i, column j. We
call [B]X the matrix representing B relative to the ordered basis X. We have just shown
that each bilinear form B : V × V →F is uniquely determined by its matrix [B]X relative
to a given ordered basis X of V .
Conversely, suppose A ∈Mn(F) is any n × n matrix with entries in F, and let X =
(x1, . . . , xn) be a given ordered basis of V . We can use the matrix A to define a bilinear
form B : V × V →F, as follows. Given v, w ∈V , write v = P
i cixi and w = P
j djxj for
unique scalars ci, dj ∈F, and set
B(v, w) =
n
X
i=1
n
X
j=1
cidjA(i, j).
(14.5)
You can check that this function B really is bilinear. We illustrate by verifying that
B(v, w + z) = B(v, w) + B(v, z) for all v, w, z ∈V . Write v = P
i cixi, w = P
j djxj,
z = P
j ejxj with ci, dj, ej ∈F. Then w + z = P
j(dj + ej)xj. Using the definition (14.5)
of B and algebraic manipulation, we get
B(v, w + z) =
n
X
i=1
n
X
j=1
ci(dj + ej)A(i, j) =
n
X
i=1
n
X
j=1
(cidjA(i, j) + ciejA(i, j))
=
n
X
i=1
n
X
j=1
cidjA(i, j) +
n
X
i=1
n
X
j=1
ciejA(i, j) = B(v, w) + B(v, z).
Also, [B]X is the matrix A we started with. To verify this, take v = xi and w = xj in the
definition of B. Here, ci = 1, dj = 1, and all other ck and dk are 0. So the formula (14.5)
for B(xi, xj) simplifies to 1 · 1 · A(i, j) = A(i, j).
This means that if we start with a matrix A, use A and X to define a bilinear form B
via (14.5), and form the matrix [B]X, then [B]X = A. On the other hand, if we start with
any bilinear form B′ on V , let A be the matrix [B′]X, and define a new bilinear form B
by (14.5), then B = B′. We can rephrase these comments as follows. Let Bilin(V ) be the
set of all bilinear forms on V and Mn(F) be the set of all n × n matrices with entries in F.
The map sending B to [B]X is a bijection from Bilin(V ) onto Mn(F). The inverse of this
bijection sends an input A ∈Mn(F) to the bilinear form B specified by (14.5).
In fact, the set Bilin(V ) of all bilinear forms on V can be made into an F-vector space
372
Advanced Linear Algebra
using pointwise operations on functions. In more detail, given any bilinear forms B, B′ in
Bilin(V ) and c ∈F, define B + B′ : V × V →F and cB : V × V →F by
(B + B′)(v, w) = B(v, w) + B′(v, w) and (cB)(v, w) = c(B(v, w))
for all v, w ∈V .
You can check that B +B′ and cB really are bilinear forms on V , so that the set Bilin(V ) is
closed under these operations of addition and scalar multiplication. Also, the zero function
sending each pair (v, w) to 0F is bilinear. So Bilin(V ) is a subspace of the vector space of
all functions from V × V →F (see §4.2) and is therefore a vector space, as claimed.
Using this vector space structure on Bilin(V ), the bijection from bilinear forms to
matrices becomes an F-vector space isomorphism from Bilin(V ) to Mn(F). In other words,
[B + B′]X = [B]X + [B′]X and [cB]X = c[B]X
for all B, B′ ∈Bilin(V ) and all c ∈F.
To verify this, compare the i, j-entries of the matrices. Both [B+B′]X and [B]X +[B′]X have
i, j-entry (B + B′)(xi, xj) = B(xi, xj) + B′(xi, xj). Both [cB]X and c[B]X have i, j-entry
(cB)(xi, xj) = c(B(xi, xj)). The following theorem summarizes our work so far.
Theorem on Matrix Representation of Bilinear Forms. Let V be an F-vector space
of dimension n. For each ordered basis X of V , the map sending a bilinear form B to its
representing matrix [B]X is a vector space isomorphism of Bilin(V ) onto Mn(F).
Let us find the matrices representing some of the bilinear forms in §14.2. The dot product
on Rn and the analogous form on F n have matrix In (the n × n identity matrix) relative
to the standard ordered basis (e1, . . . , en). The zero bilinear form on any n-dimensional
F-vector space V has matrix 0n (the n × n zero matrix) relative to any basis of V . Relative
to X = (e1, e2), the forms B1, B2, B3 defined in (14.2) have matrices
[B1]X =
 1
0
0
−1

,
[B2]X =
 0
1
1
0

,
[B3]X =

0
1
−1
0

.
(14.6)
Note that Z = ((1/
√
2, 1/
√
2), (1/
√
2, −1/
√
2)) is an ordered basis for R2. The calculation
at the end of §14.2 shows that [B2]Z =

1
0
0
−1

= [B1]X. In general, finite-dimensional
bilinear spaces (V, B) and (V∗, B∗) are isomorphic iff V has an ordered basis X and V∗has
an ordered basis X∗such that [B]X = [B∗]X∗(Exercise 14).
Let Y be the standard ordered basis of F 2n. The forms Bjn defined in (14.4) have
matrices given in block form by
[B1n]Y =

In
0n
0n
−In

,
[B2n]Y =

0n
In
In
0n

,
[B3n]Y =

0n
In
−In
0n

.
Consider the reordered standard basis Y ′ = (e1, en+1, e2, en+2, . . . , en, e2n). You can check
that for j = 1, 2, 3, the matrix [Bjn]Y ′ is block diagonal, consisting of n copies of the 2 × 2
matrices [Bj]X in (14.6). More generally, suppose (V1, B1), . . . , (Vk, Bk) are bilinear spaces
with ordered bases X1, . . . , Xk. The external direct sum V = V1 × · · · × Vk has an ordered
basis X that can be regarded as the concatenation of X1, . . . , Xk. You can check that for the
form B defined in (14.3), [B]X is block diagonal with diagonal blocks [B1]X1, . . . , [Bk]Xk.
We return to the general case of a bilinear form B on V represented by the matrix
A = [B]X, where X is an ordered basis of V . Special properties of the form B translate to
analogous properties of the matrix A, as follows.
(a) The bilinear form B is symmetric iff the matrix A is symmetric (meaning AT = A).
(b) The bilinear form B is antisymmetric iff the matrix A is skew (meaning AT = −A, and
in particular 2F A(i, i) = 0F for all i).
Bilinear Forms
373
(c) The bilinear form B is alternate iff AT = −A and A(i, i) = 0F for all i.
(d) The bilinear form B is nondegenerate iff the matrix A is invertible.
We prove (c) and (d) here, asking you to prove (a) and (b) in the exercises. Write X =
(x1, x2, . . . , xn). First assume B is an alternate bilinear form. Then A(i, i) = B(xi, xi) = 0
for all i, and (since alternate forms are antisymmetric) A(i, j) = B(xi, xj) = −B(xj, xi) =
−A(j, i) for all i, j. Conversely, assume the entries of A satisfy these conditions. Fix arbitrary
v ∈V , and write v = P
i cixi with ci in F. By general bilinearity, we compute
B(v, v) = B


n
X
i=1
cixi,
n
X
j=1
cjxj

=
n
X
i=1
n
X
j=1
cicjB(xi, xj).
Each summand indexed by (i, j) with i = j is zero, since B(xi, xi) = A(i, i) = 0. The
remaining summands, indexed by (i, j) with i ̸= j, break up into pairs that cancel:
B(v, v) =
X
i<j
[cicjB(xi, xj) + cjciB(xj, xi)] =
X
i<j
cicj(A(i, j) + A(j, i)) =
X
i<j
0 = 0.
To prove (d), we translate the condition in the definition of degenerate bilinear forms
into a condition on matrices and vectors. Suppose w ∈V satisfies B(v, w) = 0 for all v ∈V .
Write w = P
j cjxj for scalars cj ∈F, and let z be the column vector in F n with entries
(c1, . . . , cn). Observe that w = 0V iff z = 0, since X is linearly independent. For each i
between 1 and n, we can take v to be xi, the ith vector in the ordered basis X. We have
0 = B(xi, w) =
n
X
j=1
cjB(xi, xj) =
n
X
j=1
A(i, j)cj
for 1 ≤i ≤n.
This says that Az = 0. Conversely, Az = 0 implies B(xi, w) = 0 for all i, and hence (by
linearity of B in the first position) B(v, w) = 0 for all v ∈V . We now see that the following
conditions are equivalent: B is degenerate; some nonzero w ∈V satisfies B(v, w) = 0 for all
v ∈V ; some nonzero z ∈F n satisfies Az = 0; A is not invertible.
14.4
Congruence of Matrices
Given an ordered basis X of V , there is a corresponding vector space isomorphism from
Bilin(F) to Mn(F), sending a bilinear form B to its matrix [B]X relative to X. We get one
such isomorphism for each ordered basis of V . What happens to the matrix representing
B if we change from X to a new ordered basis Y of V ? In other words, how is the matrix
[B]Y related to the matrix [B]X?
To answer this question, we need some notation. Write X = (x1, x2, . . . , xn), Y =
(y1, y2, . . . , yn), and let pi,j ∈F be the unique scalars such that yj = Pn
i=1 pi,jxi for
1 ≤i, j ≤n. Define the matrix P = [pi,j] and P T = [pj,i]. (Note that P = X[idV ]Y is
the transition matrix from input basis Y to output basis X defined in §6.4, and P T is the
transpose of P.) We use general bilinearity to compute the i, j-entry of [B]Y :
B(yi, yj) = B
 n
X
s=1
ps,ixs,
n
X
k=1
pk,jxk
!
=
n
X
s=1
n
X
k=1
ps,ipk,jB(xs, xk)
=
n
X
s=1
n
X
k=1
P T(i, s)[B]X(s, k)P(k, j).
374
Advanced Linear Algebra
By definition of matrix multiplication, the right side equals the i, j-entry of the matrix
product P T[B]XP. To summarize:
[B]Y = P T[B]XP,
where P is the transition matrix from Y to X.
Compare this rule to the analogous rule for linear maps: if T : V →V is F-linear, then
[T]Y = P −1[T]XP, where P is the transition matrix from Y to X.
We may now consider two equivalence relations on the set Mn(F) of n × n matrices
with entries in F. Fix matrices A, D ∈Mn(F). First, A is similar to D iff A = P −1DP
for some invertible matrix P ∈Mn(F). Second, A is congruent to D iff A = P TDP for
some invertible matrix P ∈Mn(F). Let us check that congruence really is an equivalence
relation on Mn(F). Fix A, D, C ∈Mn(F). First, A = IT
n AIn where In is the n × n identity
matrix; so congruence is reflexive on Mn(F). To check symmetry, assume A is congruent
to D, say A = P TDP, where P is invertible with inverse Q. The inverse of P T is QT, so
QTAQ = QTP TDPQ = D, which means D is congruent to A. To check transitivity, assume
A is congruent to D and D is congruent to C, say A = P TDP and D = RTCR for some
invertible P, R ∈Mn(F). Then A = P TRTCRP = (RP)TC(RP) where RP is invertible,
so A is congruent to C.
For fixed A ∈Mn(F), the equivalence class of A relative to congruence is the set of
matrices P TAP as P varies through all invertible matrices in Mn(F). Suppose A = [B]X
for some bilinear form B on V and some ordered basis X of V . Then the congruence
equivalence class of A (briefly, the congruence class of A) consists of exactly those matrices
that represent the bilinear form B relative to some ordered basis of V . We can gain structural
information about the bilinear form B by looking for matrices in the congruence class of
A that have a simple form, such as diagonal matrices. This is the strategy for several
classification theorems for bilinear forms, to be proven below.
As an example of congruence, the matrices A =
 1
0
0
−1

and D =
 0
1
1
0

in M2(R)
are congruent, since A = P TDP holds with P =
1
√
2

1
1
1
−1

. The congruence of A and
D also follows from (14.6) and the calculation below it, which shows that A and D both
represent the same bilinear form B2 relative to certain ordered bases of R2. The congruence
relation can change if we change the field of scalars. For example, A is congruent to the
identity matrix I2 in M2(C), since A = QTI2Q holds with Q =
 1
0
0
i

. But A is not
congruent to I2 in M2(R), since A = QTI2Q with Q ∈M2(R) implies −1 = det(A) =
det(QT) det(Q) = det(Q)2, which is impossible since det(Q) is real.
14.5
Orthogonality in Bilinear Spaces
We now generalize the idea of perpendicular vectors in Rn to the setting of a bilinear space
(V, B) over a field F. Given vectors v, w ∈V , we say v is orthogonal to w and write v ⊥w
to mean that B(v, w) = 0. Given subsets S, T of V , we say S is orthogonal to T and write
S ⊥T to mean that B(s, t) = 0 for all s ∈S and all t ∈T. If S consists of a single vector
v, we write v ⊥T to abbreviate {v} ⊥T.
Geometric intuition tells us that orthogonality should be a symmetric relation on V ,
meaning that for all v, w ∈V , v ⊥w implies w ⊥v. If B is symmetric, antisymmetric, or
alternate, then orthogonality is symmetric (see Exercise 33 for a converse). For example, in
Bilinear Forms
375
the antisymmetric case, B(v, w) = 0 implies B(w, v) = −B(v, w) = −0 = 0. In geometric
settings, we focus mainly on bilinear forms that are either symmetric or alternate.
For an alternate bilinear form B, orthogonality is also reflexive on V : for all v ∈V ,
B(v, v) = 0 and hence v ⊥v. This runs counter to our intuition coming from Euclidean
geometry, where v ⊥v is true only for v = 0. But this reflexivity is a key feature of
symplectic geometry, which is an analog of Euclidean geometry based on alternate bilinear
forms.
Given v ∈V , we call v a unit vector iff B(v, v) = 1. Call a nonzero vector v isotropic iff
B(v, v) = 0, and call v non-isotropic iff B(v, v) ̸= 0. Suppose L = (v1, . . . , vk) is any finite
list of vectors in V . We say L is an orthogonal list iff vi ⊥vj for all i ̸= j between 1 and k.
We say L is an orthonormal list iff L is an orthogonal list of unit vectors.
Lemma on Orthogonality and Linear Independence.
Every orthogonal list L of
non-isotropic vectors in V is F-linearly independent.
Proof. Let L = (v1, . . . , vk) be an orthogonal list of non-isotropic vectors in V . Suppose
c1, . . . , ck ∈F are scalars such that c1v1 + c2v2 + · · · + ckvk = 0. For each i between 1 and
k,
0 = B(0, vi) = B(c1v1 + · · · + civi + · · · + ckvk, vi)
= c1B(v1, vi) + · · · + ciB(vi, vi) + · · · + ckB(vk, vi) = ciB(vi, vi).
Since B(vi, vi) is a nonzero element of the field F, we conclude each ci is 0, as needed.
Let (V, B) be any bilinear space. For any subset S of V , define the orthogonal complement
of S (on the right) to be
S⊥= {w ∈V : for all s ∈S, B(s, w) = 0} = {w ∈V : S ⊥w}.
Similarly, the orthogonal complement of S on the left is
⊥S = {v ∈V : for all s ∈S, B(v, s) = 0} = {v ∈V : v ⊥S}.
When B is symmetric, antisymmetric, or alternate, ⊥S = S⊥. For general B and all S ⊆V ,
S⊥and ⊥S are subspaces of V . We check this for S⊥. First, 0 ∈S⊥since B(s, 0) = 0 for
all s ∈S. Second, fix w, z ∈S⊥and c ∈F. For each s ∈S, we know B(s, w) = B(s, z) = 0,
hence B(s, w + z) = B(s, w) + B(s, z) = 0 + 0 = 0 and B(s, cw) = cB(s, w) = c0 = 0. Thus,
w + z ∈S⊥and cw ∈S⊥.
For any S ⊆V , we have S ⊆⊥(S⊥). To prove this, fix s ∈S and w ∈S⊥. Then s ⊥w.
This holds for all w in S⊥, so s ∈⊥(S⊥). Similarly, S ⊆(⊥S)⊥. In the main case of interest
(where orthogonality is symmetric), we write S⊥⊥instead of ⊥(S⊥). Although S ⊆S⊥⊥
always holds, S could be a proper subset of S⊥⊥. For instance, this always happens when
S is a subset of V that is not a subspace. As another example, if B is the zero bilinear form
on V , then S⊥= S⊥⊥= V for every S ⊆V .
In Rn with the dot product, we have S ∩S⊥= {0} for any subset S, since v = 0 is
the only vector orthogonal to itself. But we may very well have S ∩S⊥̸= {0} for general
bilinear forms. For example, if v is an isotropic vector in V and S = Fv, then v ∈S ∩S⊥
since B(cv, v) = cB(v, v) = 0 for all c ∈F. In fact, S ⊆S⊥in this example. By definition,
B is a degenerate form iff there exists a nonzero w ∈V with V ⊥w. For any subset S of V
containing such a w, we have w ∈S ∩S⊥. Note that B is nondegenerate iff V ⊥= {0}.
Any subspace W of any bilinear space (V, B) becomes a bilinear space using the
restriction of B : V ×V →F to the domain W ×W. The formal notation for this restricted
function is B|W ×W , but we often write B|W or just B when W is clear from context. If
376
Advanced Linear Algebra
B is symmetric (or antisymmetric or alternate), then B|W has the same property. But
it is possible for B to be nondegenerate while B|W is degenerate, or vice versa. For any
S ⊆W, the orthogonal complement of S in W is the intersection of W and the orthogonal
complement of S in V . We may write this in symbols as (S⊥in W) = W ∩(S⊥in V ).
Taking S = W and applying the observation at the end of the last paragraph, we see that
B|W is nondegenerate iff W ∩W ⊥= {0}.
If V is an F-vector space with subspaces W1, W2, . . . , Wk, the notation V = W1 ⊕W2 ⊕
· · · ⊕Wk means that V is the (internal) direct sum of these subspaces. In other words,
V = W1 + W2 + · · · + Wk and Wi ∩(W1 + · · · + Wi−1) = {0} for 2 ≤i ≤k. In this
case, every v ∈V has a unique expression as v = w1 + w2 + · · · + wk where wi ∈Wi.
Concatenating ordered bases Xi for Wi gives an ordered basis X for V . Now suppose B is a
bilinear form on V . We say V is the orthogonal direct sum of subspaces W1, . . . , Wk, writing
V = W1 ⊞W2 ⊞· · · ⊞Wk, iff V = W1 ⊕W2 ⊕· · · ⊕Wk and Wi ⊥Wj for all i ̸= j. In this
situation, B is nondegenerate on V iff B|Wi is nondegenerate for 1 ≤i ≤k (Exercise 36).
Furthermore, (V, B) is isomorphic to the external direct sum (W1, B|W1)×· · ·×(Wk, B|Wk)
(Exercise 37). Using the ordered bases Xi and X mentioned above, [B]X is block-diagonal
with blocks [B|W1]X1, . . . , [B|Wk]Xk (Exercise 38).
14.6
Bilinear Forms and Dual Spaces
This section describes the close connection between bilinear forms and dual vector spaces.
For any F-vector space V , the dual space V ∗is the set of all F-linear maps from V to
F. The set V ∗is an F-vector space under pointwise addition and scalar multiplication of
maps. In more detail, let g, h ∈V ∗and c ∈V . The functions g + h and cg are defined
by (g + h)(v) = g(v) + h(v) and (cg)(v) = c(g(v)) for all v ∈V . You can check that
g + h ∈V ∗, cg ∈V ∗, and the zero function from V to F is in V ∗. So V ∗is a subspace of
the F-vector space of all functions from V to F. For finite-dimensional spaces V , we know
dim(V ∗) = dim(V ). This can be proved by considering matrices (see §6.7) or dual bases
(see §13.2).
Let B be a fixed bilinear form on an F-vector space V with dim(V ) = n. Define a
function R : V →V ∗by letting R(w) be the function from V to F that sends each input
v ∈V to output B(v, w) ∈F. More succinctly, we may write R(w)(v) = B(v, w) for all
v, w ∈V . For fixed w ∈V , note that R(w) is an F-linear map, since B is F-linear in the
first position. This means that the function R really does map V into the claimed codomain
V ∗. Next, R itself is an F-linear function, since B is F-linear in the second position. In
more detail, for all w, z ∈V , we have R(w + z) = R(w) + R(z) since both sides send v ∈V
to B(v, w + z) = B(v, w) + B(v, z). For all w ∈V and c ∈F, we have R(cw) = cR(w) since
both sides send v ∈V to B(v, cw) = cB(v, w). So we have an F-linear map R : V →V ∗.
For w ∈V , w is in the kernel of R iff R(w) is the zero map on V iff R(w)(v) = 0 for all v ∈
V iff B(v, w) = 0 for all v ∈V . Thus, the kernel of R is V ⊥= {w ∈V : v ⊥w for all v ∈V }.
The bilinear form B is nondegenerate iff ker(R) = {0} iff R is one-to-one. In this case, the
image R[V ] is an n-dimensional subspace of V ∗. Since dim(V ∗) = dim(V ) = n, we see that
R must also be onto. To summarize, for any nondegenerate bilinear form B on V , we get
an isomorphism R : V →V ∗of F-vector spaces. Bijectivity of R means that for each linear
functional g ∈V ∗, there exists a unique w ∈V such that for all v ∈V , g(v) = B(v, w). We
say that g is represented by w relative to B.
For example, consider the real vector space Rn and take B to be the dot product.
As a special case of the preceding discussion, for every linear functional g : Rn →R,
Bilinear Forms
377
there is a unique vector w ∈Rn such that g(v) = v • w for all v ∈Rn. We can also
verify this conclusion directly, as follows. Given g ∈(Rn)∗, let wi = g(ei) for 1 ≤i ≤n,
where ei is the standard basis vector with 1 in position i and 0 in all other positions. Any
v = (v1, . . . , vn) ∈Rn can be written uniquely in the form v = Pn
i=1 viei. By linearity of g,
g(v) =
n
X
i=1
vig(ei) =
n
X
i=1
viwi = v • w.
This proves the existence of the vector w representing g. To see that w is unique, suppose
w′ = (w′
1, . . . , w′
n) also represents g relative to the dot product. Then w′
i = ei•w′ = g(ei) =
wi for 1 ≤i ≤n, so w′ = w.
14.7
Theorem on Orthogonal Complements
View Rn as a bilinear space using the dot product. For any subspace W of Rn, we have:
(i) Rn = W ⊞W ⊥; (ii) n = dim(Rn) = dim(W) + dim(W ⊥); and (iii) W ⊥⊥= W.
Beware! These results do not always hold if we replace the dot product by a general bilinear
form. Certain conclusions do follow with appropriate nondegeneracy hypotheses, as we see
in the next theorem. Carefully memorize this theorem, as it will be used constantly.
Theorem on Orthogonal Complements. Let (V, B) be an n-dimensional bilinear space,
where B is symmetric or alternate. Let W be any subspace of V .
(a) If B|W is nondegenerate (meaning W ∩W ⊥= {0}), then dim(W) + dim(W ⊥) = n =
dim(V ) and V = W ⊞W ⊥. But W ⊥⊥= W is not guaranteed.
(b) If B is nondegenerate (meaning V ⊥= {0}), then dim(W) + dim(W ⊥) = n = dim(V )
and W ⊥⊥= W. But V = W ⊞W ⊥is not guaranteed.
With minor modifications to the conclusions, the theorem holds without assuming B is
symmetric or alternate (Exercise 29).
Proof. Step 1. We apply the Rank–Nullity Theorem to a variation of the map R : V →V ∗
considered in §14.6. Define T : V →W ∗by letting T(v)(w) = B(w, v) for all v ∈V and
w ∈W. In other words, T(v) is the function from W to F sending an input w ∈W to
the output B(w, v). By the same proof used earlier for R, bilinearity of B implies that
each function T(v) is F-linear, and that T itself is F-linear. By the Rank–Nullity Theorem,
n = dim(V ) = dim(ker(T)) + dim(img(T)).
Step 2. We compute the kernel of T. Fix v ∈V . The following conditions on v are
equivalent: v ∈ker(T); T(v) = 0, the zero map from W to F; B(w, v) = 0 for all w ∈W;
w ⊥v for all w ∈W; v ∈W ⊥. Thus, ker(T) = W ⊥.
Step 3. We prove part (a) of the theorem. Assume W ∩W ⊥= {0}. We use this to prove
that img(T) = W ∗. Let T ′ : W →W ∗be the restriction of the linear map T to the domain
W. T ′ is a linear map with kernel W ∩ker(T) = W ∩W ⊥= {0}. Thus, T ′ is one-to-one. As
W and W ∗have the same finite dimension, T ′ must also be onto. Since the restricted map
T ′ maps W onto all of W ∗, the original map T also maps V onto W ∗. Using Steps 1 and 2,
n = dim(ker(T)) + dim(img(T)) = dim(W ⊥) + dim(W ∗) = dim(W ⊥) + dim(W).
Since W ∩W ⊥= {0}, the sum of subspaces W + W ⊥is an orthogonal direct sum
W ⊞W ⊥. This direct sum has dimension dim(W) + dim(W ⊥) = n = dim(V ), so W ⊞W ⊥
must be all of V . Exercise 24 asks you to find an example where W ⊥⊥̸= W.
378
Advanced Linear Algebra
Step 4. We prove part (b) of the theorem. Assume that B is nondegenerate. We use this
to prove that img(T) = W ∗. By §14.6, there is an F-linear isomorphism R : V →V ∗given
by R(v)(x) = B(x, v) for all v, x ∈V . Given h ∈W ∗, we can extend h to a linear map
g : V →F. For example, extend a basis of W to a basis of V , then let g be the linear map
that acts as h does on basis vectors in W and sends the remaining basis vectors to 0. We
know g = R(v) for some v ∈V since R is onto. It follows that h = T(v), since for each
w ∈W, h(w) = g(w) = R(v)(w) = B(w, v) = T(v)(w). Because each h ∈W ∗has the form
T(v) for some v ∈V , T is onto. Using Steps 1 and 2,
dim(V ) = n = dim(ker(T)) + dim(img(T)) = dim(W ⊥) + dim(W ∗) = dim(W ⊥) + dim(W).
This conclusion holds for all subspaces W, so we also have dim(W ⊥) + dim(W ⊥⊥) = n.
Because n is finite, we get dim(W ⊥⊥) = dim(W). Since W ⊆W ⊥⊥and these spaces have
the same finite dimension, W = W ⊥⊥follows. Exercise 25 asks you to find an example
where V = W ⊞W ⊥is not true.
Here are three examples where part (a) of the theorem applies. First, suppose (V, B)
is a symmetric bilinear space and z ∈V satisfies B(z, z) ̸= 0. The 1-dimensional subspace
W = Fz has ordered basis (z), and the matrix of B|W relative to this basis is [B(z, z)].
This matrix is invertible, so B|W is nondegenerate, and the theorem gives V = Fz⊞(Fz)⊥.
Second, suppose (V, B) is a symmetric bilinear space and (v, w) is a list of two vectors
in V such that B(v, v) = 0 = B(w, w) and B(v, w) = 1 = B(w, v). We call such a list a
hyperbolic pair in V , and the subspace H = Fv + Fw of V is called a hyperbolic plane. It
is routine to check that X = (v, w) is an ordered basis of H, and [B|H]X =
 0
1
1
0

. This
matrix is invertible, so B|H is nondegenerate, and the theorem gives V = H ⊞H⊥.
Third, suppose (V, B) is an alternate bilinear space and (v, w) is a list of two vectors
in V such that B(v, v) = 0 = B(w, w), B(v, w) = 1, and B(w, v) = −1. We call such a
list a symplectic pair in V , and the subspace S = Fv + Fw of V is called a symplectic
plane. Again, X = (v, w) is an ordered basis of S, and [B|S]X =

0
1
−1
0

. This matrix
is invertible, so B|S is nondegenerate, and the theorem gives V = S ⊞S⊥.
14.8
Radical of a Bilinear Form
Let B be a symmetric, antisymmetric, or alternate bilinear form on an F-vector space V .
The radical of (V, B) is the subspace Rad(V ) = V ⊥, which is the set of all z ∈V such that
B(v, z) = 0 for all v ∈V . The form B is nondegenerate iff Rad(V ) = {0}. Suppose W is any
subspace that is complementary to V ⊥, meaning that V is the direct sum V = V ⊥⊕W.
Such subspaces W always exist. To find one, start with an ordered basis (x1, . . . , xk) for
V ⊥, extend this list to an ordered basis X = (x1, . . . , xk, xk+1, . . . , xn) for V , and let W be
the subspace with basis X′ = (xk+1, . . . , xn). There are often many possible choices of W,
since there can be multiple ways of extending the basis of V ⊥. For any such W, the matrix
[B]X has the block form
[B]X =

0k×k
0k×(n−k)
0(n−k)×k
[B|W]X′

.
(14.7)
The matrix has this form since for any i, j with 1 ≤i ≤k and 1 ≤j ≤n, B(xi, xj) = 0 =
B(xj, xi). We claim B|W is nondegenerate. Suppose w ∈W satisfies B(y, w) = 0 for all
Bilinear Forms
379
y ∈W; we show w = 0. Given any v ∈V , we can write v = z +y for some z ∈V ⊥and some
y ∈W. Then B(v, w) = B(z, w)+B(y, w) = 0+0 = 0 since z is orthogonal to everything in
V and B(y, w) = 0. Since v was arbitrary, we conclude w ∈V ⊥. Hence w ∈V ⊥∩W = {0},
so w = 0 as needed.
We often use this construction to reduce to the case of a nondegenerate bilinear form
when proving a property for a general symmetric, antisymmetric, or alternate bilinear form.
We can also remove the radical using a quotient construction. Given (V, B) with radical
R = Rad(V ), let V/R be the quotient vector space (described in §1.6). Given cosets x + R
and y + R in V/R, where x, y ∈V , define B(x + R, y + R) = B(x, y). To see that B is
well-defined, assume x, x′, y, y′ ∈V satisfy x + R = x′ + R and y + R = y′ + R. Then
x′ = x + r and y′ = y + s for some r, s ∈R. We compute
B(x′, y′) = B(x + r, y + s) = B(x, y) + B(r, y) + B(x, s) + B(r, s) = B(x, y).
It is now routine to check that (V/R, B) is a bilinear space, and B is symmetric,
antisymmetric, or alternate if B is. We show B is nondegenerate. Assume z + R ∈V/R
satisfies B(v + R, z + R) = 0 for all v ∈V . Then B(v, z) = 0 for all v ∈V , which means
z ∈R and z + R = 0 + R in V/R. To compare this to the previous construction, let W be
any subspace of V with V = R ⊕W. You can check that the map p : W →V/R, given
by p(w) = w + R for w ∈W, is a bilinear space isomorphism from (W, B|W) to (V/R, B).
This approach shows that the bilinear space we get by removing the radical is (up to
isomorphism) independent of the choice of the subspace W complementary to Rad(V ).
14.9
Diagonalization of Symmetric Bilinear Forms
When studying a bilinear space (V, B), it is helpful to have an ordered basis X = (x1, . . . , xn)
for V where the matrix [B]X is as simple as possible. Ideally, we would like [B]X to be a
diagonal matrix, which means that B(xi, xj) = 0 for all i ̸= j, or equivalently that the
basis X is an orthogonal list of vectors. Since diagonal matrices are symmetric, a necessary
condition for such a basis to exist is that the bilinear form B be symmetric. The next
theorem shows that this necessary condition is also sufficient for most fields.
Theorem on Diagonalizing Symmetric Bilinear Forms. Assume 2F ̸= 0F . If B is a
symmetric bilinear form on an n-dimensional F-vector space V , then there exists an ordered
basis X = (x1, . . . , xn) for V such that [B]X is a diagonal matrix.
Proof. We prove the theorem by induction on n = dim(V ). In the base case where n ≤1,
the result follows at once. Fix n > 1, and assume the theorem holds for all vector spaces
of smaller dimension than n. First consider the case where B is degenerate. Choose any
subspace W with V = V ⊥⊕W. Since B is degenerate, dim(W) < n, and we can apply
the induction hypothesis to the symmetric bilinear space (W, B|W). We obtain an ordered
basis X′ of W such that [B|W]X′ is diagonal. Since V = V ⊥⊕W, we can extend X′ by
prepending an ordered basis of V ⊥to get an ordered basis X for V . By (14.7), [B]X is a
diagonal matrix.
Next, consider the case where B is nondegenerate. We seek an x ∈V with B(x, x) ̸=
0. Start with any nonzero v in V . If B(v, v) ̸= 0, then we take x = v. Otherwise, use
nondegeneracy of B to find w ∈V with B(w, v) ̸= 0. If B(w, w) ̸= 0, then we take x = w.
If B(v, v) = 0 = B(w, w), then we take x = v + w, noting that
B(x, x) = B(v + w, v + w) = B(v, v) + B(v, w) + B(w, v) + B(w, w) = 2F B(w, v) ̸= 0F .
380
Advanced Linear Algebra
Let W = Fx be the 1-dimensional subspace of V spanned by x. By the first example at
the end of §14.7, V = W ⊞W ⊥and dim(W ⊥) = n −1 < n. By induction hypothesis, there
is an ordered basis X′ = (x2, . . . , xn) of W ⊥such that [B|W ⊥]X′ is diagonal. Since x ⊥xi
for 2 ≤i ≤n, X = (x, x2, . . . , xn) is an ordered basis of V such that [B]X is diagonal. This
completes the induction step of the proof.
We can prove a sharper version of this theorem when using a field F (such as C) where
every scalar in F has a square root in F.
Theorem on Diagonalizing Bilinear Forms over Fields with Square Roots. Let B
be a symmetric bilinear form on an n-dimensional F-vector space V . Assume 2F ̸= 0F , and
for all c ∈F, there is a ∈F with a2 = c. Then there is an ordered basis X = (x1, . . . , xn)
for V such that [B]X is a diagonal matrix with diagonal entries consisting of k copies of 0F
followed by n −k copies of 1F for some k. We must have k = dim(V ⊥), so there is only one
matrix of this form that represents B.
Proof. By the previous theorem, there is an ordered basis X′ = (x′
1, . . . , x′
n) for V such
that [B]X′ is a diagonal matrix, say with diagonal entries d1, . . . , dn ∈F. We can choose
the ordering of the list X′ so that for some k between 0 and n, di = 0F for 1 ≤i ≤k and
di ̸= 0F for k < i ≤n. For each i in the range k < i ≤n, let ai ∈F be a scalar (necessarily
nonzero) with a2
i = di. For each i between 1 and k, let ai = 1F . Define xi = a−1
i x′
i for
1 ≤i ≤n. Let X = (x1, . . . , xn); X is an ordered basis for V since it comes from X′ by
rescaling basis vectors. We compute
B(xi, xi) = B(x′
i, x′
i) = 0
for 1 ≤i ≤k;
B(xi, xi) = a−2
i B(x′
i, x′
i) = d−1
i di = 1F
for k < i ≤n;
B(xi, xj) = a−1
i a−1
j B(x′
i, x′
j) = 0F
for all i ̸= j.
Thus [B]X has the structure described in the theorem statement.
To finish, we prove k = dim(V ⊥) for any ordered basis X = (x1, . . . , xn) such that
A = [B]X is diagonal with k zeroes followed by n−k ones on the diagonal. Let v = Pn
i=1 cixi
be any element of V , where all ci ∈F. Note that B(xi, v) = ciA(i, i) for 1 ≤i ≤n. We
have v ∈V ⊥iff B(xi, v) = 0 for 1 ≤i ≤n iff ciA(i, i) = 0 for 1 ≤i ≤n iff ci = 0 for
k < i ≤n (since A(1, 1) = · · · = A(k, k) = 0). Thus, (x1, . . . , xk) spans V ⊥, and this list is
F-linearly independent since it is a sublist of the basis X. So (x1, . . . , xk) is a basis of V ⊥,
and k = dim(V ⊥).
We can prove a similar result for the real field F = R, where all positive elements have
square roots.
Theorem on Diagonalizing Real Symmetric Bilinear Forms. Let B be a symmetric
bilinear form on an n-dimensional real vector space V . There is an ordered basis X =
(x1, . . . , xn) for V such that [B]X is a diagonal matrix with diagonal consisting of k copies
of 0, then s copies of 1, then t copies of −1, for some integers k, s, t that are uniquely
determined by B.
Proof. We already know there is an ordered basis X′ = (x′
1, . . . , x′
n) for V such that [B]X′
is a diagonal matrix, say with diagonal entries d1, . . . , dn ∈F. Order the list X′ so that the
first k numbers d1, . . . , dk are 0, the next s numbers dk+1, . . . , dk+s are positive, and the
last t numbers dk+s+1, . . . , dk+s+t are negative (where k+s+t = n). Now define xi = x′
i for
1 ≤i ≤k, xi =
1
√di x′
i for k + 1 ≤i ≤k + s, and xi =
1
√
|di|x′
i for k + s + 1 ≤i ≤n. You can
check that X = (x1, . . . , xn) is an ordered basis of V such that [B]X has diagonal entries
as described in the theorem. For example, for any i satisfying k + s + 1 ≤i ≤n, we have
Bilinear Forms
381
B(xi, xi) =
1
|di|B(x′
i, x′
i) = di/|di| = −1 since di is negative. Since each xi is some scalar
multiple of x′
i, we have B(xi, xj) = 0 for all i ̸= j because B(xi, xj) is a scalar multiple of
B(x′
i, x′
j) = 0.
To prove uniqueness of k, s, and t, let Y = (y1, . . . , yn) be any ordered basis of V such
that A = [B]Y is a diagonal matrix with k′ copies of 0, then s′ copies of 1, then t′ copies of
−1 on the diagonal, where k′ + s′ + t′ = n. As in the proof of the previous theorem, we see
that k′ = dim(V ⊥) = k. Let P be the subspace of V with basis (xk+1, . . . , xk+s), and let N ′
be the subspace of V with basis (y1, . . . , yk′, yk′+s′+1, . . . , yn). Given a nonzero v ∈P, say
v = Pk+s
i=k+1 cixi with ci ∈R not all zero, we compute B(v, v) = Pk+s
i=k+1 c2
i > 0. Similarly,
any v ∈N ′ satisfies B(v, v) ≤0. So we must have P ∩N ′ = {0}, and the subspace sum
P + N ′ is a direct sum P ⊕N ′ ⊆V . Then
s + (k′ + t′) = dim(P) + dim(N ′) = dim(P ⊕N ′) ≤dim(V ) = n = s′ + k′ + t′.
We conclude that s ≤s′. Interchanging the roles of X and Y , we see likewise that s′ ≤s,
so s′ = s. Then t′ = n −s′ −k′ = n −s −k = t.
14.10
Structure of Alternate Bilinear Forms
Any matrix representing an alternate bilinear form must have all diagonal entries 0. So, the
only diagonalizable alternate bilinear form is the zero form. However, the next result shows
that we can represent any alternate bilinear form with a block-diagonal matrix consisting
of repetitions of a simple 2 × 2 block.
Theorem on Alternate Bilinear Forms.
Let B be an alternate bilinear form on an
n-dimensional F-vector space V . There is an ordered basis X of V such that [B]X is a
block-diagonal matrix consisting of s diagonal blocks C =

0
1
−1
0

followed by k diagonal
entries equal to 0. The integers s and k are uniquely determined by B: namely, k = dim(V ⊥)
and s = (n −k)/2.
An equivalent reformulation of the theorem says that any alternate bilinear space is the
orthogonal direct sum of its radical and s symplectic planes for some s ≥0.
Proof. We proceed by induction on n = dim(V ). If B = 0, which must happen in the base
cases n = 0 or n = 1, then [B]X = 0 for any ordered basis X. The theorem holds for such
B, taking s = 0 and k = n. Now fix n > 1, assume the theorem holds for all alternate
bilinear forms on all vector spaces of dimension smaller than n, and assume B is nonzero.
Then there exist x, z ∈V with B(x, z) ̸= 0. We cannot have z = cx with c ∈F, since
otherwise B(x, z) = cB(x, x) = 0. Let y = B(x, z)−1z, so B(x, y) = 1, B(y, x) = −1, and
B(x, x) = B(y, y) = 0. Then (x, y) is a symplectic pair spanning a 2-dimensional symplectic
plane S = Fx + Fy in V , and the matrix of B|S relative to the basis (x, y) is C. By the
third example at the end of §14.7, V = S ⊞S⊥. Since dim(S⊥) = n −2 < n, the induction
hypothesis says there is an ordered basis X′ of S⊥such that [B|S⊥]X′ has the structure
described in the theorem. Let X be the list X′ with x, y added at the front. Then X is an
ordered basis of V such that [B]X has the required structure.
For the uniqueness claim, suppose Y
= (x1, y1, x2, y2, . . . , xs, ys, z1, . . . , zk) is any
ordered basis of V such that [B]Y is block-diagonal with s copies of C followed by k copies of
0 on the diagonal. A typical vector v ∈V has the form Ps
i=1(aixi +biyi)+Pk
j=1 cjzj where
382
Advanced Linear Algebra
ai, bi, cj ∈F. We have v ∈V ⊥iff B(xi, v) = B(yi, v) = 0 for 1 ≤i ≤s and B(zj, v) = 0
for 1 ≤j ≤k. By the assumed structure of [B]Y , we see that B(xi, v) = bi, B(yi, v) = −ai,
and B(zj, v) = 0 for 1 ≤i ≤s and 1 ≤j ≤k. Therefore, v ∈V ⊥iff each ai and bi is 0 iff v
is in the subspace with basis (z1, . . . , zk). We conclude that k = dim(V ⊥), which depends
only on B. Then s = (n −k)/2 is also uniquely determined by B.
14.11
Totally Isotropic Subspaces
Let (V, B) be a bilinear space over a field F, where 2F ̸= 0F and B is symmetric. We say
that a subspace W of V totally isotropic iff B(w, w) = 0 for all w ∈W. In this case, we
have B(w, z) = 0 for all w, z in W, since
2B(w, z) = B(w, z) + B(z, w) = B(w + z, w + z) −B(w, w) −B(z, z) = 0 −0 −0 = 0
and 2 is invertible in F. Thus, a subspace W is totally isotropic iff B|W is the zero bilinear
form. Also, W is totally isotropic iff W ⊆W ⊥. The next theorem gives a bound on the
dimension of W when B is nondegenerate.
Dimension Theorem for Totally Isotropic Subspaces. Let (V, B) be an n-dimensional
nondegenerate symmetric bilinear space over a field F, where 2F ̸= 0F . For any totally
isotropic subspace W of V , dim(W) ≤n/2.
Proof. Let k = dim(W). Let X = (x1, . . . , xn) be an ordered basis of V that extends an
ordered basis (x1, . . . , xk) for W. The matrix A = [B]X is an invertible n × n matrix that
has a k × k zero submatrix in its upper-left corner. The list of all n columns of A is linearly
independent in F n, since A is invertible. So the sublist consisting of the first k columns of A
is also linearly independent in F n. We can regard the first k columns of A as column vectors
in F n−k by deleting all the zero entries in the first k rows. These k shortened columns must
also be linearly independent in F n−k, as is readily checked. Since F n−k has dimension n−k,
we conclude that k ≤n −k and hence k ≤n/2.
With a little more work, we can prove a stronger version of this theorem that resembles
the Structure Theorem for Alternate Bilinear Forms.
Embedding
Theorem
for
Totally
Isotropic
Subspaces. Let (V, B) be an n-
dimensional nondegenerate symmetric bilinear space over a field F, where 2F ̸= 0F . Let
W be a k-dimensional totally isotropic subspace of V . There exist k hyperbolic planes
H1, . . . , Hk in V such that W ⊆H1 ⊞· · · ⊞Hk. More specifically, for any ordered basis
(w1, . . . , wk) of W, there is an ordered basis X = (w1, z1, . . . , wk, zk, v2k+1, . . . , vn) of V
such that [B]X is block-diagonal with k diagonal blocks equal to
 0
1
1
0

followed by a
block of size (n −2k) × (n −2k).
Proof. We prove the theorem by induction on k = dim(W). The conclusion is immediate in
the base case k = 0. For the induction step, assume k > 0 and the theorem holds for smaller
values of k. Let (w1, w2, . . . , wk) be any ordered basis of a k-dimensional totally isotropic
subspace W of V . Let Y = Fw2 ⊕· · ·⊕Fwk be the (k −1)-dimensional subspace of W with
ordered basis (w2, . . . , wk). Since Y ⊆W, we have W ⊥⊆Y ⊥⊆V . As B is nondegenerate,
part (b) of the Theorem on Orthogonal Complements tells us that dim(W ⊥) = n −k and
dim(Y ⊥) = n −k + 1. So there exists z belonging to Y ⊥but not to W ⊥.
Bilinear Forms
383
Define c = B(w1, z) ∈F. We cannot have c = 0; otherwise z would be orthogonal
to w1, w2, . . . , wk and so z would belong to W ⊥. Replacing z by c−1z, we can arrange
that c = 1. For any b ∈F, we have B(w1, bw1 + z) = bB(w1, w1) + B(w1, z) = 1 and
B(bw1+z, bw1+z) = b2B(w1, w1)+2bB(w1, z)+B(z, z) = 2b+B(z, z). Define z1 = bw1+z
where b = −2−1B(z, z). Then B(z1, z1) = 0 = B(w1, w1) and B(w1, z1) = 1, so (w1, z1) is
a hyperbolic pair spanning a hyperbolic plane H1 in V .
Now B|H1 is nondegenerate, since the matrix of this restricted form relative to the
ordered basis (w1, z1) is the invertible matrix
 0
1
1
0

. By part (a) of the Theorem on
Orthogonal Complements, V = H1 ⊞H⊥
1 . This decomposition shows that B|H⊥
1 must be
nondegenerate, since B is nondegenerate. Each basis vector w2, . . . , wk for Y is orthogonal
to w1 (since W is totally isotropic), orthogonal to z (since z was chosen in Y ⊥), and
orthogonal to z1. It follows that H1 and Y are orthogonal subspaces, meaning that Y
is a subspace of H⊥
1 . Y is a (k −1)-dimensional totally isotropic space. Applying the
induction hypothesis with V replaced by H⊥
1 and W replaced by Y , we get an ordered
basis (w2, z2, . . . , wk, zk, v2k+1, . . . , vn) of H⊥
1 satisfying the conclusion of the theorem for
the subspace Y of H⊥
1 . Since V = H1 ⊞H⊥
1 , (w1, z1, w2, z2, . . . , wk, zk, v2k+1, . . . , vn) is an
ordered basis of V satisfying the conclusion of the theorem for the subspace W of V .
We can also prove a structure theorem in the case where B might be degenerate. Call
a subspace Y of a bilinear space (V, B) non-isotropic or anisotropic iff B(y, y) ̸= 0 for all
nonzero y ∈Y .
Witt’s Decomposition Theorem (Existence Part).
Any symmetric bilinear space
(V, B) has a decomposition V = R⊞H ⊞Y , where R is totally isotropic, H is an orthogonal
direct sum of k hyperbolic planes for some k ≥0, and Y is non-isotropic.
Any such decomposition of V is called a Witt decomposition of V . For any such
decomposition we have Rad(V ) = Rad(R) ⊞Rad(H) ⊞Rad(Y ) = R.
Proof. Let R = V ⊥, the radical of V . R is totally isotropic and orthogonal to all subspaces
of V . Let V ′ be any complementary subspace to R (meaning V = R ⊕R′) and B′ = B|V ′.
As we saw in §14.8, B′ is nondegenerate. Next, let W be any maximal totally isotropic
subspace of V ′, where maximal means that W is not properly contained in any totally
isotropic subspace of V ′. Since V ′ is finite-dimensional, such a subspace W must exist;
we can take W to be a totally isotropic subspace of V ′ of maximum possible dimension
k. By the Embedding Theorem, there exist k hyperbolic planes H1, . . . , Hk in V ′ with
W ⊆H1 ⊞· · ·⊞Hk. Let H = H1 ⊞· · ·⊞Hk, and let Y = H⊥, the orthogonal complement of
H in V ′. Since B′|H is nondegenerate, we know V ′ = H ⊞Y by the Theorem on Orthogonal
Complements. Thus V = R ⊞H ⊞Y . We need only check that Y is non-isotropic. If this
were not true, then there would be a nonzero y ∈Y with B(y, y) = 0. On one hand, y ̸∈W,
since W ⊆H and H ∩Y = {0}. On the other hand, since y ∈H⊥, B(y, w) = 0 for all
w ∈W. This implies W + Fy is a totally isotropic subspace of V ′ properly containing W,
contradicting the maximality of W. So Y is non-isotropic.
We have already observed that the subspace R in any Witt decomposition of V must
equal the radical of V . Although H and Y do not satisfy such a strong uniqueness property,
we prove in §14.16 that H and Y are uniquely determined up to bilinear isomorphisms. In
particular, dim(H) is uniquely determined. On the other hand, the proof just given shows
that dim(H) = 2k, where k is the dimension of any maximal totally isotropic subspace W
of V ′. When B is nondegenerate, V ′ = V , and we see that every maximal totally isotropic
subspace of V has the same dimension k. The number k is called the Witt index of (V, B).
384
Advanced Linear Algebra
14.12
Orthogonal Maps
A linear map T : Rn →Rn is called orthogonal iff T preserves dot products: T(v) • T(w) =
v•w for all v, w in Rn. It follows that an orthogonal map preserves the magnitude of vectors
and the angle between nonzero vectors. Also, an orthogonal map on Rn is invertible, and
the set of all orthogonal maps is a subgroup of the group of all invertible linear maps on Rn.
We can translate these facts into the language of matrices by fixing an orthonormal basis of
Rn and taking the matrix of each linear map relative to this basis. Call a matrix A ∈Mn(R)
orthogonal iff ATA = In, in which case AAT = In and AT = A−1 automatically follow. A
linear map T is orthogonal iff the matrix representing T (relative to an orthonormal basis)
is orthogonal. The set of orthogonal n × n real matrices forms a subgroup On(R) of the
general linear group GLn(R) of invertible n × n real matrices.
We now generalize these considerations by replacing the dot product on Rn by any
bilinear form B on an n-dimensional F-vector space V . Define a linear map T : V →V
to be B-orthogonal iff for all v, w ∈V , B(T(v), T(w)) = B(v, w). By linearity of T and
bilinearity of B, it suffices to check this condition for v, w in a spanning set (or basis) of V .
A B-orthogonal map is the same thing as a bilinear homomorphism from (V, B) to itself.
Here is a matrix formulation of B-orthogonality. Let T : V →V be a linear map. Given
an ordered basis X = (x1, . . . , xn) of V , define A = [B]X and C = [T]X, which means
A(i, j) = B(xi, xj) and T(xj) = Pn
i=1 C(i, j)xi for 1 ≤i, j ≤n. We compute
B(T(xi), T(xj)) = B
 n
X
r=1
C(r, i)xr,
n
X
s=1
C(s, j)xs
!
=
n
X
r=1
n
X
s=1
C(r, i)B(xr, xs)C(s, j)
=
n
X
r=1
n
X
s=1
CT(i, r)A(r, s)C(s, j) = [CTAC](i, j).
So, the following conditions are equivalent (where i, j range from 1 to n): T is B-orthogonal;
B(T(xi), T(xj)) = B(xi, xj) for all i, j; [CTAC](i, j) = A(i, j) for all i, j; CTAC = A. Thus,
B-orthogonality of T is equivalent to the matrix condition A = CTAC, where A = [B]X
and C = [T]X. Our motivating example (the dot product in Rn) has A = In when X is
the standard ordered basis. In this special case, we recover our original definition of an
orthogonal real matrix: C is orthogonal iff CTC = In.
Let OB(V ) be the set of B-orthogonal linear maps on V . The identity map on V is B-
orthogonal. Given B-orthogonal maps S and T, the composition S◦T is B-orthogonal, since
B(S(T(v)), S(T(w))) = B(T(v), T(w)) = B(v, w) for all v, w ∈V . Similarly, the inverse of
an invertible B-orthogonal map is B-orthogonal. It follows that the set OB(V )∗of invertible
B-orthogonal maps is a group under composition of functions. This group is a subgroup of
GL(V ), the general linear group of all invertible F-linear maps on V .
We claim that if B is nondegenerate, then every B-orthogonal map is invertible. To
prove this, let T : V →V be B-orthogonal, X be any ordered basis of V , A = [B]X,
and C = [T]X. We know A is invertible (since B is nondegenerate) and A = CTAC.
Taking determinants shows that det(A) = det(CT) det(A) det(C) = det(A) det(C)2. Since
det(A) ̸= 0, we have det(C)2 = 1, so det(C) = ±1F ̸= 0F . We conclude that T is an
invertible map, and det(T) is 1F or −1F . (Recall that the determinant of a linear map is
the determinant of any matrix representing T; all such matrices are similar and have the
same determinant.) This proves the claim. So OB(V ) is a subgroup of GL(V ) when B is
nondegenerate. Similarly, for any fixed invertible matrix A ∈Mn(F), the set of A-orthogonal
matrices On(A) = {C ∈Mn(F) : CTAC = A} is a subgroup of GLn(F), the group of all
invertible matrices in Mn(F).
Bilinear Forms
385
Let us compute OB(V ) explicitly for some low-dimensional bilinear spaces (V, B). It is
routine to check that OB(V ) = {idV , −idV } when (V, B) is a 1-dimensional nondegenerate
bilinear space (Exercise 70). For a more interesting example, assume (V, B) is a hyperbolic
plane spanned by a hyperbolic pair (v, w), so B(v, v) = 0 = B(w, w) and B(v, w) = 1 =
B(w, v). Suppose T : V →V is any linear map. We have T(v) = av+bw and T(w) = cv+dw
for some scalars a, b, c, d ∈F. The following statements are equivalent:
(a) T is B-orthogonal;
(b) B(T(v), T(v)) = B(v, v) and B(T(w), T(w)) = B(w, w) and B(T(v), T(w)) = B(v, w);
(c) B(av + bw, av + bw) = 0 and B(cv + dw, cv + dw) = 0 and B(av + bw, cv + dw) = 1;
(d) 2ab = 0 and 2cd = 0 and ad + bc = 1.
Let us assume 2F ̸= 0F ; recall 1F ̸= 0F holds in any field. To find all B-orthogonal maps
T, consider various cases. In the case a = 0, we deduce bc = 1, b ̸= 0 ̸= c, c = b−1, and
d = 0, so T is the unique linear map such that T(v) = bw and T(w) = b−1v. In the case
b = 0, we deduce ad = 1, a ̸= 0 ̸= d, d = a−1, and c = 0, so T is the unique linear map such
that T(v) = av and T(w) = a−1w. The group OB(V ) consists of all maps given by these
formulas, as a and b range over nonzero elements of F.
We can give a similar analysis of OB(V ) when V is a symplectic plane spanned by
a symplectic pair (v, w). Consider a linear map T : V →V and write T(v) = av + bw,
T(w) = cv + dw with a, b, c, d ∈F. Since B(u, u) = 0 for all u ∈V , we see that T is
B-orthogonal iff B(T(v), T(w)) = B(v, w) iff B(av + bw, cv + dw) = 1 iff ad −bc = 1 iff
det(T) = 1. In this case, OB(V ) coincides with the special linear group SL(V ) of linear
maps on V with determinant 1.
As a final 2-dimensional example, consider the bilinear space R2 with the dot product.
You can check that OB(R2) consists of all rotations around the origin, together with all
reflections across lines through the origin (Exercise 62). Each rotation has determinant +1,
and each reflection has determinant −1.
14.13
Reflections
Reflections play a central role in Euclidean geometry. Given a line ℓin R2, the geometric
reflection across ℓis the map sℓ: R2 →R2 that sends each point P in the plane to its
mirror image in ℓ. We give a formula for sℓin the case where ℓis the line through (0, 0) and
v = (v1, v2) ̸= (0, 0). Let z = (v2, −v1), which is a vector orthogonal to v. Each point P in
R2 can be written as P = av + bz for unique scalars a, b ∈R. We define sℓ(P) = av −bz.
Note that sℓsends z to −z and fixes each point on the line ℓ= Rv orthogonal to z.
We now study reflections in the more general setting of a symmetric bilinear space (V, B)
over F, where 2F ̸= 0F . For any z ∈V such that B(z, z) ̸= 0, the reflection determined by
z is the map Sz : V →V defined by
Sz(v) = v −2F B(v, z)
B(z, z) z
for v ∈V .
(14.8)
When Sz is defined, we have V = Fz ⊞(Fz)⊥by the first example in §14.7. In particular,
V has an ordered basis (z, v2, . . . , vn) such that z ⊥vi for 2 ≤i ≤n.
Properties of Reflections. Assume B(z, z) ̸= 0, so that the reflection Sz is defined.
(a) Sz is an F-linear map.
(b) Sz is the unique F-linear map on V satisfying Sz(z) = −z and Sz(v) = v for all v ∈V
such that v ⊥z. Also det(Sz) = −1F .
386
Advanced Linear Algebra
(c) Sz is invertible, with S−1
z
= Sz.
(d) Sz is B-orthogonal.
(e) For any invertible B-orthogonal map T on V , T ◦Sz ◦T −1 = ST (z).
(f) If v ∈V satisfies B(v, v) = B(z, z) ̸= 0 and B(v −z, v −z) ̸= 0, then Sv−z interchanges
v and z.
Proof. (a) Fix v, w ∈V and c ∈F. Using bilinearity of B, compute
Sz(v + w) = (v + w) −2B(v + w, z)
B(z, z)
z = v + w −2B(v, z) + 2B(w, z)
B(z, z)
z
= v −2B(v, z)
B(z, z) z + w −2B(w, z)
B(z, z) z = Sz(v) + Sz(w);
Sz(cv) = cv −2B(cv, z)
B(z, z) z = cv −2cB(v, z)
B(z, z) z = cSz(v).
(b) Taking v = z in the definition gives Sz(z) = z −2B(z,z)
B(z,z) z = z −2z = −z. For v ∈V
such that v ⊥z, we have B(v, z) = 0 and so Sz(v) = v −0z = v. To prove uniqueness,
suppose S is any linear map on V satisfying S(z) = −z and S(v) = v for all v orthogonal
to z. Then S = Sz since these linear maps agree on the ordered basis (z, v2, . . . , vn). The
matrix of Sz relative to this ordered basis is diagonal with diagonal entries −1, 1, . . . , 1. So
det(Sz) = −1.
(c) To show Sz is its own inverse, it suffices (by linearity) to show that Sz(Sz(v)) = v
for all v in the basis (z, v2, . . . , vn). By part (b), Sz(Sz(z)) = Sz(−z) = −(−z) = z, while
Sz(Sz(vi)) = Sz(vi) = vi for 2 ≤i ≤n.
(d) We prove that Sz is B-orthogonal by checking on the basis (z, v2, . . . , vn). Fix i, j
between 2 and n. Using part (c), we compute:
B(Sz(z), Sz(z)) = B(−z, −z) = (−1)2B(z, z) = B(z, z);
B(Sz(z), Sz(vi)) = B(−z, vi) = −B(z, vi) = −0 = 0 = B(z, vi);
B(Sz(vi), Sz(vj)) = B(vi, vj).
(e) Let T : V →V be an invertible B-orthogonal map. Since B(T(z), T(z)) = B(z, z) ̸=
0, the reflection ST (z) is defined. Define S′ = T ◦Sz ◦T −1, which is F-linear. We show
S′ = ST (z) by checking the conditions from part (b) that uniquely characterize ST (z). First,
S′(T(z)) = T(Sz(T −1(T(z)))) = T(Sz(z)) = T(−z) = −T(z).
Second, suppose v′ ∈V is orthogonal to T(z). Since T −1 is an orthogonal map, v = T −1(v′)
is orthogonal to T −1(T(z)) = z. So
S′(v′) = T(Sz(T −1(v′))) = T(Sz(v)) = T(v) = v′.
Thus S′ is the reflection determined by T(z).
(f) Assume v ∈V satisfies B(v, v) = B(z, z) ̸= 0 and B(v −z, v −z) ̸= 0. Then Sv−z is
defined, B(v −z, v −z) = B(v, v) −2B(v, z) + B(z, z) = 2B(v, v) −2B(v, z) = 2B(v, v −z),
and hence
Sv−z(v) = v −
2B(v, v −z)
B(v −z, v −z)(v −z) = v −(v −z) = z.
By part (c), Sv−z(z) = v.
Bilinear Forms
387
As an example, consider a hyperbolic plane (V, B) spanned by a hyperbolic pair (v, w).
We saw in §14.12 that for nonzero b ∈F, the linear map Tb sending v to bw and w to b−1v
is an orthogonal map on V . We claim that the map Tb equals the reflection Sv−bw. To verify
this, first check that (v −bw, v + bw) is a basis of V such that B(v −bw, v −bw) = −2b ̸= 0
and B(v −bw, v + bw) = 0. Next, compute
Tb(v −bw) = Tb(v) −bTb(w) = bw −b(b−1v) = bw −v = −(v −bw) = Sv−bw(v −bw);
Tb(v + bw) = Tb(v) + bTb(w) = bw + b(b−1v) = v + bw = Sv−bw(v + bw).
Since the linear maps Tb and Sv−bw agree on a basis of V , they are equal. Taking b = 1,
we have the special case T1 = Sv−w, where T1 interchanges v and w. Note that T1 ◦Tb is
the linear map sending v to bv and w to b−1w. We proved in §14.12 that OB(V ) consists of
all the maps Tb and T1 ◦Tb, as b ranges over F̸=0. Since Tb and T1 are reflections, we have
now proved that every orthogonal map on a hyperbolic plane is a composition of one or two
reflections. Similarly, every orthogonal map on R2 is a composition of one or two reflections
(Exercise 62). We generalize these results in the next section.
14.14
Writing Orthogonal Maps as Compositions of Reflections
For the two-dimensional spaces considered in the last section, every orthogonal map is
a product of at most two reflections. Cartan and Dieudonn´e generalized this result to
higher-dimensional spaces. We give Artin’s proof of their theorem, following the account
in Jacobson’s algebra text [32, §6.6].
Theorem on Orthogonal Maps and Reflections. Assume 2F ̸= 0F and (V, B) is an n-
dimensional symmetric nondegenerate bilinear space. Every B-orthogonal map T : V →V
is the composition of at most n reflections.
Proof. We use induction on n = dim(V ). The base case n ≤1 is routine (Exercise 70),
using the convention that idV is the composition of zero reflections. Assume n > 1 and the
theorem holds for all spaces of dimension less than n. Let T be an orthogonal map on an
n-dimensional space (V, B). We consider four cases.
Case 1: There exists x ∈V with B(x, x) ̸= 0 and T(x) = x. Let V ′ = (Fx)⊥. By the first
example at the end of §14.7, V = Fx⊞V ′. Since T : V →V is a bilinear space isomorphism
sending Fx onto itself, T maps V ′ onto V ′ (Exercise 23). We can therefore consider the
restricted map T ′ = T|V ′, which is an orthogonal map on the (n −1)-dimensional bilinear
space (V ′, B|V ′). Since V = Fx ⊞V ′, B|V ′ is nondegenerate. By induction hypothesis, for
some k ≤n −1 and some z1, . . . , zk ∈V ′, we have T ′ = S′
z1 ◦· · · ◦S′
zk, where S′
z stands for
the reflection on V ′ determined by z. Letting Sz be the reflection on V determined by z, we
claim that T = Sz1 ◦· · · ◦Szk. On one hand, both maps have the same effect on vectors in
V ′. On the other hand, both maps send x to itself, since x ⊥V ′. So the two linear maps are
equal since they agree on Fx and V ′. Thus we have written T as a product of k reflections
in V , where k < n.
Case 2: There exists x ∈V with B(x, x) ̸= 0 and B(x −T(x), x −T(x)) ̸= 0. Note
B(T(x), T(x)) = B(x, x) since T is orthogonal. By property (f) of §14.13, the reflection
Sx−T (x) interchanges x and T(x). Then Sx−T (x) ◦T is orthogonal and sends x to itself, so
Case 1 applies to this map. We get Sx−T (x) ◦T = Sz1 ◦· · · ◦Szk for some z1, . . . , zk ∈V
where k < n. Then T = Sx−T (x) ◦Sz1 ◦· · · ◦Szk is a product of at most n reflections.
388
Advanced Linear Algebra
Case 3: V is 2-dimensional. As a subcase, suppose every nonzero v ∈V satisfies
B(v, v) ̸= 0. Fix any nonzero x ∈V . If T(x) = x, then Case 1 applies to T. If T(x) ̸= x,
then Case 2 applies to T. For the other subcase of Case 3, suppose B(v, v) = 0 for some
nonzero v ∈V . Since B is nondegenerate, there is z ∈V with B(v, z) ̸= 0. By Exercise 72
(or by the Embedding Theorem for Totally Isotropic Spaces, applied to Fv), there is w ∈V
such that (v, w) is a hyperbolic pair and V = Fv ⊕Fw is a hyperbolic plane. Then the
discussion at the end of §14.13 proves that T is a product of at most two reflections.
Case 4: V does not satisfy Case 1 or Case 2 or Case 3. This means that n = dim(V ) ≥3
and for all x ∈V , if B(x, x) ̸= 0, then T(x) ̸= x and B(x −T(x), x −T(x)) = 0. Let
R : V →V be the linear map given by R(x) = x −T(x) for x ∈V . We claim that
B(R(x), R(x)) = 0 holds for all x ∈V . By the assumption of Case 4, it suffices to check the
claim for fixed nonzero x ∈V with B(x, x) = 0. Let W = (Fx)⊥, which is a subspace of V
containing Fx. By part (b) of the Theorem on Orthogonal Complements, dim(W) = n −1.
Since n ≥3, we have n −1 > n/2, and so W cannot be totally isotropic (§14.11). Thus
there is w ∈W with B(w, w) ̸= 0. We know B(w, x) = B(x, x) = 0, which leads to
B(w + x, w + x) = B(w −x, w −x) = B(w, w) ̸= 0. By the assumption of Case 4, we get
B(R(w + x), R(w + x)) = B(R(w −x), R(w −x)) = B(R(w), R(w)) = 0. Bilinearity of B
gives B(u + v, u + v) + B(u −v, u −v) = 2B(u, u) + 2B(v, v) for any u, v ∈V (Exercise 1).
Applying this with u = R(w) and v = R(x) gives 0 + 0 = 2 · 0 + 2B(R(x), R(x)). So
B(R(x), R(x)) = 0, proving the claim.
Let the linear map R have image I and kernel K. We prove I = K. The claim above
shows that I is a totally isotropic subspace of V , which means that I ⊆I⊥. For any x ∈K,
we have R(x) = 0 and T(x) = x. By the contrapositive of the assumption of Case 4, we
must have B(x, x) = 0. This means that K is also totally isotropic, so K ⊆K⊥. We next
show that I = K⊥. To prove I ⊆K⊥, fix y ∈I. We can write y = R(v) = v −T(v)
for some v ∈V . Fix x ∈K, so x = T(x). We compute B(x, y) = B(x, v) −B(x, T(v)) =
B(x, v)−B(T(x), T(v)) = 0 using orthogonality of T. This shows y ∈K⊥. By part (b) of the
Theorem on Orthogonal Complements and by the Rank–Nullity Theorem, dim(K⊥) = n −
dim(K) = dim(I). Since I is a subset of K⊥and both spaces have the same finite dimension,
we get I = K⊥. Taking the orthogonal complement of both sides gives I⊥= K⊥⊥= K.
Then K ⊆K⊥= I ⊆I⊥= K, and therefore K = I. The fact that I ⊆K means that
R(R(u)) = 0 for all u in V , or equivalently (idV −T)2 is the zero map on V . By Exercise 75,
det(T) = 1 follows. Also, since n = dim(I) + dim(K) and K = I, we see that n is even. To
summarize, we have proved that if Case 4 occurs when analyzing a particular map T, then
n = dim(V ) must be even and det(T) must be 1.
To finish Case 4, pick a particular u ∈V such that B(u, u) ̸= 0. Such u must exist, since
otherwise V is totally isotropic and B = 0, contradicting nondegeneracy of B. Consider
the map T ′ = Su ◦T. This map is orthogonal, being the composition of orthogonal maps,
and det(T ′) = det(Su) det(T) = −1. On one hand, this means that applying the proof to
T ′ does not lead to Case 4. So we already know there exist k ≤n and z1, . . . , zk ∈V with
T ′ = Sz1 ◦· · · ◦Szk. On the other hand, since det(Szi) = −1 for each i, det(T ′) = −1
implies (−1)k = −1 and k is odd. Since n is even in Case 4, we must have k < n. Finally,
T = Su ◦Sz1 ◦· · · ◦Szk is a product of k + 1 ≤n reflections.
Exercise 77 outlines a simpler proof of the weaker result that every orthogonal map on
an n-dimensional nondegenerate symmetric bilinear space can be written as a product of
at most 2n reflections.
Bilinear Forms
389
14.15
Witt’s Cancellation Theorem
Suppose (V, B) and (V∗, B∗) are bilinear spaces, f : V →V∗is a bilinear space isomorphism,
W is a subspace of V , and W∗= f[W] is the image of W under f. It is routine to check
that f[W ⊥] = W ⊥
∗
in this situation (Exercise 23), so that f restricts to an isomorphism
from W ⊥onto W ⊥
∗. The next theorem establishes a stronger result. We write ∼= to denote
isomorphism of bilinear spaces.
Witt’s Cancellation Theorem.
Assume 2F ̸= 0F , (V, B) and (V∗, B∗) are symmetric
bilinear spaces, and V ∼= V∗via an isomorphism f. Suppose W is a nondegenerate subspace
of V , W∗is a subspace of V∗, and W ∼= W∗via an isomorphism g. Then W ⊥∼= W ⊥
∗.
Since W (and hence W∗) are nondegenerate subspaces, we know V = W ⊞W ⊥and
V∗= W∗⊞W ⊥
∗. Informally, the theorem says that if W ⊞W ⊥∼= W∗⊞W ⊥
∗and W ∼= W∗,
then we can “cancel” the isomorphic summands W and W∗to conclude that W ⊥∼= W ⊥
∗.
The observation preceding the theorem is the special case where g = f|W. Some exercises
explore whether analogous cancellation theorems hold for other algebraic structures.
Proof. Step 1. We prove the special case of the theorem where V∗= V , B∗= B, f =
idV , and dim(W) = 1. Write W = Fx for some x ∈V . Note B(x, x) ̸= 0 since W is
nondegenerate. Let y = g(x), so W∗= Fy and B(y, y) = B(x, x) ̸= 0. By the Parallelogram
Law (Exercise 1), B(x+y, x+y)+B(x−y, x−y) = 2B(x, x)+2B(y, y) = 2F ·2F ·B(x, x) ̸= 0F .
Therefore B(x + y, x + y) ̸= 0 or B(x −y, x −y) ̸= 0. Define z = y if B(x −y, x −y) ̸= 0
and z = −y otherwise. In both cases, we have W∗= Fz, B(z, z) = B(x, x) ̸= 0, and
B(x −z, x −z) ̸= 0. By property (f) from §14.13, the reflection Sx−z : V →V is an
orthogonal map interchanging x and z. Since Sx−z is a bilinear isomorphism of V sending
W = Fx onto W∗= Fz, Sx−z restricts to a bilinear isomorphism from W ⊥onto W ⊥
∗.
Step 2. We prove the special case of the theorem where dim(W) = 1. Let W1 = f −1[W∗],
and note f −1 ◦g : W →W1 is a bilinear isomorphism. By Step 1, there is an isomorphism
h : W ⊥→W ⊥
1 . Since f maps V onto V∗and W1 onto W∗, f maps W ⊥
1 onto W ⊥
∗. Then
f ◦h is an isomorphism from W ⊥onto W ⊥
∗.
Step 3. We prove the theorem by induction on k = dim(W). The case k = 0 is immediate,
and Step 2 proves the case k = 1. Fix W with dim(W) = k > 1, and assume the theorem
holds for subspaces of dimension less than k. Since W is nondegenerate, there is x ∈W with
B(x, x) ̸= 0 (otherwise W would be totally isotropic and B|W would be zero). Fix such an
x, and let Z = (Fx)⊥be the orthogonal complement of Fx in W. We have W = Fx ⊞Z
since B|Fx is nondegenerate. Define y = g(x) ∈W∗, and let Z∗= (Fy)⊥be the orthogonal
complement of Fy in W∗. Then W∗= Fy ⊞Z∗, where the isomorphism g : W →W∗maps
Fx onto Fy and hence maps Z onto Z∗. We have
Fx ⊞Z ⊞W ⊥= V ∼= V∗= Fy ⊞Z∗⊞W ⊥
∗.
The orthogonal complement of Fx in V is Z ⊞W ⊥, and the orthogonal complement of
Fy in V∗is Z∗⊞W ⊥
∗. By Step 2, there is an isomorphism f1 : Z ⊞W ⊥→Z∗⊞W ⊥
∗. We
also have an isomorphism g1 : Z →Z∗(obtained by restricting g), and dim(Z) = k −1.
Applying the induction hypothesis, we get an isomorphism h : W ⊥→W ⊥
∗.
390
Advanced Linear Algebra
14.16
Uniqueness Property of Witt Decompositions
This section proves a uniqueness property of Witt decompositions of a symmetric bilinear
space. The proof uses Witt’s Cancellation Theorem and the following analogous cancellation
property for radicals.
Radical Cancellation Lemma. Suppose (V, B) is a symmetric bilinear space with radical
R, (V∗, B∗) is a symmetric bilinear space with radical R∗, and f : V →V∗is a bilinear space
isomorphism. If W and W∗are any subspaces such that V = R ⊕W and V∗= R∗⊕W∗,
then W ∼= W∗.
Proof. Recall from §14.8 the construction of the bilinear spaces (V/R, B) and (V∗/R∗, B∗).
We saw in that section that (W, B|W) ∼= (V/R, B) and (W∗, B∗|W∗) ∼= (V∗/R∗, B∗). It is
routine to check that f restricts to an isomorphism of R onto R∗, and that f induces an
isomorphism f : V/R →V∗/R∗given by f(x + R) = f(x) + R∗for x ∈V . Combining this
isomorphism with the ones just mentioned, we get (W, B|W) ∼= (W∗, B∗|W∗).
Witt’s Decomposition Theorem (Uniqueness Part).
Suppose (V, B) and (V∗, B∗)
are isomorphic symmetric bilinear spaces, V = R ⊞H ⊞Y is a Witt decomposition of V ,
and V∗= R∗⊞H∗⊞Y∗is a Witt decomposition of V∗. Then R ∼= R∗, H ∼= H∗, and Y ∼= Y∗.
Proof. Let f : V →V∗be a bilinear space isomorphism. As remarked in §14.11, R =
Rad(V ), R∗= Rad(V∗), and f restricts to an isomorphism Rad(V ) ∼= Rad(V∗). By the
Radical Cancellation Lemma, we get an isomorphism g : H ⊞Y →H∗⊞Y∗. Write H =
H1 ⊞· · · ⊞Hk and H∗= H∗1 ⊞· · · ⊞H∗k∗for some hyperbolic planes Hi and H∗j and some
k, k∗≥0. We can assume k ≤k∗by interchanging the roles of V and V∗if needed. Any
two hyperbolic planes over F are isomorphic and nondegenerate, so we can apply Witt’s
Cancellation Theorem k times to remove pairs of factors Hi and H∗i for 1 ≤i ≤k. We are
left with an isomorphism h : Y →H′ ⊞Y∗where H′ is the orthogonal direct sum of k∗−k
hyperbolic planes. If k∗−k > 0, then we reach a contradiction since H′ ⊞Y∗has a nonzero
isotropic vector (coming from H∗k∗) but the isomorphic non-isotropic space Y has no such
vector. Thus k∗= k, which shows that H ∼= H∗, H′ = {0}, and Y ∼= Y∗via h.
14.17
Summary
In this summary, we assume 2F ̸= 0F and V is an n-dimensional vector space over F,
although these assumptions are not always needed.
1.
Definitions. Table 14.1 summarizes definitions concerning bilinear forms.
2.
Matrix Representation of Bilinear Forms. For each ordered basis X = (x1, . . . , xn)
of V , there is a vector space isomorphism from Bilin(V ) onto Mn(F) sending a
bilinear form B to the matrix A = [B]X with i, j-entry B(xi, xj). The form B
is symmetric iff AT = A, and B is alternate iff AT = −A and A(i, i) = 0 for
all i. Changing from X to a new basis Y = (y1, . . . , yn) replaces [B]X by the
congruent matrix [B]Y = P T[B]XP, where the transition matrix P is defined by
yj = Pn
i=1 P(i, j)xi for 1 ≤i, j ≤n.
Bilinear Forms
391
TABLE 14.1
Definitions involving bilinear forms. Here, V is a finite-dimensional F-vector space.
Concept
Definition
bilinear form B : V × V →F
B(x + y, z) = B(x, z) + B(y, z), B(cx, y) = cB(x, y),
B(x, y + z) = B(x, y) + B(x, z), B(x, cy) = cB(x, y)
for all x, y, z ∈V , c ∈F
symmetric form
B(x, y) = B(y, x) for all x, y ∈V
antisymmetric form
B(x, y) = −B(y, x) for all x, y ∈V
alternate form
B(x, x) = 0 for all x ∈V
nondegenerate form
if B(v, w) = 0 for all v ∈V , then w = 0
matrix [B]X for basis
X = (x1, . . . , xn)
i, j-entry of [B]X is B(xi, xj)
matrix A is congruent to
matrix D
A = P TDP for some invertible matrix P
Bilin(V )
vector space of bilinear forms on V
dual space V ∗
vector space of linear functions from V to F
v ⊥w (where v, w ∈V )
B(v, w) = 0
S ⊥T (where S, T ⊆V )
B(s, t) = 0 for all s ∈S, t ∈T
S⊥(orthogonal complement)
{v ∈V : B(s, v) = 0 for all s ∈S}
⊥S (left orthogonal complement)
{v ∈V : B(v, s) = 0 for all s ∈S}
radical Rad(V )
V ⊥= {w ∈V : B(v, w) = 0 for all v ∈V }
V = V1 ⊕· · · ⊕Vk (direct sum)
V = V1 + · · · + Vk and Vi ∩P
j<i Vj = {0} for all i
V = V1 ⊞· · · ⊞Vk
(orthog. direct sum)
V = V1 ⊕· · · ⊕Vk and Vi ⊥Vj for all i ̸= j
isotropic vector v
B(v, v) = 0 and v ̸= 0
totally isotropic subspace W
B(w, w) = 0 for all w ∈W (so B|W = 0 and
W ⊆W ⊥)
non-isotropic subspace Y
B(y, y) ̸= 0 for all nonzero y ∈Y
Witt index of V
dimension of any maximal totally isotropic subspace
of V
hyperbolic pair (v, w)
B(v, v) = 0 = B(w, w) and B(v, w) = 1 = B(w, v)
hyperbolic plane
span of a hyperbolic pair
symplectic pair (v, w)
B(v, v) = 0 = B(w, w) and B(v, w) = 1 = −B(w, v)
symplectic plane
span of a symplectic pair
Witt decomposition of V
V = R ⊞H1 ⊞· · · ⊞Hk ⊞Y where R is totally
isotropic,
H1, . . . , Hk are hyperbolic planes, and Y is
non-isotropic
bilinear hom.
T : (V, B) →(V∗, B∗)
T is F-linear and B∗(T(x), T(y)) = B(x, y) for
x, y ∈V
B-orthogonal map T : V →V
linear map with B(T(x), T(y)) = B(x, y) for
x, y ∈V
OB(V )
set of B-orthogonal maps on V
reflection Sz (where B(z, z) ̸= 0)
Sz(v) = v −2B(v,z)
B(z,z) z for v ∈V
392
Advanced Linear Algebra
3.
Basic Orthogonality Facts. Every orthogonal list of non-isotropic vectors is
linearly independent. For S ⊆V , the orthogonal complements S⊥and ⊥S are
always subspaces of V such that S ⊆⊥(S⊥) and S ⊆(⊥S)⊥. If S ⊆T, then
T ⊥⊆S⊥. Orthogonality is a symmetric relation for symmetric and alternate
bilinear forms. A bilinear form B on V is nondegenerate iff V ⊥= {0}, while the
restriction of B to a subspace W is nondegenerate iff W ∩W ⊥= {0}. Given an
orthogonal direct sum V = W1 ⊞· · · ⊞Wk, B is nondegenerate on V iff each
B|Wi is nondegenerate on Wi. If an ordered basis X for V is the concatenation
of ordered bases Xi for Wi, then [B]X is block-diagonal with diagonal blocks
[B|Wi]Xi.
4.
Bilinear Forms and Dual Spaces. If B is any nondegenerate bilinear form on V ,
there is a vector space isomorphism R : V →V ∗given by R(w)(v) = B(v, w) for
all v, w ∈V . This means that for every g ∈V ∗, there is a unique w ∈V such
that g(v) = B(v, w) for all v ∈V .
5.
Theorem on Orthogonal Complements. Let W be any subspace of V . If B|W is
nondegenerate, then dim(W) + dim(W ⊥) = dim(V ) = dim(W) + dim(⊥W) and
V = W ⊕W ⊥= W ⊕⊥W. If B is nondegenerate, then dim(W) + dim(W ⊥) =
dim(V ) = dim(W) + dim(⊥W) and ⊥(W ⊥) = W = (⊥W)⊥.
6.
Diagonalization Theorems for Symmetric Bilinear Forms. If B is symmetric, then
there is an ordered basis X for V such that [B]X is diagonal. If square roots exist
in F, then we can arrange that all diagonal entries of [B]X are in {0, 1}. If F = R,
then we can arrange that all diagonal entries of [B]X are in {−1, 0, 1}. In both
cases, the number of −1s, 0s, and 1s is uniquely determined by B.
7.
Structure Theorem for Alternate Bilinear Forms. An alternate bilinear space
(V, B) is the orthogonal direct sum of its radical and s symplectic planes. So B can
be represented by a block-diagonal matrix containins s copies of C =

0
1
−1
0

and all other entries 0.
8.
Totally Isotropic Subspaces. Let W be a totally isotropic subspace of a nondegen-
erate symmetric space V . Then dim(W) ≤dim(V )/2, and W is contained in an
orthogonal direct sum of dim(W) hyperbolic planes.
9.
Witt Decompositions. Any symmetric bilinear space has a Witt decomposition
V = R ⊞H ⊞Y , where R is totally isotropic, H is an orthogonal direct sum of
zero or more hyperbolic planes, and Y is non-isotropic. If V = R∗⊞H∗⊞Y∗is a
second Witt decomposition, then R = R∗= Rad(V ), H ∼= H∗, and Y ∼= Y∗.
10.
Orthogonal Maps and Reflections. For nondegenerate B, the set OB(V ) of B-
orthogonal maps is a subgroup of GL(V ). For z ∈V with B(z, z) ̸= 0, the
reflection Sz is a self-inverse B-orthogonal linear map that sends z to −z and
sends each v ∈(Fz)⊥to itself. If B is symmetric and nondegenerate, then every
B-orthogonal map on V is the composition of at most dim(V ) reflections.
11.
Witt’s Cancellation Theorem. If there are isomorphisms of symmetric bilinear
spaces W ⊞W ⊥∼= W∗⊞W ⊥
∗
and W ∼= W∗, then there is an isomorphism
W ⊥∼= W ⊥
∗.
Bilinear Forms
393
14.18
Exercises
In these exercises, assume F is a field with 2F ̸= 0F , V is a finite-dimensional F-vector
space, B is a bilinear form on V , and X = (x1, . . . , xn) is an ordered basis of V , unless
otherwise stated.
1.
Parallelogram Law. Prove: for all u, v ∈V , B(u + v, u + v) + B(u −v, u −v) =
2B(u, u) + 2B(v, v).
2.
Assume B is symmetric. Define Q : V →V by Q(v) = B(v, v). Prove: for all
c ∈F and v ∈V , Q(cv) = c2Q(v). For v, w ∈V , find a formula for B(v, w) in
terms of Q.
3.
Define B((x, y), (s, t)) = xs + 2yt for x, y, s, t ∈F. Prove B is a bilinear form on
F 2. Is B symmetric? alternate? nondegenerate? What is [B]X for X = (e1, e2)?
4.
Define B((x, y), (s, t)) = det
 x
y
s
t

for x, y, s, t ∈F. Is B a bilinear form on
F 2? If so, is B symmetric? alternate? nondegenerate?
5.
Define B((x, y), (s, t)) = xy + st for x, y, s, t ∈F. Is B a bilinear form on F 2? If
so, is B symmetric? alternate? nondegenerate?
6.
Define B((x, y), (s, t)) = (x + y)(s −t) for x, y, s, t ∈F. Is B a bilinear form on
F 2? If so, is B symmetric? alternate? nondegenerate?
7.
For v, w ∈R3, define B(v, w) = v × w (the cross product of v and w). Prove B
is linear in the first and second position, and B(v, v) = 0 for all v ∈R3. But B
is not a bilinear form on R3 — why not?
8.
Check the claims about B1, B2, and B3 in the sentence following equation (14.2)
on page 369.
9.
Check the claims about B in the sentence following equation (14.3) on page 369.
10.
Suppose (V, B) and (V ′, B′) are bilinear spaces, S spans V , and T : V →V ′
is a linear map. Prove T is a bilinear homomorphism iff for all x, y ∈S,
B′(T(x), T(y)) = B(x, y).
11.
Prove the bilinear spaces (F 2, B1) and (F 2, B2) defined in (14.2) are isomorphic.
Does your proof use 2F ̸= 0F ?
12.
Let F = Z2, V = F 2, and define B((a, b), (c, d)) = ad + bc for a, b, c, d ∈Z2. Find
all hyperbolic pairs in the hyperbolic plane (V, B). Is there an ordered basis X
for V such that [B]X is diagonal?
13.
Let F = Z2, V = F 2, and define B((a, b), (c, d)) = ac −bd for a, b, c, d ∈Z2. Is
(V, B) a hyperbolic plane?
14.
Let (V, B) and (V∗, B∗) be finite-dimensional bilinear spaces. Prove these spaces
are isomorphic iff V has an ordered basis X and V∗has an ordered basis X∗such
that [B]X = [B∗]X∗.
15.
Suppose Xi is an ordered basis for the bilinear space (Vi, Bi) for 1 ≤i ≤k,
and let (V, B) be the product of these spaces. Give a precise formulation of the
statement that the concatenation X of X1, . . . , Xk is an ordered basis of V . Prove
that [B]X is block diagonal with diagonal blocks [B1]X1, . . . , [Bk]Xk.
16.
Define B′(v, w) = B(w, v) for v, w ∈V . Prove B′ is a bilinear form. How are the
matrices [B]X and [B′]X related?
394
Advanced Linear Algebra
17.
Given a linear map T : V →V , define B′(v, w) = B(T(v), w) for v, w ∈V . Prove
B′ is bilinear. Prove a formula relating [B′]X to [B]X and [T]X.
18.
Given a linear map S : V →V , define B′(v, w) = B(v, S(w)) for v, w ∈V . Prove
B′ is bilinear. Prove a formula relating [B′]X to [B]X and [S]X.
19.
Define Bs(v, w) = B(v, w) + B(w, v) and Ba(v, w) = B(v, w) −B(w, v) for v, w ∈
V . Prove Bs is a symmetric bilinear form. Prove Ba is an alternate bilinear form.
How are [Bs]X and [Ba]X related to [B]X?
20.
Define B(X, Y ) to be the trace (sum of diagonal entries) of XY for X, Y ∈Mn(F).
Prove B is a symmetric bilinear form on Mn(F). Is B nondegenerate? Find the
matrix of B relative to the ordered basis of matrix units (E1,1, E1,2, . . . , En,n),
where Ei,j ∈Mn(F) has i, j-entry 1F and all other entries 0F .
21.
Let A = [B]X. Prove B is symmetric iff AT = A. Prove B is antisymmetric iff
AT = −A.
22.
Suppose v, w ∈V have coordinates [v]X and [w]X (viewed as column vectors)
relative to the ordered basis X. Prove B(v, w) = [v]T
X[B]X[w]X.
23.
Suppose (V, B) and (V∗, B∗) are bilinear spaces, f : V →V∗is a bilinear space
isomorphism, W is a subspace of V , and W∗= f[W]. Prove f[W ⊥] = W ⊥
∗.
24.
Give an example of a symmetric bilinear space (V, B) and a subspace W ̸= {0}
where B is degenerate, B|W is nondegenerate, and W ⊥⊥̸= W.
25.
Give an example of an alternate bilinear space (V, B) and a subspace W where
B is nondegenerate, B|W is degenerate, and V ̸= W ⊞W ⊥.
26.
Say that a bilinear form B on V is degenerate on the left
if there exists w ̸= 0 in
V such that for all v ∈V , B(w, v) = 0. If no such w exists, say B is nondegenerate
on the left. Prove the following conditions are equivalent:
(a) B is degenerate (as defined on page 368);
(b) [B]X is invertible for some ordered basis X of V ;
(c) [B]X is invertible for every ordered basis X of V ;
(d) B is degenerate on the left.
27.
Suppose the bilinear form B is nondegenerate on the left. Show that for each
g ∈V ∗, there exists a unique w ∈V such that g(v) = B(w, v) for all v ∈V .
28.
Prove: if B is nondegenerate, then for each ordered basis (x1, . . . , xn) of V , there
is an ordered basis (y1, . . . , yn) for V such that for 1 ≤i, j ≤n, B(xi, yj) is 1 if
i = j and 0 if i ̸= j.
29.
Show that the Theorem on Orthogonal Complements, as stated in the chapter
summary, holds for any bilinear form B.
30.
Prove: if B is nondegenerate, then f(S) = S⊥and g(S) = ⊥S define order-
reversing, mutually inverse bijections on the lattice of subspaces of V .
31.
Suppose (V, B) ∼= (V∗, B∗) via an isomorphism f, and let R = Rad(V ), R∗=
Rad(V∗). Prove that f restricts to an isomorphism of R onto R∗, and that f
induces an isomorphism f : V/R →V∗/R∗given by f(x + R) = f(x) + R∗for
x ∈V .
32.
Prove: if (V, B) is a bilinear space containing a non-isotropic vector, then every
isotropic vector in V is the difference of two non-isotropic vectors. (Hint: Use the
Parallelogram Law.) What happens if 2F = 0F ?
33.
Suppose (V, B) is a bilinear space such that orthogonality is symmetric, meaning:
for all x, y ∈V , B(x, y) = 0 implies B(y, x) = 0. Prove that B must be symmetric
Bilinear Forms
395
or alternate, as follows. Let S be the set of v ∈V such that B(v, w) = B(w, v)
for all w ∈V . Show S is a subspace of V containing all non-isotropic vectors in
V . Use the previous exercise to complete the proof. Can you find a proof in the
case 2F = 0F ?
34.
Are A =
 1
0
0
−1

and D =
 0
1
1
0

congruent in M2(Q)?
If so, find an invertible P ∈M2(Q) with A = P TDP.
35.
Are A =
 3
0
0
4

and D =
 2
0
0
3

congruent in M2(Q)?
If so, find an invertible P ∈M2(Q) with A = P TDP.
36.
Suppose W1, . . . , Wk are subspaces of V with V
= W1 ⊞· · · ⊞Wk. Prove
Rad(V ) = Rad(W1) ⊞· · · ⊞Rad(Wk). Prove that B is nondegenerate iff B|Wi is
nondegenerate for 1 ≤i ≤k.
37.
Suppose W1, . . . , Wk are subspaces of V with V = W1 ⊞· · ·⊞Wk. Prove f : W1 ×
· · · × Wk →V , given by f((w1, . . . , wk)) = w1 + · · · + wk for wi ∈Wi, is an
isomorphism of bilinear spaces.
38.
Suppose W1, . . . , Wk are subspaces of V with V = W1 ⊞· · · ⊞Wk. Let Wi have
ordered basis Xi for 1 ≤i ≤k and X be the concatenation of X1, . . . , Xk. Prove
[B]X is block-diagonal with diagonal blocks [B|W1]X1, . . ., [B|Wk]Xk.
39.
Give examples to show that the conclusions of the last three exercises do not
always hold if we only assume V = W1 ⊕· · · ⊕Wk.
40.
Suppose W1, . . . , Wk are subspaces of V with V = W1 + · · · + Wk and Wi ⊥Wj
for all i ̸= j. Does it always follow that V = W1 ⊞· · · ⊞Wk? Does the answer
change if B is nondegenerate?
41.
Let (V, B) be a bilinear space with radical R and W be any subspace of V such
that V = R ⊕W. Prove that p : W →V/R, given by p(w) = w + R for w ∈W,
is a bilinear space isomorphism from (W, B|W) to (V/R, B).
42.
Let B be a symmetric bilinear form on F n where B has matrix A relative to the
standard basis. Formulate an algorithm, similar to the Gram–Schmidt algorithm,
that computes an ordered basis X of F n such that [B]X is diagonal. Prove that
your algorithm works.
43.
For each matrix A ∈Mn(R), find a diagonal matrix D with entries in {0, 1, −1}
that is congruent to A.
(a)


4
4
4
4
4
4
4
4
4


(b)


0
1
0
1
1
0
−1
0
0
−1
0
1
1
0
1
0


(c)


1
2
0
2
0
−1
0
−1
3


44.
Define A(i, j) = j −i for 1 ≤i, j ≤5. Find a matrix congruent to A in M5(R)
that has the structure specified in the Theorem on Alternate Bilinear Forms.
45.
How many congruence classes of symmetric matrices are there in Mn(C)?
46.
How many congruence classes of symmetric matrices are there in Mn(R)?
47.
How many congruence classes of skew-symmetric matrices are there in Mn(F)?
48.
Let G be the multiplicative group of nonzero elements of the field F. Prove
G2 = {g2 : g ∈G} is a subgroup of G. Prove that for all A, C ∈GLn(F), if A
and C are congruent, then det(A)G2 = det(C)G2 (equality of cosets in G/G2).
Prove that for a bilinear form B, the coset det([B]X)G2 does not depend on the
choice of ordered basis X. This coset is called the discriminant of B.
396
Advanced Linear Algebra
49.
Give a specific infinite list of symmetric matrices in M2(Q) that are pairwise
non-congruent.
50.
Define g ∈(R2)∗by g(x, y) = 3x −5y for x, y ∈R. For each bilinear form Bj
in (14.2), find w ∈R2 such that g(v) = Bj(v, w) for all v ∈R2.
51.
Adjoints. Given any function T : V →V , a right adjoint of T relative to B is a
function S : V →V such that for all x, y ∈V , B(T(x), y) = B(x, S(y)). Suppose
B is nondegenerate. Prove every F-linear map T : V →V has a unique right
adjoint S, and S is F-linear. We write S = T ∗.
52.
Assume B is nondegenerate. For an F-linear map T : V →V with right adjoint
T ∗, what is the relation between the matrices [T ∗]X, [T]X, and [B]X?
53.
Assume B is nondegenerate. Prove that the adjoint map (sending T to T ∗) is
F-linear. Prove: for all F-linear maps R, T, (R ◦T)∗= T ∗◦R∗.
54.
Assume B is nondegenerate. Prove that a linear map T : V →V is B-orthogonal
iff T ∗= T −1.
55.
Universal Forms. A nondegenerate symmetric bilinear form B on V is called
universal iff for all c ∈F, there exists v ∈V with B(v, v) = c. Prove: if B(x, x) =
0 for some nonzero x ∈V , then B is universal. Does your proof use 2F ̸= 0F ?
56.
Give an example of a field F, a nondegenerate symmetric universal bilinear form
B on V , and a nondegenerate subspace W of V such that B|W is not universal.
57.
Prove: if dim(V ) > 1 and |F| = 2k + 1 for some integer k, then every symmetric
nondegenerate bilinear form B on V is universal. (Reduce to showing that for
fixed nonzero a, b ∈F, for all c ∈F, there exist s, t ∈F with as2 +bt2 = c. Study
the images of the maps g, h : F →F given by g(s) = as2 and h(t) = c −bt2.)
58.
Prove: if dim(V ) > 1, F is a finite field of odd size, and B is a nondegenerate
symmetric bilinear form on V , then there is an ordered basis X of V and d ∈F
such that [B]X is diagonal with diagonal entries 1, 1, . . . , 1, d.
59.
Let A ∈Mn(F) be a fixed invertible matrix. Prove directly, without reference
to bilinear forms, that On(A) = {C ∈Mn(F) : CTAC = A} is a group under
matrix multiplication. Must this be true if A is not invertible?
60.
For θ ∈R, show that the matrix R′
θ =
 cos θ
sin θ
sin θ
−cos θ

is orthogonal. Show
that the map T(v) = R′
θv (for v ∈R2) is a reflection map Sz.
61.
For θ ∈R, show that the matrix Rθ =
 cos θ
−sin θ
sin θ
cos θ

is orthogonal. Show
that the map T(v) = Rθv (for v ∈R2) rotates v counterclockwise θ radians
around the origin. Show that T is a composition of two reflections.
62.
Show that O2(R) = {R′
θ : θ ∈R} ∪{Rθ : θ ∈R}. Deduce that every orthogonal
map T on R2 (with the dot product) is a rotation or a reflection and is a
composition of at most two reflections.
63.
Let C(P; r) be the circle in R2 with center P and radius r. Prove that any two
circles C(P; r) and C(Q; s) with unequal centers intersect in 0, 1, or 2 points.
Prove that the number of intersection points depends only on r, s, and the
distance from P to Q.
64.
Euclidean Isometries. An isometry of the Euclidean plane is a function F :
R2 →R2 (not necessarily linear) that preserves Euclidean distance, meaning
that d2(F(v), F(w)) = d2(v, w) for all v, w ∈R2. Prove that every isometry F
Bilinear Forms
397
must be invertible. (Show F is one-to-one and onto. The previous exercise may
help with the proof that F is onto.)
65.
Show that the set of isometries of R2 is a group under function composition.
66.
Show that a function F : R2 →R2 is an orthogonal linear map iff F is an isometry
such that F(0) = 0.
67.
For each z ∈R2, define translation by z to be the map τz such that τz(v) = v +z
for v ∈R2. Show that every isometry F of R2 can be written F = τz ◦T for some
orthogonal map T and some z ∈R2.
68.
If ℓis a line in R2 (not necessarily through the origin), let sℓbe the map on R2
that sends each point on ℓto itself and sends each P not on ℓto the point Q on
the other side of ℓsuch that ℓis the perpendicular bisector of PQ. We call sℓ
reflection through the line ℓ. Prove that every translation τz is a composition of
two reflections. Prove that every isometry of R2 is a composition of reflections.
How many reflections are needed at most?
69.
Suppose B is a nondegenerate symmetric bilinear form on V , z ∈V , and B(z, z) ̸=
0. Prove: for all nonzero c ∈F, Scz is defined and Scz = Sz.
70.
Suppose B is a nondegenerate symmetric bilinear form on a 1-dimensional space
V . Prove that OB(V ) = {idV , −idV }. Prove that for all nonzero z ∈V , Sz =
−idV .
71.
Suppose B is a nondegenerate symmetric bilinear form on an n-dimensional space
V . Prove that −idV cannot be written as a product of fewer than n reflections.
72.
Let B be a symmetric bilinear form on V . Prove: for all v, z ∈V , if B(v, v) =
0 ̸= B(v, z), then there exists w ∈V such that (v, w) is a hyperbolic pair and
z ∈Fv + Fw.
73.
Quadratic Forms. A function Q : V →V is called a quadratic form on V iff
(a) Q(cv) = c2Q(v) for all c ∈F and all v ∈V ; and (b) the function B given by
B(v, w) = (Q(v + w) −Q(v) −Q(w))/2F for v, w ∈V is F-bilinear. Show that
the map sending Q to B is a bijection from the set of quadratic forms on V onto
the set of symmetric bilinear forms on V .
74.
Given scalars cij ∈F for 1 ≤i, j ≤n, define Q : V →V as follows. For
v = Pn
i=1 aixi ∈V with ai ∈F, let Q(v) = Pn
i=1
Pn
j=1 cijaiaj. Show Q is a
quadratic form, and find [B]X for the associated symmetric bilinear form B.
75.
Suppose V is an n-dimensional vector space and T : V →V is a linear map such
that (T −idV )2 = 0. Prove that det(T) = 1, as follows. Extend an ordered basis
(x1, . . . , xk) of ker(T −idV ) to an ordered basis X = (x1, . . . , xn) of V . Prove
that the matrix of T relative to X has the block form [T]X =
 Ik
B
0
In−k

for
some k × (n −k) matrix B.
76.
Suppose V is an n-dimensional vector space (over any field F) and T : V →V
is a linear map such that (T −idV )p = 0 for some positive integer p. Prove
det(T) = 1. (One approach uses the Theorem Classifying Nilpotent Maps in §8.6.
Alternatively, use induction on dim(V ). Prove that T must have a fixed point
x ̸= 0, and study the action of T on the quotient space V/Fx.)
77.
Let (V, B) be an n-dimensional nondegenerate symmetric bilinear space. Com-
plete the following outline proving that every B-orthogonal map T on V is a
composition of at most 2n reflections. Use induction on n. For the induction
step, choose x ∈V with B(x, x) ̸= 0. Prove there is a product U of one or
398
Advanced Linear Algebra
two reflections with U(T(x)) = x. Show U ◦T maps (Fx)⊥into itself. Use the
induction hypothesis to complete the proof.
78.
Suppose V is an n-dimensional F-vector space and H1, . . . , Hk are (n −1)-
dimensional subspaces of V . Prove H1 ∩· · · ∩Hk has dimension at least n −k.
79.
Suppose (V, B) is a nondegenerate symmetric bilinear space and T : V →V is
B-orthogonal. Prove:
(a) If dim(V ) is odd and det(T) = 1, then T has a fixed point x ̸= 0.
(b) If dim(V ) is even and det(T) = −1, then T has a fixed point x ̸= 0.
80.
Embedding Theorem for Degenerate Subspaces.
Suppose (V, B) and
(V∗, B∗) are n-dimensional nondegenerate symmetric bilinear spaces with m-
dimensional subspaces W and W∗such that W ∼= W∗as bilinear spaces. Let
k = dim(Rad(W)). Show there exist nondegenerate subspaces U and U∗with
W ⊆U ⊆V , W∗⊆U∗⊆V∗, dim(U) = m + k, and U ∼= U∗as bilinear spaces.
81.
Witt’s Extension Theorem. Suppose (V, B) and (V∗, B∗) are n-dimensional
nondegenerate symmetric bilinear spaces with m-dimensional subspaces W and
W∗such that V ∼= V∗via an isomorphism f and W ∼= W∗via an isomorphism g.
Show that g extends to an isomorphism from V to V∗.
82.
Give an example to show that Witt’s Cancellation Theorem need not hold if we
omit the hypothesis that the subspace W is nondegenerate.
83.
Suppose (V, B) and (V∗, B∗) are nondegenerate symmetric bilinear spaces, W is
a subspace of V , W∗is a subspace of V∗, V ∼= V∗via f, and W ∼= W∗via g. Does
W ⊥∼= W ⊥
∗necessarily follow?
84.
(a) Prove: for all nonempty finite sets X, Y, Z, if |X×Y | = |X×Z|, then |Y | = |Z|.
(b) Disprove: for all nonempty sets X, Y, Z, if |X × Y | = |X × Z|, then |Y | = |Z|.
85.
Prove this cancellation theorem: for all finite-dimensional vector spaces X, Y , Z
over a field F, if X × Y ∼= X × Z, then Y ∼= Z, where ∼= denotes isomorphism of
vector spaces.
86.
Disprove this statement: for all vector spaces X, Y , Z over a field F, if X × Y ∼=
X × Z, then Y ∼= Z, where ∼= denotes isomorphism of vector spaces.
87.
Prove: for all finite commutative groups X, Y , Z, if X × Y ∼= X × Z, then
Y ∼= Z, where ∼= denotes isomorphism of groups. (You may need theorems from
Chapter 16 to solve this.)
15
Metric Spaces and Hilbert Spaces
So far, our study of linear algebra has focused mostly on finite-dimensional vector spaces
and the linear maps between such spaces. In this chapter, we introduce some ideas that are
needed to do linear algebra in the infinite-dimensional setting. Infinite-dimensional vector
spaces occur frequently in analysis, where we study vector spaces of functions. To understand
such spaces in detail, we need not only algebraic concepts from linear algebra but also
some tools from analysis and topology. In particular, the notions of the length of a vector,
the distance between two vectors, and the convergence of a sequence of vectors are key
ingredients in the study of infinite-dimensional spaces.
The first part of this chapter develops the necesssary analytic concepts in the setting of
metric spaces. A metric space is a set in which we can define the distance between any two
points. The distance function (also called a metric) satisfies a few basic axioms, from which
many fundamental properties of convergent sequences can be derived. We use convergent
sequences to define other topological concepts such as closed sets, open sets, continuous
functions, compact sets, and complete spaces. Our discussion of metric spaces is far from
comprehensive, but it does provide a self-contained account of the analytic material needed
for our coverage of Hilbert spaces in the second part of the chapter.
Intuitively, a Hilbert space is a complex vector space (often infinite-dimensional) in which
we can define both the length of a vector and the concept of orthogonality of vectors, which
generalizes the geometric idea of perpendicular vectors. Orthogonality is defined via a scalar
product similar to the dot product in R3, but using complex scalars. A crucial technical
condition in the definition of a Hilbert space is the requirement that it be complete as a
metric space. Informally, this means that if the terms of a given sequence in the Hlbert
space get arbitrarily close to each other, then the sequence must converge to some point in
the space.
This completeness assumption provides an analytic substitute for the dimension-
counting arguments that we often need when proving facts about finite-dimensional inner
product spaces. For example, given any subspace W of a finite-dimensional inner product
space V , the theorem that V = W ⊕W ⊥(see §13.10)is proved using dimension counting.
The corresponding result in Hilbert spaces requires an additional hypothesis (the assumption
that the subspace W be a closed set), and the proof uses completeness in an essential way.
Orthonormal bases play a central role in the theory of finite-dimensional inner product
spaces. The analogous concept for infinite-dimensional Hilbert spaces is the idea of a
maximal orthonormal set. We show that every Hilbert space has a maximal orthonormal
set X, and every vector in the space can be expressed as an infinite linear combination of
the vectors in X. (We needs analytic ideas to give a precise definition of what this means.)
These results lead to a classification theorem showing that every abstract Hilbert space is
isomorphic to a concrete Hilbert space consisting of square-summable functions defined on
X. After proving these results, the chapter closes with a discussion of spaces of continuous
linear maps, the identification of a Hilbert space with its dual space, and adjoint operators.
DOI: 10.1201/9781003484561-15
399
400
Advanced Linear Algebra
15.1
Metric Spaces
Metric spaces were discussed briefly in §10.5; we repeat the relevant definitions here to
keep this chapter self-contained. A metric space is a set X together with a function (called
a metric or distance function) that measures the distance between any two points in the
set X. The distance function d : X × X →R must satisfy the following conditions for all
x, y, z ∈X. First, 0 ≤d(x, y) < ∞, and d(x, y) = 0 iff x = y. Second, d(x, y) = d(y, x).
Third, d(x, z) ≤d(x, y) + d(y, z). The second axiom is called symmetry; the third axiom is
called the Triangle Inequality. If several metric spaces are being considered, we sometimes
write dX for the metric defined on X.
The set R of real numbers is a metric space, with distance function d(x, y) = |x −y|
for all x, y ∈R. More generally, for each positive integer m, Rm is a metric space under
the Euclidean distance function, defined by setting d2(x, y) =
pPm
i=1 |xi −yi|2 for all x =
(x1, . . . , xm) and y = (y1, . . . , ym) in Rm. Analogous formulas define metrics on the spaces
C and Cm. We verify the metric space axioms for these examples later in the chapter, after
we introduce Hilbert spaces.
Here are a few more abstract examples of metric spaces. Given any set X and x, y ∈X,
define d(x, y) = 0 if x = y, and d(x, y) = 1 if x ̸= y. The first two axioms for a metric
are immediately verified. To see that the triangle inequality must hold, suppose that it
failed. Then there would exist x, y, z ∈X with d(x, z) > d(x, y)+d(y, z). Since the distance
function only takes values 0 and 1, the inequality just written can only occur if d(x, z) = 1
and d(x, y) = d(y, z) = 0. But then the definition of d gives x = y = z, hence x = z, which
contradicts d(x, z) = 1. This distance function d is called the discrete metric on X.
Given any metric space (X, d) and any subset S of X, the set S becomes a metric space
by restricting the domain of the distance function d from X × X to S × S. We call S a
subspace of the metric space X.
Next, suppose X and Y are metric spaces with metrics dX and dY . Consider the product
space Z = X × Y , which is the set of ordered pairs (x, y) with x ∈X and y ∈Y . We define
the product metric d = dZ : Z × Z →R by setting
d((x1, y1), (x2, y2)) = dX(x1, x2) + dY (y1, y2)
for all (x1, y1), (x2, y2) ∈Z.
Let us verify the metric space axioms for Z and d. Fix z1 = (x1, y1), z2 = (x2, y2), and
z3 = (x3, y3) in Z, where xi ∈X and yi ∈Y for i = 1, 2, 3. First, since 0 ≤dX(x1, x2) < ∞
and 0 ≤dY (y1, y2) < ∞, we have 0 ≤d(z1, z2) = dX(x1, x2)+dY (y1, y2) < ∞. Furthermore,
since the sum of two nonnegative real numbers is zero iff both summands are zero, we have
d(z1, z2) = 0 iff dX(x1, x2) = 0 and dY (y1, y2) = 0 iff x1 = x2 and y1 = y2 iff (x1, y1) =
(x2, y2) iff z1 = z2. Second, d(z1, z2) = dX(x1, x2) + dY (y1, y2) = dX(x2, x1) + dY (y2, y1) =
d(z2, z1) by symmetry of dX and dY . Third, using the Triangle Inequality for dX and dY ,
we compute
d(z1, z3) = dX(x1, x3) + dY (y1, y3) ≤dX(x1, x2) + dX(x2, x3) + dY (y1, y2) + dY (y2, y3)
= dX(x1, x2) + dY (y1, y2) + dX(x2, x3) + dY (y2, y3) = dZ(z1, z2) + dZ(z2, z3).
Next, define d′ : Z × Z →R by d′((x1, y1), (x2, y2)) = max(dX(x1, x2), dY (y1, y2)). You can
check that d′ also satisfies the metric space axioms. The metrics d and d′ on the set Z are
not equal, but we see in Exercise 12 that they are equivalent for many purposes.
We can iterate the definition of d (or d′) to define metrics on products of finitely many
metric spaces X1, X2, . . . , Xm. Specifically, let X = X1 × X2 × · · · × Xm and xi, yi ∈Xi for
Metric Spaces and Hilbert Spaces
401
1 ≤i ≤m. The formulas
d1((x1, . . . , xm), (y1, . . . , ym))
=
dX1(x1, y1) + dX2(x2, y2) + · · · + dXm(xm, ym),
d∞((x1, . . . , xm), (y1, . . . , ym))
=
max(dX1(x1, y1), dX2(x2, y2), . . . , dXm(xm, ym))
both define metrics on the product space X. In particular, these constructions provide two
additional metrics on the spaces Rm and Cm that are not equal to the Euclidean metric
given earlier. Although all three metrics are equivalent in some respects (Exercise 7), we
see later that the Euclidean metric d2 has additional geometric structure compared to d1
and d∞.
As a final remark, note that the constructions given here do not automatically generalize
to products of infinitely many metric spaces. The reason is that the sum or maximum of
an infinite sequence of positive real numbers may be +∞, which cannot be a value of the
distance function, by the first metric space axiom.
15.2
Convergent Sequences
Formally, a sequence of points in a set X is a function x : Z≥0 →X. We often present such a
sequence as an infinite list x = (x0, x1, x2, . . . , xn, . . .) = (xn : n ∈Z≥0), where xn = x(n) ∈
X is the nth term of the sequence. We sometimes index a sequence starting at x1 instead
of x0, or starting at xi for any fixed i ∈Z. A subsequence of (x0, x1, x2, . . . , xn, . . .) is a
sequence of the form (xk0, xk1, xk2, . . . , xkn, . . .) where 0 ≤k0 < k1 < k2 < · · · < kn < · · ·
is a strictly increasing sequence of integers.
Given a metric space (X, d), a sequence (xn) of points in X, and a point y ∈X, we
say that the sequence (xn) converges to y in the metric space iff for all ϵ > 0, there exists
n0 ∈Z≥0 such that for all n ≥n0, d(xn, y) < ϵ. When this condition holds, we write
xn →y or limn→∞xn = y and call y the limit of the sequence (xn). Intuitively, the formal
definition means that for any given positive distance, no matter how small, the sequence
(xn) eventually comes closer than that distance to the limit y and remains closer than that
distance forever. (The phrasing of this intuitive description arises by viewing the subscript
n as a time index, so that xn is the location of the sequence at time n.)
Not every sequence (xn) in a metric space (X, d) converges. Those sequences that do
converge to some point y ∈X are called convergent sequences. We note that the limit
of a sequence, when it exists, must be unique. For suppose a particular sequence (xn)
converged to both y and z in X. We show that y = z. For any fixed ϵ > 0, we can
choose n1 ∈Z≥0 such that n ≥n1 implies d(xn, y) < ϵ/2. Similarly, there exists n2 ∈Z≥0
such that n ≥n2 implies d(xn, z) < ϵ/2. Picking n = max(n1, n2), we conclude that
d(y, z) ≤d(y, xn) + d(xn, z) = d(xn, y) + d(xn, z) < ϵ/2 + ϵ/2 = ϵ. Thus the fixed number
d(y, z) is less than every positive number ϵ, forcing d(y, z) = 0 and y = z.
Here are some examples of sequences in metric spaces. In any metric space, the
constant sequence (y, y, y, . . .) converges to y. In the metric space R, the sequences
(1/n : n ∈Z>0), (1/n2 : n ∈Z>0), and (1/2n : n ∈Z≥0) all converge to zero. The sequence
(1, −1, 1, −1, 1, −1, . . .) = ((−1)n : n ≥0) does not converge. However, the subsequence of
odd-indexed terms (−1, −1, −1, . . .) converges to −1, and the subsequence of even-indexed
terms (1, 1, 1, . . .) converges to 1. We see that a subsequence of a non-convergent sequence
may converge, and different subsequences might converge to different limits. On the other
hand, the sequence (n : n ∈Z≥0) in R is non-convergent and has no convergent subsequence
(+∞is not allowed as a limit, since it is not in the set R).
402
Advanced Linear Algebra
Suppose (xn : n ≥0) is a convergent sequence in a metric space (X, d) with limit y.
We now show that all subsequences of (xn) also converge to y. Fix such a subsequence
(xk0, xk1, . . .). Given ϵ > 0, there exists m0 ∈Z≥0 such that for all m ≥m0, d(xm, y) < ϵ.
Since k0 < k1 < · · · is a strictly increasing sequence of integers, we can find n0 ∈Z≥0
such that kn ≥m0 for all n ≥n0. (It suffices to take n0 = m0, although a smaller n0
may also work.) Now, for any n ≥n0, d(xkn, y) < ϵ, confirming that limn→∞xkn = y. We
develop further connections between the limit of a sequence (if any) and the limits of its
subsequences when we discuss compactness and completeness.
15.3
Closed Sets
Let (X, d) be any metric space. A subset C of X is called a closed set (relative to the metric
d) iff for every sequence (xn : n ≥0) with all terms xn belonging to C, if xn converges to
some point y ∈X, then y also belongs to C. Intuitively, the set is called “closed” because
we can never go outside the set by passing to the limit of a convergent sequence of points
all of which are in the set. In §1.4, we saw that many algebraic subsystems (subgroups,
ideals, etc.) can be defined in terms of certain closure conditions, where performing various
operations on elements of the set produces another object in that same set. Here, we can
say that C is a closed set (in the sense just defined) iff C is closed under the operation of
taking the limit of a convergent sequence of points in C. Negating the definition, note that
C is not closed means there exists a sequence (xn) with all xn ∈C such that xn →y for
some y ∈X, but y ̸∈C.
Here are some examples of closed sets. First, for any fixed y ∈X, the one-point set
C = {y} is closed. To see why, note that the only sequence of points in C is the constant
sequence (y, y, y, . . .). This sequence converges to the unique limit y, which belongs to C, and
so C is indeed closed. Second, we claim the empty set ∅is a closed subset of X. Otherwise,
there would be a convergent sequence (xn : n ≥0) with all xn ∈∅converging to a limit
y ̸∈∅. But the condition xn ∈∅is impossible, so there can be no such sequence. Third, the
entire space X is a closed subset of X. Proof: given points xn ∈X such that xn →y ∈X,
we certainly have y ∈X, so that X is closed.
We continue by giving examples of closed sets and non-closed sets in the metric space
R. For any fixed a ≤b, the closed interval [a, b] = {x ∈R : a ≤x ≤b} is a closed
set, as the name suggests. We prove this by contradiction. If [a, b] is not closed, choose a
sequence (xn) with all xn ∈[a, b], such that (xn) converges to some real number y ̸∈[a, b].
In the case y < a, note that ϵ = a −y > 0. For this ϵ, there is n0 ≥0 such that n ≥n0
implies d(xn, y) < ϵ. In particular, xn0 must satisfy y −ϵ < xn0 < y + ϵ = a, contradicting
xn0 ∈[a, b]. Similarly, the case y > b leads to a contradiction. So [a, b] is closed. On the other
hand, for fixed a < b, the open interval (a, b) = {x ∈R : a < x < b} is not closed. You can
check that the sequence defined by xn = a+(b−a)/(2n) for n ≥1 has all terms xn in (a, b)
and converges to a ̸∈(a, b). Similarly, the half-open intervals (a, b] and [a, b) are not closed.
The set Z of integers is a closed subset of R. We again prove this by contradiction. If Z is
not closed, then we can choose a convergent sequence (xn) of integers with xn →y, where
y ∈R is not an integer. We know y is between two consecutive integers, say k < y < k + 1.
Take ϵ = min(y −k, k + 1 −y) > 0. For large enough n, we must have y −ϵ < xn < y + ϵ.
But no real number in this range is an integer, which contradicts xn ∈Z.
Let us return to the case of a general metric space (X, d). Given any (possibly infinite)
collection {Ci : i ∈I} of closed sets in X, we claim the intersection C = T
i∈I Ci is closed.
Proof: suppose (xn) is a sequence in C converging to some y ∈X; we must prove y ∈C.
Metric Spaces and Hilbert Spaces
403
Fix an index i ∈I. Since all xn belong to C, we know all xn are in Ci. Because Ci is a
closed set, it follows that the limit y belongs to Ci. This holds for every i, so y belongs to
the intersection C of all the Ci.
Next, we show that if C and D are closed sets in X, then the union C ∪D is also
closed. Let (xn) be a sequence of points in C ∪D converging to a limit y ∈X; we must
prove y ∈C ∪D. Consider two cases. Case 1: there are only finitely many indices n with
xn ∈C. Let n0 be the largest index with xn ∈C, or n0 = 0 if every xn is in D. The
subsequence (xn0+1, xn0+2, . . .) of the original sequence still converges to y, and all points
in this subsequence belong to D. As D is closed, we must have y ∈D, so that y ∈C ∪D
as well. Case 2: there are infinitely many indices k0 < k1 < k2 < · · · with xkn ∈C. Then
the subsequence (xk0, xk1, . . .) of the original sequence still converges to y, and all points in
this subsequence belong to C. As C is closed, we must have y ∈C, so that y ∈C ∪D.
It follows by induction that if m is any positive integer and C1, C2, . . . , Cm are closed sets
in (X, d), then C1 ∪C2 ∪· · ·∪Cm is also closed. In particular, since one-point sets are closed,
we see that all finite subsets of X are closed. However, the union of an infinite collection
of closed subsets may or may not be closed. On one hand, we saw that Z = S
n∈Z{n} is a
closed set in R. On the other hand, the union C of the one-point sets {1/n} for n ∈Z>0 is
not closed in R, since (1/n : n > 0) is a sequence in C converging to the point 0 ̸∈C.
To summarize: in any metric space X, ∅and X are closed. Finite subsets of X are
closed. The union of finitely many closed sets is closed. The intersection of arbitrarily many
closed sets is closed.
15.4
Open Sets
Given a point x in a metric space (X, d) and a real r > 0, the open ball of radius r and
center x is B(x; r) = {y ∈X : d(x, y) < r}. For example, in R, B(x; r) is the open interval
(x −r, x + r). In C or R2 with the Euclidean metric, B(x; r) is the interior of a circle with
center x and radius r. In a discrete metric space X, B(x; r) = {x} for all r ≤1, whereas
B(x; r) = X for all r > 1.
A subset U of a metric space (X, d) is called an open set (relative to the metric d) iff
for every x ∈U, there exists ϵ > 0 (depending on x) such that B(x; ϵ) ⊆U. Intuitively, the
set U is called open because all points sufficiently close to a point in U are also in U.
Here are some examples of open sets in general metric spaces. First, every open ball is
an open set, as the name suggests. To prove this, consider an open ball U = B(y; r) and
fix x ∈U. We know d(y, x) < r, so the number ϵ = r −d(y, x) is strictly positive. We show
B(x; ϵ) ⊆U. We fix z ∈B(x; ϵ) and check z ∈U. We know d(x, z) < ϵ, so the Triangle
Inequality gives d(y, z) ≤d(y, x)+d(x, z) < d(y, x)+ϵ = r. This shows that z ∈B(y; r) = U,
as needed.
Second, the entire space X is an open subset of X. Proof: given x ∈X, we can take
ϵ = 1 and note that B(x; ϵ) is a subset of X by definition.
Third, the empty set ∅is an open subset of X. Proof: if ∅were not open, then there
exists x ∈∅such that for all ϵ > 0, B(x; ϵ) is not a subset of ∅. But the existence of x ∈∅
is impossible.
Fourth, given any collection {Ui : i ∈I} of open subsets of X, the union U = S
i∈I Ui
is also open in X. To prove this, fix x in the union U of the Ui. We know x ∈Ui for some
i ∈I. Since Ui is open, there exists ϵ > 0 with B(x; ϵ) ⊆Ui. As Ui is a subset of U, we also
have B(x; ϵ) ⊆U, so U is open.
404
Advanced Linear Algebra
Fifth, if U and V are open subsets of X, then U ∩V is open in X. For the proof, fix
x ∈U ∩V . Since x ∈U, there is ϵ1 > 0 with B(x; ϵ1) ⊆U. Since x ∈V , there is ϵ2 > 0
with B(x; ϵ2) ⊆V . For ϵ = min(ϵ1, ϵ2) > 0, we see that B(x; ϵ) is contained in both U
and V , hence is a subset of U ∩V . Thus, U ∩V is open. By induction, it follows that if
m is any positive integer and U1, . . . , Um are open subsets of X, then U1 ∩· · · ∩Um is also
open. However, the intersection of infinitely many open subsets of X need not be open. For
instance, all the sets B(0; 1/n) = (−1/n, 1/n) are open subsets of R (being open balls). But
their intersection, namely {0}, is not open in R, since for every ϵ > 0, B(0; ϵ) = (−ϵ, ϵ) is
not a subset of {0}.
Open sets and closed sets are related in the following way: a set U is open in X iff the
complement C = X \U is closed in X. We prove the contrapositive in both directions. First
assume C = X \ U is not closed in X. Then there is a sequence (xn) of points of C and a
point x ∈X such that xn →x but x ̸∈C. Note x is a point of X \ C = U. For fixed ϵ > 0,
there is n0 ≥0 such that for all n ≥n0, d(xn, x) < ϵ. Each such xn is in both C = X \ U
and B(x; ϵ), which shows that B(x; ϵ) cannot be a subset of U. As ϵ was arbitrary, we see
that U is not open.
Conversely, assume U is not open; we prove C = X \ U is not closed. There is a point
x ∈U such that for all ϵ > 0, B(x; ϵ) is not a subset of U. Taking ϵ = 1/n for each positive
integer n, we obtain points xn with xn ∈B(x; 1/n) but xn ̸∈U. Thus, (xn) is a sequence
of points in C. Furthermore, we claim xn converges to x in X. To see this, fix ϵ > 0, and
choose a positive integer n0 with 1/n0 < ϵ. For all n ≥n0, d(xn, x) < 1/n ≤1/n0 < ϵ, as
needed. Since the limit of (xn) is x ̸∈C, we see that C is not closed.
Note carefully that “S is not an open set” does not mean the same thing as “S is a
closed set.” Most subsets of metric spaces are neither open nor closed. For instance, half-
open intervals (a, b] in R are neither open nor closed. It is possible for a subset of a metric
space to be both open and closed; consider ∅and X, for example.
15.5
Continuous Functions
The concept of continuity plays a central role in calculus. This concept can be generalized
to the setting of metric spaces as follows. Let (X, dX) and (Y, dY ) be two metric spaces.
A function f : X →Y is continuous on X iff whenever (xn) is a sequence of points in X
converging to some x ∈X, the sequence (f(xn)) in Y converges to f(x). Briefly, continuity
of f means that whenever xn →x in X, f(xn) →f(x) in Y . Using limit notation, continuity
of f means
f

lim
n→∞xn

= lim
n→∞f(xn),
whenever the sequence (xn) has a limit, so that f commutes with the operation of taking
the limit of a convergent sequence. For a fixed element x∗in X, we say f is continuous at
the point x∗iff whenever xn →x∗in X, f(xn) →f(x∗) in Y .
You can check that a constant function (f(x) = y0 for all x ∈X) is continuous, as is
the identity function idX : X →X. We show later that addition and multiplication (viewed
as functions from the product metric space R × R to R) are continuous. Let us check that
f : R →R, given by f(x) = −3x for x ∈R, is continuous. Suppose xn →x in R; we
prove −3xn →−3x. Fix ϵ > 0, and choose n0 so that n ≥n0 implies d(xn, x) < ϵ/3. Now
notice that d(−3xn, −3x) = |(−3xn) −(−3x)| = 3|xn −x| = 3d(xn, x). So, n ≥n0 implies
d(f(xn), f(x)) < ϵ, as needed.
Metric Spaces and Hilbert Spaces
405
A fundamental fact about continuity is that compositions of continuous functions are
continuous. In detail, let f : X →Y and g : Y →Z be continuous functions between metric
spaces X, Y , and Z; we show g ◦f : X →Z is continuous. To do so, let (xn) be a sequence
in X converging to x ∈X. By continuity of f, the sequence (f(xn)) converges to f(x) in
Y . By continuity of g, the sequence (g(f(xn))) converges to g(f(x)) in Z. So, the sequence
((g ◦f)(xn)) converges to (g ◦f)(x) in Z, proving continuity of g ◦f.
A more subtle, but equally fundamental, property of continuity is that f : X →Y is
continuous iff for every closed set D in Y , the preimage f −1[D] = {x ∈X : f(x) ∈D} is
closed in X. To prove the forward direction, assume f is continuous, fix a closed set D in
Y , and consider a sequence (xn) of points in f −1[D] that converge to some limit x∗∈X.
To see that f −1[D] is closed, we must prove x∗∈f −1[D]. Now, each xn is in f −1[D], so
f(xn) ∈D for all n ≥0. As xn converges to x∗, continuity of f tells us that f(xn) converges
to f(x∗). Since D is closed in Y , we deduce f(x∗) ∈D, and hence x∗∈f −1[D], as needed.
For the other direction, assume that f : X →Y is not continuous. So there is a sequence
(xn) in X converging to a point x ∈X, such that f(xn) does not converge to y = f(x) in
Y . This means that there exists ϵ > 0 such that for every integer n0 ≥0 there exists n ≥n0
with d(f(xn), y) ≥ϵ. It follows that we can find a subsequence (x′
n) of (xn), consisting of
all terms xn such that d(f(xn), f(x)) ≥ϵ. We know (x′
n) still converges to x. Note that
D = {z ∈Y : dY (z, y) ≥ϵ} is a closed set in Y , being the complement of the open ball
B(y; ϵ). To complete the proof, we show f −1[D] is not a closed set in X. Note that each
term x′
n is in f −1[D], since f(x′
n) ∈D by construction. But the limit x of the sequence (x′
n)
is not in f −1[D], because f(x) = y satisfies dY (y, y) = 0, so that f(x) ̸∈D.
Since open sets are complements of closed sets, we readily deduce the following
characterization of continuous functions: f : X →Y is continuous iff for every open set V
in Y , the preimage f −1[V ] is open in X. To prove this, suppose f is continuous and V is
open in Y . Then Y \ V is closed in Y , so f −1[Y \ V ] is closed in X. From set theory, we
know f −1[Y \ V ] = f −1[Y ] \ f −1[V ] = X \ f −1[V ]. Therefore, f −1[V ] is the complement of
a closed set in X, so it is open in X. The converse is proved similarly. This characterization
of continuity is closely related to the ϵ-δ definition often given in calculus. Specifically,
f : X →Y is continuous iff for all x0 ∈X and ϵ > 0, there exists δ > 0 such that for
all x ∈X, if dX(x, x0) < δ, then dY (f(x), f(x0)) < ϵ. We ask the reader to prove the
equivalence of this condition to the condition involving open sets in Exercise 50.
15.6
Compact Sets
We know that not every sequence in a general metric space is convergent. It would be
convenient if we could take a non-convergent sequence (xn) and extract a convergent
subsequence from it. However, even this is not always possible. For example, in R, no
subsequence of the sequence (n : n ≥0) converges. On the other hand, if we insist that all
terms of the sequence (xn) come from a closed interval [a, b], it can be shown that (xn) must
have a convergent subsequence (Exercise 58). This suggests that we may have to restrict
our attention to sequences coming from a sufficiently nice subset of the metric space. In a
general metric space (X, d), a subset K of X is called (sequentially) compact iff for every
sequence (xn) of points in K, there exists a subsequence (xkn) and a point y belonging to
K such that xkn →y.
Every finite subset of X is compact. To prove this, suppose K = {y1, . . . , ym} is
nonempty and finite, and (xn) is any sequence of points in K. At least one yj must occur
infinitely often in the sequence (xn). So the sequence (xn) has a constant subsequence
406
Advanced Linear Algebra
(yj, yj, yj, . . .), which converges to the point yj ∈K. Also, ∅is compact, since the definition
of compactness is vacuously satisfied (there are no sequences with values in ∅).
Every compact subset of X must be closed in X. To prove this, suppose K ⊆X is
compact, (xn) is a sequence with all xn ∈K, and xn converges to a point x in X. On one
hand, every subsequence of (xn) converges to the unique limit x. On the other hand, the
definition of compactness shows that some subsequence of (xn) converges to a point of K.
Therefore, x must belong to K, proving that K is closed. You can show that unions of
finitely many compact sets are compact, whereas intersections of arbitrarily many compact
sets are compact. The proof imitates the proofs of the analogous properties of closed sets.
Similarly, a closed subset of a compact set is compact.
We say that a subset S of a nonempty metric space (X, d) is bounded iff there exists
z ∈X and M ∈R such that for all x ∈S, d(z, x) ≤M. Every compact subset of X must
be bounded. To prove this, suppose S ⊆X is not bounded. We construct a sequence (xn) of
points in S that has no convergent subsequence. Fix z ∈X. For each n ∈Z≥0, we can find
xn ∈S such that d(z, xn) > n. To get a contradiction, suppose some subsequence (xkn)
converges to some point y ∈X. Choose an integer n0 so that n ≥n0 implies d(y, xkn) < 1.
Then choose an integer n ≥n0 with kn ≥d(z, y) + 1. For this n, d(z, xkn) ≤d(z, y) +
d(y, xkn) < d(z, y) + 1 ≤kn, which contradicts the choice of xkn.
So far, we have seen that every compact subset of a metric space is closed and bounded.
In Rm and Cm with any of the metrics d1, d2, and d∞discussed earlier, the converse also
holds: a subset K of Rm or Cm is compact iff K is closed and bounded. The proof of this,
which is rather difficult, is sketched in the exercises and can be found in texts on advanced
calculus. On the other hand, in general metric spaces, there can exist closed and bounded
sets that are not compact. For instance, consider X = Z with the discrete metric. The entire
space X is bounded, since d(0, x) ≤1 for all x ∈Z. X is also closed in X. But X is not
compact, since (n : n ≥0) is a sequence in X with no convergent subsequence.
Continuous functions preserve compact sets, in the following sense. If f : X →Y is
a continuous function between two metric spaces and K ⊆X is compact in X, then the
direct image f[K] = {f(x) : x ∈K} is compact in Y . To prove this, assume f : X →Y
is continuous and K is a compact subset of X. To prove f[K] is compact, let (yn) be any
sequence of points in f[K]. Each yn has the form yn = f(xn) for some xn ∈K. Now (xn)
is a sequence of points in the compact set K, so there is a subsequence (xkn) converging to
some point x∗∈K. By continuity, (ykn) = (f(xkn)) is a subsequence of (yn) converging to
f(x∗) ∈f[K]. So f[K] is indeed compact.
Compactness can also be defined in terms of open sets. Given any set S in a metric
space (X, d), an open cover of S is a collection {Ui : i ∈I} of open subsets of X such that
S ⊆S
i∈I Ui. A finite subcover is a finite subcollection {Ui1, . . . , Uim} of the given open cover
such that S ⊆Sm
j=1 Uij. Such a finite subcover need not exist. We say that a subset K of
X is (topologically) compact iff for each open cover of K, there does exist a finite subcover.
It can be shown that in metric spaces, sequential compactness is equivalent to topological
compactness. The topological definition applies in more general situations, but our study of
Hilbert spaces only requires the more intuitive definition in terms of subsequences.
15.7
Completeness
A Cauchy sequence in a metric space (X, d) is a sequence (xn : n ≥0) with the following
property: for all ϵ > 0, there exists an integer n0 such that for all m, n ≥n0, d(xn, xm) < ϵ.
This definition says that the terms of a Cauchy sequence get arbitrarily close to each other
Metric Spaces and Hilbert Spaces
407
if we go far enough out in the sequence. In contrast, for a convergent sequence with limit
x, the terms of the sequence get arbitrarily close to the limit value x. Let us explore the
relationship between these concepts.
On one hand, every convergent sequence in a metric space is a Cauchy sequence. To prove
this, suppose (xn) is a sequence in (X, d) converging to x ∈X. We show (xn) is a Cauchy
sequence. Fix ϵ > 0, and choose an integer n0 so that for all n ≥n0, d(xn, x) < ϵ/2. For all
n, m ≥n0, the Triangle Inequality gives d(xn, xm) ≤d(xn, x) + d(x, xm) < ϵ/2 + ϵ/2 = ϵ.
On the other hand, a Cauchy sequence in a general metric space may not converge.
For example, consider the set R>0 of strictly positive real numbers with the metric
d(x, y) = |x −y| for x, y ∈R>0. The sequence (1/n : n ≥1) converges in the larger metric
space R to the unique limit 0, so this sequence is a Cauchy sequence (in R and in R>0). But
the sequence does not converge to any point of the set R>0. For another example, consider
the metric space Q of rational numbers with d(x, y) = |x −y| for x, y ∈Q. We can find
a sequence of rational numbers (xn : n ≥0) converging to the irrational real number
√
2
(for instance, let xn consist of the decimal expansion of
√
2 truncated n places after the
decimal). This sequence is a Cauchy sequence in Q that does not converge in Q.
A metric space (X, d) is called complete iff every Cauchy sequence (xn) in X does
converge to some point in the space X. The preceding examples show that R>0 and Q are
not complete. On the other hand, any compact metric space is complete. To prove this,
suppose (xn) is a Cauchy sequence in a compact metric space (X, d). By compactness, this
sequence has a subsequence (xkn) converging to some x ∈X. We now show that the full
sequence must also converge to x. Given ϵ > 0, choose an integer n0 so that for all n ≥n0,
d(xkn, x) < ϵ/2. Also choose an integer n1 so that for all i, j ≥n1, d(xi, xj) < ϵ/2. Fix any
i ≥n1. There is an integer j ≥n1 such that j = kn for some n ≥n0. Using this j, we see
that d(xi, x) ≤d(xi, xj) + d(xj, x) < ϵ/2 + d(xkn, x) < ϵ. This proves that (xn) converges
to x.
A subset of a complete metric space is complete iff it is closed. To prove this, suppose
(X, d) is complete, C ⊆X is closed, and (xn) is a Cauchy sequence with all xn ∈C. Then
(xn) is a Cauchy sequence in X, hence converges to some x ∈X. Since C is closed, the
limit x must belong to C. So C is complete. Conversely, if C ⊆X is not closed, there exists
a sequence (xn) with all xn ∈C converging to some x ∈X \ C. The convergent sequence
(xn) is a Cauchy sequence, and x is its unique limit. Since this limit does not belong to C,
(xn) is a Cauchy sequence in C that does not converge in C. So C is not complete.
Each closed interval [a, b] is a compact, hence complete, subset of R (see Exercise 58).
We use this fact to show that R is complete. The proof requires the lemma that a Cauchy
sequence in any metric space is bounded (Exercise 16). Given a Cauchy sequence (xn) in R,
we can therefore choose M ∈R such that every term xn is in the closed interval [−M, M].
The completeness of this interval ensures that the given Cauchy sequence converges to some
real number.
More generally, Rk (and similarly Ck) with the Euclidean metric is complete. To prove
this, let (xn) be a Cauchy sequence in Rk, where xn = (xn(1), xn(2), . . . , xn(k)) for certain
real numbers xn(i). For 1 ≤i ≤k, the inequality |xn(i) −xm(i)| ≤d2(xn, xm) shows that
(xn(i) : n ≥0) is a Cauchy sequence in R. By completeness of R, this sequence converges
to some real number x(i). Let x = (x(1), x(2), . . . , x(k)) ∈Rk. Given ϵ > 0, choose integers
n1, . . . , nk such that n ≥ni implies |xn(i) −xi| < ϵ/
√
k. Then for n ≥max(n1, . . . , nk), we
have
d2(xn, x) =
v
u
u
t
k
X
i=1
|xn(i) −x(i)|2 ≤
v
u
u
t
k
X
i=1
ϵ2/k = ϵ.
So the given Cauchy sequence (xn) in Rk converges to x.
408
Advanced Linear Algebra
15.8
Definition of a Hilbert Space
Having covered the necessary background on metric spaces, we are now ready to define
Hilbert spaces. Briefly, a Hilbert space is a complex inner product space that is complete
relative to the metric induced by the inner product. Let us spell this definition out in more
detail.
We begin with the algebraic ingredients of a Hilbert space. The Hilbert space consists of
a set H of vectors, together with operations of vector addition + : H × H →H and scalar
multiplication · : C × H →H satisfying the vector space axioms listed in Table 1.4. There
is also defined on H a complex inner product B : H × H →H, denoted B(v, w) = ⟨v, w⟩,
satisfying these axioms for all v, w, z ∈H and c ∈C:
(1) ⟨v + w, z⟩= ⟨v, z⟩+ ⟨w, z⟩;
(2) ⟨cv, z⟩= c⟨v, z⟩;
(3) ⟨w, v⟩= ⟨v, w⟩, where the bar denotes complex conjugation;
(4) if v ̸= 0, then ⟨v, v⟩is a strictly positive real number.
It follows (as in §13.11) that the inner product is linear in the first input and conjugate-
linear in the second input. In other words, for all c1, . . . , cm ∈C and v1, . . . , vm, w ∈H,
⟨c1v1 + · · · + cmvm, w⟩= c1⟨v1, w⟩+ · · · + cm⟨vm, w⟩;
⟨w, c1v1 + · · · + cmvm⟩= c1⟨w, v1⟩+ · · · + cm⟨w, vm⟩.
Note that ⟨0, w⟩= 0 = ⟨w, 0⟩for all w ∈H. We say that v, w ∈H are orthogonal iff
⟨v, w⟩= 0, which holds iff ⟨w, v⟩= 0. The inner product in a Hilbert space generalizes
the dot product from R2 and R3, and orthogonality generalizes the geometric concept of
perpendicularity in R2 and R3.
We use the inner product to define the analytic ingredients of the Hilbert space H,
namely the length (or norm) of a vector and the distance between two vectors. For all
v ∈H, define the norm of v by setting ||v|| =
p
⟨v, v⟩. For all v, w ∈H and c ∈C, the
following properties hold:
(a) ||v|| ∈R≥0; and ||v|| = 0 iff v = 0;
(b) ||cv|| = |c| · ||v||;
(c) |⟨v, w⟩| ≤||v|| · ||w|| (the Cauchy–Schwarz Inequality);
(d) ||v + w|| ≤||v|| + ||w|| (the Triangle Inequality for norms).
Property (a) is true ⟨v, v⟩is either a positive real number (when v ̸= 0) or zero (when
v = 0). Property (b) is true because
||cv|| =
p
⟨cv, cv⟩=
p
cc⟨v, v⟩=
p
|c|2⟨v, v⟩= |c|
p
⟨v, v⟩= |c| · ||v||.
The Cauchy–Schwarz Inequality has a more subtle proof. The inequality holds if ⟨v, w⟩= 0,
so we may assume ⟨v, w⟩̸= 0, hence v ̸= 0 and w ̸= 0. We first prove the inequality in the
case where ||v|| = ||w|| = 1 and ⟨v, w⟩is a positive real number. Then ⟨v, v⟩= 1 = ⟨w, w⟩
and |⟨v, w⟩| = ⟨v, w⟩= ⟨w, v⟩. Using the axioms for the complex inner product, we compute
0 ≤⟨v −w, v −w⟩= ⟨v, v⟩−⟨v, w⟩−⟨w, v⟩+ ⟨w, w⟩.
(15.1)
Using our current assumptions on v and w, this inequality becomes 0 ≤1 −2|⟨v, w⟩| + 1,
which rearranges to |⟨v, w⟩| ≤1 = ||v|| · ||w||. Still assuming ||v|| = ||w|| = 1, we next prove
the case where ⟨v, w⟩is an arbitrary nonzero complex scalar. In polar form, ⟨v, w⟩= reiθ for
some real r > 0 and some θ ∈[0, 2π). Let v0 = e−iθv. Then ||v0|| = |e−iθ| · ||v|| = ||v|| = 1,
Metric Spaces and Hilbert Spaces
409
|⟨v0, w⟩| = |e−iθ⟨v, w⟩| = |⟨v, w⟩|, and ⟨v0, w⟩= e−iθ⟨v, w⟩= r is a positive real number.
By the case already proved, |⟨v0, w⟩| ≤||v0|| · ||w||, and therefore |⟨v, w⟩| ≤||v|| · ||w||.
Finally, we drop the assumption that ||v|| = ||w|| = 1. Write v = cv1 and w = dw1, where
c = ||v||, v1 = c−1v, d = ||w||, and w1 = d−1w. We see that ||v1|| = c−1||v|| = 1 and
||w1|| = d−1||w|| = 1. On one hand, 0 ̸= |⟨v, w⟩| = |⟨cv1, dw1⟩| = |cd⟨v1, w1⟩| = cd|⟨v1, w1⟩|.
On the other hand, the cases already proved now give |⟨v1, w1⟩| ≤||v1||·||w1|| = 1. Therefore,
|⟨v, w⟩| = cd|⟨v1, w1⟩| ≤cd = ||v|| · ||w||,
completing the proof of the Cauchy–Schwarz Inequality.
We can now deduce the Triangle Inequality for norms from the Cauchy–Schwarz
Inequality. Given v, w ∈H, compute
||v +w||2 = ⟨v +w, v +w⟩= ⟨v, v⟩+⟨v, w⟩+⟨w, v⟩+⟨w, w⟩= ||v||2 +⟨v, w⟩+⟨v, w⟩+||w||2.
The two middle terms add up to twice the real part of ⟨v, w⟩, which is at most
2|⟨v, w⟩| ≤2||v|| · ||w||. Hence,
||v + w||2 ≤||v||2 + 2||v|| · ||w|| + ||w||2 = (||v|| + ||w||)2.
Taking the positive square root of both sides gives ||v + w|| ≤||v|| + ||w||, as needed.
We use the norm to define the metric space structure of H. For v, w ∈H, define the
distance between v and w by d(v, w) = ||v −w||. The properties of the norm derived above
imply the required axioms for a metric space (as we saw in §10.5). In particular, the Triangle
Inequality for the metric follows from the Triangle Inequality for the norm, because
d(v, z) = ||v −z|| = ||(v −w) + (w −z)|| ≤||v −w|| + ||w −z|| = d(v, w) + d(w, z)
for all v, w, z ∈H. Like any metric induced from a norm, the metric on a Hilbert space
is compatible with translations and dilations, meaning that d(v + w, z + w) = d(v, z) and
d(cv, cw) = |c|d(v, w) for all v, w, z ∈H and c ∈C.
The final topological ingredient in the definition of a Hilbert space is the assumption
that H is complete relative to the metric just defined. The definition requires that every
Cauchy sequence in H converges to a point of H. Writing what this means, we see that
whenever (xn : n ≥0) is a sequence of vectors in H such that limm,n→∞||xm −xn|| = 0,
there exists a (necessarily unique) x ∈H such that limn→∞||xn −x|| = 0.
We can define a real Hilbert space by the same conditions discussed above, restricting all
scalars to come from R and using a real-valued inner product. In this case, ⟨v, w⟩= ⟨w, v⟩,
and the inner product is R-linear (as opposed to conjugate-linear) in both the first and
second positions. Henceforth in this chapter, we continue to consider only complex Hilbert
spaces, letting the reader make the required modifications to obtain analogous results for
real Hilbert spaces.
15.9
Examples of Hilbert Spaces
A basic example of a Hilbert space is the space Cn of n-tuples v = (v1, . . . , vn), where
all vk ∈C. The inner product is defined by ⟨v, w⟩= Pn
k=1 vkwk, which can be written
⟨v, w⟩= w∗v if we think of v and w as column vectors. The norm of v is ||v|| =
pPn
k=1 |vk|2,
and the distance between v and w is the Euclidean distance d2(v, w) discussed in §15.1.
The completeness of Cn under this metric was proved in §15.7. The other axioms of a
410
Advanced Linear Algebra
Hilbert space (namely, that Cn is a complex vector space and inner product space) may
be routinely verified. In Cn, the Cauchy–Schwarz Inequality |⟨v, w⟩| ≤||v|| · ||w|| and the
Triangle Inequality ||v + w|| ≤||v|| + ||w|| for norms translate to the following facts about
sums of complex numbers:

n
X
k=1
vkwk
 ≤
v
u
u
t
n
X
k=1
|vk|2 ·
v
u
u
t
n
X
k=1
|wk|2;
(15.2)
v
u
u
t
n
X
k=1
|vk + wk|2 ≤
v
u
u
t
n
X
k=1
|vk|2 +
v
u
u
t
n
X
k=1
|wk|2.
(15.3)
An example of an infinite-dimensional Hilbert space is the space ℓ2 of all infinite
sequences v = (v1, v2, . . . , vk, . . .) = (vk : k ≥1) such that vk ∈C and P∞
k=1 |vk|2 < ∞.
The inner product is defined by ⟨v, w⟩= P∞
k=1 vkwk. We prove the Hilbert space axioms
for this example in §15.10.
Both of the previous examples are special cases of a general construction to be described
shortly. First, we need a technical digression on general infinite summations. Let X be any
set, which could be finite, countably infinite, or uncountable. Let F(X) be the set of all
finite subsets of X. Given a nonnegative real number pk for each k ∈X, the sum P
k∈X pk
is defined to be the least upper bound (possibly ∞) of all sums P
k∈X′ pk, as X′ ranges
over all finite subsets of X. In symbols,
X
k∈X
pk = sup
( X
k∈X′
pk : X′ ∈F(X)
)
.
Given real numbers rk for k ∈X, let pk = rk if rk ≥0 and pk = 0 otherwise; and let
nk = |rk| if rk < 0 and nk = 0 otherwise. Define P
k∈X rk to be P
k∈X pk −P
k∈X nk if
at most one of the latter sums is ∞. Finally, given complex numbers zk for k ∈X, write
zk = xk + iyk for xk, yk ∈R. Define P
k∈X zk = P
k∈X xk + i P
k∈X yk if both sums on the
right side are finite. Properties of summations over the set X are analogous to properties
of infinite series. Some of these properties are covered in the exercises.
We now describe the general construction for producing Hilbert spaces. Fix a set X, and
let ℓ2(X) be the set of all functions f : X →C such that P
x∈X |f(x)|2 < ∞. We sometimes
think of such a function as a generalized sequence or X-tuple (f(x) : x ∈X) = (fx : x ∈X).
The set ℓ2(X) becomes a Hilbert space under the following operations. Given f, g ∈ℓ2(X)
and c ∈C, define f +g and cf by setting (f +g)(x) = f(x)+g(x) and (cf)(x) = c(f(x)) for
all x ∈X. Define ⟨f, g⟩= P
x∈X f(x)g(x). The norm and metric in this Hilbert space are
given by ||f|| =
pP
x∈X |f(x)|2 < ∞and d(f, g) =
pP
x∈X |f(x) −g(x)|2. The verification
of the Hilbert space axioms, which is somewhat technical, is given in §15.10. Note that both
previous examples are special cases of this construction, since Cn is ℓ2({1, 2, . . . , n}), and
ℓ2 is ℓ2(Z>0).
The spaces ℓ2(X) are themselves special cases of Hilbert spaces that occur in the theory
of Lebesgue integration. It is beyond the scope of this book to discuss this topic in detail,
but we allude to a few facts for readers familiar with this theory. Given any measure space
X with measure µ, we let L2(X, µ) be the set of measurable functions f : X →C such that
R
X |f|2 dµ < ∞. This is a complex vector space under pointwise operations on functions,
and the inner product is defined by ⟨f, g⟩=
R
X fg dµ for f, g ∈L2(X, µ). It can be proved
that L2(X, µ) is a Hilbert space with these operations; the fact that this space is complete
is a difficult theorem. By taking µ to be counting measure on an arbitrary set X, we obtain
the examples ℓ2(X) discussed above.
Metric Spaces and Hilbert Spaces
411
Finally, given any Hilbert space H, we can form new examples of Hilbert spaces by
considering subspaces of H. An arbitrary vector subspace W of H (a subset closed under
zero, addition, and scalar multiplication) automatically satisfies all the Hilbert space axioms
except possibly completeness. Since H is complete, we know from §15.7 that the subspace
W is complete iff W is closed. It follows that closed subspaces of a Hilbert space are also
Hilbert spaces, but non-closed subspaces are not complete. This example illustrates a general
theme: to obtain satisfactory results in infinite-dimensional settings, we often need to impose
a topological condition (in this case, being closed relative to the metric) in addition to
algebraic conditions. We remark that every finite-dimensional subspace of H is automatically
closed; see Exercise 75. But there are examples of infinite-dimensional subspaces that are
not closed (Exercises 70 and 71).
15.10
Proof of the Hilbert Space Axioms for ℓ2(X)
Let X be any set. This section gives the proof that ℓ2(X), with the operations defined above,
does satisfy all the axioms for a Hilbert space. We divide the proof into three parts: checking
the vector space axioms, checking the inner product axioms, and verifying completeness of
the metric.
Vector Space Axioms. Let XC denote the set of all functions f : X →C. The space
ℓ2(X) is the subset of XC consisting of those f satisfying P
x∈X |f(x)|2 < ∞. We saw
in §4.2 that XC is a complex vector space under pointwise operations on functions. Hence,
to see that ℓ2(X) is a vector space under the same operations, it suffices to check that
ℓ2(X) is a subspace of XC. First, the zero vector in XC, which is the function sending
every x ∈X to 0, belongs to ℓ2(X) because P
x∈X |0|2 = 0 < ∞. Second, for f ∈ℓ2(X)
and c ∈C, it follows readily from the definition of summation over X (Exercise 76) that
P
x∈X |(cf)(x)|2 = P
x∈X |c(f(x))|2 = |c|2 P
x∈X |f(x)|2 < ∞, so that cf ∈ℓ2(X). Third,
fix f, g ∈ℓ2(X); we must show f + g ∈ℓ2(X). Let X′ = {x1, . . . , xn} ∈F(X) be any
finite subset of X. By definition of ℓ2(X), we know that ||f||2 = P
x∈X |f(x)|2 and ||g||2 =
P
x∈X |g(x)|2 are finite. Using the known version (15.3) of the Triangle Inequality for finite
lists of complex numbers, we see that
n
X
k=1
|f(xk) + g(xk)|2 ≤


v
u
u
t
n
X
k=1
|f(xk)|2 +
v
u
u
t
n
X
k=1
|g(xk)|2


2
.
Now, since the square root function is increasing, the definition of summation over X shows
that
pPn
k=1 |f(xk)|2 ≤
pP
x∈X |f(x)|2 = ||f||, and similarly
pPn
k=1 |g(xk)|2 ≤||g||. So
X
x∈X′
|f(x) + g(x)|2 =
n
X
k=1
|f(xk) + g(xk)|2 ≤(||f|| + ||g||)2.
This calculation shows that the finite number (||f|| + ||g||)2, which does not depend on X′,
is an upper bound in R for all the finite sums P
x∈X′ |f(x)+g(x)|2 as X′ ranges over F(X).
Thus, the least upper bound of the set of these sums is finite, proving that f + g ∈ℓ2(X).
In fact, our calculation shows that ||f + g||2 ≤(||f|| + ||g||)2, giving a direct proof of the
Triangle Inequality for norms in ℓ2(X).
Inner Product Axioms. The main technical point to be checked when verifying the inner
product axioms for ℓ2(X) is that ⟨f, g⟩= P
x∈X f(x)g(x) is a well-defined complex number
412
Advanced Linear Algebra
for all f, g ∈ℓ2(X). We first prove this under the additional assumption that f(x) and g(x)
are nonnegative real numbers for all x ∈X. Given any X′ = {x1, . . . , xn} ∈F(X), the
Cauchy–Schwarz Inequality for n-tuples (see (15.2)) tells us that
n
X
k=1
f(xk)g(xk) ≤
v
u
u
t
n
X
k=1
f(xk)2 ·
v
u
u
t
n
X
k=1
g(xk)2.
The right side is at most ||f|| · ||g||, which is a finite upper bound on P
x∈X′ f(x)g(x) that
is independent of X′. So P
x∈X f(x)g(x) < ∞in this case; in fact, the sum is bounded by
||f|| · ||g||.
We next consider the case where f, g ∈ℓ2(X) are real-valued. Note that |f| has the
same squared norm as f, namely P
x∈X |f(x)|2 < ∞, so |f| (and similarly |g|) are in
ℓ2(X). By the case already considered, P
x∈X |f(x)g(x)| < ∞. Now, using a general
property of sums over X (Exercise 76), we can conclude that P
x∈X f(x)g(x) is a finite
real number, and in fact, | P
x∈X f(x)g(x)| ≤P
x∈X |f(x)g(x)| ≤||f|| · ||g|| < ∞. Finally,
for complex-valued f, g ∈ℓ2(X), we can write f = t + iu and g = v + iw where
t, u, v, w : X →R give the real and imaginary parts of f and g. Since |t(x)| ≤|f(x)|
for all x ∈X, we see that t (and similarly u, v, w) are real-valued functions in ℓ2(X). Also,
f(x)g(x) = [t(x)v(x) + u(x)w(x)] + i[u(x)v(x) −t(x)w(x)] for all x ∈X. By cases already
considered, P
x∈X t(x)v(x), P
x∈X u(x)w(x), P
x∈X u(x)v(x), and P
x∈X t(x)w(x) are all
finite. It follows that P
x∈X f(x)g(x) is well-defined. In fact, using Exercise 76, we have the
bound | P
x∈X f(x)g(x)| ≤P
x∈X |f(x)g(x)| ≤||f|| · ||g||. This gives a direct proof of the
Cauchy–Schwarz Inequality for ℓ2(X).
Knowing that ⟨f, g⟩is a well-defined complex number, we can prove the axioms for the
inner product without difficulty. We prove ⟨f + g, h⟩= ⟨f, h⟩+ ⟨g, h⟩for all f, g, h ∈ℓ2(X),
leaving the remaining axioms as exercises. Using the general property P
x∈X(a(x)+b(x)) =
P
x∈X a(x) + P
x∈X b(x) (see Exercise 76), we compute
⟨f + g, h⟩
=
X
x∈X
(f + g)(x)h(x) =
X
x∈X
(f(x) + g(x))h(x) =
X
x∈X
[f(x)h(x) + g(x)h(x)]
=
X
x∈X
f(x)h(x) +
X
x∈X
g(x)h(x) = ⟨f, h⟩+ ⟨g, h⟩.
Completeness of the Metric Space ℓ2(X). Let (f0, f1, f2, . . .) = (fn : n ≥0) be a Cauchy
sequence in ℓ2(X); we must prove that this sequence converges to some g ∈ℓ2(X). To find
g, fix x0 ∈X and consider the sequence of complex numbers (fn(x0) : n ≥0). Given ϵ > 0,
there is an integer m0 so that for all m, n ≥m0, d(fm, fn) =
pP
x∈X |fm(x) −fn(x)|2 <
ϵ. Then for all m, n ≥m0, |fm(x0) −fn(x0)| =
p
|fm(x0) −fn(x0)|2 ≤d(fm, fn) < ϵ. So
(fn(x0) : n ≥0) is a Cauchy sequence in the complete metric space C. Therefore, there is
a unique complex number g(x0) such that limn→∞fn(x0) = g(x0). This holds for each
x0 ∈X, so we have a function g : X →C such that fn converges to g pointwise. It remains
to show that g ∈ℓ2(X) and limn→∞fn = g in the metric space ℓ2(X).
To see that g ∈ℓ2(X), fix m0 ∈Z≥0 (corresponding to ϵ = 1) so that m, n ≥m0
implies d(fm, fn) < 1. We show that (||fm0|| + 2)2 is a finite upper bound for all sums
P
x∈X′ |g(x)|2 as X′ ranges over F(X), so that P
x∈X |g(x)|2 ≤(||fm0|| + 2)2 < ∞. Fix
X′ = {x1, . . . , xN} ∈F(X). Since fn(xk) →g(xk) for k = 1, 2, . . . , N, we can choose
m1, . . . , mN such that for all n ≥mk, |fn(xk) −g(xk)| < 1/
√
N. Choose an n larger than
all of m0, m1, . . . , mN. Writing g(xk) = (g(xk) −fn(xk)) + (fn(xk) −fm0(xk)) + fm0(xk)
Metric Spaces and Hilbert Spaces
413
and using (15.3) (extended to a sum of three terms), compute
X
x∈X′
|g(x)|2 ≤


v
u
u
t
N
X
k=1
|g(xk) −fn(xk)|2 +
v
u
u
t
N
X
k=1
|fn(xk) −fm0(xk)|2 +
v
u
u
t
N
X
k=1
|fm0(xk)|2


2
≤


v
u
u
t
N
X
k=1
1
N + ||fn −fm0|| + ||fm0||


2
≤(||fm0|| + 2)2.
The proof that fn →g in ℓ2(X) requires a similar computation. Fix ϵ > 0, and choose
an integer m0 so that m, n ≥m0 implies d(fm, fn) < ϵ/2. We fix n ≥m0 and show that
||fn −g|| ≤ϵ. To do so, pick X′ = {x1, . . . , xN} in F(X), and choose m1, . . . , mN so that
m ≥mk implies |fm(xk)−g(xk)| < ϵ/2
√
N. Choose an m larger than all of m0, m1, . . . , mN.
Writing fn(xk) −g(xk) = (fn(xk) −fm(xk)) + (fm(xk) −g(xk)), we compute (using (15.3))
X
x∈X′
|fn(x) −g(x)|2
≤


v
u
u
t
N
X
k=1
|fn(xk) −fm(xk)|2 +
v
u
u
t
N
X
k=1
|fm(xk) −g(xk)|2


2
<

||fn −fm|| +
v
u
u
t
N
X
k=1
ϵ2
4N


2
< (ϵ/2 + ϵ/2)2 = ϵ2.
The upper bound of ϵ2 holds for all X′, so P
x∈X |fn(x)−g(x)|2 ≤ϵ2, and hence ||fn−g|| ≤ϵ,
as needed.
15.11
Basic Properties of Hilbert Spaces
In this section, we derive some basic algebraic and analytic properties of Hilbert spaces that
do not involve the axiom of completeness. Let H be any Hilbert space. For all x, y ∈H, we
have already derived the Triangle Inequality for norms: ||x+y|| ≤||x||+||y||. We can obtain
a sharper result called the Pythagorean Theorem when x and y are orthogonal vectors. This
theorem says that if ⟨x, y⟩= 0, then ||x + y||2 = ||x||2 + ||y||2. Geometrically, the square of
the length of the hypotenuse of a right triangle equals the sum of the squares of the two
legs. To prove the Pythagorean Theorem, we compute (cf. (15.1))
||x + y||2 = ⟨x + y, x + y⟩= ⟨x, x⟩+ ⟨x, y⟩+ ⟨y, x⟩+ ⟨y, y⟩= ||x||2 + ||y||2,
where ⟨x, y⟩= 0 by hypothesis, and ⟨y, x⟩= ⟨x, y⟩= 0. More generally, we say
that a list x1, . . . , xn of vectors in H is orthogonal iff ⟨xi, xj⟩= 0 for all i ̸= j
in [n]. By induction on n, we see that for any orthogonal list x1, . . . , xn in H,
||x1 + x2 + · · · + xn||2 = ||x1||2 + ||x2||2 + · · · + ||xn||2. More generally, if x1, . . . , xn is an
orthogonal list and c1, . . . , cn ∈C are any scalars, then
||c1x1 + c2x2 + · · · + cnxn||2 = |c1|2||x1||2 + |c2|2||x2||2 + · · · + |cn|2||xn||2.
The key to the induction proof is that cnxn is orthogonal to any linear combination
c1x1 + · · · + cn−1xn−1, so that the Pythagorean Theorem for the sum of two vectors can be
applied.
414
Advanced Linear Algebra
Here are two more identities resembling the Pythagorean Theorem. First, the Parallel-
ogram Law states that for any x, y in a Hilbert space H,
||x + y||2 + ||x −y||2 = 2||x||2 + 2||y||2.
(15.4)
Geometrically, the sum of the squares of the lengths of the two diagonals of any parallelo-
gram equals the sum of the squares of the lengths of the four sides of the parallelogram. To
prove this, compute
||x + y||2 + ||x −y||2 = ⟨x + y, x + y⟩+ ⟨x −y, x −y⟩
= [⟨x, x⟩+ ⟨x, y⟩+ ⟨y, x⟩+ ⟨y, y⟩] + [⟨x, x⟩−⟨x, y⟩−⟨y, x⟩+ ⟨y, y⟩]
= 2||x||2 + 2||y||2.
Second, the Polarization Identity states that for all x, y in a Hilbert space H,
||x + y||2 + i||x + iy||2 −||x −y||2 −i||x −iy||2 = 4⟨x, y⟩.
(15.5)
This identity is proved by a calculation similar to the one just given (Exercise 78). You can
use the Polarization Identity to prove that any complex normed vector space (as defined
in §10.4) that is complete and whose norm satisfies the Parallelogram Law must be a Hilbert
space (Exercise 81).
Next, we discuss some continuity properties of the operations appearing in the definition
of a Hilbert space. First, addition is continuous: if xn →x and yn →y in a Hilbert space
H, then xn +yn →x+y. To prove this, fix ϵ > 0 and let n0, n1 be integers such that n ≥n0
implies ||xn −x|| < ϵ/2, while n ≥n1 implies ||yn −y|| < ϵ/2. Then for n ≥max(n0, n1),
||(xn + yn) −(x + y)|| = ||(xn −x) + (yn −y)|| ≤||xn −x|| + ||yn −y|| < ϵ.
Second, scalar multiplication is continuous: if xn →x in H and cn →c in C, then cnxn →cx
in H. The convergent sequence (cn) must be bounded in C. Let M be a positive constant
such that |cn| ≤M for all n ∈Z≥0. Now, given ϵ > 0, choose integers n0, n1 so that
n ≥n0 implies ||xn −x|| < ϵ/(2M), while n ≥n1 implies |cn −c| < ϵ/(2(1 + ||x||)). Then
n ≥max(n0, n1) implies
||cnxn−cx|| = ||cn(xn−x)+(cn−c)x|| ≤|cn|·||xn−x||+|cn−c|·||x|| < M(ϵ/(2M))+ϵ/2 = ϵ.
Third, the inner product on H is continuous: if xn →x and yn →y in H, then
⟨xn, yn⟩→⟨x, y⟩in C. As before, there is a single constant M > 0 such that ||xn||, ||x||,
||yn||, and ||y|| are all bounded above by M. Given ϵ > 0, choose integers n0, n1 so that
n ≥n0 implies ||xn −x|| < ϵ/(2M), while n ≥n1 implies ||yn −y|| < ϵ/(2M). Then
n ≥max(n0, n1) implies
|⟨xn, yn⟩−⟨x, y⟩| = |⟨xn −x, yn⟩+ ⟨x, yn −y⟩| ≤|⟨xn −x, yn⟩| + |⟨x, yn −y⟩|.
By the Cauchy–Schwarz inequality in H,
|⟨xn−x, yn⟩|+|⟨x, yn−y⟩| ≤||xn−x||·||yn||+||x||·||yn−y|| < (ϵ/(2M))M +M(ϵ/(2M)) = ϵ.
Fourth, the norm on H is continuous: if xn →x in H, then ||xn|| →||x|| in R. To prove
this, note ||xn|| = ||xn −x + x|| ≤||xn −x|| + ||x||, so ||xn|| −||x|| ≤||xn −x||. Similarly,
||x|| −||xn|| ≤||x −xn|| = ||xn −x||, so the absolute value of ||xn|| −||x|| is at most
||xn −x||. Given ϵ > 0, choose n0 so n ≥n0 implies ||xn −x|| < ϵ. Then n ≥n0 implies
| ||xn|| −||x|| | < ϵ, so that ||xn|| →||x|| in R.
Metric Spaces and Hilbert Spaces
415
15.12
Closed Convex Sets in Hilbert Spaces
Recall from Chapter 11 that a subset C of a (real or complex) vector space V is called
convex iff for all x, y ∈C and all real t ∈[0, 1], tx + (1 −t)y ∈C. Geometrically, this
condition says that a convex set must contain the line segment joining any two of its points.
Every subspace W of V is a convex set. If C is a convex subset of V and z ∈V , the translate
z + C = {z + w : w ∈C} is convex. The empty set is also convex.
The following geometric lemma is a key technical tool for studying Hilbert spaces. For
every nonempty closed convex set C in a Hilbert space H, there exists a unique x ∈C of
minimum norm; so ||x|| < ||z|| for all z ̸= x in C. We prove uniqueness first. Suppose
x, y ∈C are two elements such that r = ||x|| = ||y|| ≤||z|| for all z ∈C; we must prove
x = y. Consider z = (1/2)x + (1/2)y. On one hand, by convexity of C, z ∈C and hence
r ≤||z||. On the other hand, applying the Parallelogram Law to x/2 and y/2 shows that
||(x/2) + (y/2)||2 + ||(x/2) −(y/2)||2 = 2||(x/2)||2 + 2||(y/2)||2,
(15.6)
which simplifies to ||z||2 + (1/4)||x −y||2 = (2/4)r2 + (2/4)r2 = r2. Then ||x −y||2 =
4r2 −4||z||2 ≤4r2 −4r2 = 0, forcing ||x −y|| = 0 and x = y.
Turning to the existence proof, let s be the greatest lower bound in R of the set
{||z|| : z ∈C}; this set is nonempty and bounded below by zero, so s does exist. By
definition of greatest lower bound, for each positive integer n we can find xn ∈C such
that s ≤||xn|| < s+1/n. Observe that limn→∞||xn|| = s. We first show that (xn : n > 0) is
a Cauchy sequence in H. Fix ϵ > 0; we must find an integer m0 such that m, n ≥m0 implies
d(xn, xm) = ||xn −xm|| < ϵ. For any positive integers m, n, we have (1/2)xm +(1/2)xn ∈C
by convexity of C, so that s ≤||(xm/2) + (xn/2)|| by definition of s. Applying the
Parallelogram Law to xn/2 and xm/2 gives
||(xn/2) −(xm/2)||2 = 2||xn/2||2 + 2||xm/2||2 −||(xm/2) + (xn/2)||2
≤(s + 1/n)2
2
+ (s + 1/m)2
2
−s2,
which rearranges to
||xn −xm|| ≤2
p
s/n + s/m + 1/(2n2) + 1/(2m2).
Since s is fixed, each term inside the square root approaches zero as n and m increase to
infinity. So we can choose m0 so that m, n ≥m0 implies ||xn −xm|| < ϵ.
Now we know (xn : n > 0) is a Cauchy sequence. By completeness of the Hilbert space
H, this sequence must converge to some point y ∈H. Because C is a closed subset of H,
we must have y ∈C. By continuity of the norm, s = limn→∞||xn|| = || limn→∞xn|| = ||y||.
So ||y|| is a lower bound of all the norms ||x|| for x ∈C, completing the existence proof.
Here is a slight generalization of the lemma just proved. For every nonempty, closed,
convex set C in H and all w ∈H, there exists a unique x ∈C minimizing d(x, w); we call
x the point of C closest to w. To prove this, consider the translate C′ = (−w) + C, which
is readily verified to be nonempty, closed, and convex (Exercise 84). The map x 7→x −w
is a bijection between C and C′. Moreover, d(x, w) = ||x −w||. Thus, x ∈C minimizes
d(x, w) iff x −w ∈C′ has minimum norm among all elements of C′. By the lemma already
proved, there exists a unique element of C′ with the latter property. Therefore, there exists
a unique x ∈C minimizing d(x, w), as needed.
416
Advanced Linear Algebra
15.13
Orthogonal Complements
In our study of finite-dimensional inner product spaces in Chapter 13, we introduced the
orthogonal complement of a subspace W, denoted W ⊥. For each subspace W of the inner
product space V , W ⊥is the subspace consisting of all vectors in V that are orthogonal to
every vector in W. We showed that the map W 7→W ⊥is an inclusion-reversing bijection
on the lattice of subspaces of V , and for every subspace W, W ⊥⊥= W and V = W ⊕W ⊥.
A key ingredient in proving these results was the dimension formula dim(V ) = dim(W) +
dim(W ⊥), whose proof required V to be finite-dimensional.
Our goal is to extend these results to the case of a general Hilbert space H, which may be
infinite-dimensional. To begin, we define the orthogonal complement of an arbitrary subset
S of H to be
S⊥= {v ∈H : ⟨v, w⟩= 0 for all w ∈S}.
We claim that S⊥is always a closed subspace of H. First, 0H ∈S⊥since ⟨0, w⟩= 0
for all w ∈S. Second, given u, v ∈S⊥, we know ⟨u, w⟩= 0 = ⟨v, w⟩for all w ∈S. So
⟨u+v, w⟩= ⟨u, w⟩+⟨v, w⟩= 0+0 = 0 for all w ∈S, proving u+v ∈S⊥. Third, given u ∈S⊥
and c ∈C, we find that ⟨cu, w⟩= c⟨u, w⟩= c0 = 0 for all w ∈S, so cu ∈S⊥. Fourth, to see
that S⊥is closed, define a map Rw : H →C (for each w ∈H) by letting Rw(x) = ⟨x, w⟩for
all x ∈H. If xn →x in H, then (as seen in §15.11) Rw(xn) = ⟨xn, w⟩→⟨x, w⟩= Rw(x). So,
Rw is a continuous map from H to C. In particular, since the one-point set {0} is closed in
C, the inverse image R−1
w [{0}] = {x ∈H : ⟨x, w⟩= 0} is a closed subset of H. By definition
of S⊥, we have S⊥= T
w∈S R−1
w [{0}]. This is an intersection of a family of closed sets, so
S⊥is closed. We remark that each Rw is a C-linear map (by the inner product axioms), and
R−1
w [{0}] is precisely the kernel of Rw. This gives another way to see that S⊥is a subspace,
since the kernel of a linear map is a subspace of the domain, and the intersection of a family
of subspaces is also a subspace.
To obtain a bijective correspondence W 7→W ⊥in the setting of Hilbert spaces, we
must restrict attention to the set of closed subspaces W, since applying the orthogonal
complement operator always produces a closed subspace. When proving the next theorem,
dimension-counting arguments are no longer available. Instead we invoke the geometric
lemma of §15.12, whose proof made critical use of the completeness of H.
Theorem on Orthogonal Complements. Suppose H is a Hilbert space. For any closed
subspace W of H, H = W ⊕W ⊥and W ⊥⊥= W.
Proof. Given x ∈H, we must prove there exist unique y ∈W and z ∈W ⊥with x = y + z.
We prove uniqueness first: assume y1, y2 ∈W and z1, z2 ∈W ⊥satisfy x = y1 +z1 = y2 +z2.
Let u = y1 −y2 = z2 −z1. Since W is a subspace, u = y1 −y2 ∈W. Since W ⊥is a subspace,
u = z2 −z1 ∈W ⊥. Then ⟨u, u⟩= 0, forcing u = 0 by the inner product axioms. So y1 = y2
and z1 = z2.
To prove existence of y and z, we can gain intuition from the case where W is a two-
dimensional subspace of R3 and W ⊥is the line through 0 perpendicular to W. In this case,
we could find y given x by dropping an altitude from x to the plane W. This altitude meets
W at the point y on that plane closest to x, and then x = y + (x −y) where the vector
x −y is parallel to the altitude and hence is in W ⊥. This suggests that in the general case,
we could define y to be the unique point in W closest to x, and let z = x −y. The point y
does exist, since W is a nonempty closed convex subset of H. It is evident that x = y + z,
but we must still check that z ∈W ⊥.
Fix w ∈W; we must show ⟨z, w⟩= 0. The conclusion holds for w = 0, so assume w ̸= 0.
Write w = cu, where c = ||w|| and u = c−1w ∈W satisfies ||u|| = 1. For any s ∈C,
Metric Spaces and Hilbert Spaces
417
the vector y −su is in the subspace W. Since y is the closest point in W to x, we have
||z|| = ||x −y|| ≤||x −(y −su)|| = ||z + su|| for all s ∈C. Squaring this inequality and
rewriting using scalar products,
⟨z, z⟩≤⟨z, z⟩+ s⟨z, u⟩+ s⟨u, z⟩+ |s|2⟨u, u⟩.
Since ⟨u, u⟩= 1 and ⟨u, z⟩= ⟨z, u⟩, the inequality becomes 0 ≤s⟨z, u⟩+ s⟨z, u⟩+ |s|2 for
all s ∈C. Choose s = −⟨z, u⟩to get
0 ≤−|⟨z, u⟩|2 −|⟨z, u⟩|2 + |⟨z, u⟩|2 = −|⟨z, u⟩|2,
which forces ⟨z, u⟩= 0. Then ⟨z, w⟩= c⟨z, u⟩= 0, as needed.
We have now proved H = W ⊕W ⊥for any closed subspace W; we use this to prove
W ⊥⊥= W. Recalling that ⟨x, y⟩= 0 iff ⟨y, x⟩= 0, we see from the definitions that
W ⊆W ⊥⊥without any hypothesis on the subset W. The result just proved shows that
H = W ⊕W ⊥and also H = W ⊥⊕W ⊥⊥, since W ⊥is a closed subspace. We use this to
prove W ⊥⊥⊆W, as follows. Fix x ∈W ⊥⊥. There exist unique y ∈W and z ∈W ⊥with
x = y + z. Similarly, x can be written in exactly one way as the sum of a vector in W ⊥⊥
and a vector in W ⊥. Since y ∈W ⊥⊥, one such sum is x = y + z. On the other hand, since
x ∈W ⊥⊥and 0 ∈W ⊥, another such sum is x = x + 0. By uniqueness, this forces y = x
and z = 0, so x = y is in W, as needed.
Let L be the set of all closed subspaces of the given Hilbert space H. You can check that
L (ordered by set inclusion) is a complete lattice. We have shown that f : L →L, given
by f(W) = W ⊥for W ∈L, does map into the codomain L and satisfies f(f(W)) = W.
Therefore, f is a bijection on L with f −1 = f. You can check that for W, X ∈L, W ⊆X
implies f(W) ⊇f(X), so f is order-reversing. To summarize, W 7→W ⊥is a lattice anti-
isomorphism of the lattice of all closed subspaces of H.
15.14
Orthonormal Sets
Orthonormal bases play a central role in finite-dimensional inner product spaces. Every such
space has an orthonormal basis, and every vector in the space can be written as a (finite)
linear combination of these basis elements. In the setting of Hilbert spaces, we develop an
analytic version of orthonormal bases in which infinite linear combinations of basis elements
are allowed. To prepare for this, we first study finite orthonormal sets in a general Hilbert
space H.
A subset X of H is called orthonormal iff ||x||2 = ⟨x, x⟩= 1 for all x ∈X, and
⟨x, y⟩= 0 for all x ̸= y in X. An orthonormal set X is automatically linearly independent.
To prove this, suppose {x1, . . . , xN} is any finite subset of X and c1, . . . , cN ∈C satisfy
c1x1 + · · · + cNxN = 0. Then, for all j between 1 and N,
0 = ⟨0, xj⟩= ⟨c1x1 + · · · + cNxN, xj⟩= cj⟨xj, xj⟩+
X
k̸=j
ck⟨xk, xj⟩= cj.
Now suppose X = {x1, . . . , xN} is a finite orthonormal subset of H. Let W be the
subspace of H spanned by X. We give a direct argument to show that H = W ⊕W ⊥. (This
also follows from previous results and the fact that the finite-dimensional subspace W must
be closed.) It is routine to check that W ∩W ⊥= {0}, since 0 is the only vector orthogonal
to itself. Next, we show how to write any x ∈H in the form x = y + z, where y ∈W and
418
Advanced Linear Algebra
z ∈W ⊥. Define y = PN
k=1⟨x, xk⟩xk ∈W and z = x −y. To prove z ∈W ⊥, it suffices (by
the inner product axioms) to show that ⟨z, xj⟩= 0 for 1 ≤j ≤N. We compute
⟨z, xj⟩= ⟨x, xj⟩−⟨y, xj⟩= ⟨x, xj⟩−
N
X
k=1
⟨x, xk⟩⟨xk, xj⟩= ⟨x, xj⟩−⟨x, xj⟩= 0.
Thus H = W ⊕W ⊥. We already know H = W ⊥⊥⊕W ⊥, so the argument used at the end
of §15.13 shows that W = W ⊥⊥, and hence W is a closed subspace. It now follows from
the proof in §15.13 that y = PN
k=1⟨x, xk⟩xk is the closest element of W to x. We call y the
orthogonal projection of x onto W.
Since y is orthogonal to z and the xk are orthogonal to each other, the Pythagorean
Theorem shows that ||x||2 = ||y||2 + ||z||2 = PN
k=1 |⟨x, xk⟩|2 + ||z||2. Discarding the error
term ||z||2, we obtain the inequality
N
X
k=1
|⟨x, xk⟩|2 ≤||x||2,
which is the finite version of Bessel’s Inequality. This inequality can be viewed as an
approximate version of the Pythagorean Theorem: the sum of the squared norms of the
components of x in certain orthogonal directions is at most the squared length of x itself.
Next, let X be any orthonormal set (possibly infinite) in a Hilbert space H. Given
w ∈H, define a function fw : X →C by setting fw(x) = ⟨w, x⟩for all x ∈X. The complex
scalars ⟨w, x⟩are called the Fourier coefficients of w relative to the orthonormal set X. We
claim that for all w ∈H, fw is in the space ℓ2(X) of square-summable sequences indexed
by X (see §15.9). In other words, P
x∈X |fw(x)|2 = P
x∈X |⟨w, x⟩|2 < ∞.
To verify the claim, fix any finite subset X′ = {x1, . . . , xN} of X, which is also
orthonormal. By the finite version of Bessel’s Inequality,
X
x∈X′
|fw(x)|2 =
N
X
k=1
|⟨w, xk⟩|2 ≤||w||2.
Thus ||w||2 is a finite upper bound for all these sums as X′ ranges over F(X). Thus we
obtain the general version of Bessel’s Inequality, namely
X
x∈X
|⟨w, x⟩|2 ≤||w||2
for all w ∈H and all orthonormal sets X ⊆H.
15.15
Maximal Orthonormal Sets
A maximal orthonormal set in a Hilbert space H is an orthonormal set X such that for
any set Y properly containing X, Y is not orthonormal. Some texts refer to maximal
orthonormal sets as complete orthonormal sets, but we avoid this term to prevent confusion
with the notion of a complete metric space. By appealing to Zorn’s Lemma (Exercise 89),
you can show that maximal orthonormal sets exist in any Hilbert space.
Let X be a maximal orthonormal set in a Hilbert space H. We prove that in this case,
equality holds in Bessel’s Inequality, namely,
X
x∈X
|⟨w, x⟩|2 = ||w||2
(15.7)
Metric Spaces and Hilbert Spaces
419
for all w ∈H. To get a contradiction, assume this equality fails for some w ∈H. Let
r = P
x∈X |⟨w, x⟩|2 < ||w||2. For each positive integer n, we can find a finite subset Xn of
X such that r −1/n < P
x∈Xn |⟨w, x⟩|2 ≤r. By replacing each Xn by X1 ∪X2 ∪· · · ∪Xn
(which is still finite), we can arrange that X1 ⊆X2 ⊆· · · ⊆Xn ⊆· · · . Now define xn and
yn in H by setting xn = P
x∈Xn⟨w, x⟩x and yn = w −xn for each positive integer n.
We claim (xn) is a Cauchy sequence in H. Given ϵ > 0, choose an integer m0 so that
1/m0 < ϵ. For m ≥n ≥m0, the Pythagorean Theorem gives
||xm −xn||2 =


X
x∈Xm\Xn
⟨w, x⟩x


2
=
X
x∈Xm
|⟨w, x⟩|2 −
X
x∈Xn
|⟨w, x⟩|2 < r −(r −1/n) < ϵ.
So (xn) is Cauchy, hence (xn) converges to a point z ∈H by completeness of H. Letting
y = w −z, we have yn = (w −xn) →(w −z) = y as n goes to infinity.
Now, ||z|| = limn→∞||xn|| = limn→∞
qP
x∈Xn |⟨w, x⟩|2 = √r < ||w||. It follows that
z ̸= w and y ̸= 0. We next claim that ⟨y, x⟩= 0 for all x ∈X. Once this claim is proved,
we can deduce that y ̸∈X, y/||y|| ̸∈X, yet X ∪{y/||y||} is orthonormal, contradicting
the maximality of the orthonormal set X. To prove the claim, fix x ∈X and note that
⟨y, x⟩= limn→∞⟨yn, x⟩= ⟨w, x⟩−limn→∞⟨xn, x⟩. If x ∈Xn0 for some n0, then for all n ≥
n0, xn is the sum of ⟨w, x⟩x plus other vectors orthogonal to x, so limn→∞⟨xn, x⟩= ⟨w, x⟩
and ⟨y, x⟩= 0. On the other hand, if x ̸∈Xn0 for all n0, then ⟨xn, x⟩= 0 for all n. We
show that ⟨w, x⟩= 0 in this case. If ⟨w, x⟩̸= 0, choose n so that |⟨w, x⟩|2 > 1/n. Then
|⟨w, x⟩|2 + P
u∈Xn |⟨w, u⟩|2 > 1/n + (r −1/n) = r, which contradicts the definition of r.
This completes the proof of (15.7). Recalling that fw(x) = ⟨w, x⟩for all x ∈X, we have
||fw||2 = P
x∈X |⟨w, x⟩|2 = ||w||2 for all w ∈H.
Now we can define the concept of an infinite linear combination of vectors in an
orthonormal set. We show that for any maximal orthonormal set X and all w ∈H,
w = P
x∈X⟨w, x⟩x. By definition, this means that given any ϵ > 0, there exists a
finite subset X′ ∈F(X) such that for every finite subset Y
∈F(X) containing X′,
||w −P
x∈Y ⟨w, x⟩x|| < ϵ. Given w ∈H and ϵ > 0, define the sets Xn as in the proof
above (taking r = ||w||2 here). Choose n with 1/n < ϵ2, and take X′ to be the finite set
Xn. For any finite subset Y containing Xn, we know from the calculation in §15.14 that

w −
X
x∈Y
⟨w, x⟩x


2
= ||w||2 −
X
x∈Y
|⟨w, x⟩|2 ≤||w||2 −
X
x∈Xn
|⟨w, x⟩|2 < 1/n < ϵ2,
as needed.
15.16
Isomorphism of H and ℓ2(X)
Recall that a vector space isomorphism is a bijection f : V →W between F-vector spaces
V and W such that f(x+y) = f(x)+f(y) and f(cx) = cf(x) for all x, y ∈V and all c ∈F.
Given two metric spaces X and Y , an isometry from X to Y is a function f : X →Y
that preserves distances, meaning that dY (f(u), f(v)) = dX(u, v) for all u, v ∈X. An
isometry is necessarily one-to-one, since if u, v ∈X satisfy f(u) = f(v), then dX(u, v) =
dY (f(u), f(v)) = 0 and hence u = v. Similarly, you can check that an isometry must be
continuous (Exercise 43). Given Hilbert spaces H1 and H2, a Hilbert space isomorphism
420
Advanced Linear Algebra
(also called an isometric isomorphism) is a bijection f : H1 →H2 that is both a vector
space isomorphism and an isometry.
A Hilbert space isomorphism preserves norms, meaning that ||f(u)|| = ||u|| for all u ∈
H1. This holds because ||f(u)|| = d(f(u), 0) = d(f(u), f(0)) = d(u, 0) = ||u||. It follows
from the Polarization Identity that a Hilbert space isomorphism preserves inner products,
meaning that ⟨f(u), f(v)⟩= ⟨u, v⟩for all u, v ∈H1. To see why, use (15.5) in H2 and in H1
to compute
⟨f(u), f(v)⟩= 1
4(||f(u) + f(v)||2 −||f(u) −f(v)||2 + i||f(u) + if(v)||2 −i||f(u) −if(v)||2)
= 1
4(||f(u + v)||2 −||f(u −v)||2 + i||f(u + iv)||2 −i||f(u −iv)||2)
= 1
4(||u + v||2 −||u −v||2 + i||u + iv||2 −i||u −iv||2) = ⟨u, v⟩.
Conversely, if a given linear map f : H1 →H2 preserves inner products, we see by taking
u = v that f preserves norms. By linearity, f must be an isometry.
Theorem Classifying Hilbert Spaces. Every Hilbert space H is isomorphic to a Hilbert
space of the form ℓ2(X). We can take X to be any maximal orthonormal subset of H.
Although we omit the proof, it is also true that for all sets X and Y , the Hilbert spaces
ℓ2(X) and ℓ2(Y ) are isometrically isomorphic if and only if |X| = |Y | (meaning that there
is a bijection from X onto Y ).
Proof. Let H be any Hilbert space and X be any maximal orthonormal subset of H. Such
subsets do exist, by Exercise 89. We define a map f : H →ℓ2(X) by letting f(w) = fw for
all w ∈H. Recall that fw : X →C gives the Fourier coefficients of w relative to X, namely
fw(x) = ⟨w, x⟩for x ∈X. Also recall that fw does belong to ℓ2(X), by Bessel’s Inequality.
We prove the theorem by showing that f is an isometric isomorphism.
First, we check C-linearity of f. Fix w, y ∈H and c ∈C. On one hand, f(w + y) = fw+y
is the function sending x ∈X to ⟨w + y, x⟩= ⟨w, x⟩+ ⟨y, x⟩. On the other hand,
f(w) + f(y) = fw + fy is the function sending x ∈X to fw(x) + fy(x) = ⟨w, x⟩+ ⟨y, x⟩.
These functions are equal, so f(w + y) = f(w) + f(y). Similarly, both functions f(cw) and
cf(w) send each x ∈X to ⟨cw, x⟩= c⟨w, x⟩, so f(cw) = cf(w). Next, we observe that f is
an isometry, since (15.7) says
||w|| =
sX
x∈X
|⟨w, x⟩|2 =
sX
x∈X
|fw(x)|2 = ||fw||
for all w ∈H. We deduce that f is one-to-one. It also follows that f preserves inner products,
so that
⟨fw, fz⟩=
X
x∈X
⟨w, x⟩⟨z, x⟩= ⟨w, z⟩
for all w, z ∈H. This formula is called Parseval’s Identity.
The only thing left to prove is that f is surjective. Fix g ∈ℓ2(X); we must find w ∈H
with f(w) = g. To do so, we first build a sequence of partial sums gn ∈ℓ2(X) such that
limn→∞gn = g. We know r = P
x∈X |g(x)|2 < ∞. So, for each positive integer n, there
is a finite subset Xn of X such that r −1/n < P
x∈Xn |g(x)|2 ≤r. Define gn(x) = g(x)
for x ∈Xn, and gn(x) = 0 for x ∈X \ Xn. Evidently, gn ∈ℓ2(X), and ||g −gn||2 =
P
x∈X\Xn |g(x)|2 < 1/n. It readily follows that gn →g in the Hilbert space ℓ2(X). In
particular, the convergent sequence (gn) is also a Cauchy sequence.
Metric Spaces and Hilbert Spaces
421
For each positive integer n, define wn ∈H by wn = P
x∈Xn g(x)x. By orthonormality
of X, ⟨wn, x⟩= g(x) = gn(x) for x ∈Xn, and ⟨wn, x⟩= 0 = gn(x) for x ∈X \ Xn.
Therefore, f(wn) = gn for all n > 0. Since f is an isometry and (gn) is a Cauchy sequence,
(wn) must also be a Cauchy sequence (Exercise 45). By completeness of H, wn converges
to some point w ∈H. Now f is continuous (being an isometry), so wn →w in H implies
gn = f(wn) →f(w) in ℓ2(X). On the other hand, by construction, gn →g in ℓ2(X). Since
limits are unique, g = f(w), as needed.
Compare the classification of Hilbert spaces in this section to the analogous classification
of F-vector spaces based on dimension (cardinality of a basis). Note carefuly that an infinite
maximal orthonormal set X in a Hilbert space H is not the same as a basis B for the
complex vector space H. The reason is that each z ∈H must be expressible as a finite
linear combination of basis vectors in B, as opposed to the infinite linear combinations of
the orthonormal vectors in X.
15.17
Continuous Linear Maps
We intend to study operators and linear functionals on Hilbert spaces. Before doing so, we
establish some facts about continuous linear maps in the more general setting of normed
vector spaces. Recall from Chapter 10 that a normed vector space consists of a real or
complex vector space V and a norm function || · || : V →R satisfying these axioms:
for all x, y ∈V and all scalars c,
(a) ||x|| ∈R≥0; and ||x|| = 0 iff x = 0;
(b) ||x + y|| ≤||x|| + ||y|| (the Triangle Inequality);
(c) ||cx|| = |c| · ||x||.
Any normed vector space is a metric space with distance function d(x, y) = ||x −y|| for
x, y ∈V . A Banach space is a normed vector space that is complete using this metric.
Consider a map T : V →W between two normed vector spaces with norms || · ||V and
||·||W . Recall that T is continuous iff whenever vn →v in V , T(vn) →T(v) in W. To check
the continuity of a linear map T, it suffices to check that for all sequences (zn) of points in
V such that zn →0V in V , T(zn) →0W in W. To prove this, suppose (vn) is a sequence in
V converging to v ∈V . Then the sequence (vn −v) converges to v −v = 0, so the stated
condition on T implies that T(vn −v) converges to 0W . By linearity, T(vn)−T(v) converges
to 0W , so T(vn) →T(v), and T is continuous.
A linear map T : V →W is called bounded iff there is a finite real constant M ≥0 such
that ||T(x)||W ≤M||x||V for all x ∈V . We now prove that a linear map T is continuous
iff it is bounded. On one hand, suppose T is bounded and xn →0 in V . If M = 0, then
T(x) = 0W for all x ∈V , so T is continuous. Otherwise, given ϵ > 0, choose n0 so that
n ≥n0 implies ||xn|| < ϵ/M. Then for n ≥n0, ||T(xn)||W ≤M||xn||V < ϵ. So T(xn) →0,
and T is continuous. On the other hand, suppose T is not bounded. Then for each integer
n > 0, there is xn ∈V (necessarily nonzero) with ||T(xn)||W > n||xn||V . By linearity of T
and properties of norms, this inequality still holds if we replace xn by any nonzero scalar
multiple of itself. Picking an appropriate scalar multiple, we can arrange that ||xn||V = 1/n
for each positive integer n. Then xn →0 in V , but ||T(xn)||W > 1 for all n, so T(xn) does
not converge to zero in W, and T is not continuous.
Given two normed vector spaces V and W, let B(V, W) be the set of all bounded
(i.e., continuous) linear maps from V to W. You can check that B(V, W) is a subspace of
the vector space of all linear maps from V to W under pointwise operations on functions
422
Advanced Linear Algebra
(see §4.2). We now show that B(V, W) is itself a normed vector space using the operator
norm defined by
||T|| = sup{||T(x)||W : x ∈V, ||x||V = 1}
for T ∈B(V, W). (In the special case V = {0}, define ||0B(V,W )|| = 0.) First, since the
continuous map T is bounded, we know there is a finite upper bound M for the set in
the definition, so that ||T|| is a finite nonnegative real number. Second, if ||T|| = 0, then
||T(x)||W = 0 for all x ∈V with ||x||V = 1. Multiplying by a scalar, it follows that
||T(y)||W = 0 for all y ∈V , so T(y) = 0W for all y ∈V , so T is the zero map, which is
the zero element of B(V, W). The axiom ||cT|| = |c| · ||T|| follows readily from the fact that
||(cT)(x)||W = ||c(T(x))||W = |c| · ||T(x)||W . Finally, for S, T ∈B(V, W) and any x ∈V of
norm 1,
||(S + T)(x)||W = ||S(x) + T(x)||W ≤||S(x)||W + ||T(x)||W ≤||S|| + ||T||,
so that ||S + T|| ≤||S|| + ||T||.
You can check the following additional properties of the operator norm. Here, V, W, Z
are normed linear spaces and T ∈B(V, W), U ∈B(W, Z) are arbitrary.
(a) ||T(y)||W ≤||T|| · ||y||V for all y ∈V .
(b) ||T|| = sup{||T(x)||W /||x||V : x ∈V, x ̸= 0}.
(c) ||T|| = inf{M ∈R>0 : ||T(y)||W ≤M||y||V for all y ∈V }.
(d) U ◦T ∈B(V, Z) and ||U ◦T|| ≤||U|| · ||T||.
We conclude this section by showing that if W is a Banach space, then B(V, W) is a
Banach space. In other words, completeness of the metric space W implies completeness
of B(V, W). Let (Tn : n ≥0) be a Cauchy sequence in B(V, W). For each fixed x ∈V ,
||Tn(x) −Tm(x)||W = ||(Tn −Tm)(x)||W ≤||Tn −Tm|| · ||x||V . It follows that (Tn(x)) is a
Cauchy sequence in W for each fixed x ∈V . By completeness of W, for each x ∈V there
exists a unique y ∈W (denoted T(x)) so that limn→∞Tn(x) = y = T(x).
We now have a function T : V →W; we must check that T is linear, T is bounded, and
Tn →T in B(V, W). For linearity, fix x, z ∈V and a scalar c. By continuity and linearity
of each Tn,
T(x + z) = lim
n Tn(x + z) = lim
n [Tn(x) + Tn(z)] = lim
n Tn(x) + lim
n Tn(z) = T(x) + T(z);
T(cx) = lim
n Tn(cx) = lim
n cTn(x) = c lim
n Tn(x) = cT(x).
Next, since (Tn) is a Cauchy sequence, {Tn : n ≥0} is a bounded subset of B(V, W). So there
exists a finite constant M ∈R>0 with ||Tn|| ≤M for all n ≥0. For any x ∈V , ||T(x)||W =
|| limn Tn(x)||W = limn ||Tn(x)||W , where ||Tn(x)||W ≤||Tn|| · ||x||V ≤M||x||V for all n. So
||T(x)||W ≤M||x||V for all x ∈V , proving that T is bounded. Finally, we show Tn →T
in B(V, W). Given ϵ > 0, we find n0 so that n ≥n0 implies ||Tn −T|| < ϵ. Since (Tn) is a
Cauchy sequence, we can choose n0 so that m, n ≥n0 implies ||Tn −Tm|| < ϵ/4. Next, given
x ∈V with ||x||V = 1, choose m ≥n0 (depending on x) so that ||Tm(x) −T(x)||W < ϵ/4.
Now, for n ≥n0,
||(Tn −T)(x)||W = ||Tn(x) −Tm(x) + Tm(x) −T(x)||W
≤||(Tn −Tm)(x)||W + ||Tm(x) −T(x)||W < ϵ/4 + ϵ/4 = ϵ/2.
Note that n0 is independent of x here, so we see that ||Tn −T|| ≤ϵ/2 < ϵ, completing the
proof.
Metric Spaces and Hilbert Spaces
423
15.18
Dual Space of a Hilbert Space
In Chapter 13, we studied the dual space V ∗of an F-vector space V , which is the vector
space of all linear maps from V to F. For finite-dimensional V , we saw that V and V ∗
are isomorphic. In the case of a finite-dimensional real inner product space V , we could
realize this isomorphism by mapping y ∈V to the linear functional Ry ∈V ∗given by
Ry(x) = ⟨x, y⟩for x ∈V . Similar results hold for finite-dimensional complex inner product
spaces, but here we must distinguish between linear maps and semi-linear maps.
Now let H be a Hilbert space. We define the dual space H⋆to be the set B(H, C) of all
continuous linear maps from H to C. A function f : H →C is in H⋆iff for all x, y, zn, z ∈H
and all c ∈C, f(x+y) = f(x)+f(y), f(cx) = cf(x), and zn →z in H implies f(zn) →f(z)
in C. For example, given y ∈H, consider the map Ry : H →C defined by Ry(x) = ⟨x, y⟩
for all x ∈H. As noted in §15.13, each Ry is linear and continuous, so Ry ∈H⋆for all
y ∈H.
Since C is complete, it follows from the theorem proved in §15.17 that H⋆= B(H, C)
is a Banach space with norm ||f|| = sup{|f(x)| : x ∈H, ||x|| = 1} for f ∈H⋆. Our goal
here is to define a semi-linear bijective isometry R : H →H⋆, which shows that the Hilbert
space H and the normed vector space H⋆are essentially isomorphic (up to conjugation of
scalars). We can use this semi-isomorphism to define an inner product on H⋆that makes
H⋆a Hilbert space.
The map R : H →H⋆is given by R(y) = Ry for all y ∈H, where Ry(x) = ⟨x, y⟩for all
x ∈H. You can check from the inner product axioms that R is a semi-linear map, meaning
R(y + y′) = R(y) + R(y′) and R(cy) = cR(y) for all y, y′ ∈H and all c ∈C. Next, we prove
that R is a bijection; in other words, for every f ∈H⋆, there exists a unique y ∈H with
f = Ry. Fix f ∈H⋆. To prove uniqueness of y, assume f = Rw = Ry for some w, y ∈H;
we show w = y. Compute
||w −y||2 = ⟨w −y, w −y⟩= ⟨w −y, w⟩−⟨w −y, y⟩= Rw(w −y) −Ry(w −y) = 0,
so w −y = 0 and w = y. To prove existence of y, consider the null space W of f, namely
W = {x ∈H : f(x) = 0} = f −1[{0}]. W is a subspace of H, because it is the kernel
of the linear map f. W is closed, because it is the preimage of the closed set {0} under
the continuous map f. So we can write H = W ⊕W ⊥. If W = H, then f is the zero
map, and we may take y = 0. If W ̸= H, then there exists a nonzero z ∈W ⊥. Using
the Fundamental Homomorphism Theorem for Vector Spaces, we see that W ⊥is the 1-
dimensional subspace spanned by z. Let y = cz, where c = ||z||−2f(z). We claim f = Ry.
To prove this, fix x ∈H, and write x = w + dz for some w ∈W and d ∈C. On one hand,
f(x) = f(w + dz) = f(w) + df(z) = df(z). On the other hand, Ry(x) = ⟨w + dz, cz⟩=
⟨w, cz⟩+ ⟨dz, cz⟩= 0 + dc||z||2 = df(z). So the claim holds, and R is a bijection.
To finish, we check that R is an isometry. Fix x, y ∈H with ||x|| = 1 and y ̸= 0. On one
hand, by the Cauchy–Schwarz Inequality,
|Ry(x)| = |⟨x, y⟩| ≤||y|| · ||x|| = ||y||,
so that ||Ry|| ≤||y||. On the other hand, letting u = ||y||−1y, we have ||u|| = 1 and
|Ry(u)| = |⟨y, y⟩|/||y|| = ||y||,
so that ||Ry|| ≥||y||. Thus, ||Ry|| = ||y||, and this equality also holds for y = 0. So R
is indeed an isometry. You can now check that H⋆is a Hilbert space with inner product
⟨Rx, Ry⟩= ⟨y, x⟩for x, y ∈H; we reverse the order of x and y because R is semi-linear.
424
Advanced Linear Algebra
15.19
Adjoints
Next, we discuss adjoints of operators on Hilbert spaces. We can approach this topic through
the dual space H⋆(as we did in §13.11), or as follows. An operator on a Hilbert space H
is a continuous linear map T : H →H. We write B(H) = B(H, H) for the set of all such
operators. We have seen that B(H) is a Banach space (complete normed vector space).
Given any T ∈B(H), we claim there exists a unique operator T ∗on H, called the adjoint
of T, such that ⟨T(x), y⟩= ⟨x, T ∗(y)⟩for all x, y ∈H. Fix y ∈H. The map f sending
x ∈H to ⟨T(x), y⟩is C-linear, as you can check. It is also continuous, since xn →x implies
T(xn) →T(x), hence f(xn) = ⟨T(xn), y⟩→⟨T(x), y⟩= f(x). As we saw in §15.18, there
exists a unique w ∈H with f(x) = Rw(x) = ⟨x, w⟩for all x ∈H. Writing T ∗(y) = w for
each y, we obtain a unique function T ∗: H →H satisfying ⟨T(x), y⟩= ⟨x, T ∗(y)⟩for all
x, y ∈H.
Recall that the map sending u ∈H to Ru is one-to-one. It follows that, for u, v ∈H, we
can show that u = v by checking ⟨x, u⟩= ⟨x, v⟩for all x ∈H. We use this to prove that T ∗
is a linear map. Given y, z ∈H, we show T ∗(y + z) = T ∗(y) + T ∗(z) by fixing x ∈H and
computing
⟨x, T ∗(y + z)⟩=⟨T(x), y + z⟩= ⟨T(x), y⟩+ ⟨T(x), z⟩= ⟨x, T ∗(y)⟩+ ⟨x, T ∗(z)⟩
=⟨x, T ∗(y) + T ∗(z)⟩.
Similarly, for y ∈H and c ∈C, T ∗(cy) = cT ∗(y) holds because for each x ∈H,
⟨x, T ∗(cy)⟩= ⟨T(x), cy⟩= c⟨T(x), y⟩= c⟨x, T ∗(y)⟩= ⟨x, cT ∗(y)⟩.
To finish, we check continuity of T ∗. Fix y ∈H, and use the Cauchy–Schwarz Inequality
and the boundedness of T to compute
||T ∗(y)||2 = ⟨T ∗(y), T ∗(y)⟩= ⟨T(T ∗(y)), y⟩≤||T(T ∗(y))|| · ||y|| ≤||T|| · ||T ∗(y)|| · ||y||.
If ||T ∗(y)|| > 0, we divide by ||T ∗(y)|| to see that ||T ∗(y)|| ≤||T|| · ||y||; and this inequality
also holds if ||T ∗(y)|| = 0. It follows that T ∗is bounded, hence continuous. In fact, the
proof shows that ||T ∗|| ≤||T||.
Here are some properties of adjoint operators: for all S, T ∈B(H) and all c ∈C,
(S + T)∗= S∗+ T ∗, (cS)∗= c(S∗), S∗∗= S, (S ◦T)∗= T ∗◦S∗, and ||T ∗|| = ||T||. We can
prove these by the method used above. For instance, S∗∗= S since for all x, y ∈H,
⟨x, S∗∗(y)⟩= ⟨S∗(x), y⟩= ⟨y, S∗(x)⟩= ⟨S(y), x⟩= ⟨x, S(y)⟩.
Similarly, (S ◦T)∗= (T ∗◦S∗) because for all x, y ∈H,
⟨x, (S ◦T)∗(y)⟩=⟨(S ◦T)(x), y⟩= ⟨S(T(x)), y⟩= ⟨T(x), S∗(y)⟩= ⟨x, T ∗(S∗(y))⟩
=⟨x, (T ∗◦S∗)(y)⟩.
We showed earlier that ||T ∗|| ≤||T|| for all T ∈B(H). Replacing T by T ∗gives ||T|| =
||T ∗∗|| ≤||T ∗||, so ||T ∗|| = ||T||. We leave semi-linearity of the map S 7→S∗as an exercise.
Now that we have the concept of an adjoint operator, we can generalize the special types
of matrices studied in Chapter 7 to the setting of Hilbert spaces. An operator T on a Hilbert
space H is called self-adjoint iff T ∗= T; positive iff T ∗= T and ⟨T(x), x⟩∈R≥0 for all
x ∈H; normal iff T ◦T ∗= T ∗◦T; and unitary iff T ◦T ∗= idH = T ∗◦T. We indicate
some basic properties of these operators in the exercises, but we must refer the reader to
more advanced texts for a detailed account of the structure theory of normal operators on
Hilbert spaces.
Metric Spaces and Hilbert Spaces
425
15.20
Summary
1.
Definitions for Metric Spaces. Table 15.1 summarizes definitions of concepts
related to metric spaces, sequences, and continuous functions.
2.
Examples of Metric Spaces. Rm and Cm are metric spaces with the Euclidean
metric d2(x, y) =
pPm
k=1 |xk −yk|2. Any set X has the discrete metric defined
by d(x, y) = 0 for x = y, d(x, y) = 1 for x ̸= y. A product X = X1 × · · · × Xm
of metric spaces Xk has metrics d1(x, y) = Pm
k=1 dXk(xk, yk) and d∞(x, y) =
max{dXk(xk, yk) : 1 ≤k ≤m}.
3.
Convergent Sequences and Cauchy Sequences. The limit of a convergent sequence
is unique. Convergent sequences are Cauchy sequences; the converse holds in
complete spaces. Convergent sequences and Cauchy sequences are bounded. Every
subsequence of a convergent sequence converges to the same limit as the full
sequence. If one subsequence of a Cauchy sequence converges, then the full
sequence converges to the same limit.
4.
Closed Sets. In any metric space X, ∅and X are closed. Finite subsets of X
are closed. The union of finitely many closed sets is closed. The intersection of
arbitrarily many closed sets is closed. C is closed iff X\C is open. Closed intervals
[a, b] in R are closed sets. If C1, . . . , Cm are closed in X1, . . . , Xm respectively, then
C1 × · · · × Cm is closed in the product metric space X1 × · · · × Xm (with metric
d1 or d∞).
5.
Open Sets. In any metric space X, ∅and X are open. The union of arbitrarily
many open sets is open. The intersection of finitely many open sets is open. U is
open iff X\U is closed. Open balls B(x; r) are open sets. Open intervals (a, b) in R
are open sets, and every open set in R is a disjoint union of countably many open
TABLE 15.1
Definitions of concepts for metric spaces.
Concept
Definition
metric space (X, d)
∀x, y ∈X, 0 ≤d(x, y) < ∞; d(x, y) = 0 ⇔x = y
∀x, y ∈X, d(x, y) = d(y, x) (symmetry)
∀x, y, z ∈X, d(x, z) ≤d(x, y) + d(y, z) (Triangle Ineq.)
convergent sequence (xn)
xn →x means ∀ϵ > 0, ∃n0, ∀n ≥n0, d(xn, x) < ϵ.
Cauchy sequence (xn)
∀ϵ > 0, ∃n0, ∀m, n ≥n0, d(xn, xm) < ϵ.
closed set C
If xn →x and all xn ∈C, then x ∈C.
open set U
∀x ∈U, ∃r > 0, ∀y ∈X, d(x, y) < r ⇒y ∈U.
bounded set S
∃x ∈X, ∃M ∈R, ∀y ∈S, d(x, y) ≤M.
totally bounded set S
∀ϵ > 0, ∃m < ∞, ∃x1, . . . , xm ∈X, S ⊆Sm
k=1 B(xk; ϵ).
(Exc. 62)
sequentially compact set K
Any sequence (xn) in K has a
subsequence converging to a point of K.
topologically compact set K
Whenever K ⊆S
i∈I Ui with all Ui open,
there is a finite F ⊆I with K ⊆S
i∈F Ui.
complete set K
Every Cauchy sequence in K converges to a point of K.
continuous f : X →Y
Whenever xn →x in X, f(xn) →f(x) in Y .
426
Advanced Linear Algebra
intervals. If U1, . . . , Um are open in X1, . . . , Xm respectively, then U1 × · · · × Um
is open in the product metric space X1 × · · · × Xm (with metric d1 or d∞).
6.
Continuous Functions. The continuity of f : X →Y is equivalent to each of these
conditions: xn →x in X implies f(xn) →f(x) in Y ; for all open V ⊆Y , f −1[V ]
is open in X; for all closed D ⊆Y , f −1[D] is closed in X; for all ϵ > 0 and x ∈X,
there exists δ > 0 such that for all z ∈X with dX(x, z) < δ, dY (f(x), f(z)) < ϵ.
If K is compact in X and f is continuous, then f[K] is compact in Y . The image
of an open (or closed) subset of X under a continuous map need not be open (or
closed) in Y .
7.
Compact Sets. A subset K of a metric space X is sequentially compact iff every
sequence of points in K has a subsequence converging to a point of K. In metric
spaces, sequential compactness is equivalent to topological compactness (every
open cover of K has a finite subcover). Finite subsets of X are compact. Finite
unions and arbitrary intersections of compact subsets are compact. Compact
subsets of X must be closed in X and bounded, but the converse does not hold
in general. However, in Rm and Cm with the metrics d1, d2, and d∞, a subset
K is compact iff K is closed and bounded. A closed subset of a compact set is
compact. The image of a compact set under a continuous function is compact;
but the preimage of a compact set may not be compact. K is compact iff K is
complete and totally bounded.
8.
Complete Spaces. A metric space X is complete iff every Cauchy sequence in X
converges to a point of X. Compactness implies completeness, but not conversely.
Rm and Cm with the metrics d1, d2, and d∞are complete but not compact. A
subset S of a complete space is complete iff S is closed.
9.
Hilbert Spaces. A Hilbert space is a complex inner product space, with norm
||x|| =
p
⟨x, x⟩and metric d(x, y) = ||x −y||, that is complete as a metric
space. Cm is an m-dimensional Hilbert space. For any set X, the set ℓ2(X) of
functions f : X →C with P
x∈X |f(x)|2 < ∞is a Hilbert space with inner
product ⟨f, g⟩= P
x∈X f(x)g(x) and norm ||f|| =
pP
x∈X |f(x)|2. The set of
integrable functions on a measure space (X, µ) is a Hilbert space with inner
product ⟨f, g⟩=
R
X f(x)g(x) dµ and norm ||f|| =
qR
X |f(x)|2 dµ.
10.
Properties of Hilbert Spaces. All x, y in a Hilbert space H satisfy:
(a) Cauchy–Schwarz Inequality: |⟨x, y⟩| ≤||x|| · ||y||.
(b) Parallelogram Law: ||x + y||2 + ||x −y||2 = 2||x||2 + 2||y||2.
(c) Polarization Identity: ||x+y||2 −||x−y||2 +i||x+iy||2 −i||x−iy||2 = 4⟨x, y⟩.
(d) Pythagorean Theorem: If ⟨x, y⟩= 0, then ||x ± y||2 = ||x||2 + ||y||2.
If x1, . . . , xm ∈H is an orthogonal list (⟨xi, xj⟩= 0 for all i ̸= j) and c1, . . . , cm ∈
C, then ||c1x1 +· · ·+cmxm||2 = |c1|2||x1||2 +· · ·+|cm|2||xm||2. The Hilbert space
operations are continuous: if xn →x and yn →y in H and cn →c in C, then
xn + yn →x + y, cnxn →cx, ⟨xn, yn⟩→⟨x, y⟩, and ||xn|| →||x||.
11.
Closed Convex Sets. For any nonempty closed convex subset C of a Hilbert space
H, C has a unique element of minimum norm. For each w ∈H, there exists a
unique point of C closest to w.
12.
Orthogonal Complements. For each subset S of a Hilbert space H, the orthogonal
complement S⊥= {v ∈H : ⟨v, w⟩= 0 for all w ∈S} is a closed subspace of
H. The map W 7→W ⊥is an order-reversing bijection on the lattice of all closed
subspaces of H. For each closed subspace W, H = W ⊕W ⊥, W ⊥⊥= W, and in
the unique expression of x ∈H as a sum of y ∈W and z ∈W ⊥, y (resp. z) is
Metric Spaces and Hilbert Spaces
427
the closest point to x in W (resp. W ⊥). For any subset S, S⊥⊥is the smallest
closed subspace containing S.
13.
Orthonormal Sets. A subset X of a Hilbert space H is orthonormal iff ⟨x, x⟩= 1
and ⟨x, y⟩= 0 for all x ̸= y in X. For X orthonormal and w ∈H, Bessel’s
Inequality states that P
x∈X |⟨w, x⟩|2 ≤||w||2. Equality holds for all w ∈H iff X
is a maximal orthonormal set.
14.
The Isomorphism H ∼= ℓ2(X). By Zorn’s Lemma, every Hilbert space H has a
maximal orthonormal subset X. The map f : H →ℓ2(X) sending w ∈H to
the function fw : X →C, given by fw(x) = ⟨w, x⟩for x ∈X, is a bijective
continuous linear isometry (Hilbert space isomorphism). So P
x∈X |⟨w, x⟩|2 =
||w||2 and P
x∈X⟨w, x⟩⟨z, x⟩= ⟨w, z⟩for all w, z ∈H (Parseval’s Identity).
15.
Continuous Linear Maps. A linear map T : V →W between two normed vector
spaces is continuous (vn →v in V implies T(vn) →T(v) in W) iff T is continuous
at zero (vn →0 in V implies T(vn) →0 in W) iff T is bounded (for some finite M,
||T(v)|| ≤M||v|| for all v ∈V ). The set B(V, W) of continuous linear maps from
V to W is a normed vector space with norm ||T|| = sup{||T(x)|| : x ∈V, ||x|| = 1}.
If W is complete, then B(V, W) is complete.
16.
Dual of a Hilbert Space. Given a Hilbert space H, the dual space H⋆is B(H, C),
the set of bounded (continuous) linear maps from H to C. For every f ∈H⋆,
there exists a unique y ∈H such that f = Ry, where Ry(x) = ⟨x, y⟩for x ∈H.
The map R : H →H⋆is a bijective semi-linear isometry. So, every Hilbert space
is semi-isomorphic to its dual space.
17.
Adjoint Operators. For each operator T ∈B(H) = B(H, H), there exists a unique
adjoint operator T ∗∈B(H) satisfying ⟨T(x), y⟩= ⟨x, T ∗(y)⟩for all x, y ∈H.
For S, T ∈B(H) and c ∈C, (S + T)∗= S∗+ T ∗, (cS)∗= c(S∗), S∗∗= S,
(S ◦T)∗= T ∗◦S∗, and ||T ∗|| = ||T||. The operator T is self-adjoint iff T = T ∗;
positive iff T = T ∗and ⟨T(x), x⟩≥0 for all x ∈H; normal iff T ◦T ∗= T ∗◦T;
and unitary iff T ◦T ∗= idH = T ∗◦T.
15.21
Exercises
Unless otherwise specified, assume X and Y are arbitrary metric spaces and H is an
arbitrary Hilbert space in these exercises.
1.
Which of the following functions define metrics on R2? Explain.
(a) d((x, y), (u, v)) = |x −u|.
(b) d((x, y), (u, v)) = (x −u)2 + (y −v)2.
(c) d((x, y), (u, v)) =
3p
(x −u)3 + (y −v)3.
(d) d((x, y), (u, v)) = |x| + |y| if (x, y) ̸= (u, v), and 0 otherwise.
(e) d((x, y), (u, v)) = 3 if (x, y) ̸= (u, v), and 0 otherwise.
2.
Let X be the set of all bounded functions f : [0, 1] →R. For f, g ∈X, let
d(f, g) = sup{|f(x) −g(x)| : x ∈[0, 1]} Prove d is a metric on X. Find d(x2, x3).
3.
Let Y be the set of all continuous functions f : [0, 1] →R. For f, g ∈Y , let
d(f, g) =
R 1
0 |f(x) −g(x)| dx. Prove d is a metric on Y . Find d(x2, x3). Is d a
metric on the set Z of all Riemann-integrable functions f : [0, 1] →R?
428
Advanced Linear Algebra
4.
Let p be a fixed prime integer. Define dp : Q × Q →R by letting dp(x, y) = 0
if x = y, dp(x, y) = 1/pk if x −y is a nonzero integer and pk is the largest
power of p dividing x −y, and dp(x, y) = 1 otherwise. Prove: for all x, y, z ∈Q,
dp(x, z) ≤max(dp(x, y), dp(y, z)). Show that (Q, dp) is a metric space.
5.
Equivalent Metrics. We say that two metrics d and d′ defined on the same
set X are equivalent iff they have the same open sets, i.e., for all U ⊆X, U is
open in the metric space (X, d) iff U is open in the metric space (X, d′). Prove
that equivalent metrics have the same closed sets, the same compact sets, and the
same convergent sequences. Prove that f : X →Y is continuous relative to d iff
f is continuous relative to d′. Show that equivalence of metrics is an equivalence
relation on the set of all metrics on a fixed set X.
6.
Let (X, d) be a metric space. Fix m ∈R>0, and define d′ : X × X →R by
d′(x, y) = min(m, d(x, y)) for x, y ∈X. Prove d′ is a metric on X. Prove d′ is
equivalent to d (see Exercise 5). Show that all subsets of X are bounded relative
to d′. Construct an example of a closed, bounded, non-compact subset of a metric
space.
7.
Comparable Metrics. We say that two metrics d and d′ defined on the same
set X are comparable iff there exist finite positive constants M, N such that for
all x, y ∈X, d(x, y) ≤Md′(x, y) and d′(x, y) ≤Nd(x, y).
(a) Prove: if d and d′ are comparable, then they are equivalent (Exercise 5).
(b) Prove d2 and d∞are comparable metrics on Rn and Cn.
(c) Prove d1 and d∞are comparable metrics on Rn and Cn.
(d) Prove comparability is an equivalence relation on the set of metrics for X.
(e) Give an example of two equivalent metrics on R that are not comparable.
8.
Let X have metric d, Z be any set, and f : X →Z be a bijection. Show there
exists a unique metric on Z such that f is an isometry.
9.
Give an example of two equivalent metrics on R (Exercise 5) such that R
is complete with respect to one of the metrics but not the other. (Study
f : (−π/2, π/2) →R given by f(x) = tan x.)
10.
Given two comparable metrics d1 and d2 on a set X (Exercise 7), show that
(X, d1) is complete iff (X, d2) is complete.
11.
Verify the metric space axioms for d′, d1, and d∞(defined in §15.1).
12.
Given metric spaces X1, . . . , Xm, prove the metrics d1 and d∞on X1 × · · · × Xm
are comparable.
13.
Negate the definition of convergent sequence to obtain the formal definition of a
non-convergent sequence. Use this to prove carefully that the sequence (n : n ≥0)
is non-convergent in the metric space R.
14.
A sequence (xn : n ≥0) is called eventually constant iff there exists an integer
n0 such that for all n ≥n0, xn = xn0. Prove every eventually constant sequence
in (X, d) converges to xn0. If d is the discrete metric on X, prove that every
convergent sequence is eventually constant.
15.
Fix k ∈Z≥0. Show that a sequence (xn : n ≥0) converges to x iff the tail sequence
(xk+n : n ≥0) converges to x. Show that (xn : n ≥0) is Cauchy iff (xk+n : n ≥0)
is Cauchy. Show that (xn : n ≥0) is bounded iff (xk+n : n ≥0) is bounded.
16.
Prove: if (xn : n ≥0) converges in X, then {xn : n ≥0} is bounded.
17.
Prove: if (xn : n ≥0) is a Cauchy sequence, then {xn : n ≥0} is bounded.
Metric Spaces and Hilbert Spaces
429
18.
Let f : X →Y be a continuous map between metric spaces and (xn) be a
sequence in X. If (xn) is convergent, or Cauchy, or bounded, must the same be
true of the sequence (f(xn))? If (f(xn)) is convergent, or Cauchy, or bounded,
must the same be true of (xn)?
19.
Let (xn) = ((xn(1), xn(2), . . . , xn(m)) : n ≥0) be a sequence in a product metric
space X = X1 × X2 × · · · × Xm with metric d1. Prove that (xn) converges to
y = (y1, . . . , ym) iff (xn(k)) converges to yk for all k between 1 and m. Prove (xn)
is Cauchy iff (xn(k)) is Cauchy for all k between 1 and m.
20.
Repeat Exercise 19 using the metric d∞on X.
21.
Repeat Exercise 19 taking X to be Rm or Cm using the Euclidean metric d2.
22.
Let (xn : n ≥0) be a sequence in (X, d). Show that the set S of all limits of
subsequences of (xn : n ≥0) is a closed subset of X.
23.
Give an example of a sequence (xn : n ≥0) in R such that every k ∈Z is a limit
of some subsequence of (xn).
24.
Give an example of a sequence (xn : n ≥0) in R such that every r ∈[0, 1] is a
limit of some subsequence of (xn).
25.
Show that every subset of a discrete metric space is both open and closed.
26.
Decide (with proof) whether each subset of (R2, d2) is closed.
(a) {(x, y) ∈R2 : y ≥0}
(b) {(x, y) ∈R2 : x2 + y2 < 1}
(c) Z × Z
(d) the graph {(x, f(x)) : x ∈R}, where f : R →R is continuous
27.
Let Z ⊆Y ⊆X, where the metric on Y is the restriction of the metric on X.
(a) Prove: if Z is closed in Y and Y is closed in X, then Z is closed in X.
(b) Give an example to show that (a) can fail if Y is not closed in X.
(c) Prove: if Z is open in Y and Y is open in X, then Z is open in X.
(d) Give an example to show that (c) can fail if Y is not open in X.
28.
Closure of a Set. Given a subset S of a metric space (X, d), the closure of S in
X, denoted S, is the intersection of all closed subsets of X containing S. Show
that S ⊆S, S is a closed set, and for any closed set T, if S ⊆T then S ⊆T.
Show that S equals the set S′ of all y ∈X such that there exists a sequence (xn)
with all xn ∈S and limn→∞xn = y. (First show S′ is closed.)
29.
In (R, d2), find the closure of these sets: the interval (a, b), Q, R, and Z.
30.
Boundary of a Set. Given a subset S of the metric space (X, d), the boundary of
S is the set Bdy(S) consisting of all x ∈X such that for all ϵ > 0, B(x; ϵ) contains
at least one point in S and at least one point not in S. Prove Bdy(S) = S ∩X \ S
(see Exercise 28). Prove Bdy(S) is a closed set.
31.
In (R, d2), find the boundary of these sets: the interval (a, b), Q, R, and Z.
32.
In (R2, d2), find the boundary of R × {0}, {(x, y) : x2 + y2 < 1}, and [0, 1] × Q.
33.
Consider a product metric space X = X1 × · · · × Xm with metric d1.
(a) Prove: if Si is closed in Xi for all i, then S1 × · · · × Sm is closed in X.
(b) Prove: if Si is open in Xi for all i, then S1 × · · · × Sm is open in X.
(c) Prove: if Si is compact in Xi for all i, then S1 × · · · × Sm is compact in X.
34.
Which results in the previous exercise are true if we use the metric d∞on X?
Which results hold when X is (Rm, d2) and each Xi is (R, d2)?
35.
Let W be a subspace of a normed vector space V . Show that the closure W
(Exercise 28) is a closed subspace of V .
430
Advanced Linear Algebra
36.
Sketch the open ball B((2, 1); 1) in R2 for each of the metrics d1, d2, and d∞.
37.
Interior of a Set. Given a subset S of the metric space (X, d), the interior of S
in X, denoted Int(S) or S◦, is the union of all open subsets of X contained in S.
Show that Int(S) ⊆S, Int(S) is an open set, and for any open set U, if U ⊆S
then U ⊆Int(S). Show that Int(S) = X ∼X ∼S.
38.
In (R, d2), give an example of a set S with Int(S) = ∅and S = R.
39.
For x ∈X and r > 0, define the closed ball B[x; r] = {y ∈X : d(x, y) ≤r}.
(a) Prove B[x; r] is a closed subset of X.
(b) Show that open and closed balls in normed vector spaces are convex.
(c) Give an example to show that B(x; r) can be a proper subset of B[x; r].
40.
Given S ⊆X and r > 0, let Nr(S) = {y ∈X : d(x, y) < r for some x ∈S}. Show
that Nr(S) is always an open set.
41.
Let Nr[S] = {y ∈X : d(x, y) ≤r for some x ∈S}. Must Nr[S] be a closed set?
Explain.
42.
Prove: every open set in R is the union of countably many disjoint open intervals.
43.
Prove that every isometry is continuous. Deduce that idX is continuous.
44.
A contraction is a function f : X →Y such that dY (f(x1), f(x2)) ≤dX(x1, x2)
for all x1, x2 ∈X. Prove that every contraction is continuous. Deduce that
constant functions are continuous.
45.
Let f : X →Y be an isometry. Prove: for all xn ∈X, (xn) is a Cauchy sequence
in X iff (f(xn)) is a Cauchy sequence in Y . What can you say about Cauchy
sequences if f is a contraction?
46.
Let X = X1 × · · · × Xm be a product metric space using the metric d1 defined
in §15.1. For 1 ≤i ≤m, define pi : X →Xi by pi((x1, . . . , xm)) = xi.
(a) Prove each pi is continuous.
(b) Prove: g : Y →X is continuous iff pi ◦g is continuous for 1 ≤i ≤m.
(c) Explain why (a) and (b) also hold for (X, d∞) and (Cm, d2).
47.
Let Z = {z ∈C : |z| = 1} with the Euclidean metric. Define f : [0, 2π) →Z by
f(x) = (cos x, sin x) for x ∈[0, 2π). Verify that f is a continuous bijection, but
f −1 is not continuous.
48.
Prove that if X has the discrete metric, then every function f : X →Y is
continuous. Use this to give an example of a continuous bijection whose inverse
is not continuous.
49.
Let V, W, Z be normed vector spaces. Suppose (Tn) is a sequence in B(V, W)
converging to T, and (Un) is a sequence in B(W, Z) converging to U. Prove
Un ◦Tn →U ◦T in B(V, Z).
50.
Prove f : X →Y is continuous iff for all x0 ∈X and all ϵ > 0, there exists δ > 0
such that for all x ∈X, if dX(x, x0) < δ, then dY (f(x), f(x0)) < ϵ.
51.
Uniform Continuity. A function f : X →Y is called uniformly continuous iff
for all ϵ > 0, there exists δ > 0 such that for all x1, x2 ∈X, if dX(x1, x2) < δ,
then dY (f(x1), f(x2)) < ϵ. Note that the δ appearing here depends on ϵ but not
on x1 and x2, whereas the δ in Exercise 50 depends on both ϵ and x0. Prove:
(a) Contractions and isometries are uniformly continuous.
(b) Bounded linear maps on normed vector spaces are uniformly continuous.
(c) Addition (viewed as a map from (H × H, d1) to H) is uniformly continuous.
(d) The norm on a normed vector space is uniformly continuous.
Metric Spaces and Hilbert Spaces
431
52.
Prove f : R →R, given by f(x) = x2 for x ∈R, is continuous but not uniformly
continuous.
53.
Prove that the union of a finite collection of compact subsets of X is compact.
54.
Show that a subset S of a compact set is compact iff S is closed.
55.
Prove that the intersection of any collection of compact subsets of X is compact.
56.
Suppose X is a discrete metric space. Under what conditions is X compact?
Under what conditions is X complete?
57.
Let f : X →Y be continuous. Answer each question with full explanation.
(a) If U is open in X, must f[U] be open in Y ?
(b) If C is closed in X, must f[C] be closed in Y ?
(c) If K is compact in Y , must f −1[K] be compact in X?
58.
Theorem: For all a < b in R, the closed interval [a, b] is (sequentially) compact.
Prove this theorem by completing the following outline. Let x = (xn : n ∈Z≥0)
be a sequence of points in [a, b]. We construct a sequence of nested closed intervals
I0, I1, . . . , Ik, . . ., where I0 = [a, b], Ik+1 ⊆Ik for all k ≥0, and the length of Ik is
(b −a)/2k for all k ≥0. Ik+1 is either the left half or the right half of Ik. We also
construct a list of sequences x0 = x, x1, x2, . . ., such that each sequence xk+1 is a
subsequence of the previous sequence xk, and for all k, all terms of xk belong to
Ik. The construction proceeds recursively, with I0 and x0 given above. Suppose
that Ik and xk have been constructed with the stated properties. Each term in
xk belongs to either the left half or the right half of Ik. So one of these two halves
must contain infinitely many terms of the sequence xk. Let Ik+1 be the left half
of Ik if this half contains infinitely many terms of xk; otherwise, let Ik+1 be the
right half of Ik. Define xk+1 to be the subsequence of xk consisting of all terms
of xk that are in Ik+1. (a) Verify by induction that xk and Ik have the properties
stated above. (b) Use the fact that every bounded subset of R has a greatest
lower bound and least upper bound to see that T
k≥0 Ik must consist of a single
point x∗∈R. (c) Consider the diagonal sequence (yk), where yk = xk
k. Check
that this sequence is a subsequence of the original sequence (xn), and yk ∈Ik for
all k. (d) Show that (yk) converges to x∗.
59.
Use Exercise 58 and Exercise 33 to prove that a subset K of Rm (using any of
the metrics d1, d2, or d∞) is compact iff K is closed and bounded.
60.
Extreme Value Theorem. Prove: if X is a compact metric space and f :
X →R is continuous, then there exist x1, x2 ∈X such that for all x ∈X,
f(x1) ≤f(x) ≤f(x2). In other words, a continuous real-valued function on a
compact domain attains its maximum and minimum value somewhere on that
domain.
61.
Let cn be positive real constants with P∞
n=1 c2
n < ∞. Let K = {f ∈ℓ2(Z>0) : 0 ≤
|f(n)| ≤cn for all n}. Prove K is a compact subset of the Hilbert space ℓ2(Z>0).
(Use ideas from the proof in Exercise 58.)
62.
Call a subset S of X totally bounded iff for all ϵ > 0, there exist finitely many
points x1, . . . , xm ∈X such that S ⊆Sm
i=1 B(xi; ϵ). Prove that S is (sequentially)
compact iff S is complete and totally bounded.
63.
Prove that a topologically compact metric space X is sequentially compact. (If
X is not sequentially compact, fix a sequence (xn) in X with no convergent
subsequence. Without loss of generality, assume no two xn are equal. For each
x ∈X, prove that there is an open ball Ux = B(x; ϵ(x)) containing x such that
432
Advanced Linear Algebra
at most one xn is in Ux. Show that {Ux : x ∈X} is an open cover of X with no
finite subcover.)
64.
A
metric
space
X
is
called
separable
iff
there
exists
a
countable
set
S = {xn : n ∈Z>0} with S = X. (a) Prove that sequentially compact metric
spaces are separable. (b) Prove that given any open cover {Ui : i ∈I} of a
separable metric space X, there exists a countable subcover {Uim : m ∈Z≥0}
with X = S∞
m=0 Uim. (Fix a set Ui0 in the open cover. Given S as above, for
each n, k ∈Z>0 let Vn,k be one particular set Ui in the open cover such that
B(xn; 1/k) ⊆Ui, if such a set exists; otherwise let Vn,k = Ui0. Show that the set
of all Vn,k gives a countable subcover.)
65.
Prove that a sequentially compact metric space X is topologically compact.
(By Exercise 64, reduce to showing that every countably infinite open cover
{Vn : n ∈Z>0} of X has a finite subcover. If no such finite subcover exists, then
all of the closed sets Cn = X \ (V1 ∪· · · ∪Vn) are nonempty. Pick xn ∈Cn and
study the sequence (xn).)
66.
Let X and Y be complete metric spaces with subsets X1 ⊆X and Y1 ⊆Y .
Assume f1 : X1 →Y1 is an isometry, X1 = X, and Y1 = Y . Show that there
exists a unique extension of f1 to an isometry f : X →Y .
67.
Find necessary and sufficient conditions on v, w ∈H for equality to hold in the
Cauchy–Schwarz Inequality.
68.
Find necessary and sufficient conditions on v, w ∈H for equality to hold in the
Triangle Inequality for norms.
69.
Show that every finite-dimensional complex inner product space is complete
relative to the metric induced from the inner product. (Use orthonormal bases
to show that such a space is isometrically isomorphic to Cn with the Euclidean
metric.)
70.
Let W be the subset of H = ℓ2(Z≥0) consisting of all sequences with only finitely
many nonzero terms. Show that W is a subspace of H that is not closed.
71.
Let H be the Hilbert space of (Lebesgue-measurable) functions f : [0, 1] →C such
that
R 1
0 |f(x)|2 dx < ∞. Show that the set of continuous f ∈H is a subspace of
H that is not closed.
72.
Let S be any subset of H. (a) Let ⟨S⟩be the span of S, consisting of all finite C-
linear combinations of elements of S. Prove S⊥= ⟨S⟩⊥. (b) Let S be the closure
of S (see Exercise 28). Prove S⊥= (S)⊥. (c) Show that S⊥⊥= ⟨S⟩, which is the
smallest closed subspace of H containing S. (d) Show that for any subspace W
of H, W ⊥⊥= W.
73.
Gram–Schmidt Algorithm in a Hilbert Space. Let (x1, x2, . . . , xm, . . .) be
an infinite sequence of linearly independent vectors in H. Give a constructive
procedure for computing a sequence (z1, z2, . . . , zm, . . .) of orthonormal vectors
in H such that for all m ≥1, (x1, . . . , xm) and (z1, . . . , zm) span the same
subspace. Conclude that every infinite-dimensional Hilbert space has an infinite
orthonormal set.
74.
(a) Verify that the set L of closed subspaces of H is a complete lattice.
(b) Explain why the map W 7→W ⊥for W ∈L is a lattice anti-isomorphism.
(c) Prove or disprove: given {Wi : i ∈I} ⊆L, the least upper bound of this
collection must be the sum of subspaces P
i∈I Wi.
Metric Spaces and Hilbert Spaces
433
75.
Show that if W and Z are closed subspaces of H such that ⟨w, z⟩= 0 for all
w ∈W and z ∈Z, then W +Z is a closed subspace. Show that every 1-dimensional
subspace of H is closed. Use these facts and the existence of orthonormal bases
to show that every finite-dimensional subspace of H must be closed.
76.
Let X be a set, wk, zk ∈C for each k ∈X, and c ∈C.
(a) Prove P
k∈X wk + P
k∈X zk = P
k∈X(wk + zk) if both sums on the left side
are finite. (Proceed in three steps, assuming first that all wk, zk ≥0, next that
all wk, zk are real, and then handling the complex case.)
(b) Prove c P
k∈X wk = P
k∈X(cwk) when the sum on the left is finite.
(c) Prove: if P
k∈X |wk| < ∞, then P
k∈X wk < ∞and | P
k∈X wk| ≤P
k∈X |wk|.
(d) Prove: if 0 ≤wk ≤zk for each k ∈X, then P
k∈X wk ≤P
k∈X zk.
77.
In §15.10, prove the remaining axioms for the inner product.
78.
Prove the Polarization Identity (15.5).
79.
Prove: For all n ∈Z≥3, for all x, y ∈H, Pn−1
k=0 e2πik/n||x + e2πik/ny||2 = n⟨x, y⟩.
80.
For fixed x, y ∈H, evaluate the integral
R 2π
0
eit||x + eity||2 dt.
81.
Let V be a complex Banach space in which the norm satisfies the Parallelogram
Law (15.4). Take the Polarization Identity (15.5) as the definition of ⟨x, y⟩for
x, y ∈V . Show that the axioms for a complex inner product hold, and show that
⟨x, x⟩= ||x||2, where ||x|| is the given norm on V . (Informally, this says that
Hilbert spaces are the Banach spaces where the Parallelogram Law holds.)
82.
Give an example to show that the Parallelogram Law is not true in Cn if we
use the 1-norm or the sup-norm (see §10.4). Conclude that these normed vector
spaces do not have the structure of a Hilbert space.
83.
If C and D are closed subsets of H, must C + D = {x + y : x ∈C, y ∈D} be
closed in H?
84.
For fixed z ∈H, define Tz : H →H by Tz(x) = z + x for x ∈H. Prove Tz is an
isometry with inverse T−z. Prove Tz maps closed sets to closed sets and convex
sets to convex sets.
85.
Given a closed subspace W of H and x ∈H, we proved that x = y + z for unique
y ∈W and z ∈W ⊥. Prove that z is the closest point to x in W ⊥.
86.
Given a closed subspace W of H, define maps P : H →H and Q : H →H as
follows. Given x ∈H, we know x = y + z for unique y ∈W and z ∈W ⊥. Define
P(x) = y and Q(x) = z. Prove that P and Q are linear maps such that P ◦P = P,
Q ◦Q = Q, and P ◦Q = 0 = Q ◦P. Prove that P and Q are continuous maps.
Find the image and kernel of P and Q.
87.
Let H be the Hilbert space of Lebesgue integrable functions with domain [0, 2π].
For n ∈Z, define un ∈H by un(t) = eint/
√
2π for t ∈[0, 2π]. Prove that
{un : n ∈Z} is an orthonormal set in H. (This orthonormal set, which can be
shown to be maximal, plays a central role in the theory of Fourier series.)
88.
Let S = {un : n ∈Z>0} be an infinite orthonormal set in H.
(a) Compute ||un −um|| for all n ̸= m.
(b) Prove that a sequence (xn) with all xn ∈S converges in H iff the sequence is
eventually constant, i.e., for some integer n0, xn = xn0 for all n ≥n0.
(c) Using (b), show that S is closed and bounded in H, but not compact.
(d) Prove: if C is a closed set in H that contains B(0; ϵ) for some ϵ > 0, then
C is not compact. (This shows infinite-dimensional Hilbert spaces are not locally
compact.)
434
Advanced Linear Algebra
89.
Use Zorn’s Lemma (see §17.13) to prove that in any Hilbert space H, there exists
a maximal orthonormal set.
90.
Let X be an orthonormal subset of H. Prove:
(a) If ||w||2 =
X
x∈X
|⟨w, x⟩|2 for all w ∈H, then X is a maximal orthonormal set.
(b) If
X
x∈X
⟨w, x⟩⟨z, x⟩= ⟨w, z⟩for all w, z ∈H, then X is maximal.
91.
Prove: if h : X →Y is a bijection, then there is a Hilbert space isomorphism
from ℓ2(X) to ℓ2(Y ) sending f ∈ℓ2(X) to f ◦h−1 ∈ℓ2(Y ).
92.
Prove: if i : X →Y is an injection, then i induces a linear isometry mapping
ℓ2(X) onto a closed subspace of ℓ2(Y ).
93.
Prove: for any two Hilbert spaces H1 and H2, H1 is isometrically isomorphic to
a closed subspace of H2, or vice versa.
94.
Let V, W, Z be normed vector spaces, T ∈B(V, W), and U ∈B(W, Z). Prove:
(a) ||T(y)|| ≤||T|| · ||y|| for all y ∈V .
(b) ||T|| = sup{||T(x)||/||x|| : 0 ̸= x ∈V }.
(c) ||T|| = inf{M ∈R>0 : ||T(y)|| ≤M||y|| for all y ∈V }.
(d) U ◦T ∈B(V, Z) and ||U ◦T|| ≤||U|| · ||T||.
95.
Given a nonzero f ∈H⋆, let W = ker(f). Prove dim(W ⊥) = 1.
96.
Check that H⋆is a Hilbert space with the inner product ⟨Rx, Ry⟩H⋆= ⟨y, x⟩H
for x, y ∈H.
97.
Define E : H →H⋆⋆by letting E(z) be the evaluation map Ez : H⋆→C given
by Ez(f) = f(z) for f ∈H⋆and z ∈H. In §15.18, we defined a semi-linear
bijective isometry RH : H →H⋆. Prove that E = RH⋆◦RH. Conclude that E is
an isometric isomorphism, so that H ∼= H⋆⋆as Hilbert spaces.
98.
Prove: for all S, T ∈B(H) and c ∈C, (S + T)∗= S∗+ T ∗and (cS)∗= c(S∗).
99.
Prove: for all T ∈B(H), ||T ∗◦T|| = ||T||2 = ||T ◦T ∗|| (calculate ||T(x)||2).
100.
Given T ∈B(H), show that the dual map T ⋆: H⋆→H⋆, given by T ⋆(f) = f ◦T
for f ∈H⋆, is in B(H⋆). Prove that dual maps satisfy properties analogous to
the properties of adjoint maps listed in §15.19. Let R : H →H⋆be the semi-
isomorphism from §15.18. Prove: for all T ∈B(H), T ∗= R−1 ◦T ⋆◦R.
101.
Prove: the set of self-adjoint operators on H is a closed real subspace of B(H).
102.
If S, T ∈B(H) are self-adjoint, must S ◦T be self-adjoint?
103.
(a) Prove: if T ∈B(H) satisfies ⟨T(x), x⟩= 0 for all x ∈H, then T = 0.
(b) Prove: if S, T ∈B(H) satisfy ⟨T(x), x⟩= ⟨S(x), x⟩for all x ∈H, then S = T.
(c) Prove: T ∈B(H) is self-adjoint iff ⟨T(x), x⟩is real for all x ∈H.
104.
Let Z be the set of normal operators in B(H). Is Z closed under addition? scalar
multiplication? composition? Is Z a closed set? Prove: T ∈Z iff ||T ∗(x)|| =
||T(x)|| for all x ∈H. Prove: if T ∈Z, then ||T 2|| = ||T||2.
105.
Define T : ℓ2(Z>0) →ℓ2(Z>0) by setting T((c1, c2, . . .)) = (0, c1, c2, . . .) for cn ∈
C. Prove T ∈B(ℓ2(Z>0)) and compute ||T||. Find an explicit formula for the map
T ∗. Show that T ∗◦T = id, but T ◦T ∗̸= id. Is T self-adjoint? normal? unitary?
106.
Prove that T ∈B(H) is unitary iff T : H →H is an isometric isomorphism.
Part V
Modules and
Classification Theorems
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
16
Finitely Generated Commutative Groups
A major goal of abstract algebra is the classification of algebraic structures. An example of
such a classification is the theorem of linear algebra stating that every finite-dimensional
real vector space V is isomorphic to Rn for some integer n ≥0. Moreover, the number n is
uniquely determined by V and is the dimension of V as a vector space. This classification
theorem is useful because, in principle, we can use it to reduce the study of abstract vector
spaces such as V to the specific concrete vector spaces Rn.
We could hope for similar classification theorems in the theory of groups. A complete
classification of all groups seems far too difficult to ever be achieved. However, much more
can be said about special classes of groups, such as simple groups or commutative groups.
For example, here is a structure theorem for finite commutative groups.
Theorem Classifying Finite Commutative Groups. Every finite commutative group
G is isomorphic to a direct product of cyclic groups, where each cyclic group has size pe
for some prime p and some integer e > 0. The list of prime power sizes of the cyclic groups
(written in decreasing order) is uniquely determined by G.
This chapter proves a more general classification theorem describing the structure of all
finitely generated commutative groups. The techniques used to obtain this structural result
for groups can also be applied in a linear-algebraic context to derive canonical forms for
matrices and linear transformations. Further abstraction of these arguments eventually leads
to a structure theorem classifying finitely generated modules over principal ideal domains,
which is a cornerstone of abstract algebra. These topics (modules over PIDs and canonical
forms) are covered in Chapter 18, which can be read independently from this chapter.
To obtain the structure theorem for finitely generated commutative groups, we first
develop the theory of free commutative groups. Free commutative groups are the analogs
in group theory of the vector spaces that appear so prominently in linear algebra. Thus,
our development of free commutative groups has a distinctively linear-algebraic flavor.
In particular, integer-valued matrices emerge as a key tool for understanding group
homomorphisms between two free commutative groups. As you study this chapter, look
for analogies between the material presented here and the parallel theory of vector spaces,
linear transformations, and matrices.
16.1
Commutative Groups
We begin by reviewing some basic facts about commutative groups (see also Chapter 1). A
commutative group consists of a set G together with a binary operation on G (denoted by
the symbol +), satisfying these five axioms:
(i) Closure: for all g, h ∈G, g + h belongs to the set G.
(ii) Associativity: for all g, h, k ∈G, (g + h) + k = g + (h + k).
(iii) Commutativity: for all g, h ∈G, g + h = h + g.
DOI: 10.1201/9781003484561-16
437
438
Advanced Linear Algebra
(iv) Additive Identity: there exists 0 ∈G so that for all g ∈G, g + 0 = g = 0 + g.
(v) Additive Inverses: for all g ∈G, there exists −g ∈G such that g +(−g) = 0 = (−g)+g.
Some examples of commutative groups are the number systems Z, Q, R, and C, where the
group operation is ordinary addition of numbers.
Let H be a subset of a commutative group G. H is called a subgroup of G iff the following
closure conditions hold:
(a) 0 (the additive identity of G) is in the set H.
(b) For all h, k ∈H, h + k is in H.
(c) For all h ∈H, −h is in H.
In this case, the set H is a commutative group using the addition operation + from G,
restricted to inputs in H.
Given a subgroup H of G, we can form the quotient group G/H whose elements are the
cosets x + H = {x + h : h ∈H}, with x ranging through G. The Coset Equality Theorem
states that for all x, y ∈G, x + H = y + H if and only if x −y ∈H. Here, x −y is defined
to mean x + (−y). The group operation in G/H is given by
(x + H) + (y + H) = (x + y) + H
for all x, y ∈G.
See §1.6 for a fuller discussion of quotient groups.
Recall Z is the commutative group of integers under addition. In our later work, we need
to know all the subgroups of Z. The next theorem classifies these subgroups.
Theorem on Subgroups of Z. Every subgroup of Z has the form nZ = {nk : k ∈Z} for
some uniquely determined integer n ≥0.
Proof. It is routine to verify that for each n ∈Z≥0, the subset nZ is indeed a subgroup of
Z, and different values of n produce unequal subgroups. Given an arbitrary subgroup H
of Z, we show that H = nZ for some n ∈Z≥0. If H consists of 0 alone, then H = 0Z.
Otherwise, there must exist a nonzero element h ∈H. Since h ∈H implies −h ∈H, we see
that H contains at least one positive integer. Using the Least Natural Number Axiom, let
n be the smallest positive integer belonging to H. We claim that nZ = H. You can check
the inclusion nZ = {nk : k ∈Z} ⊆H by induction on k, since n ∈H and H is closed under
addition and inverses. We now prove the reverse inclusion H ⊆nZ. Fix h ∈H. Dividing h
by the nonzero integer n, we can write h = nq+r for some integers q, r such that 0 ≤r < n.
Now, since nZ ⊆H, nq is in H. It follows that r = h + −(nq) is in H as well, since H is
closed under addition and inverses. Because n is the least positive integer in H and r < n,
we must have r = 0. This means h = nq is in nZ. So H ⊆nZ, and hence H = nZ.
For each integer n ≥1, the quotient group Z/nZ gives one way of defining the
commutative group of integers modulo n. Using integer division with remainder, you can
check that Z/nZ consists of the n distinct cosets
0 = 0 + nZ, 1 = 1 + nZ, . . . , n −1 = (n −1) + nZ.
Furthermore, this quotient group is isomorphic to the group Zn = {0, 1, 2, . . . , n −1} with
operation ⊕(addition modulo n), as defined in §1.1. You can check that the quotient group
Z/0Z is isomorphic to Z itself.
Suppose G1, . . . , Gk are commutative groups. Consider the product set
G1 × G2 × · · · × Gk = {(x1, x2, . . . , xk) : xi ∈Gi for all i between 1 and k}.
This product set becomes a commutative group under the operation
(x1, x2, . . . , xk) + (y1, y2, . . . , yk) = (x1 + y1, x2 + y2, . . . , xk + yk)
for all xi, yi ∈Gi,
Finitely Generated Commutative Groups
439
as you can verify. We call the group G1×G2×· · ·×Gk the direct product of G1, . . . , Gk. When
working with commutative groups, the direct product is also written G1 ⊕G2 ⊕· · · ⊕Gk
and called the (external) direct sum of the Gi. If every Gi equals the same group G, we
may write Gk instead of G1 × · · · × Gk.
16.2
Generating Sets for Commutative Groups
Suppose G is a commutative group, x ∈G, and n ∈Z. Define the integer multiple nx by
nx =





x + x + · · · + x
(sum of n copies of x)
if n > 0;
0
(the identity of G)
if n = 0;
−x + −x + · · · + −x
(sum of n copies of −x)
if n < 0.
For all x, y ∈G and all m, n ∈Z, the following rules hold:
(m + n)x = (mx) + (nx),
m(nx) = (mn)x,
1x = x,
−(mx) = (−m)x,
(16.1)
m(x + y) = (mx) + (my).
The first four rules are the Laws of Exponents, translated from multiplicative notation
(involving powers xn) to additive notation (involving multiples nx). These laws hold in
all groups, not just commutative groups. The fifth rule requires commutativity of G. For
positive m, we can justify this rule informally by writing
m(x + y)
=
(x + y) + (x + y) + · · · + (x + y)
(m copies of x + y)
=
(x + x + · · · + x
|
{z
}
m copies of x
) + (y + y + · · · + y
|
{z
}
m copies of y
)
(since G is commutative)
=
(mx) + (my).
A more formal verification of this rule (see Exercise 7) uses induction to establish its validity
for m ≥0, followed by a separate argument for negative m.
Next, suppose v1, . . . , vk are elements of a commutative group G. A Z-linear combination
of these elements is an element of the form
c1v1 + c2v2 + · · · + ckvk =
k
X
i=1
civi,
where each ci is an integer. The set H of all Z-linear combinations of v1, . . . , vk is a subgroup
of G, denoted by ⟨v1, v2, . . . , vk⟩or Zv1 + · · · + Zvk. This follows from the rules in (16.1)
and commutativity of addition in G. For instance, H is closed under addition since
k
X
i=1
civi +
k
X
i=1
divi =
k
X
i=1
(civi + divi) =
k
X
i=1
(ci + di)vi
for all ci, di ∈Z. We call ⟨v1, v2, . . . , vk⟩the subgroup of G generated by v1, . . . , vk. If there
exist finitely many elements v1, . . . , vk that generate G, then G is called a finitely generated
commutative group. If G can be generated by a single element v1, then G is called a cyclic
group. Given any commutative group G and any element x ∈G, the set of multiples
⟨x⟩= {nx : n ∈Z} = Zx is a cyclic subgroup of G.
440
Advanced Linear Algebra
Every finite commutative group G is finitely generated, since we can take all the elements
of G as a generating set. Of course, there are often other generating sets that are much
smaller. The groups Z and Z/nZ (for n ≥1) are finitely generated groups. Indeed, they
are cyclic groups since Z = ⟨1⟩and Z/nZ = ⟨1 + nZ⟩. Generators of cyclic groups are
usually not unique; for instance, Z = ⟨1⟩= ⟨−1⟩. As another example, for each prime p,
Zp is generated by each of its elements other than 0 (Exercise 11). Some finitely generated
groups are not finite groups. For example, Z is generated by 1 element but is infinite. More
examples appear in the next paragraph.
Let G1, . . . , Gk be finitely generated commutative groups, say Gi = ⟨vi,1, . . . , vi,ni⟩for
some integers ni ∈Z>0 and vi,j ∈Gi. Given the product group G = G1 × · · · × Gk, we
can associate with each vi,j ∈Gi the k-tuple (0, 0, . . . , vi,j, . . . , 0) ∈G, where vi,j appears
in position i. You can check that the finite list of all such k-tuples generates G. Thus, the
direct product of finitely many finitely generated groups is finitely generated. For example,
since Z = ⟨1⟩, Zk is generated by the k elements ei = (0, 0, . . . , 1, . . . , 0), where the 1 occurs
in position i. Note that ei is the image in Zk of the generator 1 of the ith copy of Z. For
each k > 0, Zk is an infinite commutative group that is finitely generated.
Not every commutative group is finitely generated. For example, consider the additive
group Q of rational numbers. To prove that Q is not finitely generated, we use proof by
contradiction. Assume that v1, . . . , vk ∈Q is a finite generating set for Q. Write vi = ai/bi
for some integers ai, bi with bi > 0. By finding a common denominator, we can change
notation so that vi = ci/d for some integers ci, d with d > 0. For instance, let d = b1b2 · · · bk
and ci = dai/bi ∈Z. Now consider an arbitrary Z-linear combination of v1, . . . , vk:
x = n1v1 + · · · + nkvk = n1c1 + · · · + nkck
d
where each ni ∈Z.
Since the numerator n1c1 + · · · + nkck is an integer, any such x is either 0 or has absolute
value at least 1/d. But the rational number 1/(2d) cannot be expressed in this form, because
its absolute value is too small.
Although this chapter is concerned mainly with finitely generated commutative groups,
it is possible to define the notion of an infinite generating set. Let S be an arbitrary subset
(possibly infinite) of a commutative group G. By definition, a Z-linear combination of
elements of S is a Z-linear combination of some finite subset S′ of S. We often write
P
x∈S nxx for such a linear combination, understanding that all but a finite number of
the coefficients nx ∈Z must be zero. You can check that the set of Z-linear combinations
of elements of S is a subgroup of G, written ⟨S⟩or P
s∈S Zs. We say S generates G iff
G = ⟨S⟩. For example, using the Fundamental Theorem of Arithmetic, you can verify that
S = {1/pe : p is prime, e ≥1} generates the commutative group Q.
Our main goal in this chapter is to prove the following theorem.
Theorem Classifying Finitely Generated Commutative Groups. Every finitely
generated commutative group G is isomorphic to a direct product of cyclic groups,
specifically G ∼= Zb × Za1 × Za2 × · · · × Zas, where b ≥0, s ≥0, a1 ≥a2 ≥· · · ≥as > 1, and
every ai is a power of a prime. The nonnegative integer b and the prime powers a1, . . . , as
are uniquely determined by G.
16.3
Z-Independence and Z-Bases
A generating set for a commutative group resembles a spanning set for a vector space.
The only difference is that the scalars multiplying the group elements are integers rather
Finitely Generated Commutative Groups
441
than field elements. The analogy to linear algebra motivates the following definitions. Let
(v1, . . . , vk) be a finite list of elements in a commutative group G. We call this list Z-linearly
independent (or Z-independent) iff for all integers c1, . . . , ck,
if c1v1 + c2v2 + · · · + ckvk = 0G, then c1 = c2 = · · · = ck = 0.
This means that no linear combination of v1, . . . , vk produces 0G except the one where
all coefficients are 0. If S is a subset of G (possibly infinite), we say that S is Z-linearly
independent iff every finite nonempty list of distinct elements of S is Z-independent.
An ordered Z-basis of G is a list B = (v1, . . . , vk) of elements vi ∈G such that B is
Z-independent and G = ⟨v1, . . . , vk⟩. A subset S of G (finite or not) is called a Z-basis
of G iff G = ⟨S⟩and S is Z-linearly independent. If G has a basis, then G is called a
free commutative group. If G has a k-element basis for some k ∈Z≥0, then G is called a
finitely generated free commutative group with dimension (or rank) k. We prove later that
the dimension of such a group is uniquely determined.
Not every commutative group is free. For instance, suppose G is a finite group with size
at least 2. Consider any list (v1, . . . , vk) of elements of G. By group theory, there exists an
integer n > 0 with nv1 = 0; for instance, n = |G| has this property (see Exercise 11 in
Chapter 1 for a proof). Then the relation nv1 + 0v2 + · · · + 0vk = 0 shows that the given
list must be Z-linearly dependent. On the other hand, if G = {0} is a 1-element group, then
the empty set is a Z-basis of G. This follows from the convention ⟨∅⟩= {0} and the fact
that the empty set is Z-independent, which is a logical consequence of the definition.
For more interesting examples of bases, consider the group Zk of all k-tuples of integers,
with group operation
(a1, a2, . . . , ak) + (b1, b2, . . . , bk) = (a1 + b1, a2 + b2, . . . , ak + bk)
for ai, bi ∈Z.
For 1 ≤i ≤k, let ei = (0, . . . , 0, 1, 0, . . . , 0), where the 1 occurs in position i. Let us
verify explicitly that the list B = (e1, e2, . . . , ek) is an ordered basis of Zk; B is called the
standard ordered basis of Zk. First, B spans Zk, because for any (a1, . . . , ak) ∈Zk, we have
(a1, . . . , ak) = Pk
i=1 aiei. Second, B is Z-independent, because the relation Pk
i=1 biei = 0
(where bi ∈Z) means that (b1, . . . , bk) = (0, . . . , 0), which implies that each bi is 0 by
equating corresponding components. In summary, we have proved that Zk is a k-dimensional
free commutative group with basis B.
16.4
Elementary Operations on Z-Bases
As in the case of vector spaces, most free commutative groups have many different
bases. There are three elementary operations that produce new ordered bases from given
ordered bases. We now define how each of these operations affects a given ordered Z-basis
B = (v1, . . . , vk) of a finitely generated free commutative group G.
(B1) For any i ̸= j, we can interchange the position of vi and vj in the ordered list B.
Using commutativity, we see (Exercise 28) that the new ordered list still generates G and
is still Z-independent, so this new list is a Z-basis of G.
(B2) For any i, we can replace vi in B by −vi. Since civi = (−ci)(−vi), it readily follows
(Exercise 29) that the new ordered list is still a Z-basis of G.
442
Advanced Linear Algebra
(B3) For any i ̸= j and any integer c, we can replace vi in B by wi = vi +cvj. Let us check
that this gives another Z-basis of G. Using (B1), it suffices to consider the case i = 1 and
j = 2. Write B = (v1, v2, . . . , vk) and B′ = (v1 + cv2, v2, . . . , vk). First, does B′ generate
G? Given any g ∈G, we can write
g = n1v1 + n2v2 + · · · + nkvk
for some ni ∈Z, because B is known to generate G. Manipulating this expression gives
g = n1(v1 + cv2) + (n2 −cn1)v2 + n3v3 + · · · + nkvk
where all coefficients are integers. Thus, g is a linear combination of the elements in the
list B′. Second, is B′ a Z-independent list? Assume that d1, . . . , dk are fixed integers such
that
d1(v1 + cv2) + d2v2 + · · · + dkvk = 0;
we must prove every di = 0. Rewriting the assumption gives
d1v1 + (cd1 + d2)v2 + d3v3 + · · · + dkvk = 0.
By the known Z-independence of B, we conclude that d1 = cd1 + d2 = d3 = · · · = dk = 0.
Then d2 = (cd1 + d2) −cd1 = 0 −c0 = 0. So every di is 0.
For example, starting with the standard ordered basis (e1, e2, e3) of Z3, we can apply
a sequence of elementary operations to produce new ordered bases of this group. As
a specific illustration, you can check that the list (e2 −5e3, −e3, e1 + 2e2 + 3e3) =
((0, 1, −5), (0, 0, −1), (1, 2, 3)) can be obtained from (e1, e2, e3) by appropriate elementary
operations. Hence, this list is an ordered Z-basis of Z3.
We remark that similar elementary operations can be applied to ordered bases of vector
spaces over a field F. In operation (B3), we replace the integer c by a scalar c ∈F. In
operation (B2), we may now select any nonzero scalar c ∈F and replace vi by cvi. You can
check that the corresponding operation for commutative groups only produces a Z-basis
when c = ±1. The reason is that +1 and −1 are the only integers whose multiplicative
inverses are also integers.
16.5
Coordinates and Z-Linear Maps
The next few sections derive some fundamental facts about bases and free commutative
groups. We focus on the case of finitely generated groups, but all the proofs can be extended
to the case of groups with infinite bases.
Let G be a free commutative group with ordered basis B = (v1, . . . , vk). We first prove:
for every g ∈G, there exist unique integers n1, . . . , nk such that g = n1v1 + · · · + nkvk. We
call (n1, . . . , nk) the coordinates of g relative to B. The existence of the integers ni follows
immediately from the fact that the list v1, . . . , vk generates G. To prove uniqueness, suppose
g ∈G and
g = n1v1 + · · · + nkvk = m1v1 + · · · + mkvk
for some ni, mi ∈Z. Subtracting the equations and using the rules in (16.1), we get
(n1 −m1)v1 + · · · + (nk −mk)vk = 0.
Finitely Generated Commutative Groups
443
By Z-independence of B, it follows that ni −mi = 0 for all i. Thus, ni = mi for all i,
proving that the coordinates ni are uniquely determined by g and B.
To continue our study of free commutative groups, we need the idea of Z-linear maps.
A Z-linear map is a group homomorphism T : G →H between two commutative groups
G and H. To say that T is a homomorphism means that T(x + y) = T(x) + T(y) for all
x, y ∈G. It follows by induction that T(nx) = nT(x) for all n ∈Z and all x ∈G. w
The analogy to the condition T(cx) = cT(x) (for linear transformations on vector spaces)
explains the terminology “Z-linear.” Another induction proof shows that any Z-linear map
T : G →H preserves Z-linear combinations:
T
 k
X
i=1
nivi
!
=
k
X
i=1
niT(vi)
for all k > 0, ni ∈Z, vi ∈G.
This continues the analogy to linear transformations between F-vector spaces; such maps
preserve F-linear combinations. The kernel of a Z-linear map T : G →H is ker(T) =
{x ∈G : T(x) = 0H}. The image of T is img(T) = {T(x) : x ∈G}. A Z-linear map
T is injective iff ker(T) = {0G}. A Z-linear map T is surjective iff img(T) = H. The
Fundamental Homomorphism Theorem for Commutative Groups states that any Z-linear
map T : G →H with kernel K and image I induces a Z-linear isomorphism T ′ : G/K →I
given by T ′(x + K) = T(x) for all x ∈G. See §1.7 for a proof of this theorem.
We pause to establish two basic facts about Z-linear maps. Let T : G →H be a Z-linear
map between two commutative groups. Assume H is generated by elements w1, . . . , wm. The
first fact says that the image of T is all of H iff every generator wj is in the image of T. The
forward implication is immediate. Conversely, suppose each wj = T(xj) for some xj ∈G.
Given h ∈H, h can be written (not necessarily uniquely) in the form h = n1w1+· · ·+nmwm
where ni ∈Z. Choosing x = n1x1 + · · · + nmxm ∈G, Z-linearity implies that T(x) = h. To
state the second fact, let S : G →H be another Z-linear map, and assume G is generated by
elements v1, . . . , vk. The second fact says T = S iff T(vi) = S(vi) for all i with 1 ≤i ≤k. In
other words, two Z-linear maps are equal iff they agree on a generating set for the domain.
The forward implication is immediate. To prove the converse, suppose T(vi) = S(vi) for all
i, and let g ∈G. We can write g = p1v1 + · · · + pkvk for some integers pi. Using Z-linearity
and the hypothesis on T and S, we see that
T(g) =
k
X
i=1
piT(vi) =
k
X
i=1
piS(vi) = S(g).
16.6
UMP for Free Commutative Groups
The next theorem states a fundamental property of free commutative groups.
Universal Mapping Property (UMP) for Free Commutative Groups.
Suppose
X = {v1, . . . , vk} is a basis of a free commutative group G. For every commutative group
H and every function f : X →H, there exists a unique Z-linear map Tf : G →H such
that Tf(vi) = f(vi) for all vi ∈X.
We call Tf the Z-linear extension of f from X to G.
Proof. First we prove existence of Tf. To define the value of Tf at a given g ∈G, write g
in the form
g = n1v1 + · · · + nkvk
with all nj ∈Z.
444
Advanced Linear Algebra
We already proved that the integers nj in this expression are uniquely determined by g.
Therefore, the formula
Tf(g) = n1f(v1) + · · · + nkf(vk)
gives a well-defined element of H for each g ∈G. We must check that the function Tf is
a group homomorphism extending f. First, we prove Tf(g + g′) = Tf(g) + Tf(g′) for all
g, g′ ∈G. Given g, g′ ∈G, write g = P
i nivi and g′ = P
i mivi where ni, mi ∈Z. By the
definition of Tf,
Tf(g) = n1f(v1) + · · · + nkf(vk);
Tf(g′) = m1f(v1) + · · · + mkf(vk).
On the other hand, note that g + g′ = P
i(ni + mi)vi is the unique expression for g + g′ as
a linear combination of v1, . . . , vk. Applying the definition of Tf gives
Tf(g + g′) = (n1 + m1)f(v1) + · · · + (nk + mk)f(vk).
Comparing to the previous formulas, we see that Tf(g+g′) = Tf(g)+Tf(g′). Next, we show
Tf extends f. Fix an index j between 1 and k. Observe that vj = 0v1 + · · · + 1vj + · · · + 0vk
is the unique expansion of vj in terms of v1, . . . , vk. Therefore, by definition,
Tf(vj) = 0f(v1) + · · · + 1f(vj) + · · · + 0f(vk) = f(vj).
This completes the existence proof.
To prove uniqueness of Tf, suppose S : G →H is any Z-linear map extending f. Then
Tf(vj) = f(vj) = S(vj) for all j between 1 and k. This says Tf and S agree on a generating
set for G, so Tf = S.
The UMP holds (with an analogous proof) for all free commutative groups, not just
finitely generated ones.
Next, we use the UMP to prove that every k-dimensional free commutative group G
is isomorphic to Zk. More precisely, let B = (v1, . . . , vk) be an ordered basis of G. Define
a function f : {v1, . . . , vk} →Zk by setting f(vi) = ei, where ei is the k-tuple with a
1 in position i and 0s elsewhere. By the UMP, there is a unique Z-linear map T : G →
Zk extending f. Now, T is surjective because the image of T contains the generating set
{e1, . . . , ek} of Zk. To see that T is injective, recall the formula defining T:
T(n1v1 + · · · + nkvk) = n1f(v1) + · · · + nkf(vk) = n1e1 + · · · + nkek
where ni ∈Z.
If T sends x = n1v1 + · · · + nkvk ∈G to 0, then n1 = · · · = nk = 0 by the Z-independence
of e1, . . . , ek. Hence, x = 0, proving that T has kernel {0}. It follows that T is a bijective
Z-linear map, so T is an isomorphism from G to Zk.
16.7
Quotient Groups of Free Commutative Groups
Our goal in this chapter is to classify all finitely generated commutative groups. More
precisely, part of our goal is to show that every finitely generated commutative group is
isomorphic to a direct product of cyclic groups (namely Z or Z/nZ ∼= Zn). We achieved
part of this goal at the end of the last section, by showing that every finitely generated free
commutative group is isomorphic to one of the groups Zk. This fact is analogous to the
linear algebra theorem stating that every finite-dimensional vector space over a field F is
Finitely Generated Commutative Groups
445
isomorphic to F k for some k ≥0. However, in the case of commutative groups, there is more
work to do since not all commutative groups are free. (We must also eventually address the
question of the uniqueness of k.)
The next step toward our goal is to prove that every finitely generated commutative
group is isomorphic to a quotient group F/P, for some finitely generated free commutative
group F and some subgroup P. (This statement remains true, with the same proof, if the two
occurrences of “finitely generated” are deleted.) Let H be a commutative group generated by
w1, . . . , wk. Let Zk be the free commutative group with standard ordered basis (e1, . . . , ek).
Define a map f : {e1, . . . , ek} →H by setting f(ei) = wi for 1 ≤i ≤k. Next, use the
UMP to obtain a Z-linear extension T : Zk →H. The image of T is all of H, since all
the generators wj of H are in the image of T. Let P be the kernel of T. Applying the
Fundamental Homomorphism Theorem to T, we see that T induces a group isomorphism
T ′ : Zk/P →H. Thus, H is isomorphic to a quotient group of a free commutative group
whose dimension is the same as the size of the given generating set for H.
To see why this result helps us in the classification of finitely generated commutative
groups, consider the special case where the subgroup P of Zk has the particular form
P = n1Z × n2Z × · · · × nkZ,
(16.2)
for some integers n1, . . . , nk ≥0. Define a map S : Zk →(Z/n1Z)×· · ·×(Z/nkZ) by setting
S((a1, . . . , ak)) = (a1 + n1Z, . . . , ak + nkZ) for ai ∈Z. You can check that S is a surjective
group homomorphism with kernel P. Hence, by the Fundamental Homomorphism Theorem,
we obtain an isomorphism
H ∼= Zk/P ∼=
Z
n1Z ×
Z
n2Z × · · · ×
Z
nkZ
∼= Zn1 × Zn2 × · · · × Znk.
Thus, in the case where P is a subgroup of the form (16.2), we have succeeded in writing
H as a direct product of cyclic groups.
Unfortunately, when k > 1, not every subgroup of Zk has the form given in (16.2). For
example, when k = 2, consider the subgroup P = {(m, 2m) : m ∈Z} ⊆Z × Z. You can
check that the set P is not of the form A × B for any choice of A, B ⊆Z. In general,
the subgroup structure of the free commutative groups Zk is rich and subtle. A deeper
study of these subgroups is needed to achieve our goal of classifying all finitely generated
commutative groups.
Before continuing our investigation of the subgroups of Zk, let us revisit the analogy to
vector spaces to gain some intuition. The analog (for vector spaces) of the statement “not
every P has the form (16.2)” is the statement “not every subspace of Rk is spanned by a
subset of the standard basis vectors ei.” The analog of the subgroup P = {(n, 2n) : n ∈Z}
is the subspace {(t, 2t) : t ∈R} of R2. This 1-dimensional subspace of R2 is not spanned by
either of the vectors (1, 0) or (0, 1). However, it is spanned by the vector (1, 2). We know
from linear algebra that the one-element set {(1, 2)} can be extended to a basis of R2. This
suggests the possibility of changing the basis of Zk to force the subgroup P to assume the
nice form given in (16.2). We pursue this idea in the next few sections.
16.8
Subgroups of Free Commutative Groups
To begin our more detailed study of subgroups of Zk, we prove a key technical point
about subgroups of free commutative groups. We show that if G is a k-dimensional free
446
Advanced Linear Algebra
commutative group and H is a subgroup of G, then H is also a free commutative group with
dimension at most k. Since we know G is isomorphic to Zk, we can prove this result for the
particular group G = Zk without loss of generality. The proof uses induction on k.
The case k = 0 is immediate. Suppose k = 1. In §16.1, we saw that every subgroup
of Z1 has the form nZ for some integer n ≥0. If n = 0, then 0Z is a zero-dimensional
free commutative group with an empty basis. For n > 0, you can check that nZ ∼= Z is
a 1-dimensional free commutative group with basis {n}. (We observe in passing that, for
n > 1, this basis of the subgroup nZ cannot be extended to a basis of Z. This reveals one
notable difference between free commutative groups and vector spaces.)
For the induction step, fix k > 1 and assume the theorem holds for all free commutative
groups of dimension less than k. Let H be a fixed subgroup of Zk. To apply our induction
hypothesis, we need a subgroup of Zk−1. To obtain such a subgroup, let H′ = H ∩(Zk−1 ×
{0}) be the set of all elements of H with last coordinate 0. Note that H′ is a subgroup of
Zk−1 × {0}, which is a (k −1)-dimensional free commutative group isomorphic to Zk−1.
Applying the induction hypothesis, we conclude that H′ is free and has some ordered basis
(v1, . . . , vm−1) where m −1 ≤k −1. We must somehow pass from this basis to a basis of
the full subgroup H.
Toward this end, consider the projection map P : Zk →Z given by P((a1, . . . , ak)) = ak.
It is routine to check that P is a group homomorphism. Therefore, P[H] = {P(h) : h ∈H}
is a subgroup of Z. By our earlier classification of the subgroups of Z, we know there is some
integer q ≥0 such that P[H] = qZ. Let vm be any fixed element of H such that P(vm) = q.
So vm = (a1, . . . , ak−1, q) ∈H for some integers ai.
Now consider two cases. First, suppose q = 0. Then P[H] = {0}, which means that
every element of H has last coordinate 0. Then H = H′, and we already know that H′ is a
free commutative group of dimension m −1 ≤k −1 < k.
The second case is that q > 0. We claim that X = (v1, . . . , vm) is an ordered basis of H, so
that H is free with dimension m ≤k. To prove the claim, we first check the Z-independence
of X. Suppose c1v1 + · · · + cmvm = 0 for some integers ci. Apply the Z-linear projection
map P to this relation to obtain c1P(v1) + · · · + cm−1P(vm−1) + cmP(vm) = P(0) = 0. For
i < m, P(vi) = 0 since vi ∈H′. On the other hand, P(vm) = q by choice of vm. So the
relation reduces to cmq = 0, which implies cm = 0 because q > 0 and Z has no zero divisors.
However, once we know that cm = 0, the original relation becomes c1v1+· · ·+cm−1vm−1 = 0.
We can now conclude that c1 = · · · = cm−1 = 0 because v1, . . . , vm−1 are already known
to be Z-linearly independent. To finish the proof, we must show that X generates H. Fix
h = (b1, . . . , bk) ∈H, where bi ∈Z. We have P(h) = bk ∈P[H] = qZ, so bk = tq for some
integer t. Note that
h −tvm = (b1, . . . , bk) −t(a1, . . . , ak−1, q) = (b1 −ta1, . . . , bk−1 −tak−1, 0).
So h −tvm ∈H and h −tvm has last coordinate 0, meaning that h −tvm ∈H′. Since
(v1, . . . , vm−1) is known to generate H′, we have h −tvm = d1v1 + · · · + dm−1vm−1 for
certain integers di. Then h = d1v1 + · · · + dm−1vm−1 + tvm, showing that h is a Z-linear
combination of v1, . . . , vm.
More generally, every subgroup of an infinite-dimensional free commutative group is also
free. We omit the proof, which is similar to the preceding proof but requires transfinite
induction.
Finitely Generated Commutative Groups
447
16.9
Z-Linear Maps and Integer Matrices
We have not yet finished our analysis of the subgroups of Zk. However, to complete our
work in this area, we must first develop some machinery for understanding general Z-linear
maps. Eventually, we apply this material to the Z-linear inclusion map of a subgroup into
Zk to gain information about that subgroup.
Suppose T : G →H is a Z-linear map between two finitely generated free commutative
groups. Let X = (v1, . . . , vn) be an ordered basis of G and Y = (w1, . . . , wm) be an ordered
basis of H. We first show how to use the ordered bases X and Y to represent T by an m×n
matrix with integer entries. The following construction is exactly analogous to the procedure
used in linear algebra associating a matrix of scalars with a given linear transformation
between two vector spaces (see Chapter 6).
We first remark that the Z-linear map T is completely determined by its effect on
the generators v1, . . . , vn of G. Thus, to specify T, we need only record the n elements
T(v1), . . . , T(vn). Next, for each j, we know that T(vj) can be expressed uniquely as a
Z-linear combination of w1, . . . , wm. In other words, for all j with 1 ≤j ≤n, we can write
T(vj) =
m
X
i=1
aijwi
(16.3)
for certain uniquely determined integers aij. The m×n matrix A = [aij] is called the matrix
of T relative to the bases X and Y . As long as X and Y are fixed and known, the passage
from T to A is completely reversible. In other words, we could start with the matrix A and
use (16.3) as the definition of T on the generators vj. By the UMP for G, this definition
extends uniquely to a Z-linear map from G to H.
For example, consider the matrix
A =


7
2
−1
5
0
4
0
2
3
3
1
0

.
Take G = Z4, H = Z3, v1 = (1, 0, 0, 0), v2 = (0, 1, 0, 0), v3 = (0, 0, 1, 0), v4 = (0, 0, 0, 1),
w1 = (1, 0, 0), w2 = (0, 1, 0), and w3 = (0, 0, 1), so that X and Y are the standard ordered
bases of Z4 and Z3, respectively. Given this data, we obtain a Z-linear map T : Z4 →Z3
defined on basis elements by
T(v1)
=
7w1 + 0w2 + 3w3 = (7, 0, 3);
T(v2)
=
2w1 + 4w2 + 3w3 = (2, 4, 3);
T(v3)
=
−1w1 + 0w2 + 1w3 = (−1, 0, 1);
T(v4)
=
5w1 + 2w2 + 0w3 = (5, 2, 0).
Note that the jth column of A contains the coordinates of the image of the jth input basis
element relative to the given output basis. We can use Z-linearity to compute explicitly the
image of an arbitrary element (a, b, c, d) ∈Z4. In detail, for any a, b, c, d ∈Z,
T((a, b, c, d))
=
T(av1 + bv2 + cv3 + dv4)
=
aT(v1) + bT(v2) + cT(v3) + dT(v4)
=
a(7, 0, 3) + b(2, 4, 3) + c(−1, 0, 1) + d(5, 2, 0)
=
(7a + 2b −c + 5d, 4b + 2d, 3a + 3b + c).
448
Advanced Linear Algebra
The same answer can be found by computing the following matrix-vector product:


7
2
−1
5
0
4
0
2
3
3
1
0




a
b
c
d

=


7a + 2b −c + 5d
4b + 2d
3a + 3b + c

.
We can interpret matrix addition and matrix multiplication in terms of Z-linear maps.
Returning to the general setup, suppose T, U : G →H are two Z-linear maps. Let A = [aij]
and B = [bij] be the integer matrices representing T and U relative to the ordered bases X
and Y . Then, by definition, we must have
T(vj) =
m
X
i=1
aijwi
for 1 ≤j ≤n;
U(vj) =
m
X
i=1
bijwi
for 1 ≤j ≤n.
The sum of T and U is the map T + U : G →H defined by (T + U)(x) = T(x) + U(x) for
all x ∈G. We see at once that T + U is Z-linear. What is the matrix of T + U relative to X
and Y ? To find the jth column of this matrix, we must find the coordinates of (T + U)(vj)
relative to Y . We find that
(T + U)(vj) = T(vj) + U(vj) =
m
X
i=1
aijwi +
m
X
i=1
bijwi =
m
X
i=1
(aij + bij)wi.
Thus, T +U is represented by the m×n matrix [aij +bij] = A+B. This shows that addition
of matrices corresponds to addition of Z-linear maps.
Products are a bit more subtle. Take T and A as above, and suppose V : H →K
is a Z-linear map from H into a third free commutative group K with ordered basis
Z = (z1, . . . , zp). There are unique integers cik such that
V (wk) =
p
X
i=1
cikzi
for 1 ≤k ≤m;
here, C = [cik] is the p × m matrix of V relative to the ordered bases Y and Z. We know
that the composite map V ◦T : G →K is Z-linear. What is the matrix of this map relative
to the ordered bases X and Z? As before, we discover the answer by applying this map to
a general basis vector vj ∈X. Using Z-linearity of the maps, commutativity of addition,
and the distributive law, we calculate:
(V ◦T)(vj)
=
V (T(vj)) = V
 m
X
k=1
akjwk
!
=
m
X
k=1
akjV (wk) =
m
X
k=1
akj
 p
X
i=1
cikzi
!
=
m
X
k=1
p
X
i=1
akjcikzi =
p
X
i=1
m
X
k=1
cikakjzi
=
p
X
i=1
 m
X
k=1
cikakj
!
zi.
Finitely Generated Commutative Groups
449
So, the ij-entry of the matrix of V ◦T is Pm
k=1 cikakj, which is precisely the ij-entry of the
matrix product CA. We thereby see that CA is the matrix of V ◦T relative to the ordered
bases X and Z. So, matrix multiplication corresponds to composition of Z-linear maps, as
long as the matrices are found using the same ordered basis Y for the middle group H.
16.10
Elementary Operations and Change of Basis
Let T : G →H be a Z-linear map between two finitely generated free commutative groups
G and H. Once we fix an ordered basis X for G and an ordered basis Y for H, we obtain
a unique integer-valued matrix A representing T. A key point is that this matrix depends
on the ordered bases X and Y as well as the map T.
This raises the possibility of changing the matrix of T by replacing X and Y by other
ordered bases. Our eventual goal is to select ordered bases for G and H judiciously, so that
the matrix of T takes an especially simple form. Before pursuing this objective, we need to
understand precisely how modifications of the ordered bases X and Y affect the matrix A.
Recall from §16.4 that there are three elementary operations on ordered bases: (B1)
interchanges the positions of two basis elements; (B2) multiplies a basis element by −1; and
(B3) adds an integer multiple of one basis element to a different basis element. We shall soon
see that these operations on ordered bases correspond to analogous elementary operations
on the rows and columns of A. In particular, we consider the following elementary row
operations on an integer-valued matrix (analogous to the row operations used to solve
linear equations via Gaussian elimination): (R1) interchanges two rows of the matrix;
(R2) multiplies some row of the matrix by −1; (R3) adds an integer multiple of one row to a
different row. There are similar elementary operations (C1), (C2), and (C3) that act on the
columns of the matrix. The critical question is how performing the operations (B1), (B2),
(B3) on Y or X causes an associated row or column operation on the matrix representing
T.
The rules are most readily understood by considering a concrete example. Consider once
again the matrix
A =


7
2
−1
5
0
4
0
2
3
3
1
0

,
which is the matrix of a Z-linear map T : Z4 →Z3 relative to the standard ordered bases
X = (v1, v2, v3, v4) and Y = (w1, w2, w3) of Z4 and Z3. Let us first study the effect of
applying one of the basis operations (B1), (B2), and (B3) to the input basis X.
In the case of (B1), let us find the matrix A1 of T relative to the ordered bases
(v1, v4, v3, v2) and (w1, w2, w3). Here, we have switched the second and fourth basis elements
in X. Since T(v1) = 7w1 + 0w2 + 3w3, the first column of A1 has entries 7, 0, 3. Since
T(v4) = 5w1 + 2w2 + 0w3, the second column of A1 has entries 5, 2, 0. Continuing in this
way, we thereby obtain
A1 =


7
5
−1
2
0
2
0
4
3
0
1
3

.
Note that A1 is obtained from A by interchanging columns 2 and 4. You can check that this
holds in general: if we modify the input basis by switching the vectors in positions i and j,
then the new matrix is obtained by interchanging columns i and j.
450
Advanced Linear Algebra
In the case of (B2), let us find the matrix of T relative to the ordered bases
(v1, v2, −v3, v4) and (w1, w2, w3). Here, we have multiplied the third basis element in X
by −1. Evidently, columns 1, 2, and 4 of the matrix are unchanged by this modification. To
find the new column 3, compute
T(−v3) = −T(v3) = w1 + 0w2 −w3.
So the new matrix is
A2 =


7
2
1
5
0
4
0
2
3
3
−1
0

,
which was obtained from A by multiplying column 3 by −1. This remark holds in general:
if we modify the input basis by negating the vector in position i, then the new matrix is
obtained by negating column i.
In the case of (B3), let us find the matrix of T relative to the ordered bases
(v1, v2, v3 + 2v4, v4) and (w1, w2, w3). As before, columns 1, 2, and 4 of the matrix are
the same as in the original matrix A. To find the new column 3, compute
T(v3 + 2v4) = T(v3) + 2T(v4) = (−w1 + w3) + 2(5w1 + 2w2) = 9w1 + 4w2 + w3.
So the new matrix is
A3 =


7
2
9
5
0
4
4
2
3
3
1
0

,
which was obtained from A by adding two times column 4 to column 3. You can verify that
this holds in general: if we modify the ith input basis vector by adding c times the jth basis
vector to it, then the new matrix is obtained by adding c times column j to column i.
To summarize, performing elementary operations on the input basis causes elementary
column operations on the matrix of T. Next, we show that performing elementary operations
on the output basis causes elementary row operations on the matrix of T.
In the case of (B1), let us find the matrix of T relative to the ordered bases (v1, v2, v3, v4)
and (w1, w3, w2). Taking into account the new ordering of the output basis, we have
T(v1) = 7w1 + 0w2 + 3w3 = 7w1 + 3w3 + 0w2.
So the first column of the new matrix has entries 7, 3, 0 (in this order) instead of 7, 0, 3. The
other columns are affected similarly. So the new matrix is
A4 =


7
2
−1
5
3
3
1
0
0
4
0
2

,
which is obtained from A by interchanging rows 2 and 3. This holds in general: if we modify
the output basis by switching the elements in positions i and j, then the associated matrix
is found by interchanging rows i and j.
In the case of (B2), let us find the matrix of T relative to the ordered bases (v1, v2, v3, v4)
and (−w1, w2, w3). First,
T(v1) = 7w1 + 0w2 + 3w3 = (−7)(−w1) + 0w2 + 3w3,
so the first column of the new matrix has entries −7, 0, 3. Second,
T(v2) = 2w1 + 4w2 + 3w3 = (−2)(−w1) + 4w2 + 3w3,
Finitely Generated Commutative Groups
451
so the second column of the new matrix has entries −2, 4, 3. Continuing similarly, we obtain
the new matrix
A5 =


−7
−2
1
−5
0
4
0
2
3
3
1
0

,
which is obtained from A by multiplying the first row by −1. This holds in general: if we
modify the output basis by negating the ith element, then the associated matrix is found by
negating the ith row.
In the case of (B3), let us find the matrix of T relative to the ordered bases (v1, v2, v3, v4)
and (w1 + 2w3, w2, w3). Compute
T(v1)
=
7w1 + 0w2 + 3w3 = 7(w1 + 2w3) + 0w2 + (3 −7 · 2)w3;
T(v2)
=
2w1 + 4w2 + 3w3 = 2(w1 + 2w3) + 4w2 + (3 −2 · 2)w3;
T(v3)
=
−1w1 + 0w2 + 1w3 = −1(w1 + 2w3) + 0w2 + (1 −(−1) · 2)w3;
T(v4)
=
5w1 + 2w2 + 0w3 = 5(w1 + 2w3) + 2w2 + (0 −5 · 2)w3.
This leads to the new matrix
A6 =


7
2
−1
5
0
4
0
2
−11
−1
3
−10

,
which is obtained from A by adding −2 times row 1 to row 3. This result, which may not be
what you were expecting, generalizes as follows: if we modify the output basis by replacing
wi by wi + cwj, then the associated matrix is found by adding −c times row i to row j.
Here is the proof of the general case, where we take i = 1 and j = 2 solely for notational
convenience. Relative to the original input basis (v1, . . . , vn) and output basis (w1, . . . , wm),
T(vk) = a1kw1 + a2kw2 +
m
X
i=3
aikwi
for 1 ≤k ≤n.
We now replace w1 by w1 + cw2. To maintain equality, we must subtract the term ca1kw2.
Regrouping terms, we get
T(vk) = a1k(w1 + cw2) + (a2k −ca1k)w2 +
m
X
i=3
aikwi.
Thus, for all k, the kth column of the new matrix has entries a1k, a2k −ca1k, a3k, . . . , amk.
So, the new matrix is indeed found by adding −c times row 1 to row 2.
16.11
Reduction Theorem for Integer Matrices
Now that we understand the connection between Z-linear maps and integer matrices, we
prove a theorem describing how much an integer matrix can be simplified by row and column
operations. This theorem helps us uncover structural properties of Z-linear maps between
finitely generated free commutative groups.
452
Advanced Linear Algebra
Reduction Theorem for Integer Matrices. Let A be an m × n matrix with integer
entries. There is a finite sequence of elementary row and column operations that reduces A
to a matrix


a1
0
0
. . .
0
0
a2
0
. . .
0
0
0
a3
. . .
0
...
...

,
(16.4)
where there are r positive integers a1, . . . , ar on the main diagonal, ai divides ai+1 for
1 ≤i < r, and all other entries in the matrix are 0.
By Exercise 100, the matrix (16.4) satisfying the stated conditions is uniquely determined
by A. This reduced matrix is sometimes called the Smith normal form of A.
Proof. We prove the theorem by induction. This proof can be translated into an explicit
recursive algorithm for reducing a given input matrix A to the required form. The base
cases of the induction occur when m = 0 or n = 0 or when every entry of A is 0. In these
cases, A already has the required form, so there is nothing to do.
For the induction step, assume that the reduction theorem is known to hold when the
total number of rows and columns of A is less than m + n. Our strategy in this step is to
transform A to a matrix of the form


a1
0
· · ·
0
0
...
A′
0

,
(16.5)
where a1 > 0 and A′ is an (m −1) × (n −1) integer-valued matrix all of whose entries
are divisible by a1. Assuming that this has been done, we can use the induction hypothesis
to continue to reduce A′ to a matrix with some positive entries a2, . . . , ar on its diagonal,
such that ai divides ai+1 for 2 ≤i < r, and with zeroes elsewhere. The operations used to
reduce A′ do not affect the zeroes in row 1 and column 1 of the overall matrix. Furthermore,
you can check that if an integer a1 divides every entry of a matrix, and if we perform an
elementary row or column operation on that matrix, then a1 still divides every entry of the
new matrix (Exercise 63). So, as we continue to reduce A′, a1 always divides every entry of
all the matrices obtained along the way. In particular, at the end, a1 divides a2, and A has
been reduced to a matrix of the required form.
To summarize, we need only find a way of reducing A to a matrix of the form (16.5).
We assume that A is not a zero matrix, since that situation was handled in the base cases.
There are two possibilities to consider. Case 1: There exists an entry d = aij in A such that
d divides all the entries of A. In this case, switch rows 1 and i, and then switch columns 1
and j, to bring d into the 1, 1-position. Since d divides every other entry in its column, we
can subtract appropriate integer multiples of row 1 from the other rows to produce zeroes
below d in column 1. Similarly, we can use column operations to produce zeroes in row 1
to the right of d. By the remark in the last paragraph, d continues to divide all the entries
in the matrix as we perform these various operations on rows and columns. Finally, we can
multiply row 1 by −1 to make d positive, if necessary. We now have a matrix of the form
(16.5), which completes the proof in this case.
Case 2: There does not exist an entry of A that divides all the entries of A. When
this case occurs, we adopt the following strategy. Let m(A) be the smallest of the integers
|aij| as aij ranges through the nonzero entries of A. We aim to reduce A to a new matrix
A2 such that m(A) > m(A2). We then repeat the whole reduction algorithm on A2. If A2
Finitely Generated Commutative Groups
453
satisfies Case 1, then we can finish reducing as above. On the other hand, if A2 satisfies
Case 2, we reduce A2 to a new matrix A3 such that m(A2) > m(A3). Continuing in this
way, either Case 1 eventually occurs (in which case the reduction succeeds), or Case 2 occurs
indefinitely. But in the latter situation, we have an infinite strictly decreasing sequence of
positive integers
m(A) > m(A2) > m(A3) > · · · ,
which violates the Least Natural Number Axiom for Z>0. So the reduction of A must always
terminate after a finite number of steps.
We still need to explain how to obtain the matrix A2 such that m(A) > m(A2). For
this, let e = aij be a nonzero entry of A with minimum absolute value (so that m(A) = |e|).
By definition of Case 2, there exist entries in the matrix that are not divisible by e. We
consider various subcases.
Case 2a: There exists k ̸= j such that e does not divide aik. In other words, there is
an entry in the same row as e that is not divisible by e. Dividing aik by e, we then have
aik = qe + r where 0 < r < |e|. Subtracting q times column j from column k produces a
matrix A2 with an r in the i, k-position. Now m(A2) ≤r < |e|, so we have achieved our
goal in this case.
Case 2b: There exists k ̸= i such that e does not divide akj. In other words, there is an
entry in the same column as e that is not divisible by e. Dividing akj by e, we then have
akj = qe + r where 0 < r < |e|. Subtracting q times row i from row k produces a matrix A2
with an r in the k, j-position. As before, m(A2) ≤r < |e|, so we have achieved our goal in
this case.
Case 2c: e divides every entry in its row and column, but for some k ̸= i and t ̸= j, e
does not divide akt. So, for some u, v ∈Z, the matrix A looks like this (where we only show
the four relevant entries in rows i, k and columns j, t):


· · ·
e
· · ·
ue
...
...
...
...
ve
· · ·
akt
· · ·


.
Now, add (1 −v) times row i to row k, obtaining:


· · ·
e
· · ·
ue
...
...
...
...
e
· · ·
akt + (1 −v)ue
· · ·


.
The new k, t-entry is not divisible by e, since otherwise e would divide akt. So we can
proceed as in Case 2a, subtracting an appropriate multiple of column j from column t to
get a new k, t-entry r < |e|. The new matrix A2 satisfies m(A2) ≤r < |e|. The case analysis
is finally complete.
An example of the reduction process for a specific integer-valued matrix appears
in §16.14.
454
Advanced Linear Algebra
16.12
Structure of Z-Linear Maps of Free Commutative Groups
We now apply the Reduction Theorem for Integer Matrices to study the structure of Z-linear
maps and finitely generated commutative groups. First, suppose T : G →H is a Z-linear
map between two finitely generated free commutative groups. Start with any ordered basis
X for G and any ordered basis Y for H, and let A be the matrix of T relative to X and Y .
Use row and column operations to reduce A to the form given in the theorem, modifying
the bases X and Y accordingly. At the end, we have a new ordered basis X′ = (x1, . . . , xn)
for G and a new ordered basis Y ′ = (y1, . . . , ym) for H such that the matrix of T relative
to X′ and Y ′ has the form


a1
0
0
. . .
0
0
a2
0
. . .
0
0
0
a3
. . .
0
...

,
where a1, . . . , ar are positive integers such that ai divides ai+1 for 1 ≤i < r. Inspection of
the columns of this matrix tells us how T acts on each input xi. Specifically, T(xi) = aiyi
for 1 ≤i ≤r and T(xi) = 0 for r < i ≤n. We call the matrix displayed above the Smith
normal form for the Z-linear map T; it is uniquely determined by T (cf. Exercise 100).
Compare these results to the corresponding fact about linear transformations of vector
spaces over fields. In that setting, we are allowed to multiply rows and columns by arbitrary
nonzero scalars. The net effect of this extra ability is that we can ensure that each ai in
the final matrix equals 1 (Exercise 66). The reduction process is also easier because we can
use any nonzero scalar to create zeroes in all the other entries in its row and column —
no integer division is needed. So, if T : V →W is a linear map between vector spaces,
there exist ordered bases X = (x1, . . . , xn) for V and Y = (y1, . . . , ym) for W such that
T(xi) = yi for 1 ≤i ≤r, and T(xi) = 0 for r < i ≤n. The number r is called the rank of
the linear map T. We can also reach this result without reducing any matrices. Start with
a basis (xr+1, . . . , xn) for the null space of T, and extend it to a basis of (x1, . . . , xn) of
V . You can check that the list T(x1), . . . , T(xr) is linearly independent, so this list can be
extended to a basis of the target space Y . We used this argument to prove the Rank–Nullity
Theorem in §1.8.
16.13
Structure of Finitely Generated Commutative Groups
Returning to commutative groups, we are now ready to prove the existence part of the
Theorem Classifying Finitely Generated Commutative Groups. Suppose H is a commutative
group generated by m elements. We have seen (§16.7) that H is isomorphic to a quotient
group Zm/P, where P is some subgroup of the free commutative group Zm. We have also
seen (§16.8) that the subgroup P must also be free, with a basis of size n ≤m.
Consider the inclusion map I : P →Zm, given by I(x) = x for x ∈P. I is certainly
Z-linear, and it is a map between two finitely generated free commutative groups. So our
structural result for such maps can be applied. We see, therefore, that there is a basis
(x1, . . . , xn) for P, a basis (y1, . . . , ym) of Zm, an integer r ≥0, and positive integers
a1, . . . , ar with ai dividing ai+1 for 1 ≤i < r, such that I(xi) = aiyi for 1 ≤i ≤r, and
Finitely Generated Commutative Groups
455
I(xi) = 0 for r < i ≤n. Since I is an inclusion map, this says that xi = aiyi for 1 ≤i ≤r,
and xi = 0 for r < i ≤n. But basis elements are never 0 (by Z-independence), so we
deduce that r = n ≤m. To summarize, H ∼= Zm/P, where Zm has some ordered basis
(y1, . . . , yn, . . . , ym) and P has an ordered basis (a1y1, . . . , anyn).
We now apply an isomorphism of Zm with itself that forces the subgroup P to assume
the special form given in (16.2). Consider the function f : {y1, . . . , ym} →Zm such that
f(yi) = ei (the standard basis vector) for 1 ≤i ≤m. By the UMP for free commutative
groups, f extends to a Z-linear map T : Zm →Zm. As seen at the end of §16.6, T is an
isomorphism. The isomorphism T maps P to a new subgroup P1 of Zm with ordered basis
(a1e1, . . . , anen). You can check that T induces an isomorphism from the quotient group
Zm/P to the quotient group Zm/P1. Now, we can write P1 as the Cartesian product
a1Z × a2Z × · · · × anZ × 0Z × · · · × 0Z,
where there are m −n factors equal to {0}. Applying the Fundamental Homomorphism
Theorem (see the discussion below (16.2)), we conclude that
H ∼= Zm/P ∼= Zm/P1 ∼= Za1 ⊕· · · ⊕Zan ⊕Zm−n.
Thus, every finitely generated commutative group is isomorphic to a direct sum of finitely
many cyclic groups, where the sizes of the finite cyclic summands (if any) successively divide
each other. Note that if some of the ai are equal to 1, we can omit these factors from the
product, since Z1 is the one-element group.
We now derive another version of this result, obtained by splitting apart the cyclic
groups Zai based on the prime factorizations of each ai. First we need a group-theoretic
lemma. Suppose a > 1 is an integer with prime factorization a = pe1
1 pe2
2 · · · pes
s , where each
ei ≥1 and p1, . . . , ps are distinct primes. We claim that Za ∼= Zpe1
1 ⊕Zpe2
2 ⊕· · · ⊕Zpes
s , or
equivalently,
Z/aZ ∼= (Z/pe1
1 Z) ⊕(Z/pe2
2 Z) ⊕· · · ⊕(Z/pes
s Z).
(16.6)
To prove this, call the product group on the right side K, and consider the map sending
the integer 1 to the s-tuple of cosets (1 + pe1
1 Z, . . . , 1 + pes
s Z) ∈K. Since Z is free with basis
{1}, the UMP furnishes a Z-linear extension T : Z →K such that
T(n) = nT(1) = (n + pe1
1 Z, . . . , n + pes
s Z)
for all n ∈Z.
Let us find the kernel of T. By the formula just written, n belongs to the kernel iff all cosets
n + pei
i Z are zero iff n ∈pei
i Z for all i iff pei
i
divides n for all i iff lcm(pei
i
: 1 ≤i ≤s)
divides n iff a = pe1
1 · · · pes
s divides n (since p1, . . . , ps are distinct primes). In other words,
the kernel of T is aZ. By the Fundamental Homomorphism Theorem, the quotient group
Z/aZ ∼= Za is isomorphic to the image of T in K. But the size of the product group K is
pe1
1 pe2
2 · · · pes
s
= a, which is the same size as Z/aZ. It follows that the isomorphic copy of
Z/aZ in K must be all of K, completing the proof of the claim.
Apply this result to each of the integers ai in the preceding decomposition of H. We
conclude that every finitely generated commutative group is isomorphic to a direct sum of
finitely many cyclic groups, each of which is either infinite or has size equal to a prime
power. This concludes the proof of the existence part of the Theorem Classifying Finitely
Generated Commutative Groups. We must still address the question of the uniqueness of
the two decompositions of H found above. We consider this issue shortly, but first we give a
concrete example illustrating the reduction algorithm and the ideas in the proofs just given.
456
Advanced Linear Algebra
16.14
Example of the Reduction Algorithm
Let P be the subgroup of Z4 generated by v1 = (10, 0, −8, 4), v2 = (12, 6, −6, −6), and
v3 = (20, 48, 14, −82). We use the ideas in the last few sections to determine the structure
of the quotient group H = Z4/P.
You can verify that v1, v2, v3 are Z-independent. (This also follows from the calculations
below; see Exercise 69.) So we can consider the matrix of the inclusion map I : P →Z4
relative to the ordered basis X = (v1, v2, v3) of P and the standard ordered basis Y =
(e1, e2, e3, e4) of Z4. The jth column of this matrix gives the coordinates of vj relative to
the standard ordered basis, so the matrix is
A =


10
12
20
0
6
48
−8
−6
14
4
−6
−82

.
We proceed to reduce this matrix. Inspection reveals that no entry of A divides every other
entry. So our first goal (following the proof of Case 2 of the reduction theorem) is to reduce
the magnitude of the smallest nonzero entry of A. This entry is 4, which fails to divide the
entry 10 in its column. As prescribed by Case 2b of the reduction proof, we replace row 1
by row 1 minus 2 times row 4, obtaining
A1 =


2
24
184
0
6
48
−8
−6
14
4
−6
−82

.
This is the matrix of I relative to the bases (v1, v2, v3) and (e1, e2, e3, e4 + 2e1).
The new 1, 1-entry, namely 2, does divide every entry of the matrix. So we are in Case 1
of the reduction proof. First, we use two column operations to produce zero entries in the
rest of row 1:
A2 =


2
0
0
0
6
48
−8
90
750
4
−54
−450

.
This is the matrix of I relative to the bases (v1, v2−12v1, v3−92v1) and (e1, e2, e3, e4+2e1).
Second, we use two row operations to produce zero entries in the rest of column 1:
A3 =


2
0
0
0
6
48
0
90
750
0
−54
−450

.
This is the matrix of I relative to the input basis (v1, v2 −12v1, v3 −92v1) and output basis
(e1 −4e3 + 2(e4 + 2e1), e2, e3, e4 + 2e1) = (5e1 −4e3 + 2e4, e2, e3, e4 + 2e1).
Now, we proceed to reduce the 3×2 submatrix in the lower-right corner. The upper-left
entry of this submatrix (namely 6) already divides every other entry of this submatrix.
Adding −8 times column 2 to column 3 gives
A4 =


2
0
0
0
6
0
0
90
30
0
−54
−18

;
Finitely Generated Commutative Groups
457
the new bases are (v1, v2−12v1, v3−92v1−8(v2−12v1)) and (5e1−4e3+2e4, e2, e3, e4+2e1).
Next, two row operations produce
A5 =


2
0
0
0
6
0
0
0
30
0
0
−18

;
the new bases are (v1, v2 −12v1, 4v1 −8v2 + v3) and
(5e1 −4e3 + 2e4, e2 + 15e3 −9(e4 + 2e1), e3, e4 + 2e1).
To continue, we must reduce the 2 × 1 submatrix with entries 30 and −18. We are in
Case 2b again. Adding 2 times row 4 to row 3 gives
A6 =


2
0
0
0
6
0
0
0
−6
0
0
−18

;
the new bases are (v1, v2 −12v1, 4v1 −8v2 + v3) and
(5e1 −4e3 + 2e4, −18e1 + e2 + 15e3 −9e4, e3, e4 + 2e1 −2e3).
Next, multiply row 3 by −1, obtaining the matrix
A7 =


2
0
0
0
6
0
0
0
6
0
0
−18


and bases (v1, v2 −12v1, 4v1 −8v2 + v3) and
(5e1 −4e3 + 2e4, −18e1 + e2 + 15e3 −9e4, −e3, e4 + 2e1 −2e3).
Finally, add 3 times row 3 to row 4 to get the reduced matrix
A8 =


2
0
0
0
6
0
0
0
6
0
0
0


and bases (v1, v2 −12v1, 4v1 −8v2 + v3) and
(5e1 −4e3 + 2e4, −18e1 + e2 + 15e3 −9e4, −e3 −3(e4 + 2e1 −2e3), e4 + 2e1 −2e3).
To check our work, note that the final ordered input basis for P is (x1, x2, x3), where:
x1
=
v1 = (10, 0, −8, 4);
x2
=
v2 −12v1 = (−108, 6, 90, −54);
x3
=
4v1 −8v2 + v3 = (−36, 0, 30, −18);
and the final ordered output basis for Z4 is (y1, y2, y3, y4), where:
y1
=
5e1 −4e3 + 2e4 = (5, 0, −4, 2);
y2
=
−18e1 + e2 + 15e3 −9e4 = (−18, 1, 15, −9);
y3
=
−6e1 + 5e3 −3e4 = (−6, 0, 5, −3);
y4
=
2e1 −2e3 + e4 = (2, 0, −2, 1).
458
Advanced Linear Algebra
As predicted by the proof, we have x1 = 2y1, x2 = 6y2 and x3 = 6y3. Therefore,
H ∼= Z4
P
∼=
Z × Z × Z × Z
2Z × 6Z × 6Z × 0Z
∼= Z2 ⊕Z6 ⊕Z6 ⊕Z.
Using the prime factorization 6 = 2 · 3, we also have
H ∼= Z2 ⊕Z2 ⊕Z2 ⊕Z3 ⊕Z3 ⊕Z.
16.15
Some Special Subgroups
To finish our classification of finitely generated commutative groups, we prove the following
two uniqueness theorems.
Theorem on Uniqueness of Invariant Factors. Suppose G is a commutative group
such that
Zb ⊕Za1 ⊕· · · ⊕Zar ∼= G ∼= Zd ⊕Zc1 ⊕· · · ⊕Zct,
(16.7)
where b, d, r, t ∈Z≥0, all ai and cj are in Z>1, ai divides ai+1 for 1 ≤i < r, and cj divides
cj+1 for 1 ≤j < t. Then b = d and r = t and ai = ci for 1 ≤i ≤r.
We call b the Betti number of G, and we call a1, . . . , ar the invariant factors of G. The
theorem says these integers are isomorphism invariants of G.
Theorem on Uniqueness of Elementary Divisors. Suppose G is a commutative group
such that
Zb ⊕Za1 ⊕· · · ⊕Zar ∼= G ∼= Zd ⊕Zc1 ⊕· · · ⊕Zct,
(16.8)
where b, d, r, t ∈Z≥0, a1 ≥a2 ≥· · · ≥ar > 1, c1 ≥c2 ≥· · · ≥ct > 1, and every ai and
every cj is a prime power. Then b = d and r = t and ai = ci for 1 ≤i ≤r.
The integer b is the Betti number of G, and the prime powers a1, . . . , ar are called
elementary divisors of G. The theorem says these integers are isomorphism invariants of G.
We prove these results in stages, first considering the cases where r = t = 0 (which
means G is free) and where b = d = 0 (which means G is finite). To aid our proofs, we must
first introduce some special subgroups of G that are invariant under group isomorphisms.
Let G be an arbitrary commutative group and n be any fixed integer. Consider the map
Mn : G →G given by Mn(g) = ng for g ∈G. Because G is commutative, Mn is a group
homomorphism: for each g, h ∈G,
Mn(g + h) = n(g + h) = ng + nh = Mn(g) + Mn(h).
(This result need not hold for non-commutative groups.) Define nG = {ng : g ∈G} and
G[n] = {g ∈G : ng = 0}. The sets nG and G[n] are subgroups of G, since nG is the image
of the homomorphism Mn, and G[n] is the kernel of Mn.
We assert that these subgroups are preserved by group isomorphisms. More precisely,
suppose f : G →H is a group isomorphism. We claim the restriction of f to G[n] ⊆G is a
bijection of G[n] onto H[n]. To prove this, suppose x ∈G[n]. Then nx = 0, so f(nx) = 0,
so nf(x) = 0, so f(x) ∈H[n]. So the restriction of f to the domain G[n] does map
into the codomain H[n]. Applying the same argument to the inverse isomorphism f −1, we
see that f −1 maps H[n] into G[n]. Hence, G[n] ∼= H[n] via the restriction of f to this
domain and codomain. Similar reasoning proves that f restricts to a group isomorphism
Finitely Generated Commutative Groups
459
nG ∼= nH. Here, the key point is that for x ∈nG, we have x = ng for some g ∈G, hence
f(x) = f(ng) = nf(g) where f(g) ∈H, hence f(x) ∈nH. So f maps nG into nH, and
likewise f −1 maps nH into nG. We can conclude that f induces isomorphisms of quotient
groups G/G[n] ∼= H/H[n] and G/nG ∼= H/nH. For instance, the first of these isomorphisms
follows by applying the Fundamental Homomorphism Theorem to the homomorphism from
G to H/H[n] sending g ∈G to f(g)+H[n], which is a surjective homomorphism with kernel
G[n].
Another special subgroup of G is the set tor(G) = S
n≥1 G[n], which consists of all
elements of G of finite order: g ∈tor(G) iff g ∈G and ng = 0 for some n ∈Z>0. We
call tor(G) the torsion subgroup of G. To see that tor(G) is a subgroup, first note that
0G ∈tor(G) since 1(0G) = 0. Given g ∈tor(G), we know ng = 0 for some n ∈Z>0.
Then n(−g) = −(ng) = −0 = 0, so −g ∈tor(G). To check closure under addition, suppose
g, h ∈tor(G), so that ng = 0 = mh for some n, m > 0. Because G is commutative,
nm(g + h) = nm(g) + nm(h) = m(ng) + n(mh) = m0 + n0 = 0.
So g + h ∈tor(G), and tor(G) is indeed a subgroup. (This result is false in some infinite
non-commutative groups.) As with the previous subgroups, the torsion subgroup is an
isomorphism invariant: if f : G →H is an isomorphism, then f restricts to an isomorphism
tor(G) ∼= tor(H). So f induces an isomorphism of quotient groups G/ tor(G) ∼= H/ tor(H).
16.16
Uniqueness Proof: Free Case
To see how the subgroups introduced in the preceding section can be relevant, let us prove
the uniqueness theorem for finitely generated free commutative groups. Suppose G is a
free commutative group with an n-element basis X and an m-element basis Y . We prove
that m = n. This shows that the dimension (Betti number) of a finitely generated free
commutative group is well-defined. The assumptions on G imply that G ∼= Zn and G ∼= Zm
(§16.6). Combining these isomorphisms gives an isomorphism f : Zn →Zm. We show that
the existence of f forces n = m.
We know from the previous section that the isomorphism f induces an isomorphism
f ′ : Zn/2Zn →Zm/2Zm. Now, 2Zn = {2v : v ∈Zn} = {2(a1, . . . , an) : ai ∈Z} =
{(2a1, . . . , 2an) : ai ∈Z} = 2Z ⊕2Z ⊕· · · ⊕2Z. This is a subgroup of Zn of the form (16.2).
So, as shown below (16.2),
Zn
2Zn =
Z ⊕· · · ⊕Z
2Z ⊕· · · ⊕2Z
∼= Z
2Z ⊕· · · ⊕Z
2Z
∼= Zn
2.
Similarly, Zm/2Zm ∼= Zm
2 . So we obtain an isomorphism Zn
2 ∼= Zm
2 . Now, the product group
Zn
2 has 2n elements, while Zm
2 has 2m elements. Since isomorphic groups have the same size,
we deduce 2n = 2m, which in turn implies n = m.
An analogous result holds in linear algebra: any two bases of a vector space have the
same cardinality (see Chapter 17 for a proof). In particular, if the real vector spaces Rn
and Rm are isomorphic, then m = n. However, we cannot prove this result by the counting
argument used above. Remarkably, the commutative groups Rn and Rm are isomorphic
for all positive integers m and n (see Exercise 101). Intuition suggests that the topological
spaces Rn and Rm should be homeomorphic iff m = n. This is true, but it is quite difficult
to prove when n, m > 1. We need the tools of algebraic topology to establish that Rn and
Rm are not homeomorphic when m ̸= n. See the texts of Munkres [46] or Rotman [53] for
460
Advanced Linear Algebra
details. We remark that algebraic topology makes heavy use of the classification theorems
for commutative groups proved in this chapter.
16.17
Uniqueness Proof: Prime Power Case
As the next step in the uniqueness proof, we prove the following result about commutative
groups whose size is a prime power. Suppose p is prime, and there are integers a1 ≥a2 ≥
· · · ≥ar > 0 and c1 ≥c2 ≥· · · ≥ct > 0 such that
Zpa1 ⊕Zpa2 ⊕· · · ⊕Zpar ∼= Zpc1 ⊕Zpc2 ⊕· · · ⊕Zpct.
Then r = t and ai = ci for 1 ≤i ≤r.
The proof is greatly clarified by introducing the notion of an integer partition (cf.
Chapter 8). A partition of an integer n is a weakly decreasing sequence (a1 ≥a2 ≥· · · ≥ar)
of positive integers that sum to n. For example, the seven partitions of n = 5 are:
(5),
(4, 1),
(3, 2),
(3, 1, 1),
(2, 2, 1),
(2, 1, 1, 1),
(1, 1, 1, 1, 1).
We can visualize a partition by drawing a collection of n squares such that there are ai
squares in row i. This picture is called the diagram of the partition. For example, the
diagrams of the seven partitions of 5 are shown here:
Each partition encodes a possible commutative group of size pn (where p is any fixed prime).
For example, the seven partitions above correspond to the commutative groups listed here:
Zp5,
Zp4 ⊕Zp,
Zp3 ⊕Zp2,
Zp3 ⊕Zp ⊕Zp,
Zp2 ⊕Zp2 ⊕Zp,
Zp2 ⊕Zp ⊕Zp ⊕Zp,
Zp ⊕Zp ⊕Zp ⊕Zp ⊕Zp.
The existence part of the classification theorem says that every commutative group of size
p5 is isomorphic to one of these seven groups. The uniqueness part of the theorem (to be
proved momentarily) says that no two of these seven groups are isomorphic.
Before beginning the proof, let us see how to use partition diagrams to gain algebraic
information about the associated commutative group. For definiteness, let us consider the
commutative group
G = Z74 ⊕Z74 ⊕Z72 ⊕Z72 ⊕Z72 ⊕Z7,
which corresponds to p = 7 and the partition µ = (4, 4, 2, 2, 2, 1). A typical element of G is
a 6-tuple
x = (n1, n2, n3, n4, n5, n6)
where 0 ≤n1 < 74, 0 ≤n2 < 74, 0 ≤n3 < 72, 0 ≤n4 < 72, 0 ≤n5 < 72, and 0 ≤n6 < 7.
Suppose we write n1 as a 4-digit number in base 7:
n1 = d373 + d272 + d171 + d070
with 0 ≤di < 7.
Finitely Generated Commutative Groups
461
Writing similar expressions for n2, . . . , n6, we can represent the group element x by filling
the squares of the partition diagram of µ with arbitrary digits in the range {0, 1, 2, 3, 4, 5, 6}.
For example, the element x = (3600, 0250, 55, 41, 30, 0) (where we write all entries of x in
base 7 with leading zero digits, if needed) is represented by the following filled diagram:
x =
3
6
0
0
0
2
5
0
5
5
4
1
3
0
0
.
To multiply an integer written in base 7 by 7, we shift all the digits one place left and
append a zero. In the example above, this multiplication produces
(36000, 02500, 550, 410, 300, 00).
However, to obtain 7x in the group G, we must now reduce the first two entries modulo
74, the next three entries modulo 72, and the last entry modulo 7. The effect of all these
reductions is to make the leading digit of each entry disappear, leaving us with
7x = (6000, 2500, 50, 10, 00, 0).
The associated filled diagram is
7x =
6
0
0
0
2
5
0
0
5
0
1
0
0
0
0
.
This diagram arises from the diagram for x by shifting the entries of each row one step
left, erasing the entries that fall off the left end and bringing in zeroes at the right end.
Similarly, we can compute 72x from the original filled diagram for x by shifting all the digits
two places to the left, filling in zeroes on the right side:
72x =
0
0
0
0
5
0
0
0
0
0
0
0
0
0
0
.
Inspection of the diagram shows that 73x = 0, since the only nonzero digits in the diagram
for x occur in the first three columns. More generally, for any y ∈G, 7iy = 0 iff all the
nonzero digits in the filled diagram for y occur in the first i columns of µ. This observation
462
Advanced Linear Algebra
allows us to determine the size of the various subgroups G[7i] = {y ∈G : 7iy = 0}. For
example, how large is G[72]? Consider the following schematic diagram:
⋆
⋆
0
0
⋆
⋆
0
0
⋆
⋆
⋆
⋆
⋆
⋆
⋆
.
By the previous observation, we obtain a typical element of G[72] by choosing an arbitrary
base-7 digit for each of the starred positions (possibly zero), and filling in the remaining
squares of the diagram with zeroes. There are seven choices for each star, leading to the
conclusion that
|G[72]| = 711.
These observations generalize to arbitrary diagrams. Suppose p is prime, n is a positive
integer, µ = (a1 ≥a2 ≥· · · ≥ar) is a partition of n, and G = Zpa1 ⊕· · · ⊕Zpar . Let
a′
1 ≥a′
2 ≥· · · ≥a′
s denote the number of squares in each column of the diagram of µ,
reading from left to right. As a convention, set ak = 0 for k > r and a′
k = 0 for k > s. As in
the example, for each j ∈Z≥0, we obtain a typical element of G[pj] by filling the squares
in the first j columns with arbitrary base-p digits, and filling the remaining squares with
zeroes. By the Product Rule from combinatorics, we see that
|G[pj]| = pa′
1+a′
2+···+a′
j
for all j ∈Z≥0.
We finally have the tools needed to attack the uniqueness proof for commutative groups
of prime power size. Fix a prime p, and assume
G = Zpa1 ⊕· · · ⊕Zpar ∼= H = Zpc1 ⊕· · · ⊕Zpct,
where a1 ≥· · · ≥ar > 0 and c1 ≥· · · ≥ct > 0. We need to show that the two partitions
(ai : i ≥1) and (ci : i ≥1) are the same. Let (a′
j : j ≥1) and (c′
j : j ≥1) be the column
lengths of the associated partition diagrams. To show that the row lengths ai and ci are
the same for all i, it suffices to show that the column lengths a′
j and c′
j are the same for all
j. Equivalently, it suffices to show that a′
1 + · · · + a′
j = c′
1 + · · · + c′
j for all j ≥1. To prove
these equalities, note on the one hand that
|G[pj]| = pa′
1+···+a′
j,
while on the other hand
|H[pj]| = pc′
1+···+c′
j.
Now G ∼= H implies that G[pj] ∼= H[pj], so that
pa′
1+···+a′
j = pc′
1+···+c′
j.
Since p > 1, this finally leads to a′
1 + · · · + a′
j = c′
1 + · · · + c′
j for all j, completing the proof.
Finitely Generated Commutative Groups
463
16.18
Uniqueness of Elementary Divisors
The next step is to prove the uniqueness theorems for finite commutative groups. Suppose
Za1 ⊕· · · ⊕Zar ∼= G ∼= Zc1 ⊕· · · ⊕Zct,
(16.9)
where a1 ≥· · · ≥ar > 1 are prime powers and c1 ≥· · · ≥ct > 1 are prime powers. We
must prove r = t and ai = ci for 1 ≤i ≤r.
The idea here is to somehow isolate those ai and cj that are powers of a given prime p,
so that we can apply the uniqueness result from the previous section. To see how this can
be done, suppose p and q are distinct primes and d, e ≥1. For d ≤e, every element x of
the cyclic group Zpd satisfies pex = pe−d(pdx) = 0, so Zpd[pe] = Zpd. On the other hand,
for arbitrary d, Zqd[pe] = {0}. To see why, suppose y ∈Zqd satisfies pey = 0. Then qd must
divide the product pey computed in Z, so qd divides y (since p and q are distinct primes).
As 0 ≤y < qd, it follows that y = 0. Next, you can check that for any integer b,
(G1 ⊕· · · ⊕Gk)[b] = (G1[b]) ⊕· · · ⊕(Gk[b]).
Now assume the situation in (16.9). Let p be any fixed prime and e be the highest
power of p dividing |G|. Compute G[pe] in two ways, using the two isomorphic versions of
G as direct sums of cyclic groups appearing in (16.9). On one hand, by the remarks in the
previous paragraph, we obtain a direct sum of the form H1 ⊕· · · ⊕Hr, where Hi = Zai if ai
is a power of p, and Hi = {0} if ai is a power of another prime. Deleting zero factors and
rearranging, we see that this direct sum is isomorphic to the direct sum involving precisely
those ai that are powers of p, arranged in decreasing order. On the other hand, applying
the same reasoning to the other representation of G, we see that G[pe] is also isomorphic to
the direct sum involving those cj that are powers of p, arranged in decreasing order. Since
G[pe] is an isomorphism invariant, the two direct sums just mentioned are isomorphic. By
the results in the previous section, those ai and cj that are powers of the given prime p
must be the same (counting multiplicities). Since p was arbitrary and every ai and cj is a
power of some prime, we see that all the ai and cj must match.
16.19
Uniqueness of Invariant Factors
We can use the last result to deduce the other uniqueness theorem for finite commutative
groups. Suppose now that
Za1 ⊕· · · ⊕Zar ∼= G ∼= Zc1 ⊕· · · ⊕Zct
(16.10)
where the ai and cj are integers larger than 1 such that ai divides ai+1 and cj divides cj+1
for 1 ≤i < r and 1 ≤j < t. Our goal is to prove that r = t and ai = ci for all i. Recall
(§16.13) that we can split and merge cyclic groups based on their prime factorizations, using
the isomorphism
Zpe1
1 ···pes
s ∼= Zpe1
1 ⊕· · · ⊕Zpes
s
(for distinct primes pk).
We use this fact to split each of the cyclic groups of size ai and cj in (16.10) into direct
sums of cyclic groups of prime power size. Let |G| have prime factorization pe1
1 · · · pes
s . Write
464
Advanced Linear Algebra
ai = Qs
k=1 pfki
k
and cj = Qs
k=1 pgkj
k
for some integers fki, gkj ≥0. Then
s
M
k=1
r
M
i=1
Zp
fki
k
∼= G ∼=
s
M
k=1
t
M
j=1
Zp
gkj
k
.
By the previously proved uniqueness result, for each fixed k between 1 and s, the list of
numbers pfki
k
with fki > 0 (with multiplicities) equals the list of numbers pgkj
k
with gkj > 0
(with multiplicities). So it suffices to show that the list a1, . . . , ar is uniquely determined
by the list of powers pfki
k
together with the divisibility conditions linking successive ai (and
similarly, the list c1, . . . , ct is uniquely determined by the list of pgkj
k ).
We describe an algorithm for reconstructing a1, . . . , ar from the list of all prime powers
pfki
k
that exceed 1. Construct an s × r matrix whose kth row contains the powers pfki
k
arranged in increasing order, padded on the left with 1s so that there are exactly r entries
in the row. Note that s is known, being the number of distinct prime factors of |G|.
Furthermore, letting nk be the number of positive powers of pk that occur in the given
list, r can be calculated as the maximum of n1, . . . , ns. This follows since a1 > 1, which
means that at least one prime pk divides all r integers a1, . . . , ar. Every prime power in the
matrix arises by splitting the prime factorization of some ai into prime powers. Taking the
divisibility relations among the ai into account, it follows that ar must be the product of
the largest possible power of each prime. So, ar is the product of the prime powers in the
rth (rightmost) column of the matrix. Proceeding inductively from right to left, it follows
similarly that ar−1 must be the product of the prime powers in column r −1 of the matrix,
and so on. Thus we can recover ar, ar−1, . . . , a1 uniquely from the given matrix of prime
powers.
The following example of the reconstruction algorithm may clarify the preceding
argument. Suppose we are given the list of prime powers:
[2, 24, 24, 24, 27, 32, 32, 32, 7, 7, 75, 78].
We see that s = 3 and r = 5; the matrix of prime powers is


2
24
24
24
27
1
1
32
32
32
1
7
7
75
78

.
Multiplying the entries in each column, we get
a1 = 2,
a2 = 24 · 7,
a3 = 24 · 32 · 7,
a4 = 24 · 32 · 75,
a5 = 27 · 32 · 78.
Evidently, we do have a1|a2, a2|a3, a3|a4, a4|a5, and splitting each ai into prime power
divisors does produce the given list of prime powers. Furthermore, you can check in this
example that the choice of a5 (then a4, etc.) really is forced by these conditions.
16.20
Uniqueness Proof: General Case
To finish proving the uniqueness assertion of the classification theorem in full generality, we
use torsion subgroups to separate the torsion part and the free part of a finitely generated
commutative group. More precisely, consider a group G = Zb × H, where b ≥0 and H
Finitely Generated Commutative Groups
465
is a (possibly empty) direct sum of finitely many finite cyclic groups. You can check that
tor(G) = {0} × H ∼= H, from which it follows that
G/ tor(G) = Zb × H
{0} × H
∼= Zb
{0} × H
H
∼= Zb.
Let us begin the uniqueness proof. Suppose G1 = Zb ×H is isomorphic to G2 = Zd ×K,
where H = Za1 × · · · × Zar, K = Zc1 × · · · × Zct, b, d, r, t ∈Z≥0, every ai and cj is in
Z>1, and either (i) ai divides ai+1 and ci divides ci+1 for all applicable i, or (ii) both lists
a1, . . . , ar and c1, . . . , ct consist of prime powers in weakly decreasing order. We must show
b = d and r = t and ai = ci for all i.
Since G1 ∼= G2, we know tor(G1) ∼= tor(G2) and G1/ tor(G1) ∼= G2/ tor(G2) (§16.15).
Using the preceding remarks, this means that H ∼= K and Zb ∼= Zd. The isomorphism
between H and K guarantees that r = t and ai = ci for all i (as shown in §16.18 and
§16.19). The isomorphism between Zb and Zd guarantees that b = d (by §16.16). This
completes the proof of the Theorem Classifying Finitely Generated Commutative Groups.
16.21
Summary
We now review the facts about commutative groups established in this chapter.
1.
Definitions. A commutative group is a set G closed under a commutative,
associative binary operation (written as addition) that has an identity element
and additive inverses. A map T : G →H between commutative groups is Z-
linear iff T(x + y) = T(x) + T(y) for all x, y ∈G, which automatically implies
T(nx) = nT(x) for all x ∈G and n ∈Z. A Z-linear combination of elements
v1, . . . , vk ∈G is an element of the form n1v1 + · · · + nkvk where n1, . . . , nk are
integers. G is generated by v1, . . . , vk iff every g ∈G is a Z-linear combination
of v1, . . . , vk. The list v1, . . . , vk is Z-independent iff for every n1, . . . , nk ∈Z,
n1v1 + · · · + nkvk = 0 implies n1 = · · · = nk = 0. The list v1, . . . , vk is an
ordered Z-basis of G iff the list generates G and is Z-independent. Equivalently,
v1, . . . , vk is a Z-basis of G iff for each g ∈G there exist unique integers ni
with g = Pk
i=1 nivi. A commutative group is finitely generated iff it has a finite
generating set; the group is free iff it has a basis; the group is k-dimensional iff
it has a basis with k elements.
2.
Properties of Generating Sets. Two Z-linear maps S, T : G →H that agree on a
generating set of G must be equal. A Z-linear map T : G →H is surjective iff its
image contains a generating set for H.
3.
Universal Mapping Property for Free Commutative Groups. Suppose X is a Z-
basis of a commutative group G. Given any commutative group H and any
function f : X →H, there exists a unique Z-linear extension Tf : G →H
such that Tf(v) = f(v) for all v ∈X.
4.
Consequences of the Universal Mapping Property. Every k-dimensional free com-
mutative group G is isomorphic to Zk. Different isomorphisms can be obtained
by choosing an ordered basis (v1, . . . , vk) for G and sending n1v1 +· · ·+nkvk ∈G
to the k-tuple of coordinates (n1, . . . , nk) ∈Zk. Every commutative group with a
k-element generating set is isomorphic to a quotient group of Zk.
466
Advanced Linear Algebra
5.
Matrix Representation of Z-Linear Maps. Given a Z-linear map T : G →H, an
ordered basis X = (v1, . . . , vn) of G, and an ordered basis Y = (w1, . . . , wm) of
H, the matrix of T relative to the input basis X and output basis Y is the unique
m × n integer-valued matrix A = [aij] such that
T(vj) =
m
X
i=1
aijwi
for 1 ≤j ≤n.
Matrix addition corresponds to pointwise addition of Z-linear maps, while matrix
multiplication corresponds to composition of linear maps.
6.
Elementary Operations on Bases, Rows, and Columns. Given an ordered Z-basis
of a free commutative group G, we can create new ordered bases of G by the
following operations: switch two basis vectors; negate a basis vector; add an
integer multiple of one basis vector to another basis vector. Similar operations can
be performed on the rows and columns of integer matrices. Column operations on
the matrix of a Z-linear map T correspond to changes in the input basis, while
row operations correspond to changes in the output basis.
7.
Reduction of Integer Matrices. Using row and column operations, we can reduce
any m × n integer-valued matrix A to a new matrix B such that the nonzero
entries of B (if any) occupy the first r positions on the main diagonal of B, and
these nonzero entries are positive integers each of which divides the next one. B
is uniquely determined by A and is called the Smith normal form of A.
8.
Canonical Form for Z-Linear Maps. Suppose T : G →H is a Z-linear map
between two finite-dimensional free commutative groups. There exist an ordered
basis X = (x1, . . . , xn) for G, an ordered basis Y = (y1, . . . , ym) for H, an integer
r ≥0, and positive integers a1, . . . , ar such that ai divides ai+1 for all i with
1 ≤i < r, T(xi) = aiyi for 1 ≤i ≤r, and T(xi) = 0 for r < i ≤n.
9.
Subgroups of Finitely Generated Free Commutative Groups. Any subgroup P of
a k-dimensional free commutative group G is also free with dimension at most k.
By choosing appropriate bases for P and G, we can find an isomorphism G ∼= Zk
such that P maps under this isomorphism to a subgroup of the form
a1Z ⊕· · · ⊕amZ ⊕{0} ⊕· · · ⊕{0},
where each ai divides the next one. It follows that G/P is isomorphic to a direct
sum of cyclic groups.
10.
Invariant Subgroups. For any commutative group G and integer n, the subsets
G[n] = {g ∈G : ng = 0}, nG = {ng : g ∈G}, and tor(G) = S
n≥1 G[n] are
subgroups of G. Any isomorphism f : G →H restricts to give isomorphisms
G[n] ∼= H[n], nG ∼= nH, and tor(G) ∼= tor(H), and f induces isomorphisms of
quotient groups G/G[n] ∼= H/H[n], G/nG ∼= H/nH, and G/ tor(G) ∼= H/ tor(H).
11.
Theorem Classifying Finitely Generated Commutative Groups (Version 1). Every
finitely generated commutative group G is isomorphic to a direct sum of cyclic
groups
Zb ⊕Za1 ⊕· · · ⊕Zar,
where b, r ≥0, every ai > 1, and ai divides ai+1 for 1 ≤i < r. The integers b, r,
and a1, . . . , ar (satisfying these conditions) are uniquely determined by G.
Finitely Generated Commutative Groups
467
12.
Theorem Classifying Finitely Generated Commutative Groups (Version 2). Every
finitely generated commutative group G is isomorphic to a direct sum of cyclic
groups
Zb ⊕Za1 ⊕· · · ⊕Zar,
where b, r ≥0, a1 ≥· · · ≥ar > 1, and every ai is a prime power. The integers b,
r, and a1, . . . , ar (satisfying these conditions) are uniquely determined by G.
13.
Restatement of Uniqueness in the Classification Theorem. Suppose
G ∼= Zb ⊕Za1 ⊕· · · ⊕Zar ∼= Zd ⊕Zc1 ⊕· · · ⊕Zct,
where b, d, r, t ∈Z≥0, all ai and cj are in Z>1, and either: (i) both lists a1, . . . , ar
and c1, . . . , ct consist of prime powers in weakly decreasing order; or (ii) ai divides
ai+1 for 1 ≤i < r and cj divides cj+1 for 1 ≤j < t. Then b = d and r = t and
ai = ci for 1 ≤i ≤r. In particular (when r = t = 0), this says that the
dimension (or Betti number) b of a finite-dimensional free commutative group
is well-defined. In case (i), a1, . . . , ar are called the elementary divisors of G. In
case (ii), a1, . . . , ar are called the invariant factors of G.
14.
Remarks for Non-Finitely Generated Commutative Groups. A subset S of a
commutative group G generates G iff every g ∈G is a Z-linear combination of
some finite subset of S; S is Z-independent iff every finite list of distinct elements
of S is Z-independent; S is a Z-basis of G iff S is Z-independent and generates G.
A commutative group G is called free iff it has a Z-basis. The additive group Q is
an example of a commutative group that is not finitely generated. The Universal
Mapping Property is valid for free commutative groups. Every free commutative
group G is isomorphic to a direct sum of copies of Z, where the number of factors
in the direct sum is the cardinality of a basis of G. Every commutative group is
isomorphic to a quotient group of a free commutative group.
16.22
Exercises
Unless otherwise specified, assume (G, +) and (H, +) are commutative groups in these
exercises.
1.
(a) For fixed n ∈Z, verify that nZ = {ni : i ∈Z} is a subgroup of Z.
(b) Find all m, n ∈Z with mZ = nZ.
2.
Let H be a subgroup of G. Prove: for all x ∈G, ⟨x⟩⊆H iff x ∈H.
3.
Fix n ∈Z>0. Prove that every element of Z/nZ equals one of the cosets 0 + nZ,
1 + nZ, . . . , (n −1) + nZ, and show that these cosets are all distinct. Then prove
that the groups Z/nZ and (Zn, ⊕) are isomorphic.
4.
Prove that Z/0Z ∼= Z.
5.
In the text, we constructed a cyclic group of size n by forming the quotient group
Z/nZ. For each group, find a specific subgroup of that group that is isomorphic
to Z/nZ. (a) (Sn, ◦)
(b) C̸=0 under multiplication
(c) GLn(R)
6.
Let G1, . . . , Gk be commutative groups. Show G = G1×· · ·×Gk is a commutative
group under componentwise addition. For 1 ≤i ≤k, let ji : Gi →G be given by
ji(xi) = (0, . . . , xi, . . . , 0) for xi ∈Gi, where the xi occurs in position i. Show each
468
Advanced Linear Algebra
ji is an injective group homomorphism. Are these results still true for arbitrary
groups G1, . . . , Gk?
7.
Let n ∈Z≥0 and x ∈G. The definition of nx in §16.2 can be stated more
precisely in the following recursive way: 0x = 0, and (n + 1)x = (nx) + x. Use
this recursive definition and induction to prove the following laws of multiples
from (16.1) assuming m, n ∈Z≥0 and x, y ∈G (a separate argument is needed
when m or n is negative). (a) (m + n)x = (mx) + (nx) (b) m(nx) = (mn)x
(c) 1x = x (d) m(x + y) = (mx) + (my)
8.
Give a specific example of a non-commutative group (G, +), x, y ∈G, and m ∈
Z>0 with m(x + y) ̸= mx + my.
9.
Let v1, . . . , vk ∈G and H = {c1v1 + · · · + ckvk : ci ∈Z}.
(a) Prove that 0G ∈H and that H is closed under inverses.
(b) If (G, +) is not commutative, must H be closed under the group operation?
(c) Let K be the set of all group elements of the form e1w1 + e2w2 + · · · + esws,
where s ∈Z≥0, each ei ∈{1, −1}, and each wi is some vj (with repeats allowed).
Prove: for all G (commutative or not), K is a subgroup of G, and prove K = H
when G is commutative.
10.
Find all generators of each of these groups. (a) Z8 (b) Z12 (c) Z30 (d) Z3 × Z5
11.
(a) Prove that for prime p and all nonzero a in Zp, Zp = ⟨a⟩.
(b) For prime p and e ∈Z>0, how many a ∈Zpe satisfy Zpe = ⟨a⟩?
12.
Given n ∈Z>0 and k ∈Zn, find and prove a criterion for when Zn = ⟨k⟩.
13.
(a) Suppose G1 and G2 are commutative groups with G1 = ⟨v1, . . . , vm⟩and
G2 = ⟨w1, . . . , wn⟩. Prove G1 × G2 = ⟨(v1, 0), . . . , (vm, 0), (0, w1), . . . , (0, wn)⟩.
(b) Generalize (a) to direct products G1 × G2 × · · · × Gk where k ∈Z>0.
14.
(a) Describe an explicit generating set for the product group G = Z × · · · × Z ×
Za1 ×· · ·×Zas, where there are b copies of Z and every ai = pei
i is a prime power.
(b) How many generating sets S does G have where |S| = b+s and every element
of S has exactly one nonzero component?
15.
Let S ⊆G and H be the set of Z-linear combinations of elements of S. Prove H
is a subgroup of G.
16.
Prove that S = {1/pe : p is prime, e ∈Z>0} generates (Q, +).
17.
Prove or disprove: the set T = {1/p : p is prime} generates (Q, +).
18.
Find a generating set for Q>0, the group of positive rational numbers under
multiplication.
19.
Show that Z[x] (polynomials with integer coefficients, under addition) is a
commutative group that is not finitely generated. Find an infinite generating
set for this group.
20.
Give an example of a commutative group (G, +) that is not finitely generated
and is not isomorphic to Z[x] or to (Q, +).
21.
Prove or disprove: the commutative groups Z[x] and (Q, +) are isomorphic.
22.
Prove or disprove: the commutative groups Z[x] and Q>0 (positive rationals under
multiplication) are isomorphic.
23.
Let B = (v1, . . . , vk) be a list of elements in G. Say what it means for B to be
Z-dependent, by negating the definition of Z-linear independence. Say what it
means for a subset S of G (possibly infinite) to be Z-dependent. Then explain
why ∅is Z-independent relative to G.
Finitely Generated Commutative Groups
469
24.
Which of the following commutative groups are free? Explain.
(a) 3Z (a subgroup of Z) (b) Z × Z5 (c) Z[x] under addition
(d) {a + bi : a ∈2Z, 2b ∈Z} under complex addition
(e) Q under addition
25.
Let V be a Q-vector space. Prove that a list (v1, . . . , vk) of elements of V is
Z-linearly independent iff the list is Q-linearly independent.
26.
Show that for all a ∈Z>0 with a not a perfect square, (1, √a) is a Z-linearly
independent list in R.
27.
Show that (1,
√
2,
√
3,
√
6) is Z-linearly independent.
28.
Let B = (v1, . . . , vk) be a list of elements in G. Suppose C is obtained from B
by switching vi and vj, for some i ̸= j. Prove G = ⟨B⟩iff G = ⟨C⟩. Prove B is
Z-independent iff C is Z-independent. Prove B is an ordered basis of G iff C is
an ordered basis of G.
29.
Repeat Exercise 28, but now assume C is obtained from B by replacing some vi
by −vi. Which implications in Exercise 28 are true if C is obtained from B by
replacing some vi by nvi, where n ̸∈{−1, 0, 1} is a fixed integer?
30.
Check that B = ((0, 1, −5), (0, 0, −1), (1, 2, 3)) is an ordered basis of Z3: (a) by
proving from the definitions that B generates Z3 and is Z-linearly independent;
(b) by showing how to obtain B from the known ordered basis (e1, e2, e3) by a
sequence of elementary operations.
31.
Repeat Exercise 30 for B = ((12, −7, −2), (8, 5, 3), (1, 2, 1)).
32.
(a) Find and prove necessary and sufficient conditions on a, b, c, d ∈Z so that
((a, b), (c, d)) is an ordered basis of Z2. (b) Can you generalize your answer to (a)
to characterize ordered bases of Zk for all k ≥1?
33.
Suppose F is a field and B = (v1, . . . , vn) is an ordered basis for an F-vector
space V . Prove that the three elementary operations in §16.4 (as modified in the
last paragraph of that section) send the basis B to another ordered basis of V .
34.
Prove or disprove: for all k ≥1, every ordered basis of the free commutative
group Zk can be obtained from the standard ordered basis (e1, e2, . . . , ek) by a
finite sequence of elementary operations (B1), (B2), and (B3).
35.
Give a justified example of a finitely generated free commutative group G and
a Z-linearly independent list (v1, . . . , vk) in G that cannot be extended to an
ordered Z-basis (v1, . . . , vk, . . . , vs) of G.
36.
Give a justified example of a finitely generated free commutative group G and a
generating set {v1, . . . , vk} of G such that no subset of this generating set is a
Z-basis of G.
37.
Let T : G →H be homomorphism of commutative groups.
(a) Prove by induction on n that T(nx) = nT(x) for all x ∈G and all n ∈Z≥0.
(b) Prove T(nx) = nT(x) for all x ∈G and all n ∈Z.
(c) Prove by induction on k that T(Pk
i=1 nivi) = Pk
i=1 niT(vi) for all k ∈Z≥0,
ni ∈Z and vi ∈G.
38.
Find the coordinates of (1, 2, 3) and (4, −1, 1) relative to each ordered basis for Z3.
(a) (e1, e2, e3) (b) ((0, 1, −5), (0, 0, −1), (1, 2, 3)) (c) ((−12, 7, 2), (8, 5, 3), (1, 2, 1))
39.
Let T : G →H be a Z-linear map and X ⊆G.
(a) Prove: If X generates G, then {T(x) : x ∈X} generates img(T).
(b) Prove or disprove: If X generates G, then X ∩ker(T) generates ker(T).
470
Advanced Linear Algebra
40.
Let G be a free commutative group with ordered basis B = (v1, v2, . . . , vk).
Use the UMP for free commutative groups to construct an isomorphism
T : G →Zv1 ⊕Zv2 ⊕· · · ⊕Zvk. Show that Zvi ∼= Z, preferably by using the
UMP, and conclude that G ∼= Zk.
41.
Let G and H be finitely generated free commutative groups of dimensions n and
m, respectively. Use the UMP for free commutative groups to prove:
(a) If n ≤m, then there exists an injective Z-linear map S : G →H.
(b) If n ≥m, then there exists a surjective Z-linear map T : G →H.
(c) If n = m, then there exists a bijective Z-linear map U : G →H.
42.
Give a justified example of a free commutative group G, a commutative group H,
a finite spanning set X for G, and a function f : X →H that has no extension
to a Z-linear map with domain G.
43.
Give a justified example of a free commutative group G, a commutative group
H, a Z-independent subset X of G, and a function f : X →H that has infinitely
many extensions to Z-linear maps with domain G.
44.
Let G be a free commutative group with infinite basis X. (a) Prove: for every
g ∈G, there exist unique integers {nx : x ∈X} such that nx ̸= 0 for only
finitely many x ∈X and g = P
x∈X nxx. (b) Prove: if S, T : G →H are Z-
linear maps that agree on X, then S = T. (c) Prove: for every commutative
group H and every function f : X →H, there exists a unique Z-linear map
Tf : G →H extending f. (d) For any set Y , let Z(Y ) be the set of all functions
f : Y →Z such that f(y) = 0 for all but finitely many y ∈Y . Check Z(Y ) is
a free commutative group under pointwise addition of functions, with a basis in
bijective correspondence with the set Y . (e) Prove G is isomorphic to the group
Z(X). (f) Prove that every commutative group is isomorphic to a quotient group
of a free commutative group.
45.
Fix k, n ∈Z>0, and let X = {e1, . . . , ek} where each ei = (0, . . . , 1, . . . , 0) is
viewed as an element of G = Zk
n. Prove that G and X satisfy the following UMP:
for all commutative groups H such that ny = 0 for all y ∈H and for all functions
f : X →H, there exists a unique Z-linear map Tf : G →H extending f.
46.
Use the UMP for the free commutative group Z, together with the Fundamental
Homomorphism Theorem for Groups, to show that every cyclic group is isomor-
phic to Z or to Z/nZ for some n ∈Z>0.
47.
Prove that S defined below (16.2) is a surjective Z-linear map with kernel P.
48.
Assume Hi is a normal subgroup of a group Gi for 1 ≤i ≤k. Prove
(G1 × · · · × Gk)/(H1 × · · · × Hk) ∼= (G1/H1) × · · · × (Gk/Hk).
49.
Suppose G is a free commutative group, A and B are commutative groups,
f : A →B is a surjective group homomorphism, and g : G →B is a group
homomorphism. Prove there exists a group homomorphism h : G →A with
f ◦h = g.
50.
In §16.8, we proved that every subgroup of a finitely generated free commutative
group is free. (a) Trace through the construction in that proof, applied to
the subgroup H = {(t, 2t) : t ∈Z} of Z2, to construct an ordered basis
for this subgroup. (b) Similarly, use the proof to find an ordered basis for
H = ⟨(2, 4, 15), (4, 6, 6)⟩, which is a subgroup of Z3.
Finitely Generated Commutative Groups
471
51.
Let F be a field. Modify the proof in §16.8 to prove that for all k ∈Z≥0, every
subspace of F k has an ordered basis of size at most k. Use only the definitions
and induction, avoiding any theorems whose conclusions involve the existence of
a basis.
52.
Let e1 = (1, 0), e2 = (0, 1), f1 = (2, 5), f2 = (1, 3). Let T : Z2 →Z2 be the
Z-linear map given by T((a, b)) = (4a −b, 2a + 3b) for a, b ∈Z. Find the matrix
of T relative to each pair of ordered bases.
(a) input basis (e1, e2), output basis (e1, e2)
(b) input basis (e1, e2), output basis (f1, f2)
(c) input basis (f1, f2), output basis (e1, e2)
(d) input basis (f1, f2), output basis (f1, f2)
53.
Let v1 = (7, 2, 2), v2 = (2, −1, 0), and v3 = (3, 1, 1). Let I : Z3 →Z3 be the
identity map, X = (e1, e2, e3), and Y = (v1, v2, v3). Check that Y is an ordered
Z-basis of Z3. Then find the matrix of I relative to each pair of ordered bases.
(a) input basis X, output basis X
(b) input basis Y , output basis X
(c) input basis X, output basis Y
(d) input basis Y , output basis Y
54.
Let T, U : Z3 →Z3 be the Z-linear maps whose matrices (using the standard
ordered basis (e1, e2, e3) as both input and output basis) are
A =


2
0
−1
4
4
1
0
3
−2

and B =


−1
5
7
2
0
−2
−3
1
4

.
(a) Find T((a, b, c)) and U((a, b, c)) for all a, b, c ∈Z.
(b) Find the matrix of T + U relative to the standard ordered basis.
(c) Find the matrix of TU relative to the standard ordered basis.
(d) Find the matrix of UT relative to the standard ordered basis.
55.
Let T : Zn →Zm be a Z-linear map with matrix A relative to the standard
ordered bases of Zn and Zm. Prove: for v ∈Zn, T(v) = Av (the product of the
matrix A and the column vector v).
56.
Suppose T : G →H is Z-linear, X = (x1, . . . , xn) is an ordered basis of G,
Y = (y1, . . . , ym) is an ordered basis of H, and A is the matrix of T relative
to these bases. Show that if g ∈G has coordinates v ∈Zn relative to X, then
Av ∈Zm gives the coordinates of T(g) relative to Y . (View v and Av as column
vectors.)
57.
Let G and H be finitely generated free commutative groups with ordered bases X
and Y , respectively. Suppose T : G →H is a Z-linear map whose matrix relative
to X and Y is A. Given c ∈Z, show that cT : G →H, defined by (cT)(x) = cT(x)
for x ∈G, is Z-linear. Find the matrix of cT relative to X and Y .
58.
Dual Groups. For any commutative group G, define the dual group of G, written
G∗or HomZ(G, Z), to be the set of all Z-linear maps f : G →Z.
(a) Prove G∗is a commutative group under pointwise addition of functions.
(b) Prove Z∗∼= Z.
(c) For commutative groups G1, . . . , Gk, prove (G1×· · ·×Gk)∗∼= (G∗
1)×· · ·×(G∗
k).
(d) Prove: if G is a free commutative group of dimension k, then G∗is a free
commutative group of dimension k.
59.
(a) Suppose G is free with ordered basis X = (v1, . . . , vn). Show there exists a
unique dual basis X∗= (v∗
1, . . . , v∗
n) of the dual group G∗(defined in Exercise 58)
472
Advanced Linear Algebra
satisfying v∗
i (vj) = 1 if i = j and 0 if i ̸= j. (b) Suppose H is also free with
ordered basis Y = (w1, . . . , wm). Given a Z-linear map T : G →H, show the
map T ∗: H∗→G∗given by T ∗(f) = f ◦T for f ∈H∗is Z-linear. (c) How is
the matrix of T ∗relative to the bases Y ∗and X∗related to the matrix A of T
relative to the bases X and Y ?
60.
Assume the setup in Exercise 54. For each input basis X and output basis Y ,
find the matrix of T relative to these bases.
(a) X = (e1 + 4e3, e2, e3), Y = (e1, e2, e3) (b) X = (e1, e2, −e3), Y = (e1, e2, e3)
(c) X = (e1, e2, e3), Y = (e1, e3, e2) (d) X = (e1, e2, e3), Y = (−e1, −e2, e3)
(e) X = (e2, e1, e3), Y = (e1, e2, e3) (f) X = (e1, e2, e3), Y = (e1, e2 −3e3, e3)
(g) X = (e1, e2 −2e1, e3 + e1), Y = (e1 + e3, e2 −2e3, e3)
61.
Suppose G is a free commutative group with ordered basis (v1, v2, v3, v4), H is
a free commutative group with ordered basis (w1, w2, w3), and T : G →H is a
Z-linear map whose matrix (relative to these bases) is
A =


2
−4
−3
0
1
−2
3
3
4
−3
−2
4

.
Compute T(3v2 −v3 + 2v4). Find the matrix of T relative to each pair of bases.
(a) input basis (v3, v1, v2, v4), output basis (w3, w2, w1)
(b) input basis (v1, −v2, v3, v4), output basis (−w1, w2, −w3)
(c) input basis (v1, v2 −v1, v3 + 3v1, v4 + v1), output basis (w1, w2, w3)
(d) input basis (v1, v2, v3, v4), output basis (w1, w1 + w2 + w3, 2w2 + w3)
62.
Carefully prove the italicized statements in §16.10, which indicate how elementary
operations on input and output bases affect the matrix of a Z-linear map.
63.
Suppose A is in Mm,n(Z) and an integer b divides every entry of A. Let C be
obtained from A by applying a single elementary row or column operation. Show
that b divides every entry of C. Show that the gcd of all entries of A equals the
gcd of all entries of C (interpret the gcd as zero in the case of a zero matrix).
Show that the 1, 1-entry of the Smith normal form of A is the gcd of all entries
of A. (For a generalization, see Exercise 100.)
64.
Use elementary row and column operations in Z to reduce each integer matrix to
its Smith normal form (16.4).
(a)

9
18
36
6

(b)


9
8
7
6
5
4
3
2
1


(c)


12
0
9
15
−21
−9
27
18
0
15
33
−21


65.
Let T : Z2 →Z2 be the Z-linear map T((a, b)) = (4a −b, 2a + 3b). Find Z-bases
X = (g1, g2) and Y = (h1, h2) of Z × Z such that the matrix of T with respect to
the input basis X and the output basis Y has the form
 c
0
0
d

, where c, d ∈Z≥0
and c divides d. (Reduce the matrix found in Exercise 52(a), keeping track of how
each operation changes the input or output basis.)
66.
Let F be a field. Prove: For any matrix A ∈Mm,n(F), we can perform finitely
many elementary row and column operations on A to obtain a matrix B such
that, for some r with 0 ≤r ≤min(m, n), B(i, i) = 1F for 1 ≤i ≤r and all other
entries of B are 0F . (Imitate the first part of the proof of the Reduction Theorem
for Integer Matrices.)
Finitely Generated Commutative Groups
473
67.
Write a computer program that takes as input a matrix A ∈Mm,n(Z) and returns
as output the Smith normal form (16.4) of A.
68.
Prove that a Z-linear map between two finitely generated free commutative groups
is invertible iff the Smith normal form for the map is an identity matrix.
69.
Prove that the columns of A ∈Mm,n(Z) are Z-linearly dependent iff the Smith
normal form of A has at least one column of zeroes.
70.
Let M be the subgroup of Z3 generated by v1 = (6, 6, 9) and v2 = (12, 6, 6).
(a) Explain why (v1, v2) is an ordered Z-basis of M. (b) Define T : M →Z3 by
T(x) = x for x ∈M. Find the matrix of T relative to the input basis (v1, v2) and
output basis (e1, e2, e3). (c) Use matrix reduction to find a new basis (x1, x2) for
M, a new basis (y1, y2, y3) for Z3, and positive integers d1, d2 (where d1 divides d2)
such that x1 = d1y1 and x2 = d2y2. (d) Find a product of cyclic groups (satisfying
the conclusions of the classification theorem for finitely generated commutative
groups) that is isomorphic to Z3/M.
71.
In the proof in §16.13, show that the map T extending the function f (send-
ing each yi to ei) is an isomorphism. Prove P1 = T[P] has ordered basis
(a1e1, . . . , anen). Prove T induces a group isomorphism from Zm/P to Zm/P1.
72.
Let m and n be relatively prime positive integers.
(a) Use (16.6) to prove that Z/(mnZ) ∼= (Z/mZ) × (Z/nZ).
(b) Imitate the proof of (16.6) to prove that Z/(mnZ) ∼= (Z/mZ) × (Z/nZ).
(c) Which steps in the proof in (b) fail when gcd(m, n) > 1?
73.
Assume the setup in Exercise 54. Use matrix reduction to find new bases X and
Y for Z3 such that the matrix of T relative to X and Y is in Smith normal form.
74.
Assume the setup in Exercise 54. Use matrix reduction to find new bases X and
Y for Z3 such that the matrix of U relative to X and Y is in Smith normal form.
75.
A certain Z-linear map T : Z4 →Z3 is represented by the following matrix
relative to the standard ordered bases of Z4 and Z3:


15
0
−10
20
30
−20
20
10
25
−15
55
40

.
Give a formula for T((i, j, k, p)), where i, j, k, p ∈Z. Use matrix reduction to find
an ordered basis X = (v1, v2, v3, v4) of Z4, an ordered basis Y = (w1, w2, w3) of
Z3, and a matrix B in Smith normal form such that B is the matrix of T relative
to X and Y .
76.
Use matrix reduction to determine the Betti numbers, elementary divisors, and
invariant factors for each of the following quotient groups.
(a) Z2/⟨(−4, 4), (−8, −4)⟩
(b) Z3/⟨(−255, −12, −60), (−114, −6, −27)⟩
(c) Z4/⟨(50, 160, 70, 210), (69, 213, 81, 282), (29, 88, 31, 117)⟩
77.
Give a specific example of a group (G, ⋆) and a positive integer n such that the
map Mn : G →G given by Mn(g) = gn for g ∈G is not a group homomorphism,
the image of Mn is not a subgroup of G, and the set {g ∈G : Mn(g) = eG} is
not a subgroup of G.
78.
Suppose f : G →H is a group homomorphism.
(a) Prove: for all n ∈Z>0, f[G[n]] ⊆H[n].
(b) Give an example where strict inclusion holds in (a).
474
Advanced Linear Algebra
(c) If f is injective, must equality hold in (a)? Explain.
(d) If f is surjective, must equality hold in (a)? Explain.
79.
Suppose f : G →H is a group homomorphism.
(a) Prove: for all n ∈Z>0, f[nG] ⊆nH.
(b) Give an example where strict inclusion holds in (a).
(c) If f is injective, must equality hold in (a)? Explain.
(d) If f is surjective, must equality hold in (a)? Explain.
80.
Let |G| = m. Prove: for all n ∈Z with gcd(m, n) = 1, nG = G and G[n] = {0}.
81.
Suppose f : G →H is a group homomorphism.
(a) Prove f[tor(G)] ⊆tor(H).
(b) Give an example where strict inclusion holds in (a).
(c) Prove: for an isomorphism f, f restricts to an isomorphism tor(G) ∼= tor(H).
(d) Deduce from (c) that for an isomorphism f, we get an induced isomorphism
G/ tor(G) ∼= H/ tor(H).
82.
In §16.16, we proved that Zn ∼= Zm implies n = m. Where does this proof break
down if we try to use it to show that Rn ∼= Rm (isomorphism of additive groups)
implies n = m?
83.
For 1 ≤n ≤4, list all integer partitions of n and draw their diagrams.
84.
Let p be a fixed prime. Use partitions to make a complete list of all non-isomorphic
commutative groups of size p6.
85.
Let G be the group Z55 × Z55 × Z55 × Z53 × Z53 × Z52 × Z52.
Draw pictures of partition diagrams to help answer the following questions.
(a) For each i ≥1, find the size of the subgroup G[5i].
(b) For each i ≥1, find the size of the subgroup 5iG.
(c) Find the size of G[125] ∩25G (explain).
86.
Suppose p is prime and G = Zpa1 ×Zpa2 ×· · ·×Zpak , where (a1 ≥a2 ≥· · · ≥ak)
is a partition. What is the size of the subgroup pG? Describe how to use partition
diagrams to find the size of piG for all i ≥1. Describe how to use partition
diagrams to find the size of piG ∩G[pj] for all i, j ∈Z≥0.
87.
Prove that two integer partitions (ai : i ≥1) and (ci : i ≥1) are equal iff for all
j ≥1, the column sums a′
1 + · · · + a′
j and c′
1 + · · · + c′
j are equal.
88.
Let G1, . . . , Gk be commutative groups and b ∈Z>0.
(a) Prove (G1 ⊕· · · ⊕Gk)[b] = (G1[b]) ⊕· · · ⊕(Gk[b]).
(b) Prove b(G1 ⊕· · · ⊕Gk) = (bG1) ⊕· · · ⊕(bGk).
(c) Prove tor(G1 ⊕· · · ⊕Gk) = tor(G1) ⊕· · · ⊕tor(Gk).
89.
For each n below, make a complete list of all non-isomorphic commutative groups
of size n. Use decompositions that display the elementary divisors of each group.
(a) 400 (b) 1001 (c) 666 (d) p2q3, where p and q are distinct primes.
90.
For each n below, make a complete list of all non-isomorphic commutative groups
of size n. Use decompositions that display the invariant factors of each group.
(a) 24 (b) 300 (c) 32 (d) p3q3, where p and q are distinct primes.
91.
For each commutative group, find its elementary divisors.
(a) Z9900
(b) Z60 × Z100 × Z80
(c) Z48 × Z111 × Z99 × Z1001
92.
For each commutative group, find its invariant factors.
(a) Z32×Z16×Z4×Z4×Z9×Z9×Z3 (b) Z60×Z100×Z80 (c) Z48×Z111×Z99×Z1001
Finitely Generated Commutative Groups
475
93.
Let p and q be distinct primes. How many non-isomorphic commutative groups
of size p5q3 are there?
94.
Let P be a logical property such that every cyclic group has property P; if a group
G has property P and G ∼= H, then H has property P; and whenever groups G
and H have property P, the product group G × H has property P. Prove that
all finitely generated commutative groups have property P.
95.
Use the classification of finite commutative groups to characterize all positive
integers n such that every commutative group of size n is cyclic. Give two proofs,
one based on elementary divisors and one based on invariant factors.
96.
Use the classification of finite commutative groups to prove that for all n-element
commutative groups G and all positive divisors d of n, G has a subgroup of size
d. Can you prove this without using the classification results from this chapter?
97.
Prove that if G is an n-element commutative group that has at most one subgroup
of size d for each positive divisor d of n, then G must be cyclic.
98.
Suppose A, B, and C are finitely generated commutative groups such that A ×
C ∼= B × C. Prove that A ∼= B. Give an example to show this result can fail if C
is not finitely generated.
99.
(a) Let G be a commutative group of size pe1
1 pe2
2 · · · pek
k , where p1, . . . , pk are
distinct primes and e1, . . . , ek > 0. Prove: for 1 ≤i ≤k, G has a unique
subgroup Pi of size pei
i . (b) Give an example to show that (a) can fail if
G is not commutative. (c) Show that if the conclusion in (a) holds, then
G ∼= P1 × P2 × · · · × Pk even if G is not commutative. (Use Exercise 12 from
Chapter 1.)
100.
Let A ∈Mm,n(Z). For 1 ≤k ≤min(m, n), a minor of A of order k is the
determinant of some k ×k submatrix of A obtained by looking at the entries in k
fixed rows and k fixed columns of A. Let Gk(A) be the gcd of all kth order minors
of A, with the convention that gcd(0, 0, . . . , 0) = 0. (a) Show that performing one
elementary row or column operation on A does not change any of the integers
Gk(A). (First show that if c ∈Z divides all kth order minors of A, then c divides
all kth order minors of the new matrix.) (b) Show that if B is any Smith normal
form of A, then Gk(B) = Gk(A). (c) Show that if B is any matrix in Smith
normal form, then Gk(B) = Qk
i=1 B(i, i) for 1 ≤k ≤min(m, n). (d) Use (b) and
(c) to prove that the Smith normal form of a matrix is unique. (e) Use (b) and
(c) to compute the Smith normal form of
A =


10
8
0
−4
8
6
0
12
−8

.
101.
Let B be a basis for R, viewed as a Q-vector space. (a) For m ∈Z>0, use B
to describe a basis Bm for the Q-vector space Rm. (b) Show that |B| = |Bm|
for all m ∈Z>0. Conclude that all of the commutative groups (Rm, +) (for
m = 1, 2, 3, . . .) are isomorphic to the commutative group (R, +).
102.
Give justified examples of each of the following.
(a) an infinite commutative group that is not free
(b) a free commutative group that is not infinite
(c) a Z-independent list in R2 that is not R-independent
(d) a free commutative group with an infinite basis
476
Advanced Linear Algebra
(e) an infinite group G such that for all x ∈G, nx = 0G for some n > 0
(f) a commutative group G and a proper subgroup H with G ∼= H
(g) a group G in which tor(G) is not a subgroup of G
(h) a commutative group G with isomorphic subgroups A and B such that G/A
is not isomorphic to G/B
103.
Prove: for all finite commutative groups G and H, if G×G ∼= H ×H, then G ∼= H.
104.
True or false? Explain each answer.
(a) Every subgroup of a finitely generated free commutative group is free.
(b) Every quotient group of a finitely generated free commutative group is free.
(c) A product of finitely many finitely generated free commutative groups is free.
(d) If (x1, x2, x3) is an ordered basis for a commutative group G, then (x3, x2 +
x3, x1 + x2 + x3) is always an ordered basis for G.
(e) If (x1, . . . , xn) is any Z-independent list in a commutative group G and c is a
nonzero integer, then (cx1, x2, . . . , xn) must also be Z-independent.
(f) For all commutative groups G, (G ∼tor(G)) ∪{eG} is a subgroup of G.
(g) For all commutative groups G and all n ∈Z>0, G/G[n] ∼= nG.
(h) Every finite commutative group is isomorphic to a subgroup of Zn for some
n > 0.
(i) Every finite commutative group is isomorphic to a quotient group of Zn for
some n > 0.
(j) Every finite commutative group is isomorphic to a subgroup of Sn for some
n > 0.
(k) Every finitely generated commutative group is isomorphic to a subgroup of
Zn for some n > 0.
(l) Every finitely generated commutative group is isomorphic to a quotient group
of Zn for some n > 0.
(m) Every finitely generated commutative group G with tor(G) = {0} is free.
(n) Every k-element generating set for Zk is a Z-basis for Zk.
(o) Every k-element Z-linearly independent subset of Zk generates Zk.
17
Introduction to Modules
Introductions to linear algebra often cover the following concepts involving vector spaces
over a field F: axioms for a vector space, subspaces, quotient spaces, direct sums, linear
independence, spanning sets, bases, and linear transformations. The goal of this chapter is
to cover the same material in a more general context. We replace the field F by a general
ring R and consider vector spaces over this ring, which are now called R-modules. Intuitively,
an R-module is a set of vectors on which we define operations of vector addition and scalar
multiplication satisfying certain axioms. In this setting, the scalars come from the ring R
rather than a field. As we will see, much of the theory of vector spaces extends without
change to this more general situation, although the terminology used is a bit different.
However, we warn the reader that certain aspects of the theory of R-modules are quite
different from what might be expected based on experience with vector spaces. The most
glaring example of this phenomenon is that not every R-module has a basis. Those modules
that do have a basis are called free R-modules. Even if an R-module is free, it may possess
bases with different cardinalities, something which does not happen for vector spaces over
a field. Fortunately, for commutative rings R (and certain other classes of rings), the
cardinality of a basis is invariant for free R-modules.
Before we can even define modules, we must point out another complication that occurs
when R is non-commutative. Suppose F is a field, V is an F-vector space, c, d ∈F, and
v ∈V . We have the associative axiom for scalar multiplication, (cd)v = c(dv), which we
might also write as v(cd) = (vc)d. The point is that, although we often write scalars to the
left of the vectors on which they act, we could have written the scalars on the right instead.
However, if we replace F by a non-commutative ring R, so that we now have c, d ∈R, then
the two versions of the associative law just written are no longer equivalent. So we must
distinguish between the concepts of scalar multiplication on the left and scalar multiplication
on the right. This distinction leads to the concepts of left and right R-modules.
In the rest of this chapter, we define left and right R-modules and discuss fundamental
constructions for R-modules, including submodules, direct products, direct sums, and
quotient modules. We also discuss R-module homomorphisms, generating sets for a module,
and independent sets in a module, which generalize the concepts of linear transformations,
spanning sets, and linearly independent sets in vector spaces. We conclude by studying free
R-modules, bases, the Jordan–H¨older Theorem, and the length of a module.
17.1
Module Axioms
Let R be an arbitrary ring (see §1.2 for the definition of a ring; recall our convention that
every ring has a multiplicative identity element, denoted 1R). We now present the axioms
defining a left R-module, which are motivated by the corresponding axioms for a vector space
(see Table 1.4). A left R-module consists of a set M, an addition operation + : M ×M →M,
DOI: 10.1201/9781003484561-17
477
478
Advanced Linear Algebra
and a scalar multiplication operation · : R × M →M, often denoted by juxtaposition. The
addition operation on M must satisfy the following axioms:
(A1)
For all a, b ∈M, a + b belongs to the set M (closure under addition).
(A2)
For all a, b, c ∈M, (a + b) + c = a + (b + c) (associativity).
(A3)
For all a, b ∈M, a + b = b + a (commutativity).
(A4)
There exists an element 0 ∈M (necessarily unique) such that a + 0 = a = 0 + a
for all a ∈M (additive identity).
(A5)
For each a ∈M, there exists an element −a ∈M (necessarily unique) such that
a + (−a) = 0 = (−a) + a (additive inverses).
In other words, (M, +) is a commutative group (see §1.1). The scalar multiplication
operation must satisfy the following axioms:
(M1)
For all r ∈R and m ∈M, r · m belongs to the set M (closure under scalar
multiplication).
(M2)
For all m ∈M, 1R · m = m (identity law).
(M3)
For all r, s ∈R and m ∈M, (rs) · m = r · (s · m) (left associativity).
Also, addition and scalar multiplication are linked by the following distributive laws:
(D1)
For all r, s ∈R and m ∈M, (r + s) · m = r · m + s · m (distributive law for ring
addition).
(D2)
For all r ∈R and m, n ∈M, r ·(m+n) = r ·m+r ·n (distributive law for module
addition).
We define subtraction by setting a −b = a + (−b) for a, b ∈M. Elements of M may be
called vectors, and elements of R scalars, although this terminology is more often used when
discussing vector spaces over a field.
Now we give the corresponding definition of a right R-module. A right R-module consists
of a set N, an addition operation + : N × N →N, and a scalar multiplication operation
⋆: N ×R →N. We require (N, +) to be a commutative group (so axioms (A1) through (A5)
must hold with M replaced by N). The multiplication operation must satisfy the following
axioms:
(M1′)
For all n ∈N and r ∈R, n ⋆r belongs to the set N (closure under scalar
multiplication).
(M2′)
For all n ∈N, n ⋆1R = n (identity law).
(M3′)
For all n ∈N and r, s ∈R, n ⋆(rs) = (n ⋆r) ⋆s (right associativity).
Also, addition and scalar multiplication are linked by the following distributive laws:
(D1′)
For all n ∈N and r, s ∈R, n ⋆(r + s) = n ⋆r + n ⋆s (distributive law for ring
addition).
(D2′)
For all m, n ∈N and r ∈R, (m + n) ⋆r = m ⋆r + n ⋆r (distributive law for
module addition).
Subtraction is defined as before.
When R is commutative, there is no essential difference between left R-modules and
right R-modules. To see why, let (M, +) be a fixed commutative group. We can set up a
one-to-one correspondence between left scalar multiplications · : R × M →M and right
scalar multiplications ⋆: M × R →M, given by m ⋆r = r · m for all r ∈R and m ∈M. For
Introduction to Modules
479
any ring R, you can verify that the left module axioms (M1), (M2), (D1), and (D2) hold for
· iff the corresponding right module axioms (M1′), (M2′), (D1′), and (D2′) hold for ⋆. If R
is commutative, then (M3) holds for · iff (M3′) holds for ⋆. Thus, in the commutative case,
it makes little difference whether we multiply by scalars on the left or on the right. Later
in this chapter, the unqualified term “module” always means left R-module. All definitions
and results for left R-modules have analogs for right R-modules.
Next, we introduce the concept of R-module homomorphisms, which are analogous to
linear transformations of vector spaces. Let M and N be left R-modules. A map f : M →N
is a left R-module homomorphism iff f(x+y) = f(x)+f(y) and f(rx) = rf(x) for all x, y ∈
M and r ∈R. The definition of a right R-module homomorphism between right R-modules
M and N is analogous: we require f(x+y) = f(x)+f(y) and f(xr) = f(x)r for all x, y ∈M
and r ∈R. Homomorphisms of R-modules (left or right) are also called R-maps or R-
linear maps. The following terminology is sometimes used for R-maps satisfying additional
conditions: a module homomorphism f : M →N is called a monomorphism iff f is injective,
an epimorphism iff f is surjective, an isomorphism iff f is bijective, an endomorphism iff
M = N, and an automorphism iff M = N and f is bijective. Modules M and N are called
isomorphic or R-isomorphic iff there exists an R-module isomorphism g : M →N; in this
case, we write M ∼= N. You can verify that the following facts about linear transformations
and group homomorphisms are also true for R-maps: the identity map on M is a bijective R-
map; the composition of R-maps is an R-map (similarly for monomorphisms, epimorphisms,
etc.); the inverse of an R-isomorphism is an R-isomorphism; the relation “M is isomorphic
to N as an R-module” is an equivalence relation on any collection of left R-modules; and
for an R-map f : M →N, f(0M) = 0N and f(−x) = −f(x) for all x ∈M. You can also
check that 0Rx = 0M for all x ∈M; r0M = 0M for all r ∈R; and r(−x) = (−r)x = −(rx)
for all r ∈R and all x ∈M.
17.2
Examples of Modules
We now discuss four fundamental examples of R-modules.
First, any ring R is a left R-module, if we take addition and scalar multiplication to be
the given addition and multiplication in the ring R. In this case, the module axioms reduce
to the axioms in the definition of a ring (see §1.2). Similarly, every ring R is a right R-
module. Suppose f : R →R is a function satisfying f(x + y) = f(x) + f(y) for all x, y ∈R.
Then f is a ring homomorphism iff f(1R) = 1R and f(xy) = f(x)f(y) for all x, y ∈R.
On the other hand, f is a left R-module homomorphism iff f(xy) = xf(y) for all x, y ∈R,
while f is a right R-module homomorphism iff f(xy) = f(x)y for all x, y ∈R.
Second, generalizing the first example, let R be a subring of a ring S. (Recall from §1.4
that this means R is a subset of S containing 0S and 1S and closed under addition,
subtraction, and multiplication; R itself is a ring under the operations inherited from S.)
The ring S is a left R-module, if we take addition to be the addition in S and scalar
multiplication · : R × S →S to be the restriction of the multiplication · : S × S →S in the
ring S. Here, the conditions in the module axioms hold because they are a subset of the
conditions in the ring axioms for S. Similarly, S is a right R-module.
Third, let F be a field. Comparing the module axioms to the axioms defining a vector
space over F (see Table 1.4), we see that a left F-module V is exactly the same as an F-
vector space. An F-module homomorphism T : V →W is exactly the same as an F-linear
map from V to W.
480
Advanced Linear Algebra
Fourth, let (M, +) be any commutative group, and consider the ring R = Z. We make
M into a left Z-module by defining 0 · x = 0M, n · x = x + x + · · · + x (n copies of x), and
−n · x = −(x + x + · · · + x) (n copies of x) for all n > 0 and x ∈M. The module axioms
involving multiplication are the additive versions of the laws of exponents for commutative
groups. Moreover, you can verify (using axioms (M2) and (D1) and induction) that · is the
unique scalar multiplication map from Z×M into M that makes (M, +) into a left Z-module.
In other words, the Z-module structure of M is completely determined by the structure of
M as an additive group. Next, consider a function f : M →N between two Z-modules. On
one hand, if f is a Z-module homomorphism, then it is in particular a homomorphism of
commutative groups (meaning f(x + y) = f(x) + f(y) for all x, y ∈M). Conversely, if f is
a group homomorphism, then we know from group theory that f(n · x) = n · f(x) for all
x ∈M and n ∈Z, so that f is automatically a Z-module homomorphism. These remarks
show that Z-modules and Z-module homomorphisms are essentially the same as additive
commutative groups and group homomorphisms.
The examples illustrate one of the advantages of module theory as a unifying notational
tool: facts about vector spaces, commutative groups, and rings can all be formulated and
proved in the general framework of module theory.
17.3
Submodules
The next few sections discuss some general constructions for manufacturing new modules
from old ones: submodules, direct products, direct sums, Hom modules, quotient modules,
and changing the ring of scalars. Chapter 20 discusses more advanced constructions for
modules, such as tensor products, tensor powers, exterior powers, and symmetric powers.
Let N be a subset of a left R-module M. We say N is an R-submodule of M iff N is a
subgroup of the additive group (M, +) such that for all r ∈R and x ∈N, r·x belongs to the
set N. We may omit R from the term “R-submodule” when R is understood. A submodule
of M is a subset of M containing 0M and closed under addition, additive inverses, and left
multiplication by scalars from R. Using the fact that −1 ∈R, we see that closure under
inverses follows from the other closure conditions, since −n = −(1R · n) = (−1R) · n for
n ∈N. If we restrict the addition and scalar multiplication operations for M to the domains
N × N and R × N (respectively), then N becomes a left R-module. The axioms (A1) and
(M1) hold for N by definition of a submodule. The other axioms for N are special cases
of the corresponding axioms for M, keeping in mind that 0M ∈N and −x ∈N whenever
x ∈N.
The definition of a submodule N of a right R-module M is similar: we require N to be
an additive subgroup of M such that x ⋆r ∈N for all x ∈N and r ∈R.
Consider the examples in the preceding section. Regarding R as a left R-module, the
definition of a left R-submodule of R is exactly the definition of a left ideal of R (see §1.4).
Regarding R as a right R-module, the definition of a right R-submodule of R is exactly the
definition of a right ideal of R. If R is commutative, the R-submodules of R (viewed as a
left or right R-module) are precisely the ideals of the ring R. Next, if V is a vector space
over a field F (so V is an F-module), the F-submodules of V are precisely the subspaces
of the vector space V . Finally, if M is a commutative group and hence a Z-module, the
Z-submodules of M are precisely the subgroups of M. On one hand, Z-submodules of M
are subgroups of M by definition. On the other hand, if H is a subgroup, x ∈H, and
n ∈Z, then n · x ∈H follows by induction from the fact that H is closed under addition
and additive inverses.
Introduction to Modules
481
We now discuss intersections and sums of submodules. Let {Mi : i ∈I} be a nonempty
collection of submodules of an R-module M. The intersection N = T
i∈I Mi is a submodule
of M, hence an R-module. Proof: First, 0M ∈N since 0M belongs to every Mi. Given
x, y ∈N, we know x, y ∈Mi for all i ∈I, so x + y ∈Mi for all i ∈I, so x + y ∈N. Given
x ∈N and r ∈R, we have x ∈Mi for all i ∈I, so r · x ∈Mi for all i ∈I, so r · x ∈N. The
submodule N is the largest submodule of M contained in every Mi.
Next, let M and N be submodules of an R-module P. You can verify that the set
M + N = {m + n : m ∈M, n ∈N} is a submodule of P, called the sum of M and N.
Sums of finitely many submodules of P are defined analogously. For the general case, let
{Mi : i ∈I} be a family of submodules of P. The sum of submodules P
i∈I Mi is the set
of all finite sums mi1 + · · · + mis where ij ∈I, s ∈Z≥0, and mij ∈Mij for 1 ≤j ≤s.
You can check that P
i∈I Mi is an R-submodule of P, and this is the smallest submodule
of P containing every Mi. If I is empty, we consider the vacuous sum P
i∈I Mi to be the
submodule {0M} consisting of zero alone.
A partially ordered set is a set X and a relation ≤on X that is reflexive (for all x ∈X,
x ≤x), antisymmetric (for all x, y ∈X, if x ≤y and y ≤x, then x = y), and transitive (for
all x, y, z ∈X, if x ≤y and y ≤z, then x ≤z). A lattice is a partially ordered set in which
any two elements have a least upper bound and a greatest lower bound (see the Appendix
for more detailed definitions). A complete lattice is a partially ordered set (X, ≤) in which
any nonempty subset of X has a least upper bound and a greatest lower bound. In more
detail, given any nonempty S ⊆X, the set of upper bounds {x ∈X : s ≤x for all s ∈S}
must be nonempty and have a least element, and similarly for the set of lower bounds of S.
For any left R-module M, the set X of all submodules of M is a partially ordered set under
the ordering defined by N ≤P iff N ⊆P (for N, P ∈X). Our results on intersections and
sums of submodules show that the poset X of submodules of a left R-module is a complete
lattice, since every nonempty family of submodules {Mi : i ∈I} has a greatest lower bound
(the intersection of the Mi) and a least upper bound (the sum of the Mi).
A left R-module M is called a simple module iff M ̸= {0} and the only R-submodules
of M are {0} and M. The adjective “simple” really describes the submodule lattice of M,
which is a poset with only two elements. For example, Z2, Z3, and Z5 are simple Z-modules,
but Z1 and Z4 are not simple Z-modules. You can check that a commutative ring R is simple
(as a left R-module) iff R is a field.
17.4
Submodule Generated by a Subset
Let S be any subset of a left R-module M. Our next goal is to construct a submodule of
M containing S that is as small as possible. To do this, let {Mi : i ∈I} be the family of all
submodules of M that contain S. This family is nonempty, since M itself is a submodule
of M containing S. Let N = T
i∈I Mi. Then N is a submodule, N contains S, and N is
contained in any submodule of M that contains S (since that submodule must be one of
the Mi). We write N = ⟨S⟩, and call N the submodule generated by S. Elements of S are
called generators for N. In the case of vector spaces, we say that the subspace N is spanned
by the vectors in S, and that S forms a spanning set for N.
We can give a more explicit description of ⟨S⟩that shows how elements of this submodule
are built up from elements of S. Let N ′ be the set of all finite sums r1s1 +· · ·+rnsn, where
ri ∈R, si ∈S, and n ∈Z≥0. Such a sum is called an R-linear combination of elements of S.
If n = 0, the sum is defined to be zero, so 0M is always in N ′. We show that N ′ = N = ⟨S⟩.
First, you can check that N ′ is an R-submodule of M. Since R has an identity, each s ∈S
482
Advanced Linear Algebra
can be written as 1R · s, which implies that N ′ contains S. Therefore, N ′ is one of the
submodules Mi appearing in the definition of N, and hence N ′ ⊇N. To show the reverse
inclusion, consider an arbitrary submodule Mi that contains S. Since Mi is a submodule,
it must contain r1s1 + · · · + rnsn, which is the typical element of N ′. Thus, N ′ ⊆Mi for
every Mi, and hence N ′ ⊆N.
We draw attention to some special cases of this result. If S = {s1, . . . , sk} is finite,
then ⟨S⟩= {Pk
i=1 risi : ri ∈R}. If S = {s} has one element, then ⟨S⟩= {rs : r ∈R}.
If S is empty, then ⟨S⟩= {0M}. Given any R-module M, we say M is finitely generated
iff there exists a finite subset S of M such that M = ⟨S⟩. We say M is cyclic iff there
exists a one-element set S such that M = ⟨S⟩. If S = {x}, we often write M = ⟨x⟩or
M = Rx instead of M = ⟨{x}⟩. You can check that for any x1, . . . , xn in an R-module M,
the sum of submodules Rx1 + · · · + Rxn equals the R-submodule ⟨x1, . . . , xn⟩generated by
{x1, . . . , xn}. More generally, for any S ⊆M, P
x∈S Rx = ⟨S⟩.
One property of generating sets is that module homomorphisms that agree on a
generating set must be equal. More formally, let M be an R-module generated by S, and
let f, g : M →P be R-module homomorphisms such that f(s) = g(s) for all s ∈S. Then
f = g. To prove this, take any nonzero element x of M and write it as x = r1s1 + · · · + rksk
with ri ∈R and si ∈S. Since f and g are module homomorphisms agreeing on S,
f(x) = Pk
i=1 rif(si) = Pk
i=1 rig(si) = g(x). Also, f(0M) = 0P = g(0M), so that f = g.
Let us use the ideas of cyclic submodules and generating sets to determine all the Z-
submodules of the Z-module M = Z4 × Z2. M is an 8-element commutative group, and its
submodules are precisely the subgroups of M. It is known from group theory (Lagrange’s
Theorem) that the size of each such subgroup must be a divisor of 8, namely 1 or 2 or 4 or
8. We can start finding subgroups by looking at the cyclic submodules generated by each
element of M. We discover the following submodules:
A = Z(0, 0)
=
{(0, 0)}
B = Z(0, 1)
=
{(0, 0), (0, 1)}
C = Z(1, 0)
=
{(0, 0), (1, 0), (2, 0), (3, 0)} = Z(3, 0)
D = Z(2, 0)
=
{(0, 0), (2, 0)}
E = Z(2, 1)
=
{(0, 0), (2, 1)}
F = Z(1, 1)
=
{(0, 0), (1, 1), (2, 0), (3, 1)} = Z(3, 1).
We can build further submodules by taking sums of the cyclic submodules found above.
This produces two new submodules, namely G = B + D = {(0, 0), (0, 1), (2, 0), (2, 1)} and
B + C = Z4 × Z2 = M. Figure 17.1 displays the lattice of submodules of the Z-module M.
In the figure, we abbreviate (x, y) as xy, and we draw a thick line from a submodule U up
to a submodule V whenever U ⊆V and there is no submodule properly between U and V .
This figure is called the Haase diagram of the poset of submodules of M.
17.5
Direct Products and Direct Sums
Let I be any set, and suppose we have a left R-module Mi for each i ∈I. Let N be the set
of all functions f : I →S
i∈I Mi such that f(i) ∈Mi for all i ∈I. We may regard such a
function as an I-tuple (f(i) : i ∈I), particularly when I is a finite set such as {1, 2, . . . , n}.
Given f, g ∈N, define the sum f + g by the rule (f + g)(i) = f(i) + g(i) ∈Mi for i ∈I.
Introduction to Modules
483
00
M
00,10,20,30
00,11,20,31
00,01,21,20
00,20
00,01
00,21
FIGURE 17.1
The lattice of submodules of the Z-Module Z4 × Z2.
You can check that N becomes a commutative group with this addition operation. Next,
for f ∈N and r ∈R, define the scalar multiple rf by the rule (rf)(i) = r · [f(i)] ∈Mi
for i ∈I. You can verify that this scalar multiplication makes N into a left R-module. The
axioms for N follow from the corresponding axioms for the Mi. For instance, to check (D1),
fix r, s ∈R and f ∈N, and compare the effects of the functions (r + s)f and rf + sf on a
generic index i ∈I:
[(r + s)f](i)
=
(r + s) · [f(i)] = r · [f(i)] + s · [f(i)]
(by (D1) in Mi)
=
(rf)(i) + (sf)(i) = (rf + sf)(i).
We write N = Q
i∈I Mi and call N the direct product of the modules Mi. If I = {1, 2, . . . , n},
we write N = M1×M2×· · ·×Mn. In this case, elements of N are often regarded as n-tuples
instead of functions. Using n-tuple notation for elements of N, the module operations are
(f1, . . . , fn) + (g1, . . . , gn) = (f1 + g1, . . . , fn + gn)
for all fi, gi ∈Mi;
r(f1, . . . , fn) = (rf1, . . . , rfn)
for all fi ∈Mi, r ∈R,
where we have written fi instead of f(i).
As a special case of the direct product, we can take every Mi equal to a given module
M, and we write N = M I in this case. If I = {1, 2, . . . , n}, we write N = M n. For instance,
the vector spaces F n (for F a field) are special cases of this construction. Taking R = Z,
we see that direct products of commutative groups are also special cases. There is a similar
construction for producing the direct product of an indexed set of right R-modules.
Consider a general direct product N = Q
i∈I Mi of left R-modules. We say that a
function f ∈N has finite support iff the set {i ∈I : f(i) ̸= 0Mi} is finite. Let N0 consist of
all functions f ∈N with finite support. You can check that N0 contains 0N and is closed
under addition and scalar multiplication, so that N0 is an R-submodule of N. We call N0
the direct sum of the R-modules Mi, written N0 = L
i∈I Mi. If I is finite, the direct sum
coincides with the direct product. For example, M1 ×M2 ×· · ·×Mn = M1 ⊕M2 ⊕· · ·⊕Mn.
Let F be a field. We show that every F-vector space V is isomorphic to a direct sum
of copies of F. The proof requires the well-known fact (proved in §17.14) that every vector
space has a basis. Given V , let X be a fixed basis of V and W be the direct sum of copies
484
Advanced Linear Algebra
of F indexed by X. Thus, a typical element of W is a function g : X →F with finite
support. To get a vector space isomorphism T : W →V , define T(g) = P
x∈X g(x)x ∈V
for g ∈W. The sum is well-defined because, for each fixed g ∈W, there are only finitely
many nonzero summands. To describe the inverse map S : V →W, recall that each v ∈V
can be uniquely written as a finite linear combination v = c1x1 +· · ·+cnxn for some n ≥0,
nonzero ci ∈F, and distinct xi ∈X. Define S(v) to be the function g : X →F such that
g(xi) = ci for 1 ≤i ≤n, and g(x) = 0 for all other x ∈X. You can verify that S and T are
linear maps that are inverses of each other. This result for vector spaces does not extend to
general R-modules, because (as we see later) not every R-module has a basis.
17.6
Homomorphism Modules
Let M and N be left R-modules. Define Hom(M, N) to be the set of all group homomor-
phisms f : M →N. Note that Hom(M, N) is a subset of the direct product N M = Q
x∈M N,
which is a left R-module. We check that the subset Hom(M, N) is an R-submodule of N M.
The zero element of N M is the zero map z : M →N such that z(x) = 0N for all x ∈M.
This map is a group homomorphism, so 0NM ∈Hom(M, N). Next, fix f, g ∈Hom(M, N).
Then f + g ∈Hom(M, N) because for all x, y ∈M,
(f + g)(x + y)
=
f(x + y) + g(x + y) = f(x) + f(y) + g(x) + g(y)
=
f(x) + g(x) + f(y) + g(y)
(since + in N is commutative)
=
(f + g)(x) + (f + g)(y).
Suppose f ∈Hom(M, N) and r ∈R. Then rf ∈Hom(M, N) because for all x, y ∈M,
(rf)(x + y) = r · [f(x + y)] = r · [f(x) + f(y)] = r · f(x) + r · f(y) = (rf)(x) + (rf)(y).
Since Hom(M, N) is an R-submodule of N M, it is a left R-module under the pointwise
operations on functions inherited from N M.
Define HomR(M, N) to be the set of all left R-module homomorphisms g : M →N. The
set HomR(M, N) is a subset of N M and also a subset of Hom(M, N). We show that, if R
is commutative, then HomR(M, N) is a submodule of N M and is therefore a left R-module.
You can check that HomR(M, N) is an additive subgroup of N M (for any ring R). Fix
r ∈R, f ∈HomR(M, N), s ∈R, and x ∈M. For R commutative, we have
(rf)(s · x)
=
r · [f(s · x)] = r · [s · f(x)] = (rs) · [f(x)]
=
(sr) · [f(x)]
(by commutativity of R)
=
s · (r · [f(x)]) = s · ((rf)(x)).
The other condition for being an R-map, namely (rf)(x + y) = (rf)(x) + (rf)(y) for
x, y ∈M, was already checked above. Therefore, rf ∈HomR(M, N), as needed.
As a special case of this construction, consider F-vector spaces V and W. The set of all
linear transformations from V to W, sometimes denoted by L(V, W), becomes an F-vector
space under pointwise operations on functions. This follows by noting that L(V, W) =
HomF (V, W) and F is commutative.
Introduction to Modules
485
17.7
Quotient Modules
In §1.6, we discussed the construction of the quotient group of a commutative group by a
subgroup, and the formation of the quotient space of a vector space by a subspace. These
constructions are special cases of quotient modules, which are defined as follows.
Let N be a submodule of a left R-module M. For each x ∈M, we have the coset
x + N = {x + n : n ∈N}. The quotient set M/N = {x + N : x ∈M} is the collection
of all cosets of N in M. The Coset Equality Theorem (§1.6) states that for all x, z ∈
M, x + N = z + N iff x −z ∈N. We also proved that M is the disjoint union of the
distinct cosets of N, meaning that every u ∈M belongs to exactly one coset in M/N.
Since N is a subgroup of (M, +), we can define an addition operation on M/N by setting
(x + N) + (y + N) = (x + y) + N for all x, y ∈M. We proved in §1.6 that this binary
operation is well-defined and satisfies the axioms for a commutative group. The identity
element of this quotient group is 0M/N = 0M + N = {0 + n : n ∈N} = N, and the additive
inverse of the coset x + N is the coset (−x) + N, for each x ∈M.
To complete the definition of the quotient R-module M/N, we introduce a scalar
multiplication operation · : R × M/N →M/N. Given r ∈R and x ∈M, define
r · (x + N) = (rx) + N. We must check that this operation is well-defined. Suppose r ∈R
and x, z ∈M satisfy x + N = z + N. We prove that r · (x + N) = r · (z + N). By the Coset
Equality Theorem, x −z ∈N. Since N is an R-submodule, r(x −z) ∈N. So rx −rz ∈N,
and hence (rx)+N = (rz)+N. This shows that r ·(x+N) = r ·(z +N). Now that we know
scalar multiplication is well-defined, it is routine to check that M/N is a left R-module.
The following calculation verifies axiom (D1): for all x ∈M and r, s ∈R,
(r + s) · (x + N)
=
((r + s)x + N)
=
((rx + sx) + N)
(by (D1) in M)
=
(rx + N) + (sx + N) = [r · (x + N)] + [s · (x + N)].
You can prove the other axioms for scalar multiplication similarly.
For any left R-module M and submodule N, there is a surjection p : M →M/N defined
by p(x) = x + N for x ∈M. This map is called the natural map from M to M/N, the
canonical map from M to M/N, or the projection of M onto M/N. The map p is R-linear,
since
p(x + y) = (x + y) + N = (x + N) + (y + N) = p(x) + p(y),
and p(rx) = (rx) + N = r · (x + N) = r · p(x)
for all x, y ∈M and r ∈R.
Here are some special cases and extensions of the quotient module construction. For
R = Z, the quotient Z-module M/N is identical to the quotient group M/N (since, as
we have seen, the Z-module structure on M/N is uniquely determined by the addition
operation on this set). If R is a field F, then a submodule N is a vector subspace of the
F-vector space M, and M/N is the quotient vector space of M by N. Finally, suppose R
is any ring, and M is R viewed as a left R-module. Given any left ideal I of R, R/I is a
left R-module. However, more can be said if I is a (two-sided) ideal of R (meaning that I
is both a left ideal and a right ideal, which always happens for commutative rings R). By
analogy with the definition of scalar multiplication · : R × R/I →R/I, we can define a
multiplication operation ⋆: R/I × R/I →R/I by setting (a + I) ⋆(b + I) = (ab) + I for all
a, b ∈R. To check that ⋆is well-defined, fix a, b, a′, b′ ∈R and assume a + I = a′ + I and
b + I = b′ + I. Using the Coset Equality Theorem, we see that (ab) + I = (a′b′) + I because
a −a′ ∈I,
b −b′ ∈I,
ab −a′b′ = a(b −b′) + (a −a′)b′,
486
Advanced Linear Algebra
and the last expression is in I because I is a two-sided ideal. You can check that (R/I, +, ⋆)
is a ring, which is commutative if R is commutative. R/I is called the quotient ring of R
by the ideal I.
One nice feature of modules is that the quotient of a module by any submodule is always
defined, unlike the situation for general groups (where the subgroup must be normal) or
rings (where the subset must be a two-sided ideal).
Suppose S is a generating set for the R-module M, and N is a submodule of M. Then the
image of S in M/N, namely p[S] = {s + N : s ∈S}, is a generating set for M/N. To prove
this, note that any coset in M/N has the form x + N for some x ∈M. Write x = P
i aisi
with ai ∈R and si ∈S. Then x + N = (P
i aisi) + N = P
i((aisi) + N) = P
i ai(si + N).
17.8
Changing the Ring of Scalars
This section describes two constructions for changing the ring of scalars for a given module.
First, let M be a left R-module, and suppose S is a subring of R. Restricting the scalar
multiplication · : R × M →M to the domain S × M, we get a scalar multiplication of S
on M that satisfies the module axioms. Hence, we can regard M as a left S-module. More
generally, suppose S is any ring and f : S →R is a ring homomorphism. You can verify
that the scalar multiplication ⋆: S × M →M, defined by s ⋆x = f(s) · x for s ∈S and
x ∈M, turns M into a left S-module. The situation where S is a subring is the special case
where f : S →R is the inclusion map given by f(s) = s for all s ∈S.
Second, let M be a left R-module. Suppose I is an ideal of R, and let S be the quotient
ring R/I. Assume that I annihilates M, meaning that i · x = 0M for all i ∈I and x ∈M.
We claim that M is an S-module (i.e., an R/I-module) with scalar multiplication defined
by (r + I) • x = r · x for r ∈R and x ∈M. To check that • is well-defined, fix r, r′ ∈R
and x ∈M with r + I = r′ + I. We know r −r′ ∈I, so (r −r′) · x = 0 (by the annihilation
condition), hence r · x = r′ · x and (r + I) • x = (r′ + I) • x. The S-module axioms now
follow routinely from the corresponding R-module axioms. For instance, (M2) is true since
1R/I • x = (1R + I) • x = 1R · x = x for all x ∈M.
Let N be any R-module, not necessarily annihilated by the ideal I. Define the subset
IN of N to be the set of all finite sums of terms i · x with i ∈I and x ∈N. You can check
that IN is an R-submodule of N, so that the quotient module M = N/IN is defined. Now,
M is annihilated by I, because i · (x + IN) = (i · x) + IN = 0N + IN = 0M for all i ∈I
and x ∈N (since i · x −0N ∈IN). Therefore, M is an S-module. We restate this result for
emphasis: given any R-module N and any ideal I of R, N/IN is an R/I-module via the
rule (r + I) • (x + IN) = (r · x) + IN for r ∈R and x ∈N.
Here is one situation in which this result can be helpful. Let R be a nonzero commutative
ring, N an R-module, and I a maximal ideal of R (meaning that I ̸= R and there are no
ideals J with I ⊊J ⊊R). It can be shown that such maximal ideals exist in R (see §17.13)
and that maximality of I implies F = R/I is a field (see Exercise 53). Hence, we can pass
from the general module N over the ring R to an associated vector space over the field F,
namely N/IN. As we see later, this association lets us use known theorems about vector
spaces over a field to deduce facts about modules over a commutative ring.
Introduction to Modules
487
17.9
Fundamental Homomorphism Theorem for Modules
The next two sections describe some key theorems concerning R-module homomorphisms,
which have analogs for homomorphisms of groups, rings, and vector spaces (cf. §1.7).
Let f : M →N be a homomorphism of left R-modules. The kernel of f, written ker(f),
is {x ∈M : f(x) = 0N}. The image of f, written img(f), is {y ∈N : y = f(x) for some x ∈
M}. You can check that ker(f) is a submodule of M, and img(f) is a submodule of N.
More generally, if M ′ is any submodule of M, then the image f[M ′] = {f(x) : x ∈M ′} is a
submodule of N. The image of f is the special case where M ′ = M. If N ′ is any submodule
of N, then the preimage f −1[N ′] = {x ∈M : f(x) ∈N ′} is a submodule of M. The kernel
of f is the special case obtained by taking N ′ = {0N}. For any submodule N ′ of N, f −1[N ′]
contains ker(f).
It is immediate from the definitions that f is surjective iff img(f) = N. We now prove
that f is injective iff ker(f) = {0M}. First suppose the kernel is {0}. Assume x, z ∈M
satisfy f(x) = f(z). Then f(x −z) = f(x) −f(z) = 0, so that x −z ∈ker(f) = {0}.
Therefore, x = z, proving f is injective. Conversely, assume f is injective. On one hand,
0M ∈ker(f) since f(0M) = 0N. On the other hand, for any x ∈M unequal to 0, injectivity
of f gives f(x) ̸= f(0) = 0, so x ̸∈ker(f). Thus, ker(f) = {0M}.
Using kernels, images, and quotient modules, the next theorem builds an R-module
isomorphism from any given R-module homomorphism.
Fundamental Homomorphism Theorem for Modules. Let f : M →N be a homo-
morphism of left R-modules. There is an R-module isomorphism f ′ : M/ ker(f) →img(f)
given by f ′(x + ker(f)) = f(x) for x ∈M.
Proof. Although this result can be deduced quickly from the Fundamental Homomorphism
Theorem for Groups (see §1.7), we prove the full result here for emphasis. The crucial first
step is to check that f ′ is well-defined. Let K = ker(f), and suppose x + K = z + K for
fixed x, z ∈M. Then z = x + k for some k ∈K, and
f ′(z + K) = f(z) = f(x + k) = f(x) + f(k) = f(x) + 0 = f(x) = f ′(x + K),
so that f ′ is well-defined. Second, let us check that f ′ is an R-module homomorphism. For
fixed x, y ∈M and r ∈R, compute:
f ′((x + K) + (y + K)) = f ′((x + y) + K) = f(x + y) = f(x) + f(y) = f ′(x + K) + f ′(y + K);
f ′(r(x + K)) = f ′((rx) + K) = f(rx) = rf(x) = rf ′(x + K).
Third, f ′ is injective, since for x, y ∈M, f ′(x + K) = f ′(y + K) implies f(x) = f(y),
hence f(x −y) = 0, hence x −y ∈K, hence x + K = y + K. Fourth, the image of f ′ is
{f ′(x + K) : x ∈M} = {f(x) : x ∈M}, which is the image of f, so that f ′ is surjective as
a mapping into the codomain img(f). Thus, f ′ is an R-module isomorphism.
By modifying the proof, we obtain the following more general result (Exercise 59).
Generalized Fundamental Homomorphism Theorem for Modules. Let f : M →N
be an R-module homomorphism and H be a submodule of M such that H ⊆ker(f). There
is a well-defined R-module homomorphism f ′ : M/H →N given by f ′(x + H) = f(x) for
x ∈M. We have img(f ′) = img(f) and ker(f ′) = ker(f)/H.
The generalized theorem can also be restated as a universal mapping property (UMP)
for quotient modules.
488
Advanced Linear Algebra
Universal Mapping Property (UMP) for Quotient Modules. Let f : M →N be an
R-module homomorphism, H be a submodule of M with H ⊆ker(f), and p : M →M/H be
the canonical projection. There exists a unique R-module homomorphism f ′ : M/H →N
such that f = f ′ ◦p. Also, img(f ′) = img(f) and ker(f ′) = ker(f)/H.
The setup for the UMP is displayed in the following diagram:
M
p /
f
"
M/H
∃!f ′

N
Existence of f ′ follows from the generalized theorem, together with the observation that
(f ′ ◦p)(x) = f ′(p(x)) = f ′(x + H) = f(x) for all x ∈M. Similarly, uniqueness of f ′ holds
since the requirement f = f ′ ◦p forces us to define f ′(x+H) = f ′(p(x)) = (f ′ ◦p)(x) = f(x)
for all x ∈M.
17.10
More Module Isomorphism Theorems
We now use the Fundamental Homomorphism Theorem for Modules to deduce some further
isomorphism theorems that play a prominent role in module theory.
Diamond Isomorphism Theorem. Let M and N be submodules of a left R-module
P. The quotient modules M/(M ∩N) and (M + N)/N are isomorphic, via the R-map
g : M/(M ∩N) →(M + N)/N given by g(m + M ∩N) = m + N for m ∈M.
Figure 17.2, showing part of the submodule lattice of P, explains the name “Diamond
Isomorphism Theorem” and can aid in remembering the theorem. The edges marked by
double lines indicate which two quotient modules are isomorphic. By interchanging M and
N, the theorem also yields an R-module isomorphism (M + N)/M ∼= N/(M ∩N).
P
M + N
M
N
M ∩N
{0}
FIGURE 17.2
Picture of the Diamond Isomorphism Theorem.
Introduction to Modules
489
Proof. Define f : M →(M + N)/N by f(m) = m + N for m ∈M. The map f is the
composition of an inclusion M →M +N and a canonical map M +N →(M +N)/N, hence
is a module homomorphism. For any m in the domain M of f, f(m) = 0 in (M + N)/N iff
m+N = 0+N iff m ∈N. So the kernel of f equals M ∩N. We claim that the image of f is all
of (M +N)/N. A typical element of the set (M +N)/N is a coset (m+n)+N, where m ∈M
and n ∈N. But (m+n)+N = (m+N)+(n+N) = (m+N)+0(M+N)/N = m+N = f(m),
so that this coset is in the image of f. Applying the Fundamental Homomorphism Theorem
to f provides an isomorphism g given by the stated formula.
Nested Quotient Isomorphism Theorem. Suppose M is a left R-module, with
submodules H
and N
such that H
⊆
N. There is an R-module isomorphism
g : (M/H)/(N/H) →(M/N) given by g((x + H) + (N/H)) = x + N for x ∈M.
Proof. Consider the projection homomorphism p : M →M/N. Since H ⊆ker(p) = N by
assumption, applying the Generalized Fundamental Homomorphism Theorem to p produces
a well-defined R-map f : M/H →M/N given by f(x+H) = x+N for x ∈M. That theorem
also tells us that img(f) = img(p) = M/N and ker(f) = ker(p)/H = N/H. Applying the
Fundamental Homomorphism Theorem to f, we get the isomorphism g given by the stated
formula.
Correspondence Theorem for Modules. Let N be a fixed submodule of a left R-module
M and p : M →M/N be the projection map. Let A be the collection of all submodules
U of M such that N ⊆U ⊆M. Let B be the collection of all submodules of M/N. There
are inclusion-preserving, mutually inverse bijections T : A →B and S : B →A given by
T(U) = p[U] = U/N (the image of U under p) for U ∈A and S(V ) = p−1[V ] (the preimage
of V under p) for V ∈B.
The statement that T preserves inclusions means that for all U1, U2 ∈A, if U1 ⊆U2 then
T(U1) ⊆T(U2); similarly for S. Note that square brackets denote the image or preimage of
a subset under p, whereas round parentheses denote evaluation of a function at an input.
Each U ∈A is an element of the domain of T, while U is a subset of the domain of p.
Similarly, each V ∈B is an element of the domain of S, while V is a subset of the codomain
of p. In particular, the function T is not the same as the function p. Similarly, S ̸= p−1; in
fact, the inverse function p−1 only exists when N = {0}.
Proof. The proof of the Correspondence Theorem consists of a sequence of routine but
lengthy verifications, some details of which appear in Exercises 55 and 61. First, T does map
A into B, since the image p[U] is a submodule of M/N for any submodule U of M (whether or
not U contains N). Second, S does map B into A, since the preimage p−1[V ] is a submodule
of M containing ker(p) = N for any submodule V of M/N. Third, for any subset W of M/N,
p[p−1[W]] = W follows from the surjectivity of the function p : M →M/N. Consequently,
taking W to be any submodule V of M/N, we see that T(S(V )) = V = idB(V ) for all
V ∈B, and hence T ◦S = idB. Fourth, we prove that S(T(U)) = U = idA(U) for any
U ∈A (hence S ◦T = idA). It follows from the definition of images and preimages that
S(T(U)) = p−1[p[U]] ⊇U. To check the reverse inclusion, recall that U is a submodule of M
containing N. Let x ∈p−1[p[U]]. Then p(x) = x + N ∈p[U] = U/N, so there exists z ∈U
with x + N = z + N. This means there exists n ∈N with x = z + n. Since N ⊆U and U is
closed under addition, we have x ∈U. This establishes the inclusion p−1[p[U]] ⊆U. Fifth,
you can show that inclusions are preserved when we take images or preimages of subsets
under any function. This fact (applied to p) implies that T and S preserve inclusions.
490
Advanced Linear Algebra
The Correspondence Theorem shows that T and S are lattice isomorphisms between the
lattice A of submodules of M containing N and the lattice B of all submodules of M/N.
(A lattice isomorphism is a bijection f between two lattices such that f and its inverse
preserve the order relation, which in this situation is set inclusion.) This fact provides
some retroactive motivation for the quotient module construction: if we are studying the
submodule lattice of M, we can focus attention on the part of the lattice above the
submodule N by passing to the submodule lattice of M/N. See Figure 17.3 for a picture of
the relevant lattices.
U/N
M
N
U
0
T
N/N
S
M/N
FIGURE 17.3
The lattice isomorphisms in the Correspondence Theorem for Modules.
Recognition Theorem for Direct Products.
Suppose N and P are submodules of
a left R-module M such that N + P = M and N ∩P = {0M}. There is an R-module
isomorphism g : N × P →M given by g((x, y)) = x + y for x ∈N and y ∈P.
Proof. We check that g is a one-to-one, onto, R-linear map. To verify R-linearity, fix x1, x2 ∈
N and y1, y2 ∈P, and compute
g((x1, y1) + (x2, y2)) = g((x1 + x2, y1 + y2)) = (x1 + x2) + (y1 + y2)
= (x1 + y1) + (x2 + y2) = g((x1, y1)) + g((x2, y2)).
Given x ∈N, y ∈P, and r ∈R, we compute
g(r(x, y)) = g((rx, ry)) = rx + ry = r(x + y) = rg((x, y)).
To prove g is one-to-one, we show that ker(g) = {(0, 0)}. For any (x, y) in ker(g), 0 =
g((x, y)) = x + y, so y = −x. We know y ∈P, and since y = −x, y is also in the submodule
N. Thus y ∈N ∩P = {0M}, hence y = 0. Then x = −y = 0 and (x, y) = (0, 0), as needed.
To prove g is onto, let z ∈M be arbitrary. Since N + P = M, there exist x ∈N and y ∈P
with z = x + y = g((x, y)).
Exercise 79 generalizes the Recognition Theorem to products of more than two factors.
Introduction to Modules
491
17.11
Free Modules
The concepts of spanning sets, linearly independent sets, and bases play a key role in the
theory of vector spaces over a field. Now we consider the analogous concepts for a left R-
module M where R is any ring. For S ⊆M, we say S spans M and M is generated by S
to mean that every element of M can be written as a finite sum a1s1 + · · · + aksk for some
k ∈Z≥0, a1, . . . , ak ∈R, and s1, . . . , sk ∈S. Such a sum is called an R-linear combination
of elements of S. An ordered list (s1, . . . , sk) of elements of M is called R-independent (or
R-linearly independent) iff for all a1, . . . , ak ∈R, the relation a1s1 +· · ·+aksk = 0M implies
ai = 0R for all i. Negating this definition, we see that the list (s1, . . . , sk) is R-dependent or
R-linearly dependent iff there exist a1, . . . , ak ∈R with a1s1 + · · · + aksk = 0M and some
aj ̸= 0R. The list (s1, . . . , sk) is an ordered basis of the R-module M iff the list is R-linearly
independent and {s1, . . . , sk} spans M.
A subset S of M (finite or not) is R-independent iff every finite list of distinct elements
of S is linearly independent; otherwise, S is R-dependent. If there exists an R-independent
generating set S for the module M, then M is called a free R-module, and S is called an
R-basis or basis of M.
Certain results about generating sets, independence, and free R-modules are exactly
like the corresponding results for vector spaces, but other familiar theorems about vector
spaces and bases do not always hold for general left R-modules. Here are the key facts in
the module case.
1.
If M is a free left R-module with basis S, then every y ∈M can be uniquely
expressed as a finite R-linear combination of elements of S. The existence of
such an expression follows from the fact that S generates M. For uniqueness,
suppose that y = a1s1 + · · · + aksk = b1s1 + · · · + bksk where k ∈Z≥0, ai, bi ∈R,
and s1, . . . , sk are distinct elements of S. (We can assume that the same elements
of S are used in both expressions, by adding new terms with zero coefficients if
needed.) Subtracting gives 0 = Pk
i=1(ai −bi)si. By the R-independence of S, we
deduce ai −bi = 0 for all i. So ai = bi for all i.
2.
Universal Mapping Property (UMP) for Free R-modules. Let M be a
free left R-module with basis S. Given any left R-module N and any function
g : S →N, there exists a unique R-module homomorphism g′ : M →N whose
restriction to S equals g.
S
⊆/
g
 
M
∃!g′

N
To prove existence, take any y ∈M, and write y uniquely as y = P
x∈S axx where
ax ∈R and all but finitely many ax are 0. Define g′(y) = P
x∈S axg(x) ∈N. You
can check that g′ is R-linear and that g′(x) = g(x) for x ∈S. For uniqueness,
suppose g′′ is another R-linear map such that g′′ extends g. Then g′ = g′′ follows
because two R-linear maps that agree on the generating set S must be equal.
3.
Every free left R-module M is isomorphic to a direct sum of copies of R. We
imitate the proof of the corresponding vector space result (see §17.5). Let S be
an R-basis for the free module M. Let N = L
x∈S R. Each element of N is a
function g : S →R that is nonzero for only finitely many elements of its domain
492
Advanced Linear Algebra
S. Define a map T : N →M by setting T(g) = P
x∈S g(x) · x, which is well-
defined because the sum has only a finite number of nonzero terms. Define a map
T ′ : M →N as follows. Given y ∈M, write y uniquely as y = P
x∈S cxx where
only finitely many cx ∈R are nonzero. Define T ′(y) to be the function given by
g(x) = cx for x ∈S. You can verify that T and T ′ are R-linear maps that are
inverses of each other. We call the S-tuple T ′(y) = (cx : x ∈S) the coordinates
of y relative to the basis S.
4.
Given any set S and any nonzero ring R, there exists a free left R-module with
basis S. We let M = L
x∈S R, the direct sum of copies of R indexed by S.
Elements of M are functions g : S →R with finite support. For each s ∈S, we
have an associated function es : S →R such that es(x) = 0R for all x ̸= s in S,
and es(s) = 1R. (Note that 0R ̸= 1R in the nonzero ring R.) Each es belongs to
M; we show that {es : s ∈S} is a basis for M. Given any nonzero g ∈M, let
{s1, . . . , sk} be the elements of S for which g(si) ̸= 0. Then g = Pk
i=1 g(si)esi, as
can be seen by evaluating both sides at each s ∈S. Next, to show R-independence,
suppose P
t∈S atet = 0 for some at ∈R (with all but finitely many at equal to
zero). Evaluating this function at s ∈S, we see that each as = 0. Finally, we
change notation by replacing each element es ∈M by the corresponding element
s ∈S. Then M is a free R-module with basis S (before the notation change, we
had a basis {es} in bijective correspondence with S).
5.
For some rings R, not every left R-module is free. For example, consider R = Z.
A free R-module M is isomorphic to a direct sum of copies of Z. Hence, either M
is the zero module or M is infinite. On the other hand, any finite commutative
group G is a Z-module. Therefore, if G is not the zero group, then G is a non-free
Z-module. More generally, suppose R is an infinite ring containing a left ideal I
such that R/I is finite and nonzero. Then R/I is a left R-module that cannot be
isomorphic to a direct sum of copies of R; hence R/I is a non-free R-module.
6.
For some rings R, every left R-module is free. For example, if R is a field, then
every R-module is free because every vector space has a basis. Later in this
chapter, we prove the more general result that all modules over a division ring R
are free.
7.
For some rings R, not every left R-module is isomorphic to a submodule of a free
R-module. For example, consider R = Z. If G is a nonzero finite commutative
group, then G is not isomorphic to a submodule of any free R-module M. This
holds since M must be a direct sum of copies of Z, which implies that every
nonzero submodule of M is infinite.
8.
Every left R-module is isomorphic to a quotient module of a free left R-module.
Let M be an arbitrary left R-module. Let F be a free left R-module having
the set M as a basis. Consider the identity map idM : M →M, where the
domain is viewed as a subset of F, and the codomain is viewed as a left R-
module. By the UMP, this map extends uniquely to an R-module homomorphism
g : F →M, which is evidently surjective. By the Fundamental Homomorphism
Theorem, F/ ker(g) ∼= M. More generally, if S is any generating set for M, then we
could take F to be a free left R-module with S as a basis. The R-map g : F →M
that extends the inclusion map i : S →M must be surjective, since the image
of g is a submodule of M containing the generating set S of M. This shows that
every finitely generated left R-module M is isomorphic to a quotient module of a
finitely generated free left R-module F.
Introduction to Modules
493
17.12
Finitely Generated Modules over a Division Ring
A division ring is a ring D such that 1D ̸= 0D and for all nonzero x ∈D, there exists y ∈D
with xy = 1D = yx. This condition says that every nonzero element of D has a two-sided
multiplicative inverse. A field is the same thing as a division ring where multiplication is
commutative. The quaternion ring H (see Exercise 40 of Chapter 1) is an example of a
division ring that is not a field.
Let D be a division ring. A left D-module V is also called a left vector space over D.
Our next goal is to show that facts about bases and dimensions of vector spaces over fields
extend to the setting of modules over division rings. These facts are summarized in the next
theorem.
Theorem on Modules over Division Rings.
Suppose D is a division ring and M is a left D-module.
(a) There exists a D-basis of M.
(b) Any D-independent set in M can be extended to a basis of M.
(c) Any generating set for M contains a basis of M.
(d) For all S, T ⊆M, if S is D-independent and T generates M, then |S| ≤|T|.
(e) Any two bases of M have the same cardinality, called the dimension of M.
We prove this theorem for finitely generated D-modules here. The proofs in the general
case require Zorn’s Lemma and are covered later. Part (a) says that every module over a
division ring is free.
Lemma on Extending an Independent List. Suppose D is a division ring, M is a left
D-module, x1, . . . , xk is a D-independent list in M, and y ∈M. The list x1, . . . , xk, y is
D-independent iff y ̸∈⟨x1, . . . , xk⟩.
Proof. We give a contrapositive proof of both implications. First, we assume y ∈⟨x1, . . . , xk⟩
and prove that x1, . . . , xk, y is a D-linearly dependent list. We know there are scalars
d1, . . . , dk ∈D with y = d1x1 + · · · + dkxk. Then d1x1 + · · · + dkxk + (−1D)y = 0D
expresses 0D as a D-linear combination of the list x1, . . . , xk, y where the coefficient of y is
−1D ̸= 0D. So this list is linearly dependent.
To prove the converse, assume x1, . . . , xk, y is linearly dependent. So there exist
d1, . . . , dk, d ∈D with d1x1+· · ·+dkxk+dy = 0D and at least one of the scalars d1, . . . , dk, d
is nonzero. If d = 0, then d1x1 + · · · + dkxk = 0 with some di ̸= 0. This contradicts the
assumed linear independence of x1, . . . , xk. Therefore d ̸= 0, which means d−1 exists in the
division ring D. Rearranging the given relation, we get dy = −d1x1 −· · · −dkxk and then
y = −d−1d1x1 −· · · −d−1dkxk. So y ∈⟨x1, . . . , xk⟩, as needed.
A similar proof establishes the following variation of the lemma (Exercise 62).
Lemma on Extending an Independent Set. Suppose D is a division ring, M is a left
D-module, and S is a D-linearly independent subset of M. For all y ∈M \ S, S ∪{y} is
D-linearly independent iff y ̸∈⟨S⟩.
Lemma on Building an Ordered Basis. Suppose D is a division ring, M is a finitely
generated left D-module, x1, . . . , xk is a D-linearly independent list in M, and y1, . . . , ym
is a list generating M (where k and m are finite). Then M has an ordered D-basis of the
form x1, . . . , xk, yi1, . . . , yis for some {i1, . . . , is} ⊆{1, 2, . . . , m}.
Informally, the lemma says we can extend any given independent list to a basis of M by
appending suitable vectors from any generating list for M.
494
Advanced Linear Algebra
Proof. Let N = ⟨x1, . . . , xk⟩, which is a submodule of M. If every yi ∈N, then N = M,
since N contains a generating set for M. In this case, the list x1, . . . , xk is already a D-basis
for M. The other possibility is that some yi is not in N. Let i1 be the least index i with
this property, and note x1, . . . , xk, yi1 is D-independent by the Lemma on Extending an
Independent List.
We now apply the same reasoning to the submodule N1 = ⟨x1, . . . , xk, yi1⟩generated by
the new D-independent list. If N1 = M, then we have found a basis for M. Otherwise, some
yi is not in N1. Let i2 be the least index i with this property, and note x1, . . . , xk, yi1, yi2 is
D-independent by the lemma. Continuing in this way, the extension process must eventually
terminate with a D-basis of M, since there are only finitely many yi available.
Assuming M is a finitely generated D-module, we can now prove parts (a), (b), and
(c) of the main theorem with sets replaced by finite lists. Let y1, . . . , ym be any finite list
generating M. The lemma shows that any finite D-independent list L in M can be extended
to a basis of M, proving (b). Taking L to be the empty list, we see that the given generating
list contains an ordered basis of M, proving (a) and (c).
Comparison Lemma for Finite Independent Sets and Spanning Sets. Suppose D
is a division ring, M is a finitely generated left D-module, S = {x1, . . . , xk} is a k-element
D-linearly independent subset of M, and T = {y1, . . . , ym} is an m-element generating set
for M, where k and m are finite. Then k ≤m.
Proof. We prove the lemma by induction on ℓ= |S \ T|, the number of elements in S but
not in T. The base case is ℓ= 0, which occurs iff S ⊆T. In this case, k ≤m certainly holds
since S is a subset of T.
For the induction step, suppose |S\T| = ℓ> 0. Assume this induction hypothesis: for all
finite independent sets S1 in M and all finite generating sets T1 of M such that |S1\T1| < ℓ,
we have |S1| ≤|T1|. We must prove |S| ≤|T|.
Take a fixed s ∈S with s ̸∈T. Since s ∈M and T generates M, we have s = d1y1 +
· · · + dmym for some d1, . . . , dm ∈D. Now s ̸= 0M, since otherwise S would be D-linearly
dependent. Thus, at least one dj is nonzero. Keeping only the nonzero dj, we have s =
di1yi1+· · ·+dipyip for some p > 0. We know yi1, . . . , yip all belong to T, while s is not in T. If
all of yi1, . . . , yip belong to S, then the dependence relation di1yi1 +· · ·+dipyip +(−1D)s = 0
contradicts the linear independence of S. So some yij belongs to T but not S. By changing
notation for the elements of T, we can assume yij is y1.
To summarize the situation so far, we have s ∈S\T, y1 ∈T \S, and s = d1y1+· · ·+dmym
with d1 ̸= 0. Consider T1 = {s, y2, . . . , ym}, which is the set obtained from T by exchanging
s for y1. Since y1 = d−1
1 (s −d2y2 −· · · −dmym), T1 still generates M. Also, T1 still has size
m, since s (which is not in T) does not equal yj for any j between 2 and m. The crucial
point is that S \ T1 = (S \ T) \ {s}, so that |S \ T1| = ℓ−1 < ℓ. Applying the induction
hypothesis to the sets S and T1, we conclude that k ≤m, as needed.
This lemma proves part (d) of the main theorem for finite subsets S, T of M. Assuming
M is finitely generated, there exists a generating set T for M of finite size m. In this
situation, an infinite subset S of M cannot be linearly independent. Otherwise, S would
have a subset of size m + 1 that is necessarily linearly independent, but this contradicts the
lemma. So (in the finitely generated case), part (d) is now proved for all subsets S, T of M.
We can now prove part (e) of the theorem, still assuming M is finitely generated. Let
S and T be any two bases of M. We just proved that S and T must both be finite (since
they are linearly independent subsets of M). Since S is independent and T generates M,
|S| ≤|T|. Since T is independent and S generates M, |T| ≤|S|. Thus, |S| = |T|.
Introduction to Modules
495
17.13
Zorn’s Lemma
To extend the results of the previous section to modules that are not finitely generated, we
need an axiom of set theory called Zorn’s Lemma. This axiom is equivalent to the Axiom
of Choice, which has many other equivalent formulations. For algebraic applications of the
Axiom of Choice, it is often most convenient to use Zorn’s Lemma, so that is the version
of the axiom presented here. For more details on the Axiom of Choice and its variants, the
reader may consult the set theory texts by Halmos [27] or Monk [42].
Recall that a partially ordered set (or poset) is a structure (Z, ≤) where Z is a set and
≤is a relation on Z that is reflexive, antisymmetric, and transitive. For example, if X is
any set and Z is the set of all subsets of X, then Z with the relation ⊆(set inclusion) is a
poset. More generally, for any collection Z of subsets of X, (Z, ⊆) is a poset.
For any poset (Z, ≤), x ∈Z is a maximal element of Z iff for all y ∈Z, x ≤y implies
x = y. This definition says that no element of the poset is strictly larger than x under the
given ordering. Given any subset Y of Z, we say z ∈Z is an upper bound for Y iff y ≤z
for all y ∈Y . A subset Y of Z is a chain iff for all x, y ∈Y , x ≤y or y ≤x.
Zorn’s Lemma. For any poset (Z, ≤), if every chain Y ⊆Z has an upper bound in Z,
then Z has a maximal element.
We adopt this statement as an axiom, so it does not require proof (although, as
mentioned above, it can be proved as a consequence of the Axiom of Choice).
We frequently apply Zorn’s Lemma to posets (Z, ⊆), where Z is some collection of subsets
of a given set X. To verify the hypothesis of Zorn’s Lemma in this setting, we consider an
arbitrary chain Y of elements of Z, which we can write as an indexed set Y = {Si : i ∈I}.
Note that every Si is a subset of X. The assumption that Y is a chain means that for
all i, j ∈I, either Si ⊆Sj or Sj ⊆Si. If Z is the collection of all subsets of X, we see
immediately that S = S
i∈I Si is an upper bound for the chain Y in Z, since Si ⊆S for all
i ∈I. But in general, Z does not consist of all subsets of X, so the fact that S belongs to
Z requires proof. In fact, for some choices of Z, the union of the Si is not in Z, so some
other subset of X must be used as an upper bound for Y in the poset Z. In any case, once
we have found an upper bound for the chain Y that does belong to Z, we can conclude via
Zorn’s Lemma that Z has a maximal element. By definition, this is a subset M of X such
that no subset of X properly containing M belongs to Z.
One other subtlety that occurs when using Zorn’s Lemma is the fact that the empty
subset of (Z, ≤) is always a chain. Any element of Z is an upper bound for this chain.
Checking that the hypothesis of Zorn’s Lemma does hold for this special chain amounts to
proving that the set Z is nonempty. Depending on how Z is defined, it may be a nontrivial
task to verify this assertion. Once this verification has been done, we can assume (when
checking the hypothesis of Zorn’s Lemma) that all chains being considered are nonempty.
Before using Zorn’s Lemma to prove theorems about bases of left vector spaces, we prove
a useful result in ring theory to illustrate the application of Zorn’s Lemma. By definition,
a maximal ideal in a commutative ring R is an ideal M such that M ̸= R and there is no
ideal J of R with M ⊊J ⊊R.
Theorem on Existence of Maximal Ideals. Every nonzero commutative ring R contains
a maximal ideal.
Proof. To apply Zorn’s Lemma, we introduce the poset (Z, ⊆), where Z is the collection
of all ideals I of R with I ̸= R. Since R is a nonzero ring, the set {0R} is an ideal of R
that does not equal R, so Z is nonempty. To check the hypothesis of Zorn’s Lemma, let
496
Advanced Linear Algebra
{Jt : t ∈T} be any nonempty chain of elements of Z, so each Jt is an ideal of R with
Jt ̸= R. Our candidate for an upper bound for this chain is J = S
t∈T Jt. We must check
that J does belong to Z, meaning that J is an ideal of R and J ̸= R. On one hand, 1R
does not belong to any Jt for t ∈T, since otherwise Jt = R. Then 1R does not belong to
the union J of the Jt, so that J is a proper subset of R. On the other hand, let us check
that J is an ideal of R. First, 0R ∈J since 0R ∈Jt for every Jt (this uses the fact that
T ̸= ∅). Next, fix x, y ∈J and r ∈R. We have x ∈Js and y ∈Jt for some s, t ∈T. Then
−x ∈Js (since Js is an ideal), hence −x ∈J. Similarly, rx ∈Js and so rx ∈J. To finish,
we prove x + y ∈J. Since we are looking at a chain of ideals, either Js ⊆Jt or Jt ⊆Js. In
the first case, x and y both belong to Jt, so x + y ∈Jt ⊆J. In the second case, x, y ∈Js,
so x + y ∈Js ⊆J. Thus, J is an ideal. We have now proved that J is an upper bound for
the given chain that belongs to the poset Z.
Having checked the hypothesis of Zorn’s Lemma, we deduce from the lemma that the
poset Z has a maximal element. By definition, this is an ideal M of R such that M ̸= R
and there does not exist N ∈Z with M ⊊N. In other words, there is no ideal N of R with
M ⊊N ⊊R, which is exactly what it means for M to be a maximal ideal of R.
17.14
Existence of Bases for Modules over Division Rings
Zorn’s Lemma is exactly the tool needed to prove the following fundamental result on
existence of bases for left vector spaces over division rings.
Theorem on Building Bases. Suppose D is a division ring, M is a left D-module, S is a
D-linearly independent subset of M, and T is a generating set for M. There exists a basis
B for M with S ⊆B ⊆S ∪T.
As special cases of this theorem, we see that M has a D-basis (take S = ∅and T = M);
any D-independent set S ⊆M can be extended to a basis (take T = M); and any generating
set T of M contains a basis (take S = ∅).
Proof. Consider the poset (Z, ⊆), where Z is the collection of all D-linearly independent
subsets C such that S ⊆C ⊆S ∪T. We know Z is nonempty since the D-linearly
independent set S belongs to Z. To check the hypothesis of Zorn’s Lemma, let {Ci : i ∈I}
be any nonempty chain of elements of Z. Each Ci is a D-linearly independent set such that
S ⊆Ci ⊆S ∪T. Define C = S
i∈I Ci. We check that C is an upper bound for the given
chain that belongs to Z. Evidently, Ci ⊆C for all i ∈I, and S ⊆C ⊆S ∪T. To finish
showing C ∈Z, we need only confirm that C is D-linearly independent.
Assume, to get a contradiction, that C is linearly dependent. Then there exist a finite
list of distinct elements x1, . . . , xn ∈C and nonzero scalars d1, . . . , dn ∈D such that
d1x1 + · · · + dnxn = 0M. Because C is the union of the sets Ci, each xj belongs to Cij
for some ij ∈I. We prove by induction on n that there exists a single index i ∈I such
that x1, . . . , xn all belong to Ci. This certainly holds if n = 1. If n > 1, we can assume by
induction that x1, . . . , xn−1 all belong to Ci′ for some i′ ∈I. Now, Ci′ ⊆Cin or Cin ⊆Ci′
since {Ci : i ∈I} is a chain. In the first case, x1, . . . , xn all belong to Cin. In the second
case, x1, . . . , xn all belong to Ci′. This completes the induction proof. Now x1, . . . , xn is
a finite list of distinct elements of Ci that satisfy a nontrivial dependence relation. This
contradicts the assumed linear independence of Ci.
The hypothesis of Zorn’s Lemma has now been checked. By that lemma, there exists
a maximal element B in the poset Z. Since B ∈M, we know S ⊆B ⊆S ∪T and B is
Introduction to Modules
497
D-linearly independent. We claim B is a D-basis of M. It suffices to show that every y in
the generating set T for M is in the submodule ⟨B⟩generated by B. To get a contradiction,
assume some y ∈T does not belong to ⟨B⟩. Let B′ = B ∪{y}, so S ⊆B ⊊B′ ⊆S ∪T. By
the Lemma on Extending an Independent Set, B′ is D-linearly independent. So B′ belongs
to Z but properly contains B. This contradicts the maximality of B in Z. Thus B is a
D-basis of M, as needed.
17.15
Basis Invariance for Modules over Division Rings
As our next application of Zorn’s Lemma, we prove a result that says (informally) that an
independent set cannot be larger than a spanning set in a left vector space. First we need
another technical lemma.
Exchange Lemma. Suppose D is a division ring, M is a left D-module, X is a D-linearly
independent subset of M, and Y is a generating set for M. For any x ∈X \ Y , there exists
y ∈Y \ X such that (X \ {x}) ∪{y} is D-linearly independent.
Proof. Fix x ∈X \ Y , and let X′ = X \ {x}. We must have x ̸= 0, since otherwise the
relation 1x = 0 shows that X is linearly dependent. Since Y generates M, there exist
a positive integer n, elements y1, . . . , yn ∈Y , and nonzero scalars d1, . . . , dn ∈D with
x = d1y1 + · · · + dnyn. If y1, . . . , yn all belong to the submodule ⟨X′⟩, then x belongs to
⟨X′⟩also. But then x would be a linear combination of other elements of X, violating the
linear independence of X. So some yi, say y1, is not in ⟨X′⟩. By the Lemma on Extending
an Independent Set, X′ ∪{y1} is linearly independent. We also see that y1 ̸∈X′ and y1 ̸= x
(since x ̸∈Y ), so that y1 ∈Y \ X, as needed.
Comparison Theorem for Independent Sets and Spanning Sets. Suppose D is a
division ring, M is a left D-module, S is a D-linearly independent subset of M, and T is a
generating set for M. Then |S| ≤|T|.
Proof. By definition, |S| ≤|T| means there exists an injection (one-to-one function)
h : S →T. The idea of the proof is to use Zorn’s Lemma to assemble partial injections
(each mapping a subset of S into T) to get an injection with the largest possible domain.
The details are rather intricate, so we proceed in steps.
Step 1: We define the poset. Let Z be the set of triples (A, C, f), where A and C are sets,
S ∩T ⊆A ⊆S, S ∩T ⊆C ⊆T, f : A →C is a bijection, f(z) = z for all z ∈S ∩T, and
(S\A)∪C is D-linearly independent. Partially order Z by defining (A, C, f) ≤(A1, C1, f1) to
mean A ⊆A1, C ⊆C1, and f ⊆f1. (Here, we view the functions f and f1 as sets of ordered
pairs. For example, f = {(a, f(a)) : a ∈A}. The condition f ⊆f1 means f1(a) = f(a) for
all a ∈A ⊆A1, which says that f1 extends f.) The poset axioms for (Z, ≤) follow routinely
from the known reflexivity, antisymmetry, and transitivity of ⊆.
Step 2: We check the hypothesis of Zorn’s Lemma. First, the poset Z is nonempty, because
(S ∩T, S ∩T, idS∩T ) is in Z (note that (S \ (S ∩T)) ∪(S ∩T) = S is independent). Second,
given a nonempty chain {(Ai, Ci, fi) : i ∈I} ⊆Z, we must find an upper bound (A, C, f) for
this chain that belongs to Z. Let A = S
i∈I Ai, C = S
i∈I Ci, and f = S
i∈I fi. We certainly
have Ai ⊆A, Ci ⊆C, and fi ⊆f for all i ∈I; we need only show that (A, C, f) ∈Z. We
must first check that f, which is a certain set of ordered pairs, really is a (single-valued)
function with domain A. In more detail, we must show that for all a ∈A, there exists a
498
Advanced Linear Algebra
unique c with c ∈C and (a, c) ∈f. Fix a ∈A. Now a ∈Ai for some i ∈I, so there is
c ∈Ci ⊆C with (a, c) ∈fi ⊆f, namely c = fi(a). Suppose we also have (a, d) ∈f for some
d; we must prove d = c. Note (a, d) ∈f means fj(a) = d for some j ∈I. We are working
with a chain, so (Ai, Ci, fi) ≤(Aj, Cj, fj) or (Aj, Cj, fj) ≤(Ai, Ci, fi). In the first case, fj
extends fi, so d = fj(a) = fi(a) = c; similarly in the other case. Analogous reasoning shows
that f maps A one-to-one onto C, so that we have a well-defined bijection f : A →C. Since
f extends every fi, we also have f(z) = z for all z ∈S ∩T. Note S ∩T ⊆A ⊆S and
S ∩T ⊆C ⊆T since these set inclusions hold for every Ai and Ci.
The key point still to be checked is that (S \ A) ∪C is D-linearly independent. By
definition, we must check that every finite list of distinct elements of (S \ A) ∪C is D-
independent. Let s1, . . . , sm, c1, . . . , cn be such a list, where each sj ∈S\A and each ck ∈C.
For each k, there is an index i(k) ∈I with ck ∈Ci(k). Since we are working with a chain
and n is finite, there is a single index i0 ∈I with ck ∈Ci0 for all k between 1 and n. We also
have sj ∈(S \ A) ⊆(S \ Ai0) for all j between 1 and m. Hence, s1, . . . , sm, c1, . . . , cn is a
finite list of distinct elements of the set (S \Ai0)∪Ci0. This set is known to be independent
(since (Ai0, Ci0, fi0) ∈Z), so the list is independent, as needed. We have now proved that
(A, C, f) ∈Z, so that the given chain has an upper bound in Z.
Step 3: We analyze a maximal element of Z. By Zorn’s Lemma, there is a maximal element
(A, C, f) in the poset Z. If A = S, then f is a bijection from S onto the subset C of T. By
enlarging the codomain from C to T, we get an injection from S into T, as needed.
To finish the proof, we must rule out the case where A ⊊S. In this case, there exists
x ∈S \ A. Since A contains S ∩T, x ̸∈T but x does belong to the set X = (S \ A) ∪C. We
can apply the Exchange Lemma to the linearly independent set X and the generating set
T. By the lemma, there exists y ∈T \ X such that (X \ {x}) ∪{y} is linearly independent.
Define A′ = A∪{x}, C′ = C ∪{y}, and f ′ = f ∪{(x, y)}. The conditions on x and y ensure
that S ∩T ⊆A ⊊A′ ⊆S, S ∩T ⊆C ⊊C′ ⊆T, f ′ : A′ →C′ is a bijection extending f,
f ′(z) = z for all z ∈S ∩T, and (S \ A′) ∪C′ = (X \ {x}) ∪{y} is linearly independent.
Thus, (A′, C′, f ′) belongs to Z and is strictly larger than (A, C, f) relative to the ordering.
This contradicts the maximality of (A, C, f).
Basis Cardinality Theorem for Left Vector Spaces. Suppose D is a division ring and
M is a left D-module. Any two bases of M have the same cardinality.
Proof. Let B and C be two bases for M. Since B is linearly independent and C generates
M, |B| ≤|C|. Since C is linearly independent and B generates M, |C| ≤|B|. By the
Schr¨oder–Bernstein Theorem from set theory, |B| ≤|C| and |C| ≤|B| imply |B| = |C|, as
needed.
17.16
Basis Invariance for Free Modules over Commutative Rings
We have just proved that any two bases of a module over a division ring have the same size.
The next theorem leverages this result to show that any two bases of a free module over a
commutative ring have the same size. Later, we give an example to show that this property
need not hold for free modules over non-commutative rings.
Basis Cardinality Theorem for Free Modules over Commutative Rings. Suppose
R is a nonzero commutative ring and N is a free R-module. For any two bases X and Y of
N, |X| = |Y |.
Introduction to Modules
499
Proof. First consider the case where X and Y are finite ordered bases of N, say X =
(x1, . . . , xn) and Y = (y1, . . . , ym). Let I be a maximal ideal of R (see §17.13) and note
that F = R/I is a field (Exercise 53). We know that V = N/IN is an R/I-module, i.e., an
F-vector space. Let X′ = (x1 + IN, . . . , xn + IN) be the list of images of elements of X
in N/IN. We claim that X′ is an ordered basis for the F-vector space V . You can check
that X′ generates V , since X generates N. To show that X′ is F-linearly independent,
suppose Pn
i=1(si + I)(xi + IN) = 0 for fixed si ∈R. We must show that each si + I = 0 in
R/I, i.e., that si ∈I for all i. From the given relation and the definition of the operations
in N/IN, we deduce (Pn
i=1 sixi) + IN = 0. Therefore, the element x = Pn
i=1 sixi is in
IN. By definition of IN, we can write x = Pp
j=1 ajzj where aj ∈I and zj ∈N. Writing
zj = Pn
i=1 ci,jxi (for some ci,j ∈R) and substituting, we see that x = Pn
i=1(Pp
j=1 ajci,j)xi.
By the R-linear independence of the list X in N, we conclude that si = Pp
j=1 ajci,j for all
i. Since each aj is in the ideal I, we have si ∈I, as needed. So X′ is an ordered F-basis
for V . By the same argument, Y ′ = (y1 + IN, . . . , ym + IN) is an ordered F-basis for V .
Applying the Basis Cardinality Theorem for Vector Spaces, we deduce n = m.
Now consider the general case, where X and Y
are sets (possibly infinite). Let
p : N →N/IN be the canonical map, and set X′ = p[X], Y ′ = p[Y ]. As before, X′ and
Y ′ generate the F-vector space N/IN. The proof in the preceding paragraph, applied to
each finite list of distinct elements of X, shows that X′ is F-linearly independent. Similarly,
Y ′ is F-linearly independent. Therefore, |X′| = |Y ′| by the Basis Cardinality Theorem for
Vector Spaces. To finish, it suffices to show that the restriction of p to X is injective, so
that |X| = |X′| (and similarly |Y | = |Y ′|). Injectivity follows from the argument used
to show linear independence of X′. To see why, fix x1 ̸= x2 in X. The list (x1, x2) is R-
independent in N, so that (x1 + IN, x2 + IN) is F-independent in N/IN. In particular,
p(x1) = x1 + IN ̸= x2 + IN = p(x2) in N/IN.
Here is an example of a non-commutative ring R and a free R-module that has R-bases
with different cardinalities. Let F be any field, X = {xn : n ≥0} be a countably infinite
set, and V be an F-vector space with basis X. Let R = HomF (V, V ) be the set of linear
transformations from V to itself. We have seen that R is an F-vector space. In fact, R is also
a non-commutative ring, if we define multiplication of elements f, g ∈R to be composition
of functions. The ring axioms may be routinely verified. In particular, the distributive laws
follow since addition of functions is defined pointwise.
Like any ring, R is a left R-module. For each integer k ≥1, we produce an R-basis for R
consisting of k elements. Fix a positive integer k. We define certain elements fj and gj in R
(for 0 ≤j < k) by specifying how these linear maps operate on the basis X of V . Recall (by
integer division) that every integer n ≥0 can be written uniquely in the form n = ki + j,
for some integers i, j with 0 ≤j < k. Let fj send xki+j to xi for all i ≥0; let fj send all
other elements of X to 0. Let gj send xi to xki+j for all i ≥0. We have fjgj = idV for all
j, since both sides have the same effect on the basis X. For the same reason, fj′gj = 0R for
j ̸= j′ between 0 and k −1, and
g0f0 + g1f1 + · · · + gk−1fk−1 = idV .
(17.1)
The last identity follows because, given any xn ∈X, we can write n = ki + j for a unique
j between 0 and k −1. Then gjfj(xn) = xn for this j, while gj′fj′(xn) = 0 for all j′ ̸= j.
We can now show that Bk = (f0, . . . , fk−1) is a k-element ordered R-basis for the left
R-module R. Suppose f is any element of R. Multiplying (17.1) on the left by f, we get
(fg0)f0 + (fg1)f1 + · · · + (fgk−1)fk−1 = f where fgj ∈R. This identity shows that Bk is a
generating set for the left R-module R. Next, to test R-independence, suppose h0f0 + · · · +
hk−1fk−1 = 0R for some hj ∈R. For each j0 between 0 and k −1, multiply this equation on
500
Advanced Linear Algebra
the right by gj0 to obtain hj0 = 0R (using the relations above to simplify products fjgj0).
Thus, Bk is an R-independent list. Note how prominently the non-commutativity of R is
used in this proof.
17.17
Jordan–H¨older Theorem for Modules
If M is a left vector space over a division ring, or a free module over a commutative ring,
then the dimension of M (the cardinality of any basis) is uniquely determined by M. The
Jordan–H¨older Theorem is a more general uniqueness result regarding decompositions of
a module into building blocks that are simple modules. To see how these decompositions
occur, we must first discuss chains of submodules.
Suppose R is a ring and M is a left R-module. A chain of submodules of M is a list
(M0, M1, . . . , Mm) where each Mi is a submodule of M and M0 ⊋M1 ⊋· · · ⊋Mm. We
say that this chain has length m. Chains of submodules (M0, M1, . . . , Mn, . . .) of infinite
length are defined similarly. A chain of finite length is called a maximal chain iff it cannot
be extended to a longer chain by adding another submodule to the beginning, middle, or
end of the list. The chain (M0, . . . , Mm) is maximal iff M0 = M and Mm = {0} and
for all i between 1 and m, there exists no submodule P of M with Mi−1 ⊋P ⊋Mi.
By the Correspondence Theorem, the last condition is equivalent to saying that the only
submodules of the quotient module Mi−1/Mi are {0} = Mi/Mi and Mi−1/Mi. In other
words, the chain of submodules (M0, . . . , Mm) is maximal iff M = M0 and Mm = {0} and
Mi−1/Mi is a simple module for all i between 1 and m.
Jordan–H¨older Theorem for Modules. Suppose R is a ring and M is an R-module
such that there exists a maximal chain of submodules (M0, M1, . . . , Mm) of finite length m.
(a) Any chain (N0, N1, . . .) of submodules of M has finite length n ≤m.
(b) Any maximal chain (N0, . . . , Nn) of submodules of M satisfies n = m and, for some
permutation f of {1, 2, . . . , m}, Mi−1/Mi ∼= Nf(i)−1/Nf(i) for all i between 1 and m.
Proof. We prove the theorem by induction on m, the length of the given maximal chain.
If m = 0, then M = M0 = Mm = {0}, and the conclusions of the theorem are evident.
If m = 1, then M ∼= M/{0} = M0/M1 must be a simple module, and again the needed
conclusions follow immediately. Now assume m > 1 and that the theorem is already known
for all R-modules having a maximal chain of length less than m. Fix a chain (N0, N1, . . .)
of submodules of M. We first prove (a).
Case 1: N1 ⊆M1. We can apply the induction hypothesis to the R-module M1, which
has a maximal chain (M1, . . . , Mm) of length m −1. The chain (N1, N2, . . .) must therefore
have some finite length n −1 ≤m −1, so that (N0, N1, . . . , Nn) has finite length n ≤m.
Case 2: N1 is not a subset of M1. Note M ̸= N1 ̸= M1, but M and M1 are
the only two submodules of M containing M1. So M1 is not a subset of N1, and
P = M1 ∩N1 is a proper submodule of both M1 and N1. Moreover, M1 + N1 is a
submodule properly containing M1, so M = M1 + N1. By the Diamond Isomorphism
Theorem, N1/P = N1/(N1 ∩M1) ∼= (N1 + M1)/M1 = M/M1 is a simple module. Starting
with the chain (M1, P), we can build longer and longer chains of submodules of M1 by
repeatedly inserting a new submodule somewhere in the existing chain, as long as this can
be done. By the induction hypothesis applied to M1, this insertion process must terminate
in finitely many steps. When it does terminate, we have by definition a maximal chain
Introduction to Modules
501
(P1 = M1, P2, . . . , Pm) of submodules of M1, which must have length m −1 by induction
applied to the module M1. We know M1 ∩N1 = P = Pi for some i with 2 ≤i ≤m. Since
N1/P is simple, (N1, P = Pi, Pi+1, . . . , Pm) is a maximal chain of submodules of N1 of finite
length m −i + 1 ≤m −1. So the induction hypothesis is applicable to the R-module N1,
and we conclude that the chain (N1, N2, . . .) of submodules of N1 has finite length at most
m −i + 1 ≤m −1. So (N0, N1, . . . , Nn) has finite length n ≤m, proving (a) for the module
M.
To prove (b), we now assume that (N0, N1, . . . , Nn) is a maximal chain of submodules of
M. Part (a) says n ≤m. Using part (a) with the roles of the two maximal chains reversed, we
also get m ≤n and hence n = m. To obtain the further conclusion about the isomorphism
of quotient modules, we again consider two cases.
Case 1: N1 ⊆M1. Since M1 ⊊M, maximality of the chain (N0, N1, . . . , Nn) forces
M1 = N1. Then M0/M1 = N0/N1. We can match up the rest of the quotient modules by
applying part (b) of the induction hypothesis to the module M1 = N1 and the maximal
chains (M1, . . . , Mm) and (N1, . . . , Nn) within this module.
Case 2: N1 is not a subset of M1. As before, we let P = M1 ∩N1 and note that
M = M1 + N1. By the Diamond Isomorphism Theorem and the assumed maximality of the
two given chains, N1/P ∼= M0/M1 and M1/P ∼= N0/N1 are simple modules. As before, we
can extend the chain (M1, P) to a maximal chain (M1, P, P3, . . . , Pm) of submodules of M1.
We now have four maximal chains of submodules of M that look like this:
M
M1
N1
M2
P
N2
M3
P3
N3
...
...
...
Mm
Pm
Nm
Note that (N1, P, P3, . . . , Pm) is also a maximal chain of submodules of N1. Applying the
induction hypothesis to M1, we know that the modules M0/M1, M1/M2, . . . , Mm−1/Mm
are isomorphic (in some order) to the modules M/M1, M1/P, P/P3, . . . , Pm−1/Pm. By the
diamond isomorphisms at the top of the figure, these modules (switching the first two)
are isomorphic to M/N1, N1/P, P/P3, . . . , Pm−1/Pm. Applying the induction hypothesis
to N1, we know that the modules just listed are isomorphic (in some order) to the
modules N0/N1, N1/N2, . . . , Nm−1/Nm. Combining all these steps, the modules Mi−1/Mi
(for 1 ≤i ≤m) are isomorphic in some order to the modules Ni−1/Ni.
502
Advanced Linear Algebra
17.18
Modules of Finite Length
We say that an R-module M has finite length iff there exists a maximal chain of submodules
of M of finite length. In this case, we have proved that all maximal chains of submodules of
M have the same length m, and we define the length of the module M to be len(M) = m. If
M does not have finite length, set len(M) = ∞. The length of a module is a generalization
of the dimension of a vector space. You can check that if M is a finitely generated module
over a division ring R, then len(M) = dim(M) (Exercise 63).
We have len(M) = 0 iff M is the zero module, while len(M) = 1 iff M is a simple module.
You can check that if M ∼= M ′, then len(M) = len(M ′). Next, suppose M is any R-module
with a submodule N. We claim len(M) < ∞iff len(N) < ∞and len(M/N) < ∞, in which
case len(M) = len(N) + len(M/N). On one hand, suppose len(M) = m < ∞. Any chain of
submodules of N is also a chain of submodules of M, so has length at most m. Hence N has
finite length. On the other hand, by the Correspondence Theorem, any chain of submodules
of M/N has the form M0/N ⊋M1/N ⊋· · · ⊋Mk/N for some submodules M0 ⊋M1 ⊋
· · · ⊋Mk of M containing N. So the given chain in M/N has length at most m. Conversely,
suppose len(N) = n < ∞and len(M/N) = k < ∞. Fix a maximal chain of submodules of
N, say N = N0 ⊋N1 ⊋· · · ⊋Nn = {0}. Similarly, fix a maximal chain of submodules of
M/N, which must have the form M/N = M0/N ⊋M1/N ⊋· · · ⊋Mk/N = N/N for certain
submodules M = M0 ⊋M1 ⊋· · · ⊋Mk = N. Splicing these chains together, we get a chain
of submodules
M = M0 ⊋M1 ⊋· · · ⊋Mk = N = N0 ⊋N1 ⊋· · · ⊋Nn = {0}
of length k + n. This chain must be maximal, since otherwise the chain for N or the chain
for M/N would not be maximal. This proves len(M) = k + n = len(M/N) + len(N) < ∞.
Given R-modules M1, . . . , Mk, we show by induction that the product module
M = M1 × · · · × Mk has finite length iff all Mi have finite length, in which case
len(M) = len(M1) + · · · + len(Mk). This is evident if k = 1. Assume k > 1 and
the result is known for products of k −1 modules. On one hand, if len(M) < ∞,
then each submodule {0} × · · · × Mi × · · · × {0} has finite length. As Mi is isomor-
phic to this submodule, len(Mi) < ∞. On the other hand, suppose each Mi has
finite length. Note M = N × Mk where N = M1 × · · · × Mk−1. By the induction
hypothesis, len(N × {0}) = len(N) = len(M1) + · · · + len(Mk−1) < ∞. We have
M/(N × {0}) ∼= Mk, which has finite length. By the result in the previous paragraph,
len(M) = len(N) + len(Mk) = Pk
i=1 len(Mi) < ∞.
17.19
Summary
Let R be a ring. Here, we summarize the definitions and results for R-modules, module
homomorphisms, and free modules that were covered in this chapter.
Definitions
1.
A left R-module is an additive commutative group M and a scalar multiplication
· : R × M →M satisfying closure, left associativity, the two distributive laws,
and the identity axiom.
Introduction to Modules
503
2.
Right R-modules are defined like left modules, using a scalar multiplication
· : M × R →M obeying right associativity. The two types of modules are
equivalent for commutative rings.
3.
A homomorphism of R-modules (left or right) is a map between modules that
preserves addition and scalar multiplication. Homomorphisms of R-modules are
also called R-maps or R-linear maps.
4.
For fields F, F-modules and F-maps are the same as F-vector spaces and linear
transformations. For R = Z, Z-modules and Z-maps are the same as commutative
groups and group homomorphisms.
5.
A submodule of a module is a subset containing 0 and closed under addition and
scalar multiplication (and hence under additive inverses).
6.
Let f : M →N be an R-module homomorphism. The set of all x ∈M such that
f(x) = 0N is the kernel of f. The set of all y ∈N of the form y = f(x) for some
x ∈M is the image of f. The kernel is a submodule of M, and the image is a
submodule of N.
7.
If S is a subset of a module M, an R-linear combination of elements of S is a finite
sum P
i aisi where ai ∈R and si ∈S. The set of all such R-linear combinations
is ⟨S⟩, the submodule generated by S. If ⟨S⟩= M, we say that S spans M or S
generates M. If M is generated by some finite set S, then we say M is finitely
generated.
8.
A left R-module M is cyclic iff there exists x ∈M with M = ⟨x⟩= Rx =
{rx : r ∈R}. M is a simple R-module iff M ̸= {0} and the only R-submodules
of M are {0} and M.
9.
A division ring is a ring R with 1R ̸= 0R such that for all nonzero x ∈R, there
exists y ∈R with xy = 1R = yx. A field is a commutative division ring. A left
vector space is a left R-module over a division ring R.
10.
A subset S of a module M is called R-independent iff for all k ∈Z>0, ai ∈R,
and distinct si ∈S, Pk
i=1 aisi = 0M implies every ai = 0R.
11.
An R-independent generating set of an R-module M is called an R-basis for M.
M is called a free R-module iff M has an R-basis. If R is a division ring or
a commutative ring, then the size of any R-basis of a free R-module M is the
dimension of M.
12.
The length len(M) of an R-module M is the maximum n such that there is a
chain of submodules M0 ⊋M1 ⊋· · · ⊋Mn of M, or ∞if there is no such n.
Module Constructions
1.
Submodules: Every submodule of an R-module is itself an R-module. Intersections
and sums of submodules are submodules. The image or preimage of a submodule
under an R-linear map is a submodule.
2.
Submodule Generated by a Set: If S is any subset of an R-module M, there exists
a smallest submodule ⟨S⟩of M containing S. This submodule can be defined as
the intersection of all submodules of M containing S, or as the set of all R-linear
combinations of elements of S (including 0M).
3.
Direct Products and Direct Sums: If Mi is a left R-module for each i in a set
I, then the set of functions f with domain I such that f(i) ∈Mi for all i ∈I
is a left R-module under pointwise operations on functions. This module is the
504
Advanced Linear Algebra
direct product of the Mi, written Q
i∈I Mi. The subset consisting of functions
with f(i) = 0Mi for all but finitely many i ∈I is a submodule called the direct
sum of the Mi, written L
i∈I Mi. A special case is Rn, the module of n-tuples of
elements of R, which is a free R-module having an n-element R-basis.
4.
Hom Modules: Let M and N be left R-modules. The set Hom(M, N) of group
homomorphisms from M to N is a left R-module. For commutative R, the set
HomR(M, N) of R-module homomorphisms from M to N is a left R-module.
5.
Quotient Modules: Let N be a submodule of an R-module M. The quotient module
M/N consists of all cosets x + N with x ∈M. For all x, z ∈M, x + N =
z + N iff x −z ∈N. The operations in the quotient module are defined by
(x + N) + (y + N) = (x + y) + N and r · (x + N) = (rx) + N for x, y ∈M and
r ∈R. The canonical projection p : M →M/N, given by p(x) = x+N for x ∈M,
is a surjective R-map with kernel N. If S generates M, then p[S] generates M/N.
6.
Change of Scalars: If T is a subring of R, any R-module can be regarded as a
T-module. If f : S →R is a ring homomorphism, then an R-module M becomes
an S-module via s ⋆x = f(s) · x for s ∈S and x ∈M. If I is an ideal of R
annihilating an R-module M (meaning ix = 0 for all i ∈I and x ∈M), then M
can be regarded as an R/I-module via (r + I) • x = r · x for r ∈R and x ∈M. In
particular, for any R-module N and any ideal I of R, N/IN is an R/I-module.
Taking I to be a maximal ideal in a commutative ring R, so that R/I is a field,
we can convert R-modules to R/I-vector spaces.
Results about Module Homomorphisms and Submodules
Let M and N be left R-modules.
1.
If f, g : M →N are two R-maps agreeing on a generating set for M, then f = g.
2.
Fundamental Homomorphism Theorem for R-modules: Let f : M →N be an
R-module homomorphism. There is an R-module isomorphism f ′ : M/ ker(f) →
img(f) given by f ′(x + ker(f)) = f(x) for x ∈M.
3.
Universal Mapping Property for the Quotient Module M/H: Let f : M →N
be an R-module homomorphism, H be a submodule of M, and p : M →M/H
be the canonical map. If H ⊆ker(f), then there exists a unique R-module
homomorphism f ′ : M/H →N such that f = f ′ ◦p. Moreover, img(f ′) = img(f)
and ker(f ′) = ker(f)/H.
4.
Diamond Isomorphism Theorem: Let M and N be submodules of an R-module
P. The function g : M/(M ∩N) →(M +N)/N given by g(m+M ∩N) = m+N
(for m ∈M) is an R-module isomorphism.
5.
Nested Quotient Isomorphism Theorem: Given H ⊆N ⊆M with H and N sub-
modules of M, there is an R-module isomorphism g : (M/H)/(N/H) →(M/N)
given by g((x + H) + (N/H)) = x + N for x ∈M.
6.
Correspondence Theorem for Modules: Assume N is a submodule of M, and let
p : M →M/N be the canonical map. Let A be the collection of all submodules
U of M such that N ⊆U ⊆M. Let B be the collection of all submodules of
M/N. There are inclusion-preserving, mutually inverse bijections T : A →B
and S : B →A given by T(U) = p[U] = U/N (the image of U under p) and
S(V ) = p−1[V ] (the preimage of V under p).
Introduction to Modules
505
7.
Recognition Theorem for Direct Products: Let N and P be submodules of M
such that N + P = M and N ∩P = {0M}. There is an R-module isomorphism
g : N × P →M given by g((x, y)) = x + y for x ∈N and y ∈P.
8.
Jordan–H¨older Theorem for Modules: If M has one maximal chain of submod-
ules of finite length m, then every chain of submodules of M has length at
most m, and every maximal chain has length m. Given two maximal chains
M0 ⊋M1 ⊋· · · ⊋Mm and N0 ⊋N1 ⊋· · · ⊋Nm of M, the quotient modules
M0/M1, M1/M2, . . . , Mm−1/Mm are simple modules and are isomorphic (in some
order) to N0/N1, N1/N2, . . . , Nm−1/Nm.
9.
Results for Finite Length Modules: M = {0} iff len(M) = 0. M is simple
iff len(M) = 1. Isomorphic modules have the same length. For a submodule
N of M, M has finite length iff N and M/N have finite length, and then
len(M) = len(N) + len(M/N). For M = M1 × · · · × Mk, M has finite length
iff all Mi have finite length, and then len(M) = len(M1) + · · · + len(Mk).
Results about Free R-modules
1.
If M is a free R-module with basis S, then every y ∈M can be uniquely expressed
as a (finite) R-linear combination of elements of S.
2.
Universal Mapping Property for Free R-Modules: Let M be a free R-module with
basis X. Given any R-module N and any function g : X →N, there exists a
unique R-module homomorphism g′ : M →N whose restriction to X equals g.
3.
Every free R-module M is isomorphic to a direct sum of copies of R. Conversely,
any such direct sum is a free R-module.
4.
Given any set X and any nonzero ring R, there exists a free R-module with
R-basis X.
5.
Suppose R is a division ring. Every R-module M is free, and every R-basis of M
has the same cardinality. For all R-independent sets S in M and all generating
sets T for M: |S| ≤|T|; S can be extended to a basis of M; and T contains a
basis of M.
6.
For some rings R, not every R-module is free, and not every R-module is
isomorphic to a submodule of a free R-module. For all rings R, every R-module
M is isomorphic to a quotient module of a free R-module F. If M is finitely
generated, then F can be chosen to be finitely generated with the same number
of generators as M.
7.
If R is a nonzero commutative ring and N is a free R-module, then any two bases
for N have the same cardinality. However, there exists a non-commutative ring
R and a free R-module that has an R-basis of size k for every positive integer k.
17.20
Exercises
Unless otherwise specified, assume R is an arbitrary ring in these exercises.
1.
Let R be any nonzero ring. For each commutative group (M, +) below, show
that M is not a left R-module under the indicated scalar multiplication
· : R × M →M by pointing out one or more module axioms that fail to hold.
506
Advanced Linear Algebra
(a) M = R, r · m = 0M for all r ∈R and m ∈M.
(b) M = R, r · m = m for all r ∈R and m ∈M.
(c) M = R, r · m = r for all r ∈R and m ∈M.
(d) M = R2, r · (m1, m2) = (rm1, m2) for all r, m1, m2 ∈R.
(e) M = R, r · m = mr for all r, m ∈R (assume R is non-commutative).
2.
Fix a positive integer n. Show that the additive group Rn (viewed as a set of
column vectors) is a left Mn(R)-module if scalar multiplication A · v (for A ∈
Mn(R) and v ∈Rn) is defined to be the matrix-vector product Av. If we define
v ⋆A = Av, do we get a right Mn(R)-module structure on Rn?
3.
Given n ∈Z>0, show that Rn (viewed as a set of row vectors) is a right Mn(R)-
module if scalar multiplication w · A (for A ∈Mn(R) and w ∈Rn) is defined to
be the vector-matrix product wA.
4.
Let V be a vector space over a field F and R be the ring of all F-linear
transformations T : V →V . Show that (V, +) is a left R-module if we define
T · v = T(v) for T ∈R and v ∈V .
5.
Let (M, +) be a commutative group and · : R × M →M be a function. Define
⋆: M ×R →M by m⋆r = r ·m for all r ∈R and m ∈M. Prove that each axiom
(M1), (M2), (D1), and (D2) in §17.1 holds for · iff the analogous axiom (M1′),
(M2′), (D1′), (D2′) holds for ⋆. Prove that, if R is commutative, then axiom (M3)
holds for · iff axiom (M3′) holds for ⋆.
6.
Let M, N be left R-modules and assume f : M →N is an R-linear map. Prove
the following facts, which were stated in §17.1.
(a) The identity map idM : M →M is an automorphism.
(b) The composition of R-maps is an R-map (similarly for monomorphisms,
epimorphisms, isomorphisms, endomorphisms, and automorphisms).
(c) If f is an isomorphism, then f −1 is an isomorphism.
(d) Given a set X of left R-modules, the relation given by M ∼= N iff M and N
are isomorphic R-modules (for M, N ∈X) is an equivalence relation on X.
7.
Let f : M →N be an R-linear map between two R-modules. Prove f(0M) = 0N
and f(−x) = −f(x) for all x ∈M.
8.
Let M be any R-module.
(a) Prove 0R · x = 0M for all x ∈M.
(b) Prove r · 0M = 0M for all r ∈R.
(c) Prove r · (−x) = (−r) · x = −(r · x) for all r ∈R and all x ∈M.
9.
The complex number system C is a ring, a left C-module, and a left R-module.
(a) Show that f : C →C defined by f(a + ib) = a −ib for a, b ∈R is a ring
homomorphism and an R-linear map, but not a C-linear map.
(b) Show that g : C →C defined by g(z) = iz for z ∈C is a C-linear map, but
not a ring homomorphism.
10.
Prove or disprove: there exists exactly one function · : C × C →C such that
(C, +, ·) is a left C-module, where + denotes complex addition.
11.
Opposite Rings. Let (R, +, •) be a ring. Define an operation ⋆: R × R →R by
setting a ⋆b = b • a for all a, b ∈R.
(a) Prove (R, +, ⋆) is a ring, called the opposite ring Rop of R.
(b) Let (M, +) be a commutative group. Prove · : M × R →M satisfies the
axioms for a right R-module iff ∗: Rop × M →M, defined by r ∗m = m · r for
r ∈R and m ∈M, satisfies the axioms for a left Rop-module. (This result reduces
the study of right modules to the study of left modules over the opposite ring.)
Introduction to Modules
507
12.
Define a left R-module structure on the set of matrices Mm,n(R). Is this a free
R-module? If so, describe an R-basis.
13.
Let (M, +) be a commutative group. Prove there is at most one scalar multipli-
cation · : Z×M →M that makes (M, +, ·) a left Z-module. Prove that the scalar
multiplication · : Z × M →M, defined in §17.2, does satisfy the axioms for a left
Z-module.
14.
Let (M, +) be a commutative group and n ∈Z>0. Prove there is at most one
scalar multiplication · : Zn × M →M that makes (M, +, ·) a left Zn-module.
Find and prove a condition on (M, +) that is necessary and sufficient for there
to exist a left Zn-module with underlying additive group (M, +).
15.
Given a commutative group (M, +), recall (Exercise 67 in Chapter 1) that the
endomorphism ring End(M) is the set of all group homomorphisms f : M →M.
We add and multiply f, g ∈End(M) via the rules (f + g)(x) = f(x) + g(x) and
(f ◦g)(x) = f(g(x)) for all x ∈M. Prove: for any subring S of End(M), M is a
left S-module under the scalar multiplication f · x = f(x) for f ∈S and x ∈M.
16.
Let (M, +) be a fixed commutative group with endomorphism ring End(M).
(a) Given a scalar multiplication · : R × M →M satisfying the axioms for a left
R-module, define a map Lr : M →M (for each r ∈R) by setting Lr(x) = r·x for
x ∈M. The map Lr is called left multiplication by r. Confirm that Lr ∈End(M)
for each r ∈R. Then show that the map L : R →End(M), defined by L(r) = Lr
for r ∈R, is a ring homomorphism.
(b) Conversely, suppose T : R →End(M) is a given ring homomorphism. Define
⋆: R × M →M by r ⋆x = T(r)(x) for all r ∈R and all x ∈M. Show that this
scalar multiplication turns M into a left R-module.
(c) Let X be the set of all scalar multiplication functions · : R×M →M satisfying
the axioms for a left R-module and Y be the set of all ring homomorphisms
L : R →End(M). The constructions in (a) and (b) define maps ϕ : X →Y
and ψ : Y →X. Check that ϕ ◦ψ = idY and ψ ◦ϕ = idX , so that ϕ and
ψ are bijections. (This exercise shows that left R-module structures on a given
commutative group (M, +) correspond bijectively with ring homomorphisms of
R into the endomorphism ring End(M).)
17.
Prove that for any ring S, there exists exactly one ring homomorphism f : Z →S.
Deduce from this and Exercise 16 that for every commutative group (M, +), there
exists a unique scalar multiplication that turns M into a left Z-module.
18.
Show that N is a submodule of an R-module M iff N is a nonempty subset of M
closed under subtraction and left multiplication by scalars in R.
19.
Prove that every additive subgroup of a Z-module M is a Z-submodule.
20.
Give a specific example of a ring R, a left R-module M, and an additive subgroup
N of M that is not an R-submodule of M.
21.
Prove or disprove: for each positive integer n, every additive subgroup of any
Zn-module M is a Zn-submodule.
22.
Prove or disprove: if M and N are submodules of a left R-module P, then M ∪N
is always a submodule of P.
23.
Prove: if {Mi : i ∈I} is an indexed family of submodules of a left R-module
P such that I ̸= ∅and for all i, j ∈I, either Mi ⊆Mj or Mj ⊆Mi, then
N = S
i∈I Mi is a submodule of P.
508
Advanced Linear Algebra
24.
Given submodules M and N of a left R-module P, confirm that M + N is a
submodule of P.
25.
Given a family {Mi : i ∈I} of submodules of an R-module P, confirm that
P
i∈I Mi is a submodule of P. Prove that if N is any R-submodule of P containing
every Mi, then P
i∈I Mi ⊆N. (So the sum of the Mi is the smallest submodule
containing every Mi).
26.
Let S be any set and X be the set of all subsets of S. X is a partially ordered set
where U ≤V means U ⊆V for U, V ∈X. Prove that X is a complete lattice.
27.
Prove that every simple R-module is cyclic. Give an example to show that the
converse is not true in general.
28.
Show that a Z-module M is simple iff |M| is prime. (You may need Lagrange’s
Theorem from group theory.)
29.
Show: for all fields F, an F-module M is simple iff dimF (M) = 1.
30.
Show: for all commutative rings R, the left R-module R is simple iff R is a field.
31.
Give an example (with proof) of an infinite ring R and an infinite cyclic R-module
M such that there exists a unique x ∈M with M = Rx.
32.
For any left ideal I of R, prove R/I is a cyclic left R-module. Conversely, prove
that every cyclic left R-module M is isomorphic to a module R/I for some left
ideal I of R. (Use the Fundamental Homomorphism Theorem.)
33.
A subset I of R is called a maximal left ideal iff I is a submodule of the left
R-module R such that I ̸= R and for any submodule J with I ⊆J ⊆R, either
J = I or J = R. Prove that if I is a maximal left ideal of R, then R/I is a simple
left R-module. (Use the Correspondence Theorem.) Conversely, prove that every
simple left R-module M is isomorphic to a module R/I for some maximal left
ideal I of R.
34.
The set M2(R) of 2×2 real matrices is an additive group, a ring, a left Z-module,
a left R-module, and a left M2(R)-module. Let N be the set of matrices of the
form

0
a
0
b

for some a, b ∈R. Show that N is an R-submodule of M2(R) and
an M2(R)-submodule of M2(R), but N is not an ideal of M2(R). Show that N
is not a simple R-module, but N is a simple M2(R)-module. (Prove any nonzero
M2(R)-submodule P of N must be equal to N.)
35.
Give an example of submodules A, B, C of the R-module R × R such that the
distributive law A ∩(B + C) = (A ∩B) + (A ∩C) is not true.
36.
Let A, B, and C be submodules of a left R-module M. Prove: if A ⊆C, then
A + (B ∩C) = (A + B) ∩C.
37.
Let S be a subset of a left R-module M.
(a) Show that the set N ′ of R-linear combinations of elements of S is an R-
submodule of M by checking the closure conditions in the definition.
(b) Show that N ′ is a submodule of M by verifying that N ′ = P
s∈S Rs.
38.
Let N be a subset of a left R-module M. Let I be the set of all r ∈R such that
r · x = 0M for all x ∈N. Prove that I is a submodule of the left R-module R.
Prove that if N is a submodule of M, then I is a two-sided ideal of the ring R.
39.
Assuming R ̸= {0}, prove that R[x] is not a finitely generated R-module.
40.
Given an index set I and left R-modules Mi for each i ∈I, verify that Q
i∈I Mi
satisfies all the module axioms.
Introduction to Modules
509
41.
Given an index set I and left R-modules Mi for each i ∈I, verify that L
i∈I Mi
is a submodule of Q
i∈I Mi.
42.
Verify that the maps S and T in §17.5 are R-linear maps with S = T −1.
43.
Let M and N be left R-modules. Prove that HomR(M, N) is always an additive
subgroup of N M.
44.
Bimodules. Given rings R and S, an R, S-bimodule is a commutative group
(M, +) that has both a left R-module structure, given by · : R × M →M, and
a right S-module structure, given by ⋆: M × S →M, that are connected by the
axiom (r · x) ⋆s = r · (x ⋆s) for all r ∈R, s ∈S, and x ∈M.
(a) Prove that any ring R is an R, R-bimodule if we take · and ⋆to be the
multiplication of R.
(b) Prove that Rn (viewed as column vectors) is an Mn(R), R-bimodule using the
natural action of matrices and scalars on column vectors.
45.
Let M be a left R-module and N an R, S-bimodule. Show that the commutative
group HomR(M, N) of R-maps from M to N is a right S-module if we define
f · s (for f ∈HomR(M, N) and s ∈S) to be the function from M to N sending
x ∈M to f(x) ⋆s.
46.
Let M be an R, S-bimodule and N a left R-module. Show that the commutative
group HomR(M, N) of R-maps from M to N is a left S-module if we define s · f
(for f ∈HomR(M, N) and s ∈S) to be the function from M to N sending x ∈M
to f(x ⋆s).
47.
Let M be a left R-module with submodule N. Define a relation ≡on M by setting
x ≡y iff x −y ∈N (for all x, y ∈M). Prove ≡is an equivalence relation, and
prove the equivalence class of x is the coset x + N.
48.
Prove that R[x] is a free left R-module by finding an explicit basis for this module.
49.
Assume R is an integral domain and g ∈R[x] is monic of degree n > 0.
Let I
= R[x]g. Prove that R[x]/I is a free R-module with ordered basis
(1 + I, x + I, x2 + I, . . . , xn−1 + I).
50.
Suppose S is a ring, f : S →R is a ring homomorphism, and M is a left R-
module. (a) Prove that M is a left S-module via s ⋆x = f(s) · x for s ∈S and
x ∈M by checking the S-module axioms. (b) Give a short proof that M is a left
S-module using the results of Exercise 16.
51.
Assume M is a left R-module annihilated by an ideal I of R. Complete the
verification (from §17.8) that M is a left R/I-module via (r + I) • m = r · m for
r ∈R and m ∈M.
52.
Given a left R-module N and an ideal I of R, show IN is a submodule of N.
53.
Suppose R is a commutative ring with maximal ideal M. Prove R/M is a field.
(Given a nonzero x + M ∈R/M with x ∈R, consider the ideal M + Rx.)
54.
Let N =
 0
b
0
d

: b, d ∈R

, which is an M2(R)-submodule of M2(R) by
Exercise 34. Use the Fundamental Homomorphism Theorem to prove that there
is an M2(R)-module isomorphism M2(R)/N ∼= N.
55.
Let f : M →N be an R-map between left R-modules M and N. Prove: for any
submodule M ′ of M, f[M ′] is a submodule of N, and this submodule is contained
in img(f). Prove: for any submodule N ′ of N, f −1[N ′] is a submodule of M, and
this submodule contains ker(f).
510
Advanced Linear Algebra
56.
Let M be a left R-module. Prove M/{0M} is isomorphic to M. Prove M/M is
isomorphic to {0M}.
57.
Let M and N be left R-modules. Prove
M×N
M×{0N} ∼= N.
58.
Given an index set I, left R-modules Mi for i ∈I, and a submodule Ni of Mi for
each i ∈I, prove
Q
i∈I Mi
Q
i∈I Ni ∼= Q
i∈I(Mi/Ni).
59.
Suppose f : M →N is a homomorphism of left R-modules, and H is a submodule
of M. Try to define f ′ : M/H →N by f ′(x + H) = f(x) for all x ∈M.
(a) Prove the Generalized Fundamental Homomorphism Theorem stated in §17.9.
(b) Prove: If H is not contained in ker(f), then f ′ is not a well-defined function.
60.
Assume H and N are submodules of a left R-module M with H ⊆N. Prove: for
all x ∈M, x + H ∈N/H iff x ∈N.
61.
This exercise proves some results from set theory that are needed in the proof of
the Correspondence Theorem for Modules. Let f : X →Y be any function.
(a) Prove: for all W ⊆Y , f[f −1[W]] ⊆W.
(b) Give an example to show equality need not hold in (a).
(c) Prove: if f is surjective, then equality does hold in (a).
(d) Prove: for all U ⊆X, f −1[f[U]] ⊇U.
(e) Give an example to show equality need not hold in (d).
(f) Prove: if f is one-to-one, then equality does hold in (d).
(g) Prove: for all U1, U2 with U1 ⊆U2 ⊆X, f[U1] ⊆f[U2].
(h) Prove: for all W1, W2 with W1 ⊆W2 ⊆Y , f −1[W1] ⊆f −1[W2].
62.
Prove the Lemma on Extending an Independent Set in §17.12.
63.
Prove: if M is a finitely generated module over a division ring R, then M has
finite length and len(M) = dim(M).
64.
Prove that each structure (Z, ≤) is a poset.
(a) (Z, ⊆), where Z is a set of subsets of a fixed set
(b) (Z>0, |), where a|b means a divides b
(c) (Z>0, ⪯), where a ⪯b means b divides a
(d) Z is the set of functions f : R →R; f ≤g means f(x) ≤g(x) for all x ∈R
(e) Z is the set of functions f : D →R for some D ⊆R, and for f : D →R and
g : E →R in Z, f ≤g means D ⊆E and f(x) = g(x) for all x ∈D
65.
For each poset in (b) through (e) of Exercise 64, describe all maximal elements
of the poset, or explain why none exist.
66.
For each poset in (b) through (e) of Exercise 64, give an example of an infinite
chain in that poset, and state whether the chain has an upper bound in the poset.
67.
Let R be a commutative ring, I be an ideal of R, and S be a nonempty subset of
R \ I. Use Zorn’s Lemma to prove that the set of ideals J of R with I ⊆J and
J ∩S = ∅has a maximal element (relative to set inclusion).
68.
An ideal P in a nonzero commutative ring R is called prime iff P ̸= R and for all
x, y ∈R, xy ∈P implies x ∈P or y ∈P. Use Zorn’s Lemma to prove that for a
given prime ideal P of R, the set of prime ideals Q contained in P has a minimal
element relative to set inclusion.
69.
Give an example of a commutative group G with no maximal proper subgroups.
Suppose we try to use Zorn’s Lemma to prove that every commutative group has
a maximal proper subgroup, by adapting the proof that maximal ideals exist in
commutative rings. Exactly where does the proof break down?
Introduction to Modules
511
70.
Use Zorn’s Lemma to prove that any poset (Z, ≤) has a maximal chain (which is
a chain C ⊆Z such that any set properly containing C is not a chain).
71.
For any set X, let P′(X) be the set of nonempty subsets of X. Use Zorn’s Lemma
to prove this version of the Axiom of Choice: for any set X, there exists a function
(a set of ordered pairs) f : P′(X) →X with f(S) ∈S for all S ∈P′(X).
72.
Assume M is a finite left R-module.
(a) Given a submodule C of M, explain why |M| = |C| · |M/C|.
(b) Let A and B be submodules of M. Use an appropriate isomorphism theorem
to prove that |A + B| = |A| · |B|/|A ∩B|.
73.
Use Figure 17.1 to find all maximal chains of submodules of the Z-module Z4×Z2.
Confirm that the conclusions of the Jordan–H¨older Theorem are true for these
chains.
74.
We know every subgroup of Z has the form Zm for a unique m ≥0. Also, for all
a, b ∈Z, Za ⊆Zb iff b divides a. Use this information and the Correspondence
Theorem to draw the submodule lattices of the following quotient Z-modules:
Z/8Z, Z/35Z, and Z/60Z. Verify by inspection of the drawings that all maximal
chains of submodules have the same length.
75.
Draw the lattice of all Z2-submodules of the Z2-module Z2 × Z2 × Z2. Confirm
that the conclusions of the Jordan–H¨older Theorem are true for this module.
76.
Let R be the set of upper-triangular matrices in M2(Z2).
(a) Prove that R is a subring of M2(Z2).
(b) Draw the submodule lattice of R, viewed as a left R-module.
(c) Draw the submodule lattice of R, viewed as a right R-module.
(d) Draw the lattice of two-sided ideals of the ring R.
77.
Solve this exercise without using the Jordan–H¨older Theorem.
(a) Suppose A is a submodule of an R-module M with the property that A and
M/A are simple R-modules. Suppose B is a submodule of M different from {0},
A, and M. Prove that B and M/B are simple R-modules. (Draw a picture of
what you know about the submodule lattice of M.)
(b) Give an example of R, A, B, M satisfying the conditions in (a), such that A
and B are non-isomorphic R-modules.
78.
Fix m, n ∈Z>0 with gcd(m, n) = 1. Use the Recognition Theorem for Direct
Products to show that the Z-module Zmn = {0, 1, . . . , mn −1} (with operation
addition mod mn) is isomorphic to the direct product of its submodules Zn and
Zm. Conclude that Zmn ∼= Zm × Zn as Z-modules.
79.
Recognition Theorem for Finite Direct Products. Suppose N1, N2, . . . , Nk
are submodules of a left R-module M such that M = N1 + N2 + · · · + Nk and
Ni ∩(N1 + N2 + · · · + Ni−1) = {0} for all i between 2 and k. Prove that g : N1 ×
N2 × · · · × Nk →M, defined by g(x1, . . . , xk) = x1 + · · · + xk for xi ∈Ni, is an
R-module isomorphism.
80.
Recognition Theorem for Direct Sums. Suppose I is an index set, and {Ni :
i ∈I} is a collection of submodules of a left R-module M such that M = P
i∈I Ni
and Ni ∩P
j∈I: j̸=i Nj = {0M} for all i ∈I. Prove that M ∼= L
i∈I Ni.
81.
Find the length of the following Z-modules.
(a) Z32 (b) Z60 (c) Z12 × Z15
(d) Z
(e) Zn
p, where p is prime and n ≥1
82.
Given a positive integer n, apply the Jordan–H¨older Theorem to the Z-module
Zn to prove that n can be factored into a product of prime integers that are
uniquely determined by n (up to reordering).
512
Advanced Linear Algebra
83.
Let V be a vector space over a field F.
(a) Prove: if V has an n-element basis for some integer n, then there is a maximal
chain of subspaces of V of length n.
(b) Prove: if dim(V ) = ∞, then V has an infinitely long chain of subspaces.
(c) Use the Jordan–H¨older Theorem to prove that if V is finite-dimensional, then
all bases of V have the same (finite) size.
84.
Let (x1, . . . , xn) be a list of elements in a left R-module M, where R is a nonzero
ring. Show that this list is R-linearly dependent in each of the following situations.
(a) xi = 0M for some i; (b) xi = xj for some i ̸= j; (c) x1, . . . , xn−1 generate M.
85.
Let v1 = (2, 3) and v2 = (1, 5).
(a) Show that (v1, v2) is an ordered basis of the R-module R × R.
(b) Show that (v1, v2) is not an ordered basis of the Z-module Z × Z.
(c) Is (v1, v2) an ordered basis of the (R × R)-module R × R? Explain.
86.
Let f : N →P be an R-linear map. Prove: if f is surjective and the list
(x1, . . . , xn) generates N, then (f(x1), . . . , f(xn)) generates P.
87.
Let f : N →P be an R-linear map. Prove: if f is injective and the list (x1, . . . , xn)
is R-linearly independent in N, then (f(x1), . . . , f(xn)) is R-linearly independent
in P.
88.
Check in detail that the map g′ (from the UMP in §17.11) is an R-linear map
and g′(x) = g(x) for all x ∈S.
89.
Let I be an ideal of a commutative ring R such that {0} ̸= I ̸= R. Show that the
R-module R/I has no finite basis. Indicate where you use the hypotheses {0} ̸= I
and I ̸= R. Does the R/I-module R/I have a basis? Explain.
90.
Prove: for all x, y ∈R, the list (x, y) is an R-basis of the left R-module R iff there
exist r, s ∈R with xr = 1 = ys, yr = 0 = xs, and rx + sy = 1.
91.
Let A, B, C be submodules of a left R-module M such that A
⊆
B,
A + C = B + C, and A ∩C = B ∩C. Prove that A = B.
92.
Give an example of an R-module M and submodules A, B, C such that A + C =
B + C, A ∩C = B ∩C, and yet A ̸= B.
93.
Give a justified example of each of the following:
(a) a ring R and an R-module that is not finitely generated
(b) a ring R and an infinite non-cyclic finitely generated R-module
(c) a ring R and a finitely generated R-module that has no basis
(d) a list (v1, v2) in the Z-module Z × Z that is Z-linearly independent but does
not generate Z × Z
(e) a ring R, a commutative group (M, +), and a function • : R × M →M
satisfying every module axiom except 1R • x = x for all x ∈M
(f) a simple left M3(Q)-module.
94.
True or false? Explain each answer.
(a) The set of all subgroups of a fixed R-module M, ordered by set inclusion, is
always a complete lattice.
(b) Every submodule of a product R-module M × N must have the form A × B,
where A is a submodule of M and B is a submodule of N.
(c) For any submodules A and B of a left R-module M, there is an R-module
isomorphism (A + B)/A ∼= A/(A ∩B).
(d) A left R-module M is simple iff M has exactly two submodules.
Introduction to Modules
513
(e) There exists exactly one function · : Z × (Z × Z) →Z × Z that turns the
additive group Z × Z into a left Z-module.
(f) There exists exactly one function · : (Z × Z) × Z →Z that turns the additive
group Z into a left (Z × Z)-module.
(g) Every left R-module M is a sum of cyclic submodules.
95.
Suppose we are given ten left R-modules and thirteen R-linear maps as shown in
the following diagram:
A
f
/
α

B
g
/
β

C
h
/
γ

D
k
/
δ

E
ϵ

A′
f ′
/ B′
g′
/ C′
h′
/ D′
k′
/ E′
(This means f : A →B, α : A →A′, etc.) Assume that:
(1) img(f) = ker(g)
(2) img(g) = ker(h)
(3) img(h) = ker(k)
(4) img(f ′) = ker(g′)
(5) img(g′) = ker(h′)
(6) img(h′) = ker(k′)
(7) f ′ ◦α = β ◦f
(8) g′ ◦β = γ ◦g
(9) h′ ◦γ = δ ◦h
(10) k′ ◦δ = ϵ ◦k
(11) β is one-to-one
(12) δ is one-to-one
(13) α is onto
(a) Prove that γ is one-to-one. Indicate which of the 13 hypotheses is needed in
each step of the proof.
(b) Keep assumptions (1) through (10), but replace (11), (12), (13) by:
(11′) β is onto
(12′) δ is onto
(13′) ϵ is one-to-one
Prove that γ is onto. Indicate which of the 13 hypotheses is needed in each step
of the proof.
(c) Deduce that, if (1) through (10) hold and α, β, δ, and ϵ are isomorphisms,
then γ is an isomorphism.
18
Principal Ideal Domains, Modules over PIDs, and
Canonical Forms
Given a field F, we know that every finite-dimensional F-vector space V is isomorphic to
F n for some integer n ≥0, where F n is the direct product of n copies of the vector space F.
Similarly, given any finitely generated commutative group G, we proved in Chapter 16 that
G is isomorphic to a direct product of finitely many cyclic groups. Now, F-vector spaces are
the same thing as F-modules, and commutative groups are the same thing as Z-modules. So,
the two theorems just mentioned provide a classification of all finitely generated R-modules,
where R is either a field or the ring Z.
In this chapter, we prove a more general classification theorem that includes both of the
previous results as special cases. To obtain this theorem, we must isolate the key properties
of fields and the ring Z that made the previous theorems work. This leads us to study
principal ideal domains (abbreviated PIDs), which are integral domains where every ideal
can be generated by a single element. We will see that Z is a PID, and so is the polynomial
ring F[x] in one variable with coefficients in a field F. As in the cases of Z and F[x], we
will see that elements in general PIDs have unique factorizations into irreducible elements
(which are analogous to prime integers or irreducible polynomials).
The key theorem in this chapter asserts that for any PID R, every finitely generated
R-module M is isomorphic to a direct product of cyclic R-modules
R/Ra1 × R/Ra2 × · · · × R/Rak
for some k ∈Z>0 and some ai ∈R. As in the case of Z-modules, we can arrange that the
ai satisfy certain additional conditions (for instance, that each ai divide ai+1, or that every
nonzero ai be a prime power in R). When we impose appropriate conditions of this kind,
the ideals Ra1, . . . , Rak appearing in the decomposition of M are uniquely determined by
M.
The proof of this theorem mimics the proof of the classification theorem for commutative
groups, which occupies most of Chapter 16. The current chapter can be read independently
of that one and yields the main results of that chapter as a special case. However, the
reader is urged to study that chapter first, to get used to the essential ideas of the proof in
the very concrete setting of integer-valued matrices. This chapter does assume knowledge
of definitions and basic facts about R-modules and free R-modules, which we cover in
Chapter 17. We also need some material on one-variable polynomials over a field from
Chapter 3.
As an application of the Classification Theorem for Modules over PIDs, we prove the
Rational Canonical Form Theorem for Linear Operators. This theorem shows that every
linear map T : V →V defined on a finite-dimensional F-vector space V can be represented
(relative to an appropriate ordered basis) by a matrix with an especially simple structure.
To obtain this matrix and to prove its uniqueness, we first use T to make V into a finitely
generated F[x]-module, then apply the structure theorems of this chapter to this module.
For fields F satisfying appropriate hypotheses, we use similar techniques to give another
DOI: 10.1201/9781003484561-18
514
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
515
derivation of the Jordan Canonical Form Theorem from Chapter 8. We also discuss Smith
normal forms, rational canonical forms, and Jordan forms for matrices.
18.1
Principal Ideal Domains
We begin by spelling out the definition of a principal ideal domain in more detail. Recall
from §1.2 that an integral domain is a commutative ring (R, +, ·) such that 1R ̸= 0R and R
has no zero divisors other than 0. The last condition means that for all a, b ∈R, if a·b = 0R,
then a = 0R or b = 0R. The following Cancellation Law holds in integral domains R: for
all x, y, z ∈R with x ̸= 0R, if xy = xz then y = z. To prove this, rewrite xy = xz as
xy −xz = 0 and then as x(y −z) = 0. Since x ̸= 0, we get y −z = 0 and y = z.
Next, recall from §1.4 that an ideal of a commutative ring R is a subset I of R satisfying
these closure conditions: 0R ∈I; for all x, y ∈I, x + y ∈I; for all x ∈I, −x ∈I; and for
all x ∈I and r ∈R, r · x ∈I. You can check that for any commutative ring R and any
a ∈R, the set Ra = {r · a : r ∈R} is an ideal of R containing a. This ideal is called the
principal ideal generated by a. An ideal I of R is a principal ideal iff there exists a ∈R with
I = Ra. A principal ideal domain (PID) is an integral domain R such that every ideal of R
is a principal ideal.
We know that the ring Z is an integral domain, since the product of any two nonzero
integers is nonzero. To prove that Z is a PID, consider any ideal I of Z. By the first three
closure conditions in the definition of an ideal, we see that I is an additive subgroup of
the group (Z, +). In §16.1, we used integer division with remainder to prove that such a
subgroup must have the form Zn = {kn : k ∈Z} for some integer n ≥0. So I is a principal
ideal.
You can check (Exercise 2) that every field F is a PID. Next, we show that a one-
variable polynomial ring F[x] with coefficients in a field F is a PID. In §3.4, we used degree
arguments to see that F[x] is an integral domain. To show that every ideal I of F[x] is
principal, we use the division theorem for one-variable polynomials (§3.6). On one hand, if
I = {0}, then I = F[x]0 is a principal ideal. On the other hand, if I is a nonzero ideal, we
can choose a nonzero g ∈I of minimum possible degree. Because I is an ideal and g ∈I,
I contains the principal ideal F[x]g = {pg : p ∈F[x]}. We now prove that I ⊆F[x]g,
which shows that I = F[x]g is principal. Fix any f ∈I. Dividing f by g produces a unique
quotient q ∈F[x] and remainder r ∈F[x] with f = qg + r and r = 0 or deg(r) < deg(g). If
r = 0, then f = qg ∈F[x]g as needed. If r ̸= 0, then r = f + (−q)g ∈I since f, g ∈I and
I is an ideal. But then deg(r) < deg(g) contradicts minimality of the degree of g, so r ̸= 0
cannot occur.
You can check (Exercises 3 and 4 that Z[x] and F[x1, . . . , xn] (where n > 1) are examples
of integral domains that are not PIDs.
18.2
Divisibility in Commutative Rings
In any commutative ring R, we can define concepts related to divisibility by analogy with
the familiar definitions for integers and polynomials. Given a, b ∈R, we say a divides b in
R and write a|b iff there exists c ∈R with b = ac. When a|b, we also say b is a multiple of a
and a is a divisor of b. You can check that: for all a ∈R, a|a (reflexivity); for all a, b, c ∈R,
516
Advanced Linear Algebra
if a|b and b|c then a|c (transitivity); and for all a ∈R, 1|a and a|0. A unit of the ring R is
an element u ∈R such that u|1. This means that 1 = uv = vu for some v ∈R, so that the
units of R are precisely the invertible elements of R relative to the multiplication in R. We
let R∗be the set of units of R.
The set R under the divisibility relation | is almost a poset, since | is reflexive and
transitive on R. However, for almost all commutative rings R, antisymmetry does not hold
for |. In other words, there can exist x ̸= y in R with x|y and y|x. For all x, y ∈R, we define
x and y to be associates in R, denoted x ∼y, iff x|y and y|x. You can check that ∼is an
equivalence relation on the set R. Associate ring elements behave identically with respect
to divisibility; more precisely, given a, a′, b, c ∈R with a ∼a′, a|b iff a′|b, and c|a iff c|a′.
Similarly, u ∈R is a unit of R iff u ∼1.
If R is an integral domain, then there is an alternate description of when two elements
x, y ∈R are associates: x ∼y iff y = ux for some unit u ∈R∗. To prove one direction,
assume y = ux for some unit u of R. There is v ∈R with vu = 1, so vy = vux = 1x = x.
Since y = ux and x = vy, we see that x|y and y|x in R, hence x ∼y. Conversely, assume x
and y are associates in R. Then y = ax and x = by for some a, b ∈R. Combining these, we
get 1x = by = (ba)x and 1y = ax = aby = (ba)y. If x or y is nonzero, then the Cancellation
Law for integral domains gives ba = 1, so that a ∈R∗and y = ax as needed. If x = 0 = y,
then y = ux holds for the unit u = 1R.
For example, in Z the units are +1 and −1, so x ∼y in Z iff y = ±x. In F[x] where
F is a field, the units are the nonzero constant polynomials, so p ∼q in F[x] iff q = cp
for some nonzero c ∈F. It follows that each equivalence class of ∼in Z contains exactly
one nonnegative integer, and each equivalence class of ∼in F[x] (other than {0}) contains
exactly one monic polynomial.
Next, we define common divisors, common multiples, gcds, and lcms. Let a1, . . . , ak be
fixed elements of a commutative ring R. We say d ∈R is a common divisor of a1, . . . , ak iff
d|ai for 1 ≤i ≤k. We say e ∈R is a common multiple of a1, . . . , ak iff ai|e for 1 ≤i ≤k.
We say d ∈R is a greatest common divisor (gcd) of a1, . . . , ak iff d is a common divisor of
a1, . . . , ak such that for all common divisors c of a1, . . . , ak, c|d. We say e ∈R is a least
common multiple (lcm) of a1, . . . , ak iff e is a common multiple of a1, . . . , ak such that for
all common multiples c of a1, . . . , ak, e|c. (Compare these definitions to the definitions of
lower bounds, upper bounds, greatest lower bounds, and least upper bounds in a poset,
given in the Appendix.)
We warn the reader that greatest common divisors of a1, . . . , ak need not exist in general.
Even if gcds do exist, they may not be unique. Indeed, given associate ring elements d, d′ ∈
R, it follows from the definitions that d is a gcd of a1, . . . , ak iff d′ is a gcd of a1, . . . , ak. On
the other hand, if c, d ∈R are any two gcds of a1, . . . , ak, then c ∼d. Similar results hold
for lcms. In Z and F[x], we can remedy the non-uniqueness by using nonnegative integers
and monic polynomials as our gcds and lcms.
18.3
Divisibility and Ideals
Early in the development of abstract algebra, it was realized that divisibility of elements
in a commutative ring R can be conveniently studied by instead looking at containment of
principal ideals. To explain this, we need the fundamental observation that for all a, b ∈R,
a|b in R iff Rb ⊆Ra. Fix a, b ∈R. On one hand, assume a|b in R, say b = ac for some c ∈R.
To prove Rb ⊆Ra, fix x ∈Rb. Then x = rb for some r ∈R, hence x = r(ac) = (rc)a ∈Ra.
On the other hand, assume Rb ⊆Ra. Then b = 1b ∈Rb, so b ∈Ra, so b = sa for some
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
517
s ∈R, so a|b. When using this result, we must remember that the smaller element a in the
divisibility relation a|b corresponds to the larger ideal in the containment relation Rb ⊆Ra.
You can check that the set X of all ideals of R is a poset ordered by set inclusion. The
subset Z of X consisting of the principal ideals of R is also a poset ordered by ⊆. Let us
translate some of the definitions in the last section into statements about ideal containment.
First, a, b ∈R are associates in R iff a and b generate the same principal ideal in R, because:
a ∼b iff a|b and b|a iff Rb ⊆Ra and Ra ⊆Rb iff Ra = Rb. Second, u is a unit of R iff
Ru = R, because: u ∈R∗iff u ∼1 iff Ru = R1 = R. Third, given d, a1, . . . , ak ∈R, d is
a common divisor of a1, . . . , ak iff d|ai for all i iff Rai ⊆Rd for all i iff the ideal Rd is an
upper bound for the set of ideals {Ra1, . . . , Rak} in the poset Z. Fourth, the element d is a
common multiple of the elements ai iff the ideal Rd is a lower bound for {Ra1, . . . , Rak} in
Z. Fifth, d is a gcd of the ai iff Rd is the least upper bound of {Ra1, . . . , Rak} in Z Sixth, d
is an lcm of the ai iff Rd is the greatest lower bound of {Ra1, . . . , Rak} in Z. We illustrate
these ideas by proving the following result.
Theorem on GCDs and LCMs in PIDs. Let R be a PID. For any a1, . . . , ak in R,
d = gcd(a1, . . . , ak) exists in R and L = lcm(a1, . . . , ak) exists in R. Furthermore, there
exist r1, . . . , rk ∈R with d = r1a1 + · · · + rkak.
The last statement says that in a PID R, the gcd of a1, . . . , ak is an R-linear combination
of a1, . . . , ak. This property of gcds need not hold in more general integral domains (see
Exercise 42).
Proof. To prove that the gcd and lcm of a1, . . . , ak exist, it suffices to find a least upper
bound and a greatest lower bound for the set of ideals {Ra1, Ra2, . . . , Rak} in the poset Z.
Recall from §17.3 that the poset of all submodules of a fixed left R-module M is a lattice.
More specifically (taking M = R), the least upper bound in X of a set of ideals {I1, . . . , Ik}
is the ideal sum I1+· · ·+Ik. The greatest lower bound in X of {I1, . . . , Ik} is the intersection
I1 ∩· · · ∩Ik. The crucial point is that R is a PID, so Z and X are the same poset.
On one hand, I = Ra1 + Ra2 + · · · + Rak = {r1a1 + r2a2 + · · · + rkak : r1, . . . , rk ∈R}
is an ideal of R that is the least upper bound for {Ra1, . . . , Rak} in the poset X = Z. We
know I is principal, so I = Rd for some d ∈R. Any generator d for I is a gcd of a1, . . . , ak.
Every such generator has the form d = r1a1+r2a2+· · ·+rkak for some ri ∈R, by definition
of I. On the other hand, J = Ra1 ∩Ra2 ∩· · · ∩Rak is an ideal of R that is the greatest
lower bound for {Ra1, . . . , Rak} in the poset X = Z. We know J is principal, so J = Re
for some e ∈R, and any generator e for J is an lcm of a1, . . . , ak.
18.4
Prime and Irreducible Elements
To continue our discussion of factorization theory in a commutative ring R, we need
to generalize the definitions of prime integers and irreducible polynomials. Two different
generalizations are possible, but we will see that these generalizations coincide when R is a
PID. Let p ∈R be a nonzero element of R that is not a unit of R. We say p is a prime in
R iff for all f, g ∈R, p|(fg) implies p|f or p|g. We say p is irreducible in R iff for all q ∈R,
q|p implies q ∼p or q ∈R∗. Irreducibility of p means that the only divisors of p in R are
the units of R (which divide everything) and the associates of p.
In any integral domain R, every prime p must be irreducible. To see why, let p be prime
in R and assume q ∈R divides p. Then p = qg for some g ∈R. By primeness of p, either
p|q or p|g. If p|q, then (since also q|p) we see that q ∼p. On the other hand, if p|g, write
518
Advanced Linear Algebra
g = rp for some r ∈R. Then 1p = qg = (qr)p. As p ̸= 0 in the integral domain R, we can
cancel p to get qr = 1, so that q ∈R∗. Since q ∼p or q ∈R∗, p is irreducible as claimed.
On the other hand, there exist integral domains R and irreducible elements p ∈R that are
not prime in R (see Exercise 31).
Let us translate the definitions of prime and irreducible elements into statements about
principal ideals. First, the assumption that p is a nonzero non-unit means that Rp ̸= {0}
and Rp ̸= R, so that Rp is a proper nonzero ideal of R. Rewriting the definition of prime
element, we see that p is prime in R iff for all f, g ∈R, R(fg) ⊆Rp implies Rf ⊆Rp or
Rg ⊆Rp. Now, R(fg) ⊆Rp iff fg ∈Rp, and similarly for Rf and Rg. So we can also say
that p is prime in R iff for all f, g ∈R, fg ∈Rp implies f ∈Rp or g ∈Rp. In ring theory,
an ideal I of a commutative ring R is called a prime ideal iff I ̸= R and for all f, g ∈R,
fg ∈I implies f ∈I or g ∈I. So we have shown that p is a prime element in R iff Rp is a
nonzero prime ideal of R.
Next, rewriting the definition of irreducible element, we get that p is irreducible in R
iff for all q ∈R, Rp ⊆Rq implies Rq = Rp or Rq = R. (Recall that p still satisfies
{0} ̸= Rp ̸= R.) In other words, irreducibility of a nonzero non-unit p means that the
principal ideal Rp is a maximal element in the poset of all proper, principal ideals of R. In
ring theory, an ideal I of a commutative ring R is called a maximal ideal iff I ̸= R and for
all ideals J with I ⊆J, either I = J or I = R. In other words, maximal ideals (as just
defined) are maximal elements in the poset of all proper ideals of R. In the case of a PID
R, the poset of proper principal ideals of R is the same as the poset of proper ideals of R.
We conclude that, in a PID R, p is irreducible iff Rp is a nonzero maximal ideal of R.
You can prove that every maximal ideal in any commutative ring R is a prime ideal
(Exercise 17). By the remarks in the last two paragraphs, we conclude that every irreducible
element in a PID is also a prime element. Since the converse holds in all integral domains,
irreducible elements and prime elements coincide in a PID.
18.5
Irreducible Factorizations in PIDs
A unique factorization domain (UFD) is an integral domain R such that for each nonzero
non-unit f ∈R: (a) there exist irreducible elements p1, . . . , pk ∈R with f = p1p2 · · · pk;
(b) for any two factorizations f = up1p2 · · · pk = vq1q2 · · · qm with all pi and qj irreducible
in R and u, v ∈R∗, we must have k = m and, after reordering the qj, pi ∼qi for 1 ≤i ≤k.
Z is a UFD, and we saw in Chapter 3 that F[x] is a UFD for any field F. The next
theorem includes these two results as special cases.
Theorem: Every PID is a UFD.
Proof. We need a preliminary lemma about chains of ideals in a PID. Suppose R is a PID
and we have an infinite sequence of ideals
I1 ⊆I2 ⊆I3 ⊆· · · ⊆Ik ⊆· · · .
Then there exists k0 such that Ik = Ik0 for all k ≥k0. Informally, we say that every
ascending chain of ideals in a PID must stabilize. To prove this, let I = S∞
k=1 Ik be the
union of all the ideals in the sequence. You can verify that I is an ideal of R, and Ik ⊆I
for all k ≥1. Since R is a PID, there exists a ∈R with I = Ra. Now a ∈I, so a ∈Ik0
for some fixed index k0. Then, for any k ≥k0, I = Ra ⊆Ik0 ⊆Ik ⊆I, which proves that
I = Ik = Ik0 for all such k.
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
519
We prove (a) holds for a PID R by contradiction. Assuming (a) fails, we can find a
nonzero non-unit f0 ∈R that cannot be factored into a product of irreducible elements in
R. In particular, f0 itself cannot be irreducible, so we can write f0 = gh where g, h ∈R
are not zero, are not units of R, and are not associates of f0. In terms of ideals, these
conditions mean that Rf0 ⊊Rg ⊊R and Rf0 ⊊Rh ⊊R. Now, if g and h could both
be factored into products of irreducible elements, then f could be so factored as well by
combining the two factorizations. It follows that g or h must also be a counterexample to
(a). Let f1 = g if g is a counterexample, and f1 = h otherwise. Now Rf0 ⊊Rf1 ⊊R
and f1 is a counterexample to (a). Then we can repeat the argument to produce another
counterexample f2 with Rf0 ⊊Rf1 ⊊Rf2 ⊊R. This process can be continued indefinitely
(using the Axiom of Choice), ultimately producing an infinite strictly ascending chain of
ideals in the PID R. But this contradicts the lemma proved in the previous paragraph. So
(a) does hold for R.
To prove (b), assume f = up1p2 · · · pk = vq1q2 · · · qm, where u, v ∈R∗, k, m ∈Z>0, and
all pi and qj are irreducible in the PID R. Because R is a PID, all pi and qj are prime.
In particular, the prime element p1 divides f = vq1q2 · · · qm, so p1 must divide some qs.
(Note p1 cannot divide the unit v, or p1 would also be a unit of R.) Reordering the qj
if needed, assume that p1 divides q1. As p1 is a non-unit and q1 is irreducible, we obtain
p1 ∼q1. Write q1 = wp1 for some w ∈R∗. Then up1p2 · · · pk = (vw)p1q2 · · · qm where u
and v′ = vw are units of R. We are in an integral domain, so we can cancel p1 to obtain
up2 · · · pk = v′q2 · · · qm. Now we repeat the argument to get p2 ∼qj for some j ≥2. We can
reorder to ensure j = 2 and then modify the unit v′ to replace q2 by its associate p2. Then
cancel p2 from both sides and continue until all pi have been matched with appropriate
qj. Note that k < m is impossible, since otherwise we would obtain u = v∗qk+1 · · · qm
after k cancellation steps, contradicting the fact that qm is not a unit. Similarly, k > m is
impossible, so k = m, and pi ∼qi for 1 ≤i ≤k after reordering the qj.
18.6
Free Modules over a PID
We now begin our study of finitely generated modules over a fixed PID R. As in the case
of Z-modules (Chapter 16), the first step is to look at properties of finitely generated free
R-modules. See §17.11 for proofs of the general facts about free modules reviewed here.
An R-module M is free and finitely generated (abbreviated f.g. free) iff M has a finite
ordered basis B = (v1, . . . , vn), which is an R-linearly independent list of vectors that spans
the R-module M. In more detail, R-independence of B means that for all c1, . . . , cn ∈R,
if c1v1 + · · · + cnvn = 0M, then c1 = · · · = cn = 0R. Saying that B spans M means
that for all w ∈M, there exist d1, . . . , dn ∈R such that w = d1v1 + · · · + dnvn. Because
B is R-independent, the list of scalars (d1, . . . , dn) is uniquely determined by w. We call
(d1, . . . , dn) the coordinates of w relative to the ordered basis B. The map T : M →Rn such
that T(w) = (d1, . . . , dn) is an R-module isomorphism. Hence, every f.g. free R-module M
with an n-element basis is isomorphic to the free R-module Rn whose elements are n-tuples
of scalars in R.
The R-module M with ordered basis B = (v1, . . . , vn) satisfies the following universal
mapping property (UMP): for every R-module N and every list w1, . . . , wn ∈N, there
exists a unique R-linear map U : M →N with U(vi) = wi for 1 ≤i ≤n, namely
U(Pn
i=1 divi) = Pn
i=1 diwi. In §17.11, we used the UMP to show that for any (possibly
non-free) finitely generated R-module N generated by n elements, there is a surjective R-
linear map U : Rn →N, and hence an R-module isomorphism N ∼= Rn/ ker(U). In other
520
Advanced Linear Algebra
words, every finitely generated R-module is isomorphic to a quotient module of a f.g. free
R-module by some submodule. This explains why the study of f.g. free R-modules helps
us understand the structure of all finitely generated R-modules. Submodules of f.g. free
modules also play a role, as seen in the next theorem.
Theorem on Submodules of a Free Module over a PID. Let R be a PID. Any
submodule P of any f.g. free R-module M is also f.g. free. If M has a k-element basis, then
P has a d-element basis for some d ≤k.
Proof. We imitate the proof in §16.8 for the special case R = Z. Let M be a f.g. free module
over a PID R. We know M ∼= Rk for some integer k ≥0, so we can assume M = Rk without
loss of generality. The proof uses induction on k. If k = 0, then P and M must be the zero
module, which is f.g. free with an empty basis. Suppose k = 1; the assumption that P is a
submodule of R1 means that P is an ideal of the ring R. Because R is a PID, there exists
a ∈R with P = Ra. If a = 0, then P = {0} is f.g. free with a basis of size zero. Otherwise,
a ̸= 0, and B = (a) is a generating list for P of size 1. Is this list R-linearly independent?
Given c ∈R with ca = 0, we see that c = 0 since a ̸= 0 and R is an integral domain. So B
is a basis for P of size 1, and P is f.g. free. Note how the conditions in the definition of a
PID were exactly what we needed to make this base case work.
Proceeding to the induction step, fix k > 1 and assume the theorem is known for all
f.g. free R-modules having bases of size less than k. Let P be a fixed submodule of Rk. Define
Q = P ∩(Rk−1 × {0}), which is a submodule of the free R-module Rk−1 × {0} ∼= Rk−1. By
induction, Q is f.g. free with some ordered basis (v1, . . . , vd−1) where d−1 ≤k−1. Consider
the projection map T : Rk →R given by T((r1, . . . , rk)) = rk for ri ∈R. T is R-linear, so
T[P] = {T(x) : x ∈P} is an R-submodule of R. Since R is a PID, T[P] = Ry for some
y ∈R. Fix an element vd ∈P with T(vd) = y, so vd is a k-tuple of elements of R with last
coordinate y.
If y = 0, then P = Q is f.g. free with a basis of size d −1 < k. Assuming y ̸= 0, we now
show that B = (v1, . . . , vd) is an ordered R-basis of P, so that P is f.g. free with a basis
of size d ≤k. First, we show B is R-linearly independent. Assume c1, . . . , cd ∈R satisfy
c1v1 +· · ·+cdvd = 0. Applying the R-linear map T and noting that T(vi) = 0 for each i < d
(since v1, . . . , vd−1 ∈Q), we get 0 = c1T(v1) + · · · + cdT(vd) = cdy. As y ̸= 0 and R is an
integral domain, cd = 0 follows. Now, since c1v1 + · · · + cd−1vd−1 = 0, the known R-linear
independence of v1, . . . , vd−1 gives c1 = · · · = cd−1 = 0. So B is R-linearly independent.
Second, we check that B spans the R-module P. Fix z = (z1, . . . , zk) ∈P. Since zk =
T(z) ∈T[P] = Ry, we have zk = ry for some r ∈R. Then z −rvd is in the R-submodule P
and has last coordinate zk−ry = 0, so z−rvd ∈Q. Therefore z−rvd = e1v1+· · ·+ed−1vd−1
for some ei ∈R, and we see that z itself is an R-linear combination of v1, . . . , vd. This
completes the induction step.
18.7
Operations on Bases
Assume R is a PID and M is a f.g. free R-module with ordered basis X = (v1, . . . , vn).
We can perform various transformations on X that produce new ordered bases for M. For
example, by analogy with §16.4, there are three elementary operations we could apply to
X. Operation (B1) interchanges vi and vj for some i ̸= j; operation (B2) replaces vi by uvi
for some i and some unit u ∈R∗; and operation (B3) replaces vi by vi + bvj for some i ̸= j
and some b ∈R. You can check that applying any finite sequence of such operations to X
produces a new ordered basis of M, and each elementary operation is reversible.
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
521
We require an even more general operation on bases that includes (B1), (B2), and (B3)
as special cases. Suppose we are given a, b, c, d in a PID R such that u = ad −bc is a unit
of R. Operation (B4) acts on X by replacing vi with v′
i = avi + bvj and replacing vj with
v′
j = cvi + dvj for some i ̸= j in {1, 2, . . . , n}. We can also write this as
 v′
i
v′
j

=
 a
b
c
d
  vi
vj

.
(18.1)
We claim the new list X ′ = (v1, . . . , v′
i, . . . , v′
j, . . . , vn) is another ordered basis for M, and
X can be recovered from X ′ by another operation of type (B4).
By inverting the 2 × 2 matrix in (18.1), we get

vi
vj

=

u−1d
−u−1b
−u−1c
u−1a
  v′
i
v′
j

.
(18.2)
where (u−1d)(u−1a) −(−u−1b)(−u−1c) = u−2(da −bc) = u−1 is a unit in R since u ∈R∗.
This shows that we can go from X ′ back to X by an operation of type (B4).
Let us check that X ′ spans M and is R-linearly independent. Given w ∈M, write
w = Pn
k=1 dkvk for scalars dk ∈R. Using (18.2), we can replace divi and djvj in this
expression by R-linear combinations of v′
i and v′
j. So w is in the span of X ′. Next, assume
0 = eiv′
i + ejv′
j + P
k̸=i,j ekvk for scalars ek ∈R. Using (18.1), this equation becomes
0
=
ei(avi + bvj) + ej(cvi + dvj) +
X
k̸=i,j
ekvk
=
(eia + ejc)vi + (eib + ejd)vj +
X
k̸=i,j
ekvk.
By the known linear independence of X, it follows that ek = 0 for all k ̸= i, j and
eia + ejc = eib + ejd = 0. In matrix notation, [ei ej]
 a
b
c
d

= [0 0]. Right-multiplying
by the inverse matrix gives ei = ej = 0. So X ′ is linearly independent.
Note that the elementary operations (B1), (B2), and (B3) really are special cases of
operation (B4): for (B1), take a = d = 0 and b = c = 1; for (B2), take a = u, d = 1 (for any
j), and b = c = 0; for (B3), take a = d = 1, c = 0 and any b. You can check that (B2) can
still be applied to replace v1 by uv1 (for some u ∈R∗) in the case n = 1.
18.8
Matrices of Linear Maps between Free Modules
Our next step is to study the matrices that represent linear maps between f.g. free modules.
Assume R is a PID, M is a f.g. free R-module with ordered basis X = (v1, . . . , vn), N
is a f.g. free R-module with ordered basis Y = (w1, . . . , wm), and T : M →N is a fixed
R-linear map. For 1 ≤j ≤n, we can write T(vj) = Pm
i=1 A(i, j)wi for unique scalars
A(i, j) ∈R (these scalars are the coordinates of T(vj) relative to the basis Y). The m × n
matrix A = [A(i, j)] is called the matrix of T relative to the bases X and Y. By linearity of
T, T is uniquely determined by the matrix A when X and Y are fixed and known. As in
previously studied cases (when R = Z or R is a field), you can show that matrix addition
corresponds to pointwise addition of linear maps, and matrix multiplication corresponds to
composition of linear maps.
522
Advanced Linear Algebra
Changing the input basis X or the output basis Y changes the matrix A that represents
the given linear map T. Let us investigate how an application of basis operation (B4) to
X or to Y affects the matrix A. Suppose a, b, c, d ∈R satisfy u = ad −bc ∈R∗. Define
a′ = u−1d, b′ = −u−1b, c′ = −u−1c, and d′ = u−1a, which are the entries of the inverse
matrix shown in (18.2). Recall that A[k] denotes column k of A, and A[k] denotes row k of
A. We make two claims:
1.
Suppose X ′ is obtained from X by replacing vi and vj by v′
i = avi + bvj and
v′
j = cvi + dvj. Then the matrix B of T relative to the bases X ′ and Y satisfies
B[i] = aA[i] + bA[j], B[j] = cA[i] + dA[j], and B[k] = A[k] for all k ̸= i, j. We say
B is obtained from A by a type 4 column operation on columns i and j.
2.
Suppose Y′ is obtained from Y by replacing wi and wj by w′
i = awi + bwj and
w′
j = cwi + dwj. Then the matrix C of T relative to the bases X and Y′ satisfies
C[i] = A[i]a′ + A[j]c′, C[j] = A[i]b′ + A[j]d′, and C[k] = A[k] for all k ̸= i, j. We say
C is obtained from A by a type 4 row operation on rows i and j.
We prove the second claim, leaving the first claim as Exercise 36. For fixed p ∈{1, . . . , n},
we must find the unique scalars C(s, p) ∈R satisfying
T(vp) = C(i, p)w′
i + C(j, p)w′
j +
X
k̸=i,j
C(k, p)wk.
We know
T(vp) = A(i, p)wi + A(j, p)wj +
X
k̸=i,j
A(k, p)wk.
Recalling from (18.2) that wi = a′w′
i + b′w′
j and wj = c′w′
i + d′w′
j, we get
T(vp) = (A(i, p)a′ + A(j, p)c′)w′
i + (A(i, p)b′ + A(j, p)d′)w′
j +
X
k̸=i,j
A(k, p)wk.
Since Y′ is R-independent, we can equate coefficients to conclude that C(i, p) = A(i, p)a′ +
A(j, p)c′, C(j, p) = A(i, p)b′ + A(j, p)d′, and C(k, p) = A(k, p) for all k ̸= i, j. This holds
for all p, so the rows of C are related to the rows of A as stated in the claim.
As special cases of the result for (B4), we can deduce how elementary column and row
operations on the matrix A correspond to elementary operations of types (B1), (B2), and
(B3) on the input basis X and the output basis Y. Specifically, you can check that:
3.
Switching columns i and j in A corresponds to switching vi and vj in the input
basis X; and switching rows i and j of A corresponds to switching wi and wj in
the output basis Y.
4.
Multiplying column i of A by a unit u ∈R∗corresponds to replacing vi by uvi in
the input basis X; and multiplying row i of A by u ∈R∗corresponds to replacing
wi by u−1wi in the output basis Y.
5.
Adding b times column j of A to column i of A (where b ∈R) corresponds to
replacing vi by vi + bvj in the input basis X. Adding b times row j of A to row i
of A corresponds to replacing wj by wj −bwi in the output basis Y.
We also need the following properties, which you are asked to prove in Exercise 36:
6.
Suppose e ∈R divides every entry of a matrix A ∈Mm,n(R). If we apply any
sequence of type 4 row and column operations to A, then e still divides every
entry of the new matrix.
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
523
7.
Suppose B is obtained from A by applying a type 4 column operation as in
Claim 1. Then B = AV , where V ∈Mn(R) is an invertible matrix with entries
V (i, i) = a, V (j, i) = b, V (i, j) = c, V (j, j) = d, V (k, k) = 1R for all k ̸= i, j, and
all other entries of V are zero (cf. §4.7 and §4.9).
8.
Suppose C is obtained from A by applying a type 4 row operation as in Claim 2.
Then C = UA, where U ∈Mm(R) is an invertible matrix with entries U(i, i) = a′,
U(i, j) = c′, U(j, i) = b′, U(j, j) = d′, U(k, k) = 1R for all k ̸= i, j, and all other
entries of U are zero (cf. §4.8 and §4.9).
18.9
Reduction Theorem for Matrices over a PID
We now have all the tools needed to prove the following structural result.
Matrix Reduction Theorem for PIDs. . Let A be an m × n matrix with entries in a
PID R. There is a finite sequence of type 4 row and column operations on A that reduces
A to a new matrix
B =


a1
0
0
. . .
0
0
a2
0
. . .
0
0
0
a3
. . .
0
...
...

,
(18.3)
in which there are s ≥0 nonzero elements a1, . . . , as ∈R on the main diagonal, ai divides
ai+1 in R for 1 ≤i < s, and all other entries of B are zero.
For brevity, we write B = diag(a1, . . . , as)m×n, omitting the m × n when s = m = n. In
terms of ideals, the divisibility conditions on the aj are equivalent to Ra1 ⊇Ra2 ⊇Ra3 ⊇
· · · ⊇Ras. Later, we prove that s and the ideals satisfying this containment condition
are uniquely determined by A, and hence the aj are unique up to associates in R. So the
matrix B is unique in this sense; it is called a Smith normal form of A. By repeated use
of properties 7 and 8 from §18.8, we see that B = PAQ for some invertible P ∈Mm(R)
and Q ∈Mn(R), where P (resp. Q) is the product of all the matrices U (resp. V ) used to
accomplish the type 4 row (resp. column) operations needed to reduce A to B.
We proved the reduction theorem for R = Z in §16.11. If we try to repeat that proof in
the setting of general PIDs, a problem emerges. The old proof made critical use of integer
division with remainder, as well as the fact that there is no infinite strictly decreasing
sequence of positive integers. These proof ingredients can be generalized to a class of rings
called Euclidean domains (defined in Exercise 6), but they are not available in all PIDs. To
execute the proof at this level of generality, a new trick is needed.
Recall that all PIDs are UFDs, so that every nonzero non-unit a ∈R has a factorization
a = p1p2 · · · ps into irreducible elements pi in R; and any other irreducible factorization
a = q1q2 · · · qt has s = t and pi ∼qi after appropriate reordering. Define the length of a
in R to be len(a) = s, the number of factors appearing in any irreducible factorization of
a. For any unit u of R, let len(u) = 0; len(0R) is undefined. For any nonzero matrix A
with entries in R, let len(A) be the minimum length of all the nonzero entries of A. Given
nonzero a, b, d ∈R, you can check that: len(ab) = len(a) + len(b); if d divides a in R, then
len(d) ≤len(a) with equality iff d ∼a; and if d = gcd(a, b) where a does not divide b, then
len(d) < len(a).
We now begin the proof of the Matrix Reduction Theorem for a PID R. The theorem
holds when A = 0 or m = 0 or n = 0, so we assume m, n > 0 and A ̸= 0 throughout the
524
Advanced Linear Algebra
rest of the proof. Using induction on m (the number of rows), we can assume the theorem
is known to hold for all matrices with fewer than m rows.
Step 1: We show we can apply finitely many type 4 row and column operations to A to
produce a matrix A1 such that some nonzero entry in A1 divides all entries of A1. The proof
uses induction on len(A). If len(A) = 0, then some entry of A is a unit of R, which divides
all elements of R and hence divides all entries of A. Next, assume len(A) = ℓ> 0 and
the result of Step 1 is known to hold for all matrices of length less than ℓ. Let e = A(i, j)
be a nonzero entry of A with len(e) = ℓ. If e happens to divide all entries of A, then the
conclusion of Step 1 already holds for the matrix A. Suppose instead that there exists at
least one entry of A not divisible by e = A(i, j).
Case 1: There is k ̸= j such that e does not divide f = A(i, k). Since R is a PID, we know
g = gcd(e, f) exists in R, and g = ae + bf for some a, b ∈R. We have e = gd and f = gc for
some c, d ∈R. Cancelling g ̸= 0 in g = a(gd) + b(gc) gives 1R = ad + bc. So, we can apply a
type 4 column operation to A that replaces A[j] by aA[j] +bA[k] and A[k] by −cA[j] +dA[k].
The new matrix A′ has i, j-entry ae + bf = g, and len(g) < len(e) since e does not divide
f. So len(A′) ≤len(g) < len(A). By induction, we can apply further reduction steps to A′
to achieve the conclusion of Step 1.
Case 2: There is k ̸= i such that e = A(i, j) does not divide f = A(k, j). We argue as
in Case 1, but this time we use a type 4 row operation to replace e by g = gcd(e, f), which
lowers the length of the matrix.
Case 3: e = A(i, j) divides everything in row i and column j, but for some i1 ̸= i and
j1 ̸= j, e does not divide f = A(i1, j1). Pictorially, rows i, i1 and columns j, j1 look like this
for some u, v ∈R:


e
· · ·
ue
...
...
ve
· · ·
f

.
Adding (1 −v) times row i to row i1 produces:


e
· · ·
ue
...
...
e
· · ·
f + (1 −v)ue

.
If this new matrix has lower length than A, we are done by induction. Otherwise, note e
cannot divide f + (1 −v)ue in R, lest e divide f. So Case 1 now applies to row i1, and we
can complete Step 1 as in that case.
Step 2: Let a1 = A1(i, j) be a nonzero entry in A1 dividing all entries of A1. We show
A1 can be further reduced to the form
A2 =


a1
0
· · ·
0
0
...
A′
0

,
(18.4)
where A′ ∈Mm−1,n−1(R), and a1 divides all entries of A′ in R. To prove this, recall that
applying type 4 row and column operations to A1 never changes the property that a1 divides
every entry of the matrix. To begin, bring a1 into the 1, 1-position by switching row 1 and
row i and switching column 1 and column j. Since a1 divides every entry in row 1, we can
subtract appropriate multiples of column 1 from each later column to make the other entries
in row 1 become zero. Similarly, we can use row operations to produce zeroes below a1 in
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
525
the first column. The current matrix now looks like (18.4). Since a1 still divides all entries
of the full matrix, a1 divides every entry of A′.
Step 3: We show how A2 can be reduced to the normal form (18.3). Since A′ has
m −1 < m rows, the induction hypothesis shows that we can apply type 4 row and column
operations to this (m −1) × (n −1) matrix to obtain a matrix in normal form with entries
a2, . . . , as on the main diagonal, all other entries zero, and ai|ai+1 in R for 2 ≤i < s.
We can apply the same type 4 operations to the full matrix A2, and these operations do
not disturb the zeroes we have already created in row 1 and column 1. Furthermore, a1
continues to divide all entries of the matrix throughout the reduction of A2. In particular,
at the end, a1 divides a2, and we have reached the required normal form for A.
18.10
Structure Theorems for Linear Maps and Modules
The Matrix Reduction Theorem for PIDs translates into the following result on linear maps.
Structure Theorem for Linear Maps between Free Modules over PIDs. Let R be
a PID, N and M be f.g. free R-modules, and T : N →M be an R-linear map. There exist
an ordered basis X = (x1, . . . , xn) for N and an ordered basis Y = (y1, . . . , ym) for M such
that the matrix of T relative to X and Y is in Smith normal form (18.3). So there exist
s ∈Z≥0 and nonzero a1, . . . , as ∈R with ai|ai+1 for 1 ≤i < s, T(xi) = aiyi for 1 ≤i ≤s,
and T(xi) = 0M for s < i ≤n.
Later, we prove that s and the ideals Rai (satisfying Ra1 ⊇Ra2 ⊇· · · ⊇Ras ̸= {0})
are uniquely determined by T.
Proof. To prove existence, start with any ordered bases X0 for N and Y0 for M, and let A
be the matrix of T relative to X0 and Y0. Use a finite sequence of type 4 operations to bring
A into Smith normal form. To ensure that each new matrix still represents T, we perform
the appropriate basis operation (B4) on the input basis (when we do a column operation
on A) or on the output basis (when we do a row operation on A), using the rules explained
in §18.8. At the end, we get new ordered bases X for N and Y for M satisfying the required
properties.
Next, we prove the existence part of our main classification result.
Classification Theorem for Finitely Generated Modules over PIDs (Version 1).
For any finitely generated module M over a PID R, there exist d ∈Z≥0 and b1, . . . , bd ∈R
with R ̸= Rb1 ⊇Rb2 ⊇· · · ⊇Rbd ⊇{0} and
M ∼= R/Rb1 × R/Rb2 × · · · × R/Rbd.
(18.5)
Note that some bj could be zero, in which case R/Rbj ∼= R. Each ideal Rbj is called an
invariant factor of M. The generators bj of these ideals are also called invariant factors.
In §18.14, we prove that d and the sequence of ideals (Rb1, Rb2, . . . , Rbd) satisfying the
stated conditions are uniquely determined by M.
Proof. To prove the existence part of the theorem, suppose M is an R-module generated by
m elements. Recall (§18.6) that M ∼= Rm/P for some submodule P of Rm, where P is a free
R-module with a basis of size n ≤m. The inclusion map T : P →Rm, given by T(x) = x
for all x ∈P, is an R-linear map between free R-modules. So there exist an ordered basis
X = (x1, . . . , xn) of P, an ordered basis Y = (y1, . . . , ym) for Rm, s ∈Z≥0, and nonzero
526
Advanced Linear Algebra
a1, . . . , as ∈R with Ra1 ⊇· · · ⊇Ras ̸= {0}, T(xi) = aiyi for 1 ≤i ≤s, and T(xi) = 0
for s < i ≤n. Now T(xi) = xi ̸= 0 for all i (since X is a basis), so we must have s = n
and xi = aiyi for 1 ≤i ≤n. Define ai = 0 for n < i ≤m, so Ra1 ⊇· · · ⊇Ram ⊇{0}.
As in §16.13, the UMP for free R-modules provides an isomorphism Rm ∼= Rm sending
yi to ei (the standard basis vector) for 1 ≤i ≤m, and this isomorphism sends P to
P1 = Ra1 × Ra2 × · · · × Ram. So
M ∼= Rm/P ∼= Rm/P1 ∼= (R/Ra1) × (R/Ra2) · · · × (R/Ram),
where the last step uses the Fundamental Homomorphism Theorem for Modules (see
Exercise 58 of Chapter 17). To finish, we delete any initial factors R/Rai that are equal to
zero. Such a factor occurs iff Rai = R iff ai is a unit of R.
Continuing to imitate §16.13, we now derive a prime power version of the structure
theorem for modules.
Lemma on Splitting into Prime Powers. Suppose R is a PID and a ∈R has irreducible
factorization a = pe1
1 · · · pek
k where p1, . . . , pk are non-associate irreducible elements of R and
e1, . . . , ek ∈Z>0. There is an R-module isomorphism
R/Ra ∼= (R/Rpe1
1 ) × · · · × (R/Rpek
k ).
Proof. Define T : R →Qk
i=1(R/Rpei
i ) by T(x) = (x + Rpe1
1 , . . . , x + Rpek
k ) for x ∈R.
The map T is R-linear, and x ∈ker(T) iff x + Rpei
i
= 0 for all i iff pei
i |x for all i iff
a = lcm(pe1
1 , . . . , pek
k )|x iff x ∈Ra. So T induces an isomorphism T ′ : R/Ra →img(T). It
suffices to show T is onto, which is accomplished by showing that each generator (0, . . . , 1+
Rpei
i , . . . , 0) of Qk
i=1(R/Rpei
i ) is in the image of T. Note r = pei
i and s = Q
j̸=i pej
j have gcd
1R (by comparing unique prime factorizations), so there exist b, c ∈R with br + cs = 1R.
Consider T(cs). For any k ̸= i, the coset cs + Rpek
k is zero since pek
k divides s. On the other
hand, cs + Rpei
i
= (1 −br) + Rpei
i
= 1 + Rpei
i
since −br + Rpei
i
is the zero coset. Thus,
T(cs) = (0, . . . , 1 + Rpei
i , . . . , 0) as needed.
Applying this lemma to each nonzero bj in (18.5), we obtain the following structural
result.
Classification Theorem for Finitely Generated Modules over PIDs (Version 2).
For any finitely generated module M over a PID R, there exist k ∈Z≥0 and q1, . . . , qk ∈R
such that each qi is either zero or pei
i for some irreducible pi ∈R and ei ∈Z>0, and
M ∼= R/Rq1 × R/Rq2 × · · · × R/Rqk.
(18.6)
The ideals Rqj (as well as the generators qj of these ideals) are called the elementary divisors
of the R-module M. In §18.15, we prove that these ideals (counted with multiplicity) are
uniquely determined by the module M.
18.11
Minors and Matrix Invariants
Our next goal is to prove the uniqueness of the Smith normal form of a matrix A ∈Mm,n(R)
or a linear map between f.g. free R-modules. Before doing so, we need some preliminary
results on matrix invariants, which are quantities depending on A that do not change when
we multiply A on the left or right by an invertible matrix with entries in R.
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
527
Let A be an m × n matrix with entries in a PID R. We consider submatrices of A
formed by keeping only certain rows and columns of A. More precisely, given a subset
I = {i1 < i2 < · · · < ik} of [m] = {1, 2, . . . , m} and a subset J = {j1 < j2 < · · · < jℓ} of
[n] = {1, 2, . . . , n}, the submatrix of A with rows in I and columns in J is the matrix AI,J
with r, s-entry AI,J(r, s) = A(ir, js) for 1 ≤r ≤k and 1 ≤s ≤ℓ. For example, A[m],{2,3,5}
is the submatrix obtained by keeping all m rows of A and columns 2, 3, and 5 of A. Using
this notation, the Cauchy–Binet Formula (proved in §5.14) can be stated as follows: Given
U ∈Mk,m(R) and V ∈Mm,k(R) with k ≤m,
det(UV ) =
X
L⊆[m],|L|=k
det(U[k],L) det(VL,[k]).
Fix k with 1 ≤k ≤min(m, n). A k×k submatrix of A is a matrix AI,J with |I| = |J| = k.
For each choice of I and J of size k, let dI,J = det(AI,J) ∈R; dI,J is called the order k
minor of A indexed by I and J. Let gk(A) be a gcd in R of all the determinants dI,J with
|I| = |J| = k. We claim that for all invertible P ∈Mm(R) and all invertible Q ∈Mn(R),
gk(A) ∼gk(PAQ) in R. In terms of ideals, this says that Rgk(A) = Rgk(PAQ), so that the
ideal generated by any gcd of all the order k minors of A is a matrix invariant of A.
First, we use the Cauchy–Binet Formula to prove that gk(A)
∼
gk(PA). Fix
I = {i1 < · · · < ik} ⊆[m] and J = {j1 < · · · < jk} ⊆[n] of size k. Note that (PA)I,J =
PI,[m]A[m],J, since the r, s-entry of both sides is (PA)(ir, js) = Pm
t=1 P(ir, t)A(t, js). Since
k ≤m, we can apply the Cauchy–Binet Formula to the k × m matrix U = PI,[m] and the
m × k matrix V = A[m],J. We obtain
det((PA)I,J) = det(UV ) =
X
L⊆[m],
|L|=k
det(U[k],L) det(VL,[k]) =
X
L⊆[m],
|L|=k
det(PI,L) det(AL,J).
This formula shows that the minor det((PA)I,J) of the matrix PA is an R-linear
combination of various order k minors dL,J of the matrix A. Therefore, if e ∈R is a
common divisor of all the order k minors of A, then e divides each order k minor of PA.
In particular, e = gk(A) divides all minors det(AI,J), so gk(A) is a common divisor of all
minors det((PA)I,J), so gk(A) divides gk(PA) = gcd{det((PA)I,J) : |I| = |J| = k}. We
have now proved gk(A) divides gk(PA) for all A ∈Mm,n(R) and all invertible P ∈Mm(R).
Applying this result with A replaced by PA and P replaced by P −1, we see that gk(PA)
divides gk(P −1(PA)) = gk(A). Hence, gk(A) ∼gk(PA) as needed.
By a similar argument (Exercise 59), we can use the Cauchy–Binet Formula to show
gk(A) ∼gk(AQ) for any A ∈Mm,n(R) and any invertible Q ∈Mn(R). So gk(A) ∼gk(PAQ)
when P and Q are invertible over R. (You can also prove gk(PA) ∼gk(A) ∼gk(AQ) without
appealing to the Cauchy–Binet Formula, but instead invoking multilinearity properties of
determinants — see Exercise 60.)
18.12
Uniqueness of Smith Normal Form
Let R be a PID and A be an m×n matrix with entries in R. In §18.9, we proved that there
exist invertible matrices P ∈Mm(R) and Q ∈Mn(R) such that B = PAQ has the form
B = diag(a1, a2, . . . , as)m×n,
where s ≥0, a1, . . . , as ∈R, and Ra1 ⊇Ra2 ⊇· · · ⊇Ras ̸= {0}.
528
Advanced Linear Algebra
Our goal here is to prove the uniqueness of s and the ideals Ra1, . . . , Ras satisfying the
properties just stated. More specifically, we show that for any invertible P ′ ∈Mm(R) and
Q′ ∈Mn(R) such that
B′ = P ′AQ′ = diag(b1, b2, . . . , bt)m×n
for some t ≥0 and b1, . . . , bt ∈R with Rb1 ⊇Rb2 ⊇· · · ⊇Rbt ̸= {0}, we must have s = t
and Rai = Rbi for 1 ≤i ≤s (equivalently, ai and bi are associates in R for all i). The ideals
Rai (and their generators ai) are called the invariant factors of A, and s is called the rank
of A.
Define ai = 0 for s < i ≤min(m, n) and bj = 0 for t < j ≤min(m, n). Fix k with
1 ≤k ≤min(m, n). On one hand, we have seen that
gk(B) = gk(PAQ) ∼gk(A) ∼gk(P ′AQ′) = gk(B′).
(18.7)
On the other hand, we can use the special form of B to compute gk(B) directly from the
definition. If k > s, every k × k submatrix BI,J must have a row and column of zeroes, so
every order k minor det(BI,J) is zero. Then gk(B) = 0 since this is the gcd of a list of zeroes.
Now suppose k ≤s. You can check that every order k minor det(BI,J) is either zero or is
some product of the form ai1ai2 · · · aik ̸= 0, where 1 ≤i1 < i2 < · · · < ik ≤s. Furthermore,
one of these minors is det(B[k],[k]) = a1a2 · · · ak ̸= 0. Since ai|aj for all 1 ≤i ≤j ≤s, we
see that a1a2 · · · ak divides all the order k minors of B. So this ring element is a gcd of all
of these minors, and we can therefore take gk(B) = a1a2 · · · ak ̸= 0 for 1 ≤k ≤s. Letting
g0(B) = 1R, we see that ak = gk(B)/gk−1(B) for 1 ≤k ≤s. (More precisely, ak is the
unique x in the integral domain R solving gk−1(B)x = gk(B).) Replacing gk(B) or gk−1(B)
by associate ring elements replaces ak by an associate of ak in R.
Applying the same reasoning to B′, we see that gk(B′)
=
0 for all k
>
t,
gk(B′) = b1b2 · · · bk ̸= 0 for 0 ≤k ≤t, and bk = gk(B′)/gk−1(B′) for 1 ≤k ≤t. Returning
to (18.7), we now see that s = t = max{k : gk(A) ̸= 0R} and ak ∼gk(A)/gk−1(A) ∼bk for
1 ≤k ≤s. This completes the uniqueness proof.
We can deduce a similar uniqueness result for R-linear maps between f.g. free R-modules.
Given a PID R, f.g. free R-modules N and M, and an R-linear map T : N →M, there
exist unique s ≥0 and ideals Ra1 ⊇Ra2 ⊇· · · ⊇Ras ̸= {0} such that for some ordered
bases X = (x1, . . . , xn) for N and Y = (y1, . . . , ym) for M, T(xi) = aiyi for 1 ≤i ≤s
and T(xi) = 0M for s < i ≤n. Existence of one choice of s, a1, . . . , as, X, Y was shown
in §18.10. To prove uniqueness, suppose s′, Ra′
1, . . . , Ra′
s′, X ′, Y′ also satisfies all of the
conclusions above. Let A be the matrix of T relative to the bases X and Y. Then A is in
Smith normal form (18.3). Similarly, the matrix A′ of T relative to X ′ and Y′ is in Smith
normal form with the elements a′
j on its diagonal. Considering transition matrices between
the bases X and X ′ and the bases Y and Y′, you can check that A′ = PAQ for some
invertible P ∈Mm(R) and Q ∈Mn(R) (Exercise 40). Applying the uniqueness result for
matrices proved above, we obtain s = s′ and ai ∼a′
i (hence Rai = Ra′
i) for 1 ≤i ≤s.
18.13
Torsion Submodules
Our next task is to prove the uniqueness of the ideals Rbi and Rqj appearing in the
decompositions (18.5) and (18.6). We begin in this section by showing how to split off
the free part of a finitely generated module over a PID.
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
529
Given an integral domain R and any R-module M, the torsion submodule of M is
tor(M) = {x ∈M : for some r ∈R, r ̸= 0 and r · x = 0}.
To see that tor(M) really is a submodule, first note 1R ̸= 0R and 1R · 0M = 0M, so
0M ∈tor(M). Next, fix x, y ∈tor(M) and t ∈R. Choose nonzero r, s ∈R with rx = 0 =
sy. Then rs ̸= 0 since R is an integral domain, and (rs) · (x + y) = (rs) · x + (rs) · y =
s · (r · x) + r · (s · y) = s0 + r0 = 0, so x + y ∈tor(M). Also r · (t · x) = t · (r · x) = t · 0 = 0
since R is commutative, so t · x ∈tor(M).
Now suppose M and N are R-modules and f : M →N is an R-module isomorphism. You
can check that f[tor(M)] = tor(N), so that f restricts to an isomorphism from tor(M) to
tor(N). It follows from this and the Fundamental Homomorphism Theorem that f induces a
module isomorphism f ′ : M/ tor(M) →N/ tor(N) given by f ′(x+tor(M)) = f(x)+tor(N)
for x ∈M. To summarize: if M ∼= N, then tor(M) ∼= tor(N) and M/ tor(M) ∼= N/ tor(N).
For example, consider an R-module
P = R/Ra1 × R/Ra2 × · · · × R/Rak × Rd,
where a1, . . . , ak are nonzero elements of R and d ≥0. We claim
tor(P) = R/Ra1 × · · · × R/Rak × {0Rd}.
A typical element of the right side is z = (x1 + Ra1, . . . , xk + Rak, 0) where each xi ∈R.
Let r = a1a2 · · · ak ̸= 0; note rxi is divisible by ai, so r(xi + Rai) = rxi + Rai = 0 + Rai
for 1 ≤i ≤k, so that rz = (0, . . . , 0, 0) and hence z ∈tor(P). On the other hand, consider
y = (x1 + Ra1, . . . , xk + Rak, (r1, . . . , rd)) ∈P with some rj ̸= 0R. Multiplying y by any
nonzero s ∈R produces sy = (sx1 +Ra1, . . . , sxk +Rak, (sr1, . . . , srd)), where srj ̸= 0R. So
sy cannot be 0, hence y ̸∈tor(P). This proves the claim. From the claim, we readily deduce
that P/ tor(P) ∼= Rd.
The preceding remarks imply the following result that lets us break apart the free piece
and the torsion piece of a finitely generated module.
Splitting Theorem for Modules over a PID. Suppose R is a PID and M is a finitely
generated R-module such that
R/Ra1 × · · · × R/Rak × Rd ∼= M ∼= R/Rb1 × · · · × R/Rbℓ× Re,
where every Rai and Rbj is a nonzero ideal of R and d, e ≥0. Then
R/Ra1 × · · · × R/Rak ∼= tor(M) ∼= R/Rb1 × · · · × R/Rbℓand Rd ∼= M/ tor(M) ∼= Re,
and hence d = e.
To see why d = e follows, note M/ tor(M) is a free R-module having a basis of size d
(since this module is isomorphic to Rd) and a basis of size e (since this module is isomorphic
to Re). Since the PID R is commutative, d = e follows from the theorem proved in §17.16.
We call d the Betti number of M.
18.14
Uniqueness of Invariant Factors
We are now ready to prove the uniqueness of the sequence of ideals (the invariant factors)
appearing in the decomposition (18.5). Using the Splitting Theorem to separate out all
factors of the form R/R0 ∼= R, it suffices to prove the following statement.
530
Advanced Linear Algebra
Theorem on Uniqueness of Invariant Factors. Let R be a PID. Assume a1, . . . , ak,
b1, . . . , bℓ∈R satisfy
R ̸= Rak ⊇· · · ⊇Ra1 ̸= {0},
R ̸= Rbℓ⊇· · · ⊇Rb1 ̸= {0},
and R/Rak × · · · × R/Ra1 ∼= R/Rbℓ× · · · × R/Rb1.
(18.8)
Then k = ℓand Rai = Rbi for 1 ≤i ≤k.
We have reversed the indexing order of the ai and bj to simplify notation in the induction
proof below.
Proof. All quotient modules appearing in the theorem statement are nonzero (as Rai ̸=
R ̸= Rbj), so k = 0 iff ℓ= 0. Assume k, ℓ> 0. We first prove that Ra1 = Rb1 using the
following ideas. Given any R-module M, the annihilator of M is
annR(M) = {r ∈R : for all x ∈M, rx = 0}.
You can check that for any commutative ring R, annR(M) is an ideal of R. Also, M ∼= M ′
implies annR(M) = annR(M ′), so that isomorphic R-modules have equal annihilators.
Given M = R/Rak × · · · × R/Ra1 as above, we show that annR(M) = Ra1. A typical
element of M is a k-tuple of cosets x = (xk+Rak, . . . , x1+Ra1) with all xi ∈R. Multiplying
x by ra1 ∈Ra1 (where r ∈R) produces (ra1)x = (ra1xk + Rak, . . . , ra1x1 + Ra1).
Every ra1xi is in Ra1, which is contained in all the other ideals Rai by assumption.
So ra1xi + Rai = 0 + Rai for all i between 1 and k, proving that (ra1)x = 0. This
means that Ra1 ⊆annR(M). To prove the reverse inclusion, fix s ∈annR(M). Then
s·(0, . . . , 0, 1+Ra1) = 0M, so that s+Ra1 = 0+Ra1, so that s ∈Ra1. The same reasoning
shows that annR(R/Rbℓ×· · ·×R/Rb1) is Rb1. By the result in the last paragraph, Ra1 = Rb1
follows.
Fix i with 1 ≤i −1 ≤min(k, ℓ), and make the induction hypothesis that Ra1 = Rb1,
Ra2 = Rb2, . . ., Rai−1 = Rbi−1. We now prove that k ≥i iff ℓ≥i, in which case Rai = Rbi.
Assume k ≥i; we show ℓ≥i and bi|ai in R. The proof requires facts about the length of a
module proved in §17.18. Let P = R/Rai−1 × · · · × R/Ra1, which appears in both of the
product modules (18.8) by the induction hypothesis. You can check that len(R/Ra) = len(a)
for any nonzero a in a PID R (Exercise 63), so that len(P) = len(a1)+· · ·+len(ai−1) < ∞.
If we had ℓ= i −1, then (18.8) says
[R/Rak × · · · × R/Rai] × P ∼= P,
where the term in brackets is a nonzero module Q. On one hand, the isomorphic modules
Q × P and P have the same finite length. On the other hand, len(Q × P) > len(P) since
Q ̸= {0}. This contradiction shows ℓ≥i.
Note that for any R-module N and any c ∈R (where R is a commutative ring),
cN = {c · n : n ∈N} is a submodule of N; and if N ∼= N ′ are isomorphic R-modules,
then cN ∼= cN ′. Furthermore, for a direct product N = N1 × N2 × · · · × Nk, we have
cN = (cN1) × (cN2) × · · · × (cNk). Taking c = ai and applying these remarks to (18.8), we
get an isomorphism
[ai(R/Rak) × · · · × ai(R/Rai)] × aiP ∼= [ai(R/Rbℓ) × · · · × ai(R/Rbi)] × aiP.
(18.9)
We know ai belongs to all the ideals Rai ⊆Rai+1 ⊆· · · ⊆Rak. It follows that the product
in brackets on the left side of (18.9) is the zero module. Comparing lengths of both sides
(noting that len(aiP) ≤len(P) < ∞), we conclude that the product in brackets on the right
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
531
side must also be the zero module. In particular, ai(R/Rbi) = {0}, so ai·(1+Rbi) = 0+Rbi.
This means ai ∈Rbi, so bi|ai in R.
By interchanging the roles of the two product modules in (18.8), we prove similarly that
ℓ≥i implies k ≥i and ai|bi in R. So k ≥i iff ℓ≥i, in which case bi|ai and ai|bi, hence
ai ∼bi, hence Rai = Rbi. This completes the induction step. Taking i = min(k, ℓ) + 1, we
see that k = ℓand Raj = Rbj for 1 ≤j ≤k.
18.15
Uniqueness of Elementary Divisors
Next, we prove the uniqueness (up to reordering) of the elementary divisors appearing in
the decomposition (18.6). By invoking the Splitting Theorem from §18.13 to remove all
factors of the form R/R0 ∼= R, it suffices to prove the following statement.
Theorem on Uniqueness of Elementary Divisors. Let R be a PID. Assume q1, . . . , qk,
r1, . . . , rℓare positive powers of irreducible elements in R such that
R/Rq1 × · · · × R/Rqk ∼= R/Rr1 × · · · × R/Rrℓ.
(18.10)
Then k = ℓand the list of ideals (Rq1, . . . , Rqk) is a rearrangement of the list (Rr1, . . . , Rrℓ).
To simplify the proof, let M = [Rq1, . . . , Rqk] be the set of all rearrangements of the list
(Rq1, . . . , Rqk); we call M a multiset of ideals. This word indicates that the order in which
we list the ideals is unimportant, but the number of times each ideal occurs is significant.
Let X be the set of all such multisets arising from lists of finitely many ideals Rqj with
each qj = pej
j
for some irreducible pj ∈R and ej ∈Z>0. Let Y be the set of all finite lists
of ideals (Ra1, . . . , Ram) with R ̸= Ra1 ⊇Ra2 ⊇· · · ⊇Ram ̸= {0}. The idea of the proof
is to define bijections f : X →Y and g : Y →X that let us use the known uniqueness of
the invariant factors of a module. We saw this idea in the simpler setting of commutative
groups in §16.19.
Let Z be a fixed set of irreducible elements in R such that no two elements of Z are
associates, but every irreducible element in R is associate to some element of Z. The map
g acts on L = (Ra1, . . . , Ram) ∈Y as follows. We know each ai factors in R into a product
u Qni
j=1 peij
ij where u ∈R∗, pi1, pi2, . . . , pini are distinct irreducible elements in Z and eij ∈
Z>0. Define g(L) to be the multiset in X consisting of all ideals Rpeij
ij
for 1 ≤i ≤m and
1 ≤j ≤ni. Using the Lemma on Splitting into Prime Powers, note that Qm
i=1 R/Rai ∼=
Qm
i=1
Qni
j=1 R/Rpeij
ij , no matter what order we list the terms in the direct product on the
right side.
The map f acts on M = [Rq1, . . . , Rqk] ∈X as follows. Let p1, . . . , pn ∈Z be the
distinct irreducible elements such that each qi ∼pej
j
for some j between 1 and n. Place
the elements of M in a matrix such that row j contains all the ideals Rqi with qi ∼pej
j ,
listed with multiplicities so that the exponents ej weakly increase reading from left to right.
Suppose the longest row in the matrix has length m. Pad all shorter rows with copies of
1R on the left so that all rows have length m. Define f(M) = (Ra1, Ra2, . . . , Ram), where
ak is the product of all qi appearing in column k. You can check that f(M) ∈Y , since
the construction ensures that ai|ai+1 for all i < m. Since splitting the ak back into prime
powers recovers the qi in some order, we see that g(f(M)) = M for all M ∈X, and
Qk
i=1 R/Rqi ∼= Qm
j=1 R/Raj. You can also check that f(g(L)) = L for all L ∈Y , though we
do not need this fact below.
532
Advanced Linear Algebra
To begin the uniqueness proof, assume we have an isomorphism as in (18.10). Let M1 =
[Rq1, . . . , Rqk], M2 = [Rr1, . . . , Rrℓ], L1 = f(M1) = [Ra1, . . . , Ram], and L2 = f(M2) =
[Rb1, . . . , Rbn]. We have seen that
m
Y
j=1
R/Raj ∼=
k
Y
i=1
R/Rqi ∼=
ℓ
Y
i=1
R/Rri ∼=
n
Y
j=1
R/Rbj.
Since L1, L2 ∈Y , the uniqueness result for invariant factors shows that L1 = L2. Then
M1 = g(f(M1)) = g(L1) = g(L2) = g(f(M2)) = M2, which proves the required uniqueness
result for elementary divisors.
18.16
F[x]-Module Defined by a Linear Operator
In the rest of this chapter, we apply the structure theorems for finitely generated modules
over PIDs to derive results on canonical forms of matrices and linear operators on a vector
space. Throughout, we let F be a field and V be an n-dimensional vector space over F. We
also fix an F-linear map T : V →V . We will define a collection of matrices in Mn(F) called
rational canonical forms and show that each T is represented (relative to an appropriate
ordered basis of V ) by exactly one of these matrices.
To obtain this result from the preceding theory, we use T to turn the vector space (F-
module) V into an F[x]-module. The addition in the F[x]-module V is the given addition
in the vector space V . For v ∈V and p = Pd
i=0 pixi ∈F[x], define scalar multiplication by
p · v = Pd
i=0 piT i(v), where T 0 = idV and T i denotes the composition of i copies of T.
Let us check the F[x]-module axioms. The five additive axioms are already known to
hold. For p ∈F[x] and v ∈V as above, p · v is in V , since T and each T i map V to V and
V is closed under addition and multiplication by scalars in F. Next, 1F [x] · v = 1T 0(v) = v.
Given q = P
i≥0 qixi ∈F[x], note qp = P
i≥0
Pi
k=0 qkpi−k

xi. Using linearity of T and
its powers, we compute:
(qp) · v
=
X
i≥0
 
i
X
k=0
qkpi−k
!
T i(v) =
X
i≥0
i
X
k=0
qkT k(pi−kT i−k(v)) =
X
k≥0
X
j≥0
qkT k(pjT j(v))
=
X
k≥0
qkT k

X
j≥0
pjT j(v)

= q ·

X
j≥0
pjT j(v)

= q · (p · v).
Next,
(p + q) · v =
X
i≥0
(pi + qi)T i(v) =
X
i≥0
piT i(v) +
X
i≥0
qiT i(v) = p · v + q · v.
Finally, given w ∈V ,
p·(v+w) =
X
i≥0
piT i(v+w) =
X
i≥0
pi[T i(v)+T i(w)] =
X
i≥0
piT i(v)+
X
i≥0
piT i(w) = p·v+p·w.
Let us spell out some definitions from module theory in the setting of the particular
F[x]-module V determined by the linear operator T. First, what is an F[x]-submodule of
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
533
V ? This is an additive subgroup W of V such that p · w ∈W for all p ∈F[x] and all
w ∈W. Taking p to be a constant polynomial, we see that a submodule W must be closed
under multiplication by scalars in F. Taking p = x, we see that a submodule W must
satisfy x · w = T(w) ∈W for all w ∈W. Conversely, suppose W is a subspace such that
T[W] ⊆W; a subspace satisfying this condition is called a T-invariant subspace of V . By
induction on i, we see that T i(w) ∈W for all w ∈W and all i ≥0. Since W is a subspace,
we then see that p · w = P
i≥0 piT i(w) ∈W for all w ∈W and all p ∈F[x]. So, submodules
of the F[x]-module V are the same thing as T-invariant subspaces of V .
Second, is V finitely generated as an F[x]-module? We know V is finitely generated as
an F-module, since the vector space V has an n-element basis B = (v1, . . . , vn). We claim
{v1, . . . , vn} generates the F[x]-module V . Given v ∈V , write v = c1v1 + · · · + cnvn for
some c1, . . . , cn ∈F. Each ci is also a constant polynomial in F[x], so v has been expressed
as an F[x]-linear combination of the vi. (V might also be generated, as an F[x]-module, by
a proper subset of the vi, since we could also act on each vi by non-constant polynomials.)
Third, what does a cyclic F[x]-submodule of V look like? Recall this is a submodule of
the form W = F[x]z for some fixed z ∈W; z is called a generator of the submodule. Such a
submodule is also called a T-cyclic subspace of V . Define a map g : F[x] →W by g(p) = p·z
for all p ∈F[x]. You can check that g is a surjective F[x]-module homomorphism, which
induces an F[x]-module isomorphism g′ : F[x]/ ker(g) →W. The kernel of g is a submodule
(ideal) of F[x], called the T-annihilator of z. Since F[x] is a PID, ker(g) = F[x]h for some
h ∈F[x]. Now h cannot be zero, since otherwise W would be isomorphic to F[x] as an F[x]-
module, hence also isomorphic to F[x] as an F-module. But F[x] is an infinite-dimensional
F-vector space and W is finite-dimensional. So h ̸= 0, and we can take h to be the unique
monic generator of ker(g). Write h = h0 + h1x + · · · + hdxd, where d ≥0, each hi ∈F, and
hd = 1.
So far, we know that F[x]/F[x]h and W = F[x]z are isomorphic (both as F[x]-modules
and F-modules) via the map g′ sending p + F[x]h to p · z for all p ∈F[x]. Now, using
polynomial division with remainder (cf. §3.20), you can check that
(1 + F[x]h, x + F[x]h, x2 + F[x]h, . . . , xd−1 + F[x]h)
is an ordered basis for the F-vector space F[x]/F[x]h. Applying the F-isomorphism g′ to
this basis, we conclude that
Bz = (1 · z, x · z, x2 · z, . . . , xd−1 · z) = (z, T(z), T 2(z), . . . , T d−1(z))
is an ordered F-basis for the subspace W of V .
Since W is T-invariant, we know T restricts to a linear map T|W : W →W. What is the
matrix of T|W relative to the ordered basis Bz? Note T|W (z) = T(z), which has coordinates
(0, 1, 0, . . . , 0) relative to Bz. Note T|W (T(z)) = T(T(z)) = T 2(z), which has coordinates
(0, 0, 1, 0, . . . , 0) relative to the basis Bz. Similarly, T|W (T j(z)) = T j+1(z) for 0 ≤j < d−1.
But, when we apply T|W to the final element in Bz, T|W (T d−1(z)) = T d(z) is not in the
basis. As h ∈ker(g), we know 0 = h·z = Pd−1
i=0 hiT i(z)+T d(z), so the coordinates of T d(z)
relative to Bz must be (−h0, −h1, . . . , −hd−1). In conclusion, the matrix we want is
[T|W ]Bz =


0
0
0
. . .
0
−h0
1
0
0
. . .
0
−h1
0
1
0
. . .
0
−h2
0
0
1
. . .
0
−h3
. . .
. . .
. . .
0
0
0
. . .
1
−hd−1


d×d
.
(18.11)
534
Advanced Linear Algebra
This matrix is called the companion matrix of the monic polynomial h and is written
Ch. Conversely, let W be any T-invariant subspace such that for some z ∈W, Bz =
(z, T(z), . . . , T d−1(z)) is an F-basis of W and [T|W ]Bz = Ch. You can check that W = F[x]z
is a T-cyclic subspace isomorphic to F[x]/F[x]h (Exercise 74).
18.17
Rational Canonical Form of a Linear Map
As in the last section, let V be an n-dimensional vector space over a field F and T : V →V
be a fixed linear map. Make V into an F[x]-module via T, as described above. We know
F[x] is a PID and V is a finitely generated F[x]-module, so version 1 of the Classification
Theorem for Modules over a PID (see (18.5)) gives us an F[x]-module isomorphism
ϕ : V →F[x]/F[x]h1 × F[x]/F[x]h2 × · · · × F[x]/F[x]hk
(18.12)
for uniquely determined ideals
F[x] ̸= F[x]h1 ⊇F[x]h2 ⊇· · · ⊇F[x]hk ⊇{0}
generated by h1, . . . , hk ∈F[x]. Since V is finite-dimensional as an F-module, none of the
ideals F[x]hj can be zero. So we can assume the generators h1, . . . , hk of these ideals are
monic with respective degrees d1, . . . , dk > 0. These polynomials satisfy hi|hi+1 in F[x] for
1 ≤i < k.
Call the product module on the right side of (18.12) V ′. For 1 ≤i ≤k, let W ′
i be
the submodule {0} × · · · × F[x]/F[x]hi × · · · × {0} of V ′. Note W ′
i is a cyclic F[x]-module
generated by z′
i = (0, . . . , 1 + F[x]hi, . . . , 0). Letting Wi = ϕ−1[W ′
i] and zi = ϕ−1(z′
i) for
each i, we obtain cyclic submodules Wi = F[x]zi of V such that Wi ∼= W ′
i ∼= F[x]/F[x]hi.
You can check that hi is the monic polynomial of least degree sending the coset 1 + F[x]hi
to 0 + F[x]hi, and hence F[x]hi is the T-annihilator of zi for all i.
You can also check that we get an ordered basis for the F-vector space V ′ by
concatenating ordered bases for the submodules W ′
i corresponding to each factor in the
direct product. Applying ϕ−1, we get an ordered basis B for the F-vector space V by
concatenating ordered bases for W1, . . . , Wk. Using the bases Bz1, . . . , Bzk constructed
in §18.16, we get an ordered basis
B = (z1, T(z1), . . . , T d1−1(z1), z2, T(z2), . . . , T d2−1(z2), . . . , zk, T(zk), . . . , T dk−1(zk))
for V . The matrix of T relative to the basis B is the block-diagonal matrix
[T]B =


Ch1
0
. . .
0
0
Ch2
. . .
0
...
...
...
0
0
. . .
Chk

,
(18.13)
where Chi is the companion matrix of hi.
In general, given any square matrices A1, . . . , Ak, we write blk-diag(A1, . . . , Ak) for the
block-diagonal matrix with diagonal blocks A1, . . . , Ak. A rational canonical form is a matrix
A ∈Mn(F) of the form A = blk-diag(Ch1, . . . , Chk), where: k > 0, h1, . . . , hk ∈F[x] are
monic, non-constant polynomials, and hi|hi+1 in F[x] for 1 ≤i < k. The matrix in (18.13)
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
535
is called the rational canonical form of T, and the polynomials hi are called the invariant
factors of T. We have now proved the existence part of the following theorem.
Theorem on Rational Canonical Forms. Let V be a finite-dimensional F-vector space.
Every linear map T : V →V is represented by exactly one matrix in rational canonical
form (18.13).
To prove the uniqueness of the rational canonical form, suppose there were another
ordered basis B∗of V such that [T]B∗= blk-diag(Cg1, . . . , Cgℓ), where g1, . . . , gℓ∈F[x] are
monic polynomials in F[x] with gi|gi+1 for 1 ≤i < ℓand deg(gi) = d∗
i > 0. Let W ∗
1 be the
subspace of V generated by the first d∗
1 vectors in B∗, W ∗
2 the subspace generated by the
next d∗
2 vectors in B∗, and so on. The form of the matrix [T]B∗shows that each W ∗
i is a
cyclic F[x]-submodule of V annihilated by gi, namely W ∗
i = F[x]z∗
i ∼= F[x]/F[x]gi, where
z∗
i is the first basis vector in the generating list for W ∗
i . You can check (Exercise 74) that
V ∼= W ∗
1 × · · · × W ∗
ℓ∼= F[x]/F[x]g1 × · · · × F[x]/F[x]gℓ
as F-modules and F[x]-modules. The uniqueness result proved in §18.14 now gives ℓ= k
and F[x]hi = F[x]gi for all i, hence hi = gi for all i since hi and gi are monic.
18.18
Jordan Canonical Form of a Linear Map
Assume T : V →V is a fixed linear map on an n-dimensional F-vector space V . We know
(using (18.6)) that there is an F[x]-module isomorphism
ψ : V ∼= F[x]/F[x]q1 × · · · × F[x]/F[x]qs
where each qi is pei
i for some monic irreducible pi ∈F[x] and some ei > 0. We saw in §18.15
that s and the multiset of qi are uniquely determined by these conditions. The qi are called
elementary divisors of the linear map T.
By exactly the same argument used to derive (18.13) from (18.12), we see that V is
the direct sum of T-cyclic subspaces W1, . . . , Ws with Wi = F[x]yi ∼= F[x]/F[x]qi for some
yi ∈V , and V has an ordered basis B such that [T]B = blk-diag(Cq1, . . . , Cqs). To obtain
further structural results, we now impose the additional hypothesis that every pi has degree
1, say pi = x −ci where ci ∈F. For example, this hypothesis automatically holds when
F = C, or when F is any algebraically closed field (which means all irreducible polynomials
in F[x] have degree 1).
Our goal is to change the basis of each T-cyclic subspace Wi = F[x]yi to obtain an
even nicer matrix than the companion matrix Cqi = C(x−ci)ei. Fix c ∈F and a positive
integer e, and consider any T-cyclic subspace W = F[x]y of V with ordered basis By =
(y, T(y), . . . , T e−1(y)), such that [T|W ]By = C(x−c)e. For 0 ≤k ≤e, define yk = (x −c)k · y,
which is a vector in W. Since (x −c)k = (x −c)(x −c)k−1, we have y0 = y, ye = 0, and for
1 ≤k ≤e, yk = (x −c) · yk−1 = T(yk−1) −cyk−1, hence T(yk−1) = yk + cyk−1. Using these
facts, you can show by induction that the list (y0, . . . , yk) spans the same F-subspace as the
list (y, T(y), . . . , T k(y)) for 0 ≤k < e, and therefore B = (ye−1, . . . , y2, y1, y0) is another
ordered F-basis for W.
Let us compute the matrix [T|W ]B. First, T(ye−1) = ye + cye−1 = cye−1, which has
coordinates (c, 0, . . . , 0) relative to B. Next, for 1 < j ≤e, T(ye−j) = 1ye−j+1 + cye−j,
so that column j of the matrix has a 1 in row j −1, a c in row j, and zeroes elsewhere. In
536
Advanced Linear Algebra
other words, [T|W ]B is the Jordan block defined by
J(c; e) =


c
1
0
· · ·
0
0
c
1
· · ·
0
0
0
c
· · ·
0
0
0
0
· · ·
1
0
0
0
· · ·
c


.
(In the case e = 1, this is a 1 × 1 matrix with sole entry c.)
By concatenating bases of the form (ye−1, . . . , y0) for all the T-cyclic subspaces Wi, we
obtain an ordered basis for V such that the matrix of T relative to this basis is
J =


J(c1; e1)
0
. . .
0
0
J(c2; e2)
. . .
0
...
...
...
0
0
. . .
J(cs; es)

.
(18.14)
Any block-diagonal matrix in Mn(F) with Jordan blocks on the diagonal is called a Jordan
canonical form; the particular matrix J is called a Jordan canonical form of the linear map
T. The Jordan canonical form for T may not be unique, since we can obtain other Jordan
canonical forms by reordering the multiset of elementary divisors of T, which leads to a
permutation of the Jordan blocks of J. But, arguing as in §18.17, we can prove that these
are the only possible Jordan canonical forms for T. The only new detail is showing that for
any subspace W of V such that [T|W ]B = J(c; e) for some basis B of W, W is a T-cyclic
subspace with W = F[x]y ∼= F[x]/F[x](x−c)e for some y ∈W (Exercise 78). We summarize
these results as follows.
Theorem on Jordan Canonical Forms. Let V be a finite-dimensional F-vector space.
Assume T : V →V is a linear map such that every elementary divisor of T has the form
(x −c)e for some c ∈F and e ∈Z>0 (which always holds for F = C). Then there is an
ordered basis B for V such that the matrix J = [T]B is a Jordan canonical form. All Jordan
canonical forms for T are obtained from J by reordering the Jordan blocks.
18.19
Canonical Forms of Matrices
Let A ∈Mn(F) be an n × n matrix with entries in a field F. We use A to define a linear
map T : F n →F n such that T(v) = Av for all v ∈F n. Then F n becomes an F[x]-module
via T, where the action is given by p·v = Pd
i=0 piAiv for p = Pd
i=0 pixi ∈F[x] and v ∈F n.
The matrix of T relative to the standard ordered basis of F n is A. We know (Chapter 6)
that the matrix C of T relative to any other ordered basis of F n has the form C = S−1AS
for some invertible S ∈Mn(F); in other words, C is similar to A. Conversely, every matrix
similar to A is the matrix of T relative to some ordered basis of F n. We can rephrase our
previous theorems on canonical forms of linear maps as statements about canonical forms
of matrices.
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
537
Theorem on Canonical Forms of Matrices. Let F be a field.
(a)
Every
matrix
A
∈
Mn(F)
is
similar
to
exactly
one
matrix
of
the
form
blk-diag(Ch1, . . . , Chk), where h1, . . . , hk ∈F[x] are monic non-constant polynomials with
hi|hi+1 for 1 ≤i < k. This matrix is called the rational canonical form of A, and the
polynomials h1, . . . , hk are the invariant factors of A.
(b) If F is algebraically closed, then every matrix A ∈Mn(F) is similar to a matrix of the
form blk-diag(J(c1; e1), . . . , J(cs; es)), and the only matrices of this form that are similar
to A are those obtained by reordering the Jordan blocks. These matrices are called Jordan
canonical forms of A.
(c) Two matrices in Mn(F) are similar iff they have the same rational canonical form. For
algebraically closed F, two matrices are similar iff they have the same Jordan canonical
forms.
Recall that the minimal polynomial of A ∈Mn(F) is the unique monic polynomial
mA ∈F[x] of minimum degree such that mA(A) = 0, and the characteristic polynomial of
A is χA = det(xIn −A) ∈F[x]. You can check that similar matrices have the same minimal
polynomial and the same characteristic polynomial. For monic h ∈F[x], it follows directly
from these definitions that mCh = χCh = h (Exercise 79). For any block-diagonal matrix
B = blk-diag(B1, . . . , Bk), we have mB = lcm(mB1, . . . , mBk) and χB = Qk
i=1 χBi. In the
special case where B is the rational canonical form of A, we see that χA = χB = Qk
i=1 hi
and mA = mB = hk. These observations reprove the Cayley–Hamilton Theorem, which
states that mA|χA in F[x], or equivalently χA(A) = 0.
Theorem on Invariant Factors of a Matrix. Let F be a field. Given A ∈Mn(F), the
invariant factors of A are precisely the invariant factors of positive degree in the Smith
normal form of the matrix xIn −A ∈Mn(F[x]).
We can use this theorem and the matrix reduction algorithm in §18.9 (applied to the
matrix xIn −A) to calculate the rational canonical form of a given matrix A. Alternatively,
we can invoke the results in §18.12 to give specific formulas for each invariant factor
(involving quotients of gcds of appropriate minors of xIn −A) and to show once again
that the invariant factors of A are uniquely determined by A.
Proof. To prove the theorem, first consider the case where A = Ch is the companion
matrix of a monic polynomial h ∈F[x] of degree n. Using elementary row and column
operations, the matrix xIn −Ch ∈Mn(F[x]) can be reduced to the diagonal matrix
diag(1, . . . , 1, h)n×n (Exercise 79). This matrix is a Smith normal form of xIn −Ch. For a
general matrix A, we know there is an invertible matrix S ∈Mn(F) such that S−1AS
is the unique rational canonical form of A. Suppose S−1AS = blk-diag(Ch1, . . . , Chk),
where h1, . . . , hk are the invariant factors of A. Write deg(hi) = di > 0 for 1 ≤i ≤k.
It follows that S−1(xIn −A)S = xIn −S−1AS = blk-diag(xId1 −Ch1, . . . , xIdk −Chk). By
the observation above, for each block xIdi −Chi there are invertible Pi, Qi ∈Mn(F[x])
such that Pi(xIdi −Chi)Qi = diag(1, . . . , 1, hi)di×di. Letting P = blk-diag(P1, . . . , Pk)
and Q = blk-diag(Q1, . . . , Qk), PS−1(xIn −A)SQ is a diagonal matrix where the
diagonal entries (in some order) are h1, . . . , hk, and n −k copies of 1. By performing
row and column interchanges, we can arrange that the diagonal consists of all the
1s followed by h1, . . . , hk. So we have found invertible P ′, Q′ ∈Mn(F[x]) such that
P ′(xIn −A)Q′ = diag(1, . . . , 1, h1, . . . , hk)n×n. This matrix is a Smith normal form of
xIn −A. We see that the non-constant invariant factors in this normal form are exactly
h1, . . . , hk, which are the invariant factors of the matrix A.
538
Advanced Linear Algebra
18.20
Summary
Let R be a commutative ring.
1.
Divisibility Definitions. Given x, y in R:
• x divides y (written x|y) iff y = rx for some r ∈R iff Ry ⊆Rx;
• x is an associate of y (written x ∼y) iff x|y and y|x iff Rx = Ry;
• x is a unit of R (written x ∈R∗) iff xy = 1R for some y ∈R iff x ∼1;
• x is a zero divisor of R iff x ̸= 0 and there is y ̸= 0 in R with xy = 0R.
• When R is an integral domain, x ∼y iff there exists u ∈R∗with y = ux.
• Given a1, . . . , ak, d, e ∈R, d is a gcd of a1, . . . , ak iff d divides all ai, and every
common divisor of the ai divides d; e is an lcm of a1, . . . , ak iff all ai divide e,
and e divides every common multiple of the ai.
2.
Types of Ideals. Given an ideal I in R:
• I is principal iff there exists c ∈R with I = Rc = {rc : r ∈R};
• I is prime iff I ̸= R and for all x, y ∈R, xy ∈I implies x ∈I or y ∈I;
• I is maximal iff I ̸= R and for all ideals J with I ⊆J ⊆R, J = I or J = R.
All maximal ideals are prime, but not all prime ideals are maximal in general.
3.
Prime and Irreducible Elements. Given a nonzero non-unit x in R:
• x is prime in R iff for all y, z ∈R, x|(yz) implies x|y or x|z;
• x is irreducible in R iff for all w ∈R, w|x implies w ∼x or w is a unit.
• x is prime iff Rx is a nonzero prime ideal of R;
• x is irreducible iff Rx is a nonzero ideal that is maximal in the poset of proper,
principal ideals of R.
• In an integral domain, every prime element is irreducible.
• In PIDs and UFDs (but not for general R), every irreducible element is prime.
4.
Types of Rings.
• R is an integral domain iff 0R ̸= 1R and R has no (nonzero) zero divisors.
• R is a principal ideal domain (PID) iff R is an integral domain in which every
ideal I has the form I = Rc for some c ∈R.
• R is a unique factorization domain (UFD) iff R is an integral domain in which
every r ̸= 0 in R factors as r = up1 · · · pk for some u ∈R∗and irreducible pi ∈R;
and whenever up1 · · · pk = vq1 · · · qt with u, v ∈R∗and all pi, qj irreducible in R,
k = t and pi ∼qi for 1 ≤i ≤k after reordering the qj.
5.
Theorems about PIDs and UFDs.
• Z and F[x] (for any field F) are PIDs.
• For all x in a PID or UFD R, x is prime in R iff x is irreducible in R.
• Every PID is a UFD.
• Given a1, . . . , ak in a PID R, there exist gcds and lcms of a1, . . . , ak, and each
gcd is an R-linear combination of the aj. Specifically, d is a gcd of a1, . . . , ak iff
Rd = Ra1 + · · · + Rak, and e is an lcm of a1, . . . , ak iff Re = Ra1 ∩· · · ∩Rak.
• Every ascending chain of ideals in a PID must stabilize.
• Every nonzero prime ideal in a PID is maximal.
• In a UFD R, gcds and lcms of a1, . . . , ak exist, but the gcd may not be an
R-linear combination of the ai (if it is in all cases, then R must be a PID).
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
539
6.
Module Lemmas. Assume M and N are isomorphic R-modules.
When R is a general commutative ring:
• for any c ∈R, cM is a submodule of M with cM ∼= cN and M/cM ∼= N/cN;
• every basis of an f.g. free R-module has the same size;
• annR(M) = {r ∈R : ∀x ∈M, rx = 0} = annR(N) is an ideal of R.
When R is an integral domain:
• tor(M) = {x ∈M : ∃r ∈R, r ̸= 0 and rx = 0} is a submodule of M;
• tor(M) ∼= tor(N) and M/ tor(M) ∼= M/ tor(N).
When R is a PID:
• every submodule of an f.g. free R-module is also f.g. free;
• given a = pe1
1 · · · pek
k ∈R where the pi are non-associate irreducible elements of
R, R/Ra ∼= Qk
i=1 R/Rpei
i .
7.
Smith Normal Form of a Matrix. For a PID R and any matrix A ∈Mm,n(R),
there exist unique ideals Ra1 ⊇Ra2 ⊇· · · ⊇Ras ̸= {0} (where s ≥0)
such that for some invertible matrices P
∈
Mm(R) and Q
∈
Mn(R),
PAQ = diag(a1, . . . , as)m×n. Note ai|ai+1 for 1 ≤i < s. We say s is the rank
of A, the ideals Rai are the invariant factors of A, and the matrix PAQ is a
Smith normal form of A. We can pass from A to PAQ using a sequence of type 4
row and column operations. For 1 ≤k ≤s, a1a2 · · · ak is a gcd of all the order k
minors of A (which are determinants of k × k submatrices obtained by keeping
any k rows and any k columns of A), and s is the largest k for which some order
k minor of A is nonzero.
8.
Structure Theorem for Linear Maps between Free Modules. Let N and M be
f.g. free R-modules where R is a PID. For every R-linear map T : N →M,
there exist unique ideals Ra1 ⊇Ra2 ⊇· · · ⊇Ras ̸= {0} (where s ≥0) such
that for some ordered basis X = (x1, . . . , xn) of N and some ordered basis Y =
(y1, . . . , ym) of M, T(xi) = aiyi for 1 ≤i ≤s and T(xi) = 0M for s < i ≤n.
9.
Structure of Finitely Generated Modules over PIDs. For any finitely gener-
ated module M over a PID R, there exist unique k, d ≥0 and a unique
sequence of ideals R
̸=
Ra1
⊇
Ra2
⊇
· · ·
⊇
Rak
̸=
{0} such that
M ∼= R/Ra1 × R/Ra2 × · · · × R/Rak × Rd. There also exist unique m, d ≥0 and
a unique multiset of ideals [Rq1, . . . , Rqm], with every qi a power of an irreducible
in R, such that M ∼= R/Rq1 × · · · × R/Rqm × Rd. We call d the Betti number of
M, Ra1, . . . , Rak the invariant factors of M, and Rq1, . . . , Rqm the elementary
divisors of M.
10.
Rational Canonical Forms. Let F be any field. Given h = xd + hd−1xd−1 +
· · · + h0 ∈F[x], the companion matrix Ch has Ch(i + 1, i) = 1 for 1 ≤i < d,
Ch(i, d) = −hi−1 for 1 ≤i ≤d, and all other entries zero. A rational canonical
form is a matrix of the form blk-diag(Ch1, . . . , Chk), where h1, . . . , hk ∈F[x]
are monic and non-constant and hi|hi+1 for 1 ≤i < k. For every n-dimensional
vector space V and every linear map T : V →V , there exists a unique rational
canonical form B ∈Mn(F) such that [T]X = B for some ordered basis X of
V . For any matrix A ∈Mn(F), there exists a unique rational canonical form
B ∈Mn(F) similar to A (i.e., B = P −1AP for some invertible P ∈Mn(F)).
In this case, T and A and B have minimal polynomial hk and characteristic
polynomial h1 · · · hk, so mA|χA. The invariant factors h1, . . . , hk in the rational
canonical form of A ∈Mn(F) coincide with the non-constant monic invariant
factors in the Smith normal form of xIn −A ∈Mn(F[x]).
540
Advanced Linear Algebra
11.
Jordan Canonical Forms. Let F be an algebraically closed field (such as C). For
c ∈F, the Jordan block J(c; e) is the e × e matrix with cs on the main diagonal,
1s on the next higher diagonal, and zeroes elsewhere. A Jordan canonical form is
a matrix of the form blk-diag(J(c1; e1), . . . , J(cs; es)). For every n-dimensional F-
vector space V and every linear map T : V →V , there exists a Jordan canonical
form C ∈Mn(F) such that [T]X = C for some ordered basis X of V . For any
matrix A ∈Mn(F), there exists a Jordan canonical form C ∈Mn(F) similar
to A. The only other Jordan forms for T (or A) are obtained by reordering the
Jordan blocks of C.
12.
F[x]-Module of a Linear Operator. Given a field F, a finite-dimensional F-
vector space V , and a linear map T : V →V , V becomes an F[x]-module via
(Pd
i=0 pixi) · v = Pd
i=0 piT i(v) for pi ∈F and v ∈V . F[x]-submodules of V
are the T-invariant subspaces (subspaces W with T[W] ⊆W). A cyclic F[x]-
submodule W has an F-basis of the form Bz = (z, T(z), T 2(z), . . . , T d−1(z)) for
some z ∈V and d ≥0. Such a cyclic submodule is isomorphic to F[x]/F[x]h,
where h ∈F[x] is the monic polynomial of least degree d with h · z = 0. The
matrix of [T|W ]Bz is the companion matrix Ch.
18.21
Exercises
1.
(a) Prove: for all commutative rings R and all a ∈R, Ra is an ideal of R.
(b) Give an example to show (a) can be false if R is not commutative.
2.
Prove that every field F is a PID.
3.
Let R = Z[x]. Prove that I = {2f + xg : f, g ∈R} is an ideal of R that is not
principal. Conclude that R is not a PID.
4.
For any integral domain R and any integer n ≥2, prove S = R[x1, . . . , xn] is not
a PID.
5.
Let R = {a + bi : a, b ∈Z}. Prove R is a subring of C and hence an integral
domain. Prove: for all f, g ∈R with g ̸= 0, there exist q, r ∈R with f = qg + r
and r = 0 or |r| < |g|, where |a + bi| =
√
a2 + b2. (First modify the division
algorithm in Z to see that for all u, v ∈Z with v ̸= 0, there exist q, r ∈Z with
u = qv + r and |r| ≤v/2.) Deduce that R is a PID.
6.
A Euclidean domain is a ring R satisfying the following hypotheses. First, R is
an integral domain. Second, R has a degree function deg : R̸=0 →Z≥0 such that
for all nonzero f, g ∈R, deg(f) ≤deg(fg). Third, R satisfies a Division Theorem
relative to the degree function: for all f, g ∈R with g ̸= 0, there exist q, r ∈R
with f = qg+r and r = 0 or deg(r) < deg(g). Prove that every Euclidean domain
is a PID. Explain why it follows that fields F, polynomial rings F[x], and the ring
Z are PIDs. (It can be shown [14] that not all PIDs are Euclidean domains; the
standard example is the ring R = {a + bz : a, b ∈Z}, where z = (1 + i
√
19)/2.)
7.
Let F be a field. Prove F[[x]]∗consists of all formal power series P
i≥0 pixi with
p0 ̸= 0. Prove F[[x]] is a PID. (Show that every nonzero ideal of F[[x]] has the
form F[[x]]xj for some j ∈Z≥0.)
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
541
8.
Let R be a commutative ring. Prove: for all a, a′, b, c ∈R:
(a) a|a;
(b) if a|b and b|c then a|c;
(c) 1R|a and a|0R;
(d) 0|a iff a = 0;
(e) ∼(defined by a ∼b iff a|b and b|a) is an equivalence relation on R;
(f) if a ∼a′, then: a|b iff a′|b; and c|a iff c|a′;
(g) a ∈R∗iff a ∼1.
9.
Let R be a commutative ring. Prove: for all k > 0 and all r1, . . . , rk, a1, . . . , ak,
c ∈R, if c divides every ri, then c|(a1r1 + · · · + akrk). Translate this result into
a statement about principal ideals.
10.
Suppose R is a commutative ring and d ∈R is a gcd of a1, . . . , ak ∈R. Prove: the
set of all gcds of a1, . . . , ak in R equals the set of all d′ ∈R with d ∼d′. Prove a
similar result for lcms.
11.
Let R = {a + bi : a, b ∈Z}, which is a subring of C.
(a) Find all units of R.
(b) What are the associates of 3 −4i in R?
(c) How do the answers to (a) and (b) change if we replace R by C?
12.
Given d, a1, . . . , ak in a commutative ring R, carefully prove that d is an lcm of
a1, . . . , ak iff Rd is the greatest lower bound of {Ra1, . . . , Rak} in the poset of
principal ideals of R ordered by ⊆.
13.
Suppose R is a commutative ring and a, b ∈R are associates.
(a) Prove a is irreducible in R iff b is irreducible in R using divisibility definitions.
(b) Prove the result in (a) by considering principal ideals.
(c) Prove a is prime in R iff b is prime in R using divisibility definitions.
(d) Prove the result in (c) by considering principal ideals.
14.
Suppose p is a prime element in a commutative ring R. Prove by induction:
for all k ≥1 and all a1, . . . , ak ∈R, if p|(a1a2 · · · ak) then p divides some ai.
15.
Let p be irreducible in an integral domain R. Prove: for all k ≥1 and all b1,. . .,bk
in R, if p ∼b1b2 · · · bk, then for some i, p ∼bi and all other bj are units of R.
16.
Let R be a commutative ring.
(a) Prove an ideal P of R is a prime ideal iff R/P is an integral domain.
(b) Prove an ideal M of R is a maximal ideal iff R/M is a field.
(c) Deduce from (a) and (b) that every maximal ideal is a prime ideal.
17.
Let R be a commutative ring with a maximal ideal M. Without using quotient
rings, prove that M is a prime ideal. (Argue by contradiction, and assume x, y ∈R
satisfy x, y ̸∈M but xy ∈M. Consider the ideals M +Rx and M +Ry to deduce
1R ∈M, which cannot occur.)
18.
Suppose R is a PID and I = Ra is an ideal of R. Under what conditions on a is
R/I a PID? Explain.
19.
Consider the commutative ring R = Z12, which is not an integral domain.
(a) Find all units of Z12.
(b) Find all prime elements in Z12.
(c) Find all irreducible elements in Z12.
(d) Describe the equivalence classes of ∼in Z12.
(e) Is it true that for all a, b ∈Z12, a ∼b iff a = ub for some u ∈Z∗
12?
542
Advanced Linear Algebra
20.
Let R and S be rings. Suppose K is any ideal in the product ring R × S. Prove
there exists an ideal I in R and an ideal J in S such that K = I × J.
21.
Consider the product ring T = Z4×Z2. Find all ideals in T. Draw the ideal lattice
of T (compare to Figure 17.1). Decide, with explanation, whether each ideal is
prime and whether each ideal is maximal. Decide, with explanation, whether each
nonzero element of T is a unit, a zero divisor, a prime element, or an irreducible
element (indicate all that apply).
22.
Repeat the previous exercise for the product ring T = Z2 × Z2 × Z2. (Compare
the ideal lattice to the one in Exercise 75 of Chapter 17.)
23.
Let R be a PID that is not a field.
(a) Explain why {0} is a prime ideal of R that is not a maximal ideal.
(b) Show that every nonzero prime ideal of R is a maximal ideal.
24.
Let R be an integral domain in which every ascending chain of principal ideals
stabilizes. Show R satisfies condition (a) in the definition of a UFD (existence of
irreducible factorizations).
25.
Let R be an integral domain in which every irreducible Show R satisfies
condition (b) in the definition of a UFD (uniqueness of irreducible factorizations).
26.
Suppose R is a UFD. Prove p ∈R is irreducible in R iff p is prime in R.
27.
Let R be a UFD and {pi : i ∈I} be a set of irreducible elements of R such that
every irreducible q in R is an associate of exactly one pi. Prove: for all nonzero
r ∈R, there exist unique u ∈R∗and exponents ei ≥0 such that all but finitely
many ei are zero and r = u Q
i∈I pei
i . Write u = u(r) and ei = ei(r) to indicate
that these parameters are functions of r.
28.
Let R be a UFD. With the notation of the previous exercise, prove:
(a) for a, b, c ∈R, c = ab iff u(c) = u(a)u(b) and ei(c) = ei(a) + ei(b) for all i ∈I;
(b) for all r, s ∈R̸=0, r|s iff ei(r) ≤ei(s) for all i ∈I;
(c) for all d, a1, . . . , ak ∈R̸=0, d is a gcd of a1, . . . , ak iff ei(d) = min1≤j≤k ei(aj)
for all i ∈I. Deduce that gcds of finite lists of elements in a UFD always exist.
29.
State and prove a formula characterizing lcms of a1, . . . , ak in a UFD R, similar
to part (c) of the previous exercise.
30.
Suppose R is a Euclidean domain (Exercise 6) for which there is an algorithm
to compute the quotient and remainder (relative to deg) when f is divided by
g. Describe an algorithm that takes as input f, g ∈R and produces as output
d, a, b ∈R such that d = af + bg and d is a gcd of f and g. (Imitate the proof
in §3.8.)
31.
Let R = {a + bi
√
5 : a, b ∈Z}, which is a subring of C. Define N : R →Z by
N(a + bi
√
5) = a2 + 5b2 for a, b ∈Z. Prove N(rs) = N(r)N(s) for all r, s ∈R.
Prove: for all r ∈R, r is a unit of R iff N(r) = ±1. Find all units of R. Show
that 2, 3, 1 + i
√
5, and 1 −i
√
5 are irreducible in R but not prime in R. Is R a
UFD? Is R a PID? Why?
32.
Let R = Z6. Show that every ideal of R is principal, but give an example of a
f.g. free R-module M and a submodule N of M that is not free. Explain exactly
why the proof in §18.6 fails for this ring.
33.
Give a specific example of an integral domain R, a f.g. free R-module M, and a
submodule N of M that is not free. Why does the proof in §18.6 fail here?
34.
Prove or disprove: for every f.g. free module M over a PID R and every submodule
N of M, there is a submodule P of M with P + N = M and P ∩N = {0}.
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
543
35.
Without using operation (B4), give direct proofs that applying the operations
(B1), (B2), and (B3) in §18.7 to an ordered basis of M produces a new ordered
basis of M.
36.
Prove item 1 and items 3—8 in §18.8.
37.
Let R = Q[x], let X
= Y
= (e1, e2) be the standard ordered basis of
the R-module R2, and let T : R2 →R2 be the R-linear map with matrix
A =

x −3
x2 + 1
x3 −x
5x + 2

relative to the bases X and Y.
(a) Compute T((3x −1, x2 + x −1)).
(b) Let Z = ((x −1, 2x), (x/2, x + 1)). Show Z is obtained from X by operation
(B4), so is an ordered basis of R2.
(c) What is the matrix of T relative to input basis Z and output basis Y? How
is this matrix related to A?
(d) What is the matrix of T relative to input basis X and output basis Z? How
is this matrix related to A?
(e) What is the matrix of T relative to input basis Z and output basis Z? How
is this matrix related to A?
38.
Continuing the previous exercise:
(a) Find one type 4 column operation on A making the 1, 1-entry become 1. Find
an ordered basis X ′ such that the new matrix represents T relative to X ′ and Y.
(b) Find one type 4 row operation on A making the 1, 1-entry become 1. Find an
ordered basis Y′ such that the new matrix represents T relative to X and Y′.
39.
Let M be a f.g. free module over a PID R, X = (x1, . . . , xm) be an ordered basis
of M, and P ∈Mm(R) be an invertible matrix. Consider the operation that maps
X to X ′ = (x′
1, . . . , x′
m), where x′
i = Pm
j=1 P(j, i)xj for 1 ≤i ≤m. Prove X ′ is
an ordered basis of M. Prove that for any ordered basis Y of M, we can choose
P ∈Mm(R) such that X ′ = Y. Show that operation (B4) is a special case of the
operation considered here.
40.
Let R be a PID and T : N →M be an R-linear map between two f.g. free R-
modules. Let X = (x1, . . . , xn) be an ordered basis for N, Y = (y1, . . . , ym) be an
ordered basis for M, and A be the matrix of T relative to X and Y .
(a) Given an invertible P ∈Mn(R), define X ′ = XP as in Exercise 39, and let
A′ be the matrix of T relative to X ′ and Y. How is A′ related to A and P?
(b) Given an invertible Q ∈Mm(R), let Y′ = YQ as in Exercise 39, and let A′′
be the matrix of T relative to X and Y′. How is A′′ related to A and Q?
(c) Suppose X ′ and Y′ are any ordered bases for N and M (respectively), and B
is the matrix of T relative to X ′ and Y′. Show that B = UAV for some invertible
U ∈Mm(R) and V ∈Mn(R).
41.
Let a, b, d be nonzero elements in a PID (or UFD) R. Prove:
(a) len(ab) = len(a) + len(b);
(b) if d|a in R, then len(d) ≤len(a), and equality holds iff d ∼a;
(c) if d = gcd(a, b) and a does not divide b, then len(d) < len(a).
42.
Suppose R is a UFD such that for all x, y ∈R and every gcd d of x and y in R,
there exist a, b ∈R with d = ax + by. Prove that R must be a PID. (Given any
nonzero ideal I of R, show I is generated by any x ∈I of minimum length.) This
exercise shows that in UFDs that are not PIDs, the gcd of a list of elements is
not always an R-linear combination of those elements.
544
Advanced Linear Algebra
43.
Prove the Matrix Reduction Theorem in §18.9 for Euclidean domains R (see
Exercise 6) without using unique factorization or length, by imitating the proof
for R = Z in §16.11. Also show that the matrix reduction never requires general
type 4 operations, but can be achieved using only elementary operations (B1),
(B2), and (B3).
44.
Let R be the PID {a + bi : a, b ∈Z} (see Exercise 5). Use matrix reduction to
find a Smith normal form of these matrices:
(a)

8 −21i
12 + 51i
49 + 32i
−136 + 2i

(b)


4 + i
3
1 + i
16 + 5i
10 −7i
1 −3i
1 + 10i
8 −3i
3 −7i


45.
Solve Exercise 44 again, but find the rank and invariant factors by computing
gcds of order k minors.
46.
Let R = Q[x]. Use matrix reduction to find a Smith normal form of these matrices:
(a)
 x3 −x2 −3x + 2
x2 −4
x3 −6x + 4
x2 + x −6

(b)

x4 −x2 + x
x4 + x3 −x2
x3 −x
x5 −x4 −x3 + 2x2 −2x
x5 −2x3 + x2 −x
x4 −x3 −x2 + x

(c)


x + 1
x2 + x −1
1
x
x2 + x
x3 + x2 −x


47.
Solve Exercise 46 again, but find the rank and invariant factors by computing
gcds of order k minors.
48.
Let R be a PID and a ∈R. What is a Smith normal form of an m × n matrix all
of whose entries equal a?
49.
Given c ∈C, what is a Jordan canonical form of an n × n complex matrix all of
whose entries equal c?
50.
Given c ∈Q, what is the rational canonical form of the matrix in Mn(Q) all of
whose entries equal c?
51.
For prime p, what is the rational canonical form of the matrix in Mp(Zp) all of
whose entries equal 1?
52.
Write a computer program that finds the Smith normal form of a matrix A ∈
Mm,n(Q[x]) via the matrix reduction algorithm described in §18.9.
53.
Write a computer program to find the Smith normal form of A using the formulas
for the invariant factors as gcds of minors of A (see §18.12).
54.
Comment on the relative efficiency of the programs in the previous two exercises
for large m and n.
55.
Let R = Q[x] and T : R3 →R2 be the R-linear map
T((f, g, h)) = (x3f + (x2 + x)g + x2h, (x4 −x3)f + x2g + x3h) for f, g, h ∈R.
(a) What is the matrix of T relative to the standard ordered bases of R3 and R2?
(b) Find a Smith normal form B of T and ordered bases X of R3 and Y of R2
such that B is the matrix of T relative to X and Y.
56.
Find the Betti number, invariant factors, and elementary divisors of each R-
module M.
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
545
(a) R = Q[x], M = R4/(Rv1 + Rv2 + Rv3) where
v1 = (x4 −2x3 + 2x2, −2x2, −x2, x2),
v2 = (x3 −x2, x2, x2, 0),
v3 = (x5 −2x3 + 2x2, x4 −2x2, x4 −x2, x2).
(b) R = Q[x], M = R3/(Rw1 + Rw2 + Rw3) where w1 = (x2, x3, x3), w2 =
(x3, x3, x2), and w3 = (x2, x3, x4).
(c) R = {a + bi : a, b ∈Z}, M = R2/(Rz1 + Rz2) where z1 = (4 + 3i, 12 + 8i) and
z2 = (5 −7i, 7 −4i).
57.
Let R be a PID. Prove: if a, b ∈R satisfy gcd(a, b) = 1, then R/Rab is isomorphic
to R/Ra × R/Rb both as R-modules and as rings.
58.
Show that for R = Q[x, y], R/Rxy and R/Rx × R/Ry are not isomorphic as
R-modules (compare to the previous exercise).
59.
Let A be an m × n matrix with entries in a PID R. Use the Cauchy–Binet
Formula to prove: for all k between 1 and min(m, n) and all invertible Q ∈Mn(R),
gk(AQ) ∼gk(A). If Q is not invertible, is there any relation between gk(AQ) and
gk(A)?
60.
Let A be an m × n matrix with entries in a PID R. Without using the Cauchy–
Binet Formula, prove: for all k between 1 and min(m, n) and all invertible P ∈
Mm(R) and all invertible Q ∈Mn(R), gk(PA) ∼gk(A) ∼gk(AQ). (Use ideas
from §4.8 to show that each row of (PA)I,J is an R-linear combination of certain
rows of A[m],J. Then use multilinearity of the determinant as a function of the
rows of a matrix (§5.6) to show that det((PA)I,J) is some R-linear combination
of order k minors of A. The Cauchy–Binet Formula shows us explicitly what this
linear combination is.)
61.
Give an example of a commutative ring R and an R-module M such that tor(M)
is not a submodule of M.
62.
Suppose R is an integral domain and f : M →N is an R-module isomorphism.
Carefully check that f[tor(M)] = tor(N) and that f induces an isomorphism
f ′ : M/ tor(M) →N/ tor(N).
63.
Let a be a nonzero element in a PID R. Prove that len(R/Ra) [as defined in §17.18]
equals len(a) [as defined in §18.9].
64.
Let N and N ′ be isomorphic R-modules, where R is a commutative ring, and fix
c ∈R. Prove cN is a submodule of N. Prove cN ∼= cN ′ and N/cN ∼= N ′/cN ′. If
N = N1 × · · · × Nk, prove cN = cN1 × · · · × cNk.
65.
Define T : R4 →R4 by T(v) = Av for v ∈R4, where A = blk-diag(J(3; 2), J(5; 2))
is a Jordan canonical form. What is the rational canonical form of T? Use this
and results about submodule lattices to find all T-invariant subspaces of R4 and
a specific generator for each T-cyclic subspace.
66.
Let A = blk-diag(J(4; 2), J(4; 2)) and T(v) = Av for v ∈R4. What is the rational
canonical form of T? Show that R4 has infinitely many 1-dimensional T-invariant
subspaces and infinitely many 2-dimensional T-invariant subspaces.
67.
A matrix in Mn(Q) has invariant factors (x −1, x3 −3x + 2, x5 + x4 −5x3 −x2 +
8x −4). Find n and the elementary divisors of the matrix.
546
Advanced Linear Algebra
68.
A matrix in Mm(Q) has elementary divisors
[x −1, x −1, x −1, (x −1)3, (x −1)4, (x2 −2)2, (x2 −2)2, (x2 −2)3, x2 + 1, x2 + 1].
Find m and the invariant factors of the matrix.
69.
Let M be a finitely generated module over a PID R. Give an alternate proof of
the uniqueness of the elementary divisors of M by imitating the arguments for
Z-modules in §16.17 and §16.18.
70.
(a) In §18.15, prove f(g(L)) = L for all L ∈Y .
(b) Let R be a PID. Assume we have proved the uniqueness of the elementary
divisors of a finitely generated R-module M (see Exercise 69). Use this result and
(a) to prove the uniqueness of the invariant factors of M (cf. §16.19).
71.
In §18.16, we proved that V was an F[x]-module by checking all the module
axioms. Give a more conceptual proof of this fact by using Exercise 16 in
Chapter 17 and the Universal Mapping Property for F[x].
72.
Let T, S : V →V be linear operators on an n-dimensional vector space V over a
field F with S ◦T = T ◦S. Show that there exists a unique F[x, y]-module action
· : F[x, y] × V →V such that for all v ∈V , x · v = T(v), y · v = S(v), and for all
c ∈F, c · v is the given scalar multiplication in V .
73.
Let F be a field and h ∈F[x] have degree d > 0. Show that the F-vector space
F[x]/F[x]h has ordered basis (1 + F[x]h, x + F[x]h, . . . , xd−1 + F[x]h).
74.
Let T be a linear map on an n-dimensional F-vector space V . Let W be any T-
invariant subspace of V such that for some z ∈W, Bz = (z, T(z), . . . , T d−1(z)) is
an F-basis of W and [T|W ]Bz = Ch. Prove that W = F[x]z is a T-cyclic subspace
isomorphic to F[x]/F[x]h
75.
Define T : R6 →R6 by T((x1, x2, x3, x4, x5, x6)) = (x2, x3, x4, x5, x6, x1).
(a) For 1 ≤i ≤6, find the T-annihilator of ei in R[x].
(b) What is the T-annihilator of e1 + e3 + e5?
(c) What is the T-annihilator of e1 + e4?
(d) If possible, find v ∈R6 whose T-annihilator is R[x](x2 −x + 1).
(e) If possible, find w ∈R6 whose T-annihilator is R[x](x3 + 2x2 + 2x + 1).
76.
In the last paragraph of §18.17, confirm that each W ∗
i is a cyclic F[x]-submodule
of V generated by z∗
i , and V ∼= W ∗
1 × · · · × W ∗
ℓ∼= Qℓ
i=1 F[x]/F[x]gi.
77.
Check the claim in §18.18 that B = (ye−1, . . . , y2, y1, y0) is an ordered F-basis for
W.
78.
In §18.18, prove that if W
is a T-invariant subspace of V
such that
[T|W ]B = J(c; e) for some ordered basis B of W, then W is a T-cyclic subspace
with W = F[x]y ∼= F[x]/F[x](x −c)e for some y ∈W. Use this to show that all
Jordan canonical forms of T are obtained from (18.14) by reordering the Jordan
blocks.
79.
Let h ∈F[x] be monic of degree n > 0. Show the minimal polynomial of Ch is h.
Show that xIn −Ch ∈Mn(F[x]) can be reduced by elementary row and column
operations to diag(1, . . . , 1, h). (Start by adding −x times row i to row i −1, for
i = n, n −1, . . . , 2.) Show that χCh = h by calculating det(xIn −Ch).
80.
Prove: If A, B ∈Mn(F) are similar matrices, then mA = mB and χA = χB.
81.
Show that for a block-diagonal matrix B = blk-diag(B1, . . . , Bk) ∈Mn(F),
mB = lcm(mB1, . . . , mBk) and χB = Qk
i=1 χBi.
Principal Ideal Domains, Modules over PIDs, and Canonical Forms
547
82.
What are the minimal polynomial and characteristic polynomial of a Jordan
block J(c; e)? What are the minimal polynomial and characteristic polynomial of
a Jordan canonical form blk-diag(J(c1; e1), . . . , J(cs, es))?
83.
Let F be a field. What are the invariant factors in the rational canonical
form of the matrix bIn? What are the invariant factors for a diagonal matrix
diag(a1, . . . , an)n×n where all ai ∈F are distinct? Given that a, b, c, d ∈F are
distinct, what are the invariant factors of diag(a, a, a, a, b, b, b, b, c, c, d, d)?
84.
Let F be a field. Find (with proof) all matrices in Mn(F) whose rational canonical
form is diagonal.
85.
Use the Jordan canonical form to prove that a matrix A ∈Mn(C) is diagonalizable
iff the minimal polynomial of A in C[x] has no repeated roots.
86.
For any field F, give a simple characterization of the rational canonical forms of
diagonalizable matrices in Mn(F).
87.
Find the rational canonical form of each matrix in Mn(Q):
(a)


1
2
3
4
5
6
7
8
9


(b)


0
1
1
1
0
1
1
1
0


(c)


1
0
0
0
0
−1
−1
0
0
4
3
0
1
−4
−2
1


(d)


12
15
5
−30
1
7/2
1/2
−3
13
39/2
17/2
−39
6
9
3
−16


88.
Find a Jordan canonical form in Mn(C) for each matrix in Exercise 87.
89.
Prove: for any PID R and any B ∈Mm,n(R), B and BT have the same rank and
invariant factors.
90.
Prove: for any field F and any A ∈Mn(F), A is similar to AT.
91.
Let F be a subfield of K. Prove: for all A ∈Mn(F), the rational canonical form
of A in Mn(F) is the same as the rational canonical form of A in Mn(K).
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
Part VI
Universal Mapping Properties
and Multilinear Algebra
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
19
Introduction to Universal Mapping Properties
The concept of a Universal Mapping Property (abbreviated UMP) occurs frequently in linear
and multilinear algebra. Indeed, this concept pervades every branch of abstract algebra and
occurs in many other parts of mathematics as well. To introduce this fundamental notion,
we start by considering a well-known result about vector spaces over a field F.
Theorem on Bases and Linear Maps. Let V be an n-dimensional F-vector space with
basis X = {x1, . . . , xn}. For any F-vector space W and any function f : X →W, there
exists a unique F-linear map T : V →W extending f (i.e., T(x) = f(x) for all x ∈X).
Proof of uniqueness of T: Any v ∈V can be uniquely expressed as v = Pn
i=1 cixi for scalars
ci ∈F. If T is to be an F-linear map extending f, then we must have
T(v) = T
 n
X
i=1
cixi
!
=
n
X
i=1
ciT(xi) =
n
X
i=1
cif(xi).
(19.1)
This shows that the map T is uniquely determined by f, if T exists at all.
Proof of existence of T: To prove existence of T, we define T(Pn
i=1 cixi) = Pn
i=1 cif(xi)
for all ci ∈F, as in formula (19.1). Note that T is a well-defined map from V into
W, since every vector v ∈V has a unique expansion in terms of the basis X. Taking
v = xj = 1xj + P
i̸=j 0xi, the formula shows that T(xj) = 1f(xj) + P
i̸=j 0f(xi) = f(xj)
for all xj ∈X, so that T does extend f. To check F-linearity of T, fix v, w ∈V and a ∈F.
Write v = Pn
i=1 cixi and w = Pn
i=1 dixi for some ci, di ∈F. Then v + w = Pn
i=1(ci + di)xi
and av = Pn
i=1(aci)xi. Using the definition of T several times, we get
T(v + w) =
n
X
i=1
(ci + di)f(xi) =
n
X
i=1
cif(xi) +
n
X
i=1
dif(xi) = T(v) + T(w);
T(av) =
n
X
i=1
(aci)f(xi) = a
n
X
i=1
cif(xi) = aT(v).
Therefore, T is an F-linear map.
We now restate the Theorem on Bases and Linear Maps in three equivalent ways.
Throughout, we fix the field F, the vector space V , and the basis X. Let i : X →V
be the inclusion map given by i(x) = x for all x ∈X. Note that T extends f iff T(x) = f(x)
for all x ∈X iff T(i(x)) = f(x) for all x ∈X iff (T ◦i)(x) = f(x) for all x ∈X iff the two
functions T ◦i : X →W and f : X →W are equal.
1.
Diagram Completion Property. For any F-vector space W and any function
f : X →W, there exists a unique F-linear map T : V →W such that the
following diagram commutes.
X
i
/
f
 
V
T

W
DOI: 10.1201/9781003484561-19
551
552
Advanced Linear Algebra
(Saying the diagram commutes means, by definition, that f = T ◦i.)
2.
Unique Factorization Property. For any F-vector space W and any function
f : X →W, there exists a unique F-linear map T : V →W such that the
factorization f = T ◦i holds.
3.
Bijection between Collections of Functions. For any F-vector space W, there is a
bijection from the collection
A = {F-linear maps T : V →W}
onto the collection
B = {arbitrary maps f : X →W}.
The bijection sends T ∈A to T ◦i ∈B. The inverse bijection sends f ∈B to the
unique F-linear map T : V →W that extends f.
The first two restatements follow immediately from the original theorem and the fact
that T extends f iff f = T ◦i iff the given diagram commutes. To prove the third restatement,
define a map ϕ with domain A by ϕ(T) = T ◦i for T ∈A. For any such T, T ◦i is a function
from X to W, so that ϕ does map A into the codomain B. The unique factorization property
amounts to the statement that ϕ : A →B is a bijection. In more detail, given any f ∈B,
that property says there exists a unique T ∈A with f = T ◦i = ϕ(T). Existence of such a T
means that ϕ is onto, and uniqueness of T means that ϕ is one-to-one. So, ϕ is a bijection.
The relation f = ϕ(T) now implies that ϕ−1(f) = T, where T is the unique F-linear map
extending f.
The theorem and its three restatements are all referred to as the universal mapping
property for the basis X of the vector space V . More precisely, we might refer to this result
as the UMP for the inclusion map i : X →V . Note that composition with i establishes a
correspondence (bijection) between F-linear functions from V to another vector space W
and arbitrary functions from X to W. The term “universal” indicates that the same map
i : X →V works for all possible target vector spaces W.
The rest of this chapter discusses many examples of UMPs that occur in linear and
abstract algebra. Even more UMPs are discussed in the following chapter on multilinear
algebra. All of these universal mapping properties involve variations of the setup illustrated
by the preceding example. In that example, we are interested in understanding special types
of functions (namely, F-linear maps) from the fixed vector space V into arbitrary vector
spaces W. The universal mapping property helps us understand these functions by setting
up a bijection (for each fixed W) between these linear maps and another, simpler set of maps
(namely, the set of all functions from the finite set X into W). The central idea of studying
a linear map by computing its matrix relative to an ordered basis is really a manifestation of
this universal mapping property (the columns of the matrix give the coordinates of f(xj) for
each xj ∈X). Other UMPs have a similar purpose: roughly speaking, composition with a
universal map induces a bijection from collections of functions having one kind of structure
to collections of functions having another kind of structure. The bijection is valuable since
one of the two kinds of functions might be much easier to understand and analyze than
the other. For instance, in the next chapter, we study UMPs that convert multilinear maps
(and their variations) into linear maps, thus permitting us to use linear algebra in the study
of the more difficult subject of multilinear algebra.
Introduction to Universal Mapping Properties
553
19.1
Bases of Free R-Modules
The rest of this chapter assumes familiarity with the material on modules covered in
Chapter 17. Our introductory example, involving bases of finite-dimensional vector spaces,
readily generalizes to free modules over an arbitrary ring. Let R be a ring and M be a free
left R-module with basis X. Recall this means that for each v ∈M, there exists a unique
expression v = P
x∈X cxx in which cx ∈R and all but finitely many scalars cx are zero. Let
i : X →M be the inclusion map given by i(x) = x for all x ∈X. We have the following
equivalent versions of the Universal Mapping Property for i : X →M.
1.
Diagram Completion Property. For any left R-module N and any function
f : X →N, there exists a unique R-module homomorphism T : M →N such
that the following diagram commutes.
X
i
/
f
 
M
T

N
2.
Unique Factorization Property. For any left R-module N and any function
f : X →N, there exists a unique R-module homomorphism T : M →N such
that the factorization f = T ◦i holds.
3.
Bijection between Collections of Functions. For any left R-module N, there is a
bijection from the collection
A = {R-module homomorphisms T : M →N}
onto the collection
B = {arbitrary maps f : X →N}.
The bijection sends T ∈A to T ◦i ∈B. The inverse bijection sends f ∈B to the
unique R-module homomorphism T : M →N that extends f.
The proof proceeds as before. Given N and f : X →N, the only map T : M →N that
could possibly be an R-module homomorphism extending f must be given by the formula
T
 X
x∈X
cxx
!
=
X
x∈X
cxf(x),
where all cx ∈R.
So T is unique if it exists. To prove existence of T, use the preceding formula as the definition
of T. Then T is a well-defined function from M into N (the definition is unambiguous, since
M is free with basis X). You can check, as we did for vector spaces, that T is an R-module
homomorphism extending f. This proves the first version of the UMP, and the other two
versions follow as before.
19.2
Homomorphisms out of Quotient Modules
Let R be a ring and M be any left R-module with submodule N. Let p : M →M/N be the
R-homomorphism given by p(x) = x + N for all x ∈M. The following universal mapping
554
Advanced Linear Algebra
property of p helps explain the significance of the construction of the quotient module M/N.
As before, we can state the UMP in several equivalent ways.
1.
UMP for Projection onto M/N (Diagram Completion Formulation): For every
left R-module Q and every R-linear map f : M →Q such that f(x) = 0 for
all x ∈N, there exists a unique R-module homomorphism f ′ : M/N →Q
satisfying f = f ′ ◦p, meaning that f ′(x+N) = f(x) for all x ∈M. (Furthermore,
img(f ′) = img(f) and ker(f ′) = ker(f)/N.)
M
p /
f
"
M/N
f ′

Q
2.
UMP for Projection onto M/N (Bijective Formulation): For any left R-module
Q, there is a bijection from the collection
A = {all R-module homomorphisms f ′ : M/N →Q}
onto the collection
B = {R-module homomorphisms f : M →Q such that f(x) = 0 for all x ∈N}.
The bijection sends f ′ ∈A to f ′ ◦p ∈B. The inverse bijection sends f ∈B to
the unique R-module homomorphism f ′ : M/N →Q such that f ′ ◦p = f.
To prove the diagram completion property, let f : M →Q satisfy f(x) = 0 for all x ∈N.
Uniqueness of f ′ : M/N →Q follows from the requirement that f = f ′◦p. This requirement
means that f ′(x + N) = f ′(p(x)) = (f ′ ◦p)(x) = f(x) for all x ∈M, and every element of
the set M/N has the form x+N for some x ∈M. Thus, if f ′ exists at all, it must be defined
by the formula f ′(x + N) = f(x) for x ∈M. Naturally, then, this is the formula we must
use in the proof that f ′ does exist. To see that the formula defines a single-valued function,
suppose x, y ∈M are such that x + N = y + N. Then x −y ∈N, so that f(x −y) = 0
by assumption on f. Since f is a homomorphism, we have f(x) −f(y) = f(x −y) = 0
and hence f(x) = f(y). Therefore, f ′(x + N) = f(x) = f(y) = f ′(y + N), proving that
f ′ : M/N →Q is a well-defined (single-valued) function. Knowing this, we can now check
that f ′ is a R-module homomorphism. For all x, z ∈M and r ∈R, compute
f ′((x + N) + (z + N)) = f ′((x + z) + N) = f(x + z) = f(x) + f(z) = f ′(x + N) + f ′(z + N);
f ′(r(x + N)) = f ′((rx) + N) = f(rx) = rf(x) = rf ′(x + N).
As seen in the uniqueness proof, the very definition of f ′ guarantees that f = f ′ ◦p.
To verify the bijective version of the UMP, consider the function ϕ with domain A given
by ϕ(f ′) = f ′ ◦p for f ′ ∈A. We check ϕ does map into the claimed codomain B. Given
f ′ ∈A, f ′◦p is a R-homomorphism from M to Q, being a composition of R-homomorphisms.
Also, f ′ ◦p does send every x ∈N to 0Q, since (f ′ ◦p)(x) = f ′(x + N) = f ′(0 + N) = 0Q.
The existence and uniqueness assertions proved above show that ϕ is a bijection.
To finish, we prove the parenthetical remark about the image and kernel of f ′. Note
img(f ′) = {f ′(w) : w ∈M/N} = {f ′(x + N) : x ∈M} = {f(x) : x ∈M} = img(f).
Next, N ⊆ker(f) holds by our assumption on f, so the quotient module ker(f)/N exists.
For each x ∈M, we have x + N ∈ker(f ′) iff f ′(x + N) = 0 iff f(x) = 0 iff x ∈ker(f) iff
x + N ∈ker(f)/N.
Introduction to Universal Mapping Properties
555
19.3
Direct Product of Two Modules
Let R be a ring and M and N be left R-modules. We can form the direct product
M × N = M ⊕N = {(m, n) : m ∈M, n ∈N}, which is a left R-module under
componentwise operations (see §17.5). We define four R-module homomorphisms associated
with the module M × N:
p : M × N →M
given by
p((m, n)) = m
for m ∈M, n ∈N;
q : M × N →N
given by
q((m, n)) = n
for m ∈M, n ∈N;
i : M →M ⊕N
given by
i(m) = (m, 0)
for m ∈M;
j : N →M ⊕N
given by
j(n) = (0, n)
for n ∈N.
We call p and q the canonical projections, and we call i and j the canonical injections.
These maps satisfy the following identities:
p ◦i = idM,
q ◦j = idN,
q ◦i = 0,
p ◦j = 0,
i ◦p + j ◦q = idM⊕N .
In this section and the next, we discuss two different universal mapping properties for M×N,
one involving the canonical projections, and the other involving the canonical injections.
1.
UMP for Projections (Diagram Completion Formulation): Suppose Q is any left
R-module, and we are given two R-homomorphisms f : Q →M and g : Q →N.
There exists a unique R-homomorphism h : Q →M × N satisfying f = p ◦h and
g = q ◦h, meaning that this diagram commutes:
M
M × N
p
o
q
/ N
Q
f
c
g
;
h
O
2.
UMP for Projections (Bijective Formulation): For any left R-module Q, there is
a bijection from the collection
A = {all R-module homomorphisms h : Q →M × N} = HomR(Q, M × N)
onto the collection
B
=
{pairs (f, g) of R-homomorphisms f : Q →M, g : Q →N}
=
HomR(Q, M) × HomR(Q, N).
The bijection sends h ∈A to the pair (p ◦h, q ◦h) ∈B. The inverse bijection
sends (f, g) ∈B to the unique R-module homomorphism h : Q →M × N such
that (f, g) = (p ◦h, q ◦h).
To verify the first version of the UMP, we start by proving the uniqueness of
h : Q →M × N. If the function h exists at all, it must have the form h(x) = (h1(x), h2(x))
for all x ∈Q, where h1 : Q →M and h2 : Q →N are certain functions. The requirement
on h that p ◦h = f forces f(x) = p(h(x)) = p((h1(x), h2(x))) = h1(x) for all x ∈Q, so
that h1 = f. Similarly, the requirement q ◦h = g forces h2 = g. Therefore, h (if it exists at
all) must be given by the formula h(x) = (f(x), g(x)) for x ∈Q, so that uniqueness of h is
proved.
556
Advanced Linear Algebra
To prove existence of h, define h (as we must) by setting h(x) = (f(x), g(x)) for x ∈Q.
By a computation similar to the one in the last paragraph, we have p ◦h = f and q ◦h = g.
We must still prove that h is an R-module homomorphism. Fix x, y ∈Q and r ∈R, and
calculate:
h(x + y) = (f(x + y), g(x + y)) = (f(x) + f(y), g(x) + g(y))
= (f(x), g(x)) + (f(y), g(y)) = h(x) + h(y);
h(rx) = (f(rx), g(rx)) = (rf(x), rg(x)) = r(f(x), g(x)) = rh(x).
To verify the bijective version of the UMP, consider the function ϕ with domain A given
by ϕ(h) = (p ◦h, q ◦h) for h ∈A. Note ϕ does map A into the set B, since for any h ∈A,
p ◦h is an R-homomorphism from Q to M, and q ◦h is an R-homomorphism from Q to N.
The existence and uniqueness assertions proved above show that ϕ is a bijection.
19.4
Direct Sum of Two Modules
Let R be a ring, let M and N be left R-modules, let M ⊕N = {(m, n) : m ∈M, n ∈N},
and let i and j be the canonical injections defined in §19.3. We now consider the universal
mapping property satisfied by these maps.
1.
UMP for Injections (Diagram Completion Formulation): Suppose Q is any left
R-module, and we are given two R-homomorphisms f : M →Q and g : N →Q.
There exists a unique R-homomorphism h : M ⊕N →Q satisfying f = h ◦i and
g = h ◦j, meaning that this diagram commutes:
M
i /
f
#
M ⊕N
h

N
jo
g
{
Q
2.
UMP for Injections (Bijective Formulation): For any left R-module Q, there is a
bijection from the collection
A = {all R-module homomorphisms h : M ⊕N →Q} = HomR(M ⊕N, Q)
onto the collection
B
=
{pairs (f, g) of R-homomorphisms f : M →Q, g : N →Q}
=
HomR(M, Q) × HomR(N, Q).
The bijection sends h ∈A to the pair (h ◦i, h ◦j) ∈B. The inverse bijection
sends (f, g) ∈B to the unique R-module homomorphism h : M ⊕N →Q such
that (f, g) = (h ◦i, h ◦j).
To verify the first version of the UMP, we start by proving the uniqueness of
h : M ⊕N →Q. The requirement h ◦i = f means that h((m, 0)) = h(i(m)) = f(m) for all
m ∈M. The requirement h ◦j = g means that h((0, n)) = h(j(n)) = g(n) for all n ∈N.
Now, h is also required to be an R-homomorphism. So if h exists at all, we must have
h((m, n)) = h((m, 0) + (0, n)) = h((m, 0)) + h((0, n)) = f(m) + g(n)
for all m ∈M and n ∈N. This proves uniqueness of h.
Introduction to Universal Mapping Properties
557
To prove existence of h : M ⊕N →Q, define h (as we must) by setting h((m, n)) =
f(m)+g(n) for all m ∈M and all n ∈N. For each m ∈M, we have (h◦i)(m) = h(i(m)) =
h((m, 0)) = f(m) + g(0) = f(m) + 0 = f(m), so that h ◦i = f. Similarly, f(0) = 0 implies
that h◦j = g. To finish, we check that h is an R-homomorphism. Let m, m′ ∈M, n, n′ ∈N,
r ∈R, and calculate:
h((m, n) + (m′, n′)) = h((m + m′, n + n′)) = f(m + m′) + g(n + n′)
= f(m) + f(m′) + g(n) + g(n′) = f(m) + g(n) + f(m′) + g(n′)
= h((m, n)) + h((m′, n′));
(19.2)
h(r(m, n)) = h((rm, rn)) = f(rm)+g(rn) = rf(m)+rg(n) = r(f(m)+g(n)) = rh((m, n)).
Note that commutativity of addition in Q was needed for the fourth equality in (19.2).
To verify the bijective version of the UMP, consider the function ϕ with domain A given
by ϕ(h) = (h ◦i, h ◦j) for h ∈A. Note ϕ does map into the set B, because for any h ∈A,
h ◦i is a R-homomorphism from M to Q, and h ◦j is a R-homomorphism from N to Q.
The existence and uniqueness assertions proved above show that ϕ is a bijection.
You can check (Exercises 11 and 12) that all the constructions and results in this section
and Section 19.3 extend to modules M1 × · · · × Mn = M1 ⊕· · · ⊕Mn where there are
only finitely many factors. In the next two sections, we generalize the universal mapping
properties even further, discussing arbitrary direct products and direct sums of R-modules.
When there are infinitely many nonzero factors, the direct product Q
i∈I Mi is distinct from
the direct sum L
i∈I Mi, so that the two universal mapping properties (one for projections,
one for injections) involve different R-modules.
19.5
Direct Products of Arbitrary Families of R-Modules
Let R be a ring, I be an index set, Mi be a left R-module for each i ∈I, and M = Q
i∈I Mi
be the direct product of the Mi. Recall from §17.5 that elements x ∈M are functions
x : I →S
i∈I Mi such that x(i) ∈Mi for all i ∈I. We often visualize these functions as I-
tuples (xi : i ∈I), particularly when I = {1, 2, . . . , n}. Module operations in M are defined
pointwise: given x, y ∈M, we have (x+y)(i) = x(i)+y(i) ∈Mi and (rx)(i) = r(x(i)) ∈Mi
for all i ∈I.
For each i ∈I, we have the canonical projection map pi : M →Mi, which sends x ∈M
to x(i) ∈Mi. Each pi is an R-homomorphism, since for x, y ∈M and r ∈R,
pi(x + y) = (x + y)(i) = x(i) + y(i) = pi(x) + pi(y);
pi(rx) = (rx)(i) = r(x(i)) = rpi(x).
The family of projection maps {pi : i ∈I} satisfies the following universal mapping property.
1.
UMP for Direct Products of Modules (Diagram Completion Formulation): Sup-
pose Q is any left R-module, and for each i ∈I we have an R-homomorphism
fi : Q →Mi. There exists a unique R-homomorphism f : Q →Q
i∈I Mi satisfying
fi = pi ◦f for all i ∈I, meaning that these diagrams commute for all i ∈I:
Mi
Q
i∈I Mi
pi
o
Q
fi
c
f
O
558
Advanced Linear Algebra
2.
UMP for Direct Products of Modules (Bijective Formulation): For any left R-
module Q, there is a bijection from the collection
A = HomR
 
Q,
Y
i∈I
Mi
!
=
(
R-module homomorphisms f : Q →
Y
i∈I
Mi
)
onto the collection
B =
Y
i∈I
HomR(Q, Mi) = {I-tuples (fi : i ∈I) of R-homomorphisms fi : Q →Mi}.
The bijection sends f ∈A to (pi ◦f : i ∈I) ∈B. The inverse bijection sends
(fi : i ∈I) ∈B to the unique R-module homomorphism f : Q →M such that
fi = pi ◦f for all i ∈I.
The proof of this UMP is similar to the one given earlier for M×N. Consider the diagram
completion property for fixed Q and fixed R-maps fi : Q →Mi (i ∈I). If f : Q →M exists,
then f(x) is a function from I to S
i∈I Mi for each x ∈Q. Furthermore, the requirement that
fi = pi ◦f means that fi(x) = pi(f(x)) for all x ∈Q. By definition of pi, pi(f(x)) = f(x)(i)
is the value of the function f(x) at the point i ∈I. Therefore, f is completely determined
by the requirements fi = pi ◦f for i ∈I: we must have f(x)(i) = fi(x) for all i ∈I and all
x ∈Q. To say the same thing in I-tuple notation, we must have
f(x) = (fi(x) : i ∈I)
for all x ∈Q.
This proves uniqueness of f.
Proceeding to the existence proof, we must use the formula just written as the definition
of f : Q →M. Since f(x)(i) = fi(x) ∈Mi for all i ∈I, f(x) is a well-defined element
of M = Q
i∈I Mi, and it is true that fi = pi ◦f for all i ∈I. We need only check that
f : Q →M is an R-module homomorphism. Let x, y ∈Q and r ∈R, and calculate:
f(x + y)
=
(fi(x + y) : i ∈I) = (fi(x) + fi(y) : i ∈I)
=
(fi(x) : i ∈I) + (fi(y) : i ∈I) = f(x) + f(y);
f(rx)
=
(fi(rx) : i ∈I) = (rfi(x) : i ∈I)
=
r(fi(x) : i ∈I) = rf(x).
For the second version of the UMP, define a function ϕ with domain A by setting
ϕ(f) = (pi ◦f : i ∈I) for f ∈A. For each i ∈I, pi ◦f is an R-homomorphism from Q to
Mi, as required. So ϕ does map into B, and the first version of the UMP shows that ϕ is a
bijection.
19.6
Direct Sums of Arbitrary Families of R-Modules
Let R be a ring, I be an index set, Mi be a left R-module for each i ∈I, and M = L
i∈I Mi
be the direct sum of the Mi. Recall that M is the submodule of the direct product Q
i∈I Mi
consisting of those functions x : I →S
i∈I Mi such that x(i) ̸= 0Mi for only finitely many
indices i. If the index set I is finite, then L
i∈I Mi = Q
i∈I Mi, but these modules are
distinct when I is infinite and all Mi are nonzero. The module L
i∈I Mi is also referred to
as the coproduct of the R-modules Mi.
Introduction to Universal Mapping Properties
559
For each i ∈I, we have the canonical injection map ji : Mi →M, which sends m ∈Mi
to the function x ∈M such that x(i) = m and x(k) = 0Mk for all k ̸= i in I. Informally,
ji(m) is the I-tuple with m in position i and zeroes in all other positions. For all m ∈Mi
and i, k ∈I, let δk,im denote m if k = i and 0Mk if k ̸= i. With this notation, we can write
ji(m) = (δk,im : k ∈I) for m ∈Mi. Each ji is an R-homomorphism, since for m, n ∈Mi
and r ∈R,
ji(m + n) = (δk,i(m + n) : k ∈I) = (δk,im : k ∈I) + (δk,in : k ∈I) = ji(m) + ji(n);
ji(rm) = (δk,i(rm) : k ∈I) = r(δk,im : k ∈I) = rji(m).
Next, we claim that any x ∈M = L
i∈I Mi can be written as follows:
x =
X
i∈I
ji(x(i)).
The right side is a sum of elements of M, in which all but finitely many summands ji(x(i))
are zero, since all but finitely many of the elements x(i) are zero. To verify the claim that
the two functions x and P
i∈I ji(x(i)) are equal, we evaluate each of them at an arbitrary
k ∈I:
"X
i∈I
ji(x(i))
#
(k) =
X
i∈I
ji(x(i))(k) =
X
i∈I
δk,ix(i) = x(k).
Now we are ready to state the UMP satisfied by the family of injection maps {ji : i ∈I}.
1.
UMP for Direct Sums of Modules (Diagram Completion Formulation): Suppose
Q is any left R-module, and for each i ∈I we are given an R-homomorphism
fi : Mi →Q. There exists a unique R-homomorphism f
: L
i∈I Mi
→Q
satisfying fi = f ◦ji for all i, meaning that these diagrams commute for all
i ∈I:
Mi
fi
$
ji / L
i∈I Mi
f

Q
2.
UMP for Direct Sums of Modules (Bijective Formulation): For any left R-module
Q, there is a bijection from the collection
A = HomR
 M
i∈I
Mi, Q
!
=
(
R-module homomorphisms f :
M
i∈I
Mi →Q
)
onto the collection
B =
Y
i∈I
HomR(Mi, Q) = {I-tuples (fi : i ∈I) of R-homomorphisms fi : Mi →Q}.
The bijection sends f ∈A to (f ◦ji : i ∈I) ∈B. The inverse bijection sends
(fi : i ∈I) ∈B to the unique R-module homomorphism f : L
i∈I Mi →Q such
that fi = f ◦ji for all i ∈I.
We begin by proving uniqueness of f in the diagram completion version of the UMP.
Fix Q and the R-maps fi : Mi →Q for i ∈I. Suppose f : M = L
i∈I Mi →Q is any
560
Advanced Linear Algebra
R-homomorphism such that f ◦ji = fi for all i. Take any function x ∈M, and write
x = P
i∈I ji(x(i)), as above. We must have
f(x)
=
f
 X
i∈I
ji(x(i))
!
=
X
i∈I
f(ji(x(i)))
=
X
i∈I
(f ◦ji)(x(i)) =
X
i∈I
fi(x(i)).
This proves that f is uniquely determined on M by the fi, if f exists at all. (Note that all
sums written here and below are really finite sums, since we disregard all zero summands.
The calculations in this proof would not make sense for infinite direct products.)
To prove existence of f, define f(x) = P
i∈I fi(x(i)) for all x ∈M. This sum is a finite
sum of elements of Q, since x is an element of the direct sum of the Mi, so f : M →Q
is a well-defined function. To confirm that f ◦jk = fk for fixed k ∈I, let us check that
these functions agree at each y ∈Mk. First, (f ◦jk)(y) = f(jk(y)) = P
i∈I fi(jk(y)(i)).
Now, jk(y)(i) = 0 if i ̸= k, while jk(y)(k) = y. Therefore, the sum has at most one nonzero
summand, corresponding to i = k, and (f ◦jk)(y) = fk(jk(y)(k)) = fk(y). It remains to
check that f : M →Q is an R-module homomorphism. Let x, y ∈M and r ∈R, and
calculate:
f(x + y)
=
X
i∈I
fi((x + y)(i)) =
X
i∈I
fi(x(i) + y(i))
=
X
i∈I
[fi(x(i)) + fi(y(i))] =
X
i∈I
fi(x(i)) +
X
i∈I
fi(y(i)) = f(x) + f(y);
f(rx)
=
X
i∈I
fi((rx)(i)) =
X
i∈I
fi(r(x(i)))
=
X
i∈I
rfi(x(i)) = r
X
i∈I
fi(x(i)) = rf(x).
For the second version of the UMP, define a function ϕ with domain A by setting
ϕ(f) = (f ◦ji : i ∈I) for f ∈A. For each i ∈I, f ◦ji is an R-homomorphism from Mi to
Q, as required. So ϕ does map into B, and the first version of the UMP shows that ϕ is a
bijection.
Let I and K be index sets and Mi (for i ∈I) and Nk (for k ∈K) be left R-modules.
By combining the bijections discussed in this section and the previous one, we obtain a
bijection
HomR
 M
i∈I
Mi,
Y
k∈K
Nk
!
→
Y
i∈I
Y
k∈K
HomR(Mi, Nk)
that maps an R-module homomorphism g : L
i∈I Mi →Q
k∈K Nk to the tuple
(pk ◦g ◦ji : i ∈I, k ∈K),
where the pk are the canonical projections of Q
k Nk and the ji are the canonical injections
of L
i∈I Mi. Note that pk ◦g ◦ji is an R-map from Mi to Nk. Given R-maps gi,k : Mi →Nk
for all i ∈I and k ∈K, the inverse bijection maps the tuple (gi,k : i ∈I, k ∈K) to the
R-homomorphism from L
i∈I Mi to Q
k∈K Nk such that
(xi : i ∈I) maps to
 X
i∈I
gi,k(xi) : k ∈K
!
.
Introduction to Universal Mapping Properties
561
This function can also be written
 
x 7→
 X
i∈I
gi,k(p′
i(x)) : k ∈K
!
: x ∈
M
i∈I
Mi
!
,
where p′
i denotes the canonical projection of L
j∈I Mj onto Mi.
19.7
Solving Universal Mapping Problems
In this chapter, we have analyzed some basic algebraic constructions for modules and
discovered the universal mapping properties (UMPs) of these constructions. The next
chapter adopts the opposite point of view: we start by specifying some universal mapping
problem (also abbreviated UMP), and we then seek to construct a new object and maps
that solve this problem. If we succeed in finding a solution to the UMP, then we may inquire
to what extent our solution is unique.
For example, our discussion of direct sums (coproducts) of R-modules suggests the
analogous universal mapping problem for sets:
Problem (Coproducts for Sets). Given a family of sets {Si : i ∈I}, construct a set S
and maps ji : Si →S satisfying the following UMP: for any set T and any collection of
functions gi : Si →T, there exists a unique function g : S →T with gi = g ◦ji for all i ∈I.
Si
ji
/
gi

S
g

T
An equivalent formulation of this problem is to construct a set S and maps ji : Si →S
such that, for any set T, there is a bijection from the set
A = {all functions g : S →T}
onto the set
B = {families of functions (gi : i ∈I) where gi : Si →T}
given by g 7→(g ◦ji : i ∈I).
Note that we cannot form the direct sum of the Si, since the Si are sets, not R-modules.
So a modification of our construction for R-modules is required. Here is one possible solution.
Construction of Solution to the UMP. Let S be the disjoint union of the Si. More
formally, define
S = {(i, x) : i ∈I, x ∈Si}.
For i ∈I, define the function ji : Si →S by ji(x) = (i, x) for all x ∈Si. We must verify
that S and the maps ji do have the necessary universal mapping property. Assume T and
gi : Si →T are given. To prove uniqueness, consider any function g : S →T satisfying
gi = g ◦ji for all i ∈I. For i ∈I and x ∈Si, we then have g((i, x)) = g(ji(x)) = gi(x).
Thus, the value of g at every (i, x) ∈S is completely determined by the given gi. So g is
unique if it exists at all.
562
Advanced Linear Algebra
To prove existence, we must define g((i, x)) = gi(x) for all (i, x) ∈S. It is immediate that
g : S →T is a well-defined function such that gi = g ◦ji for all i ∈I. The bijective version
of the UMP follows, as in earlier proofs, once we note that the function g 7→(g ◦ji : i ∈I)
does indeed map the domain A into the claimed codomain B.
We have now solved the UMP posed above. But is our solution unique? Certainly not —
we can always change notation to obtain superficially different solutions to the UMP. For
instance, we could have defined S = {(x, i) : i ∈I, x ∈Si} and ji(x) = (x, i) for i ∈I and
x ∈Si. On the other hand, we claim our solution is unique up to a unique isomorphism
compatible with the UMP. In the case at hand, this means that for any solution (S′, j′
i)
to the UMP, there exists a unique bijection g : S →S′ such that j′
i = g ◦ji for all i ∈I.
Briefly, although ungrammatically, we say that the solution (S, ji) to the UMP is essentially
unique.
Proof of Essential Uniqueness of Solution. Suppose (S′, j′
i) also solves the UMP.
Applying the universal mapping property of (S, ji) to the set T = S′ and the family of
maps gi = j′
i, we conclude that there exists a unique function g : S →S′ with j′
i = g ◦ji
for all i ∈I, meaning that the following diagram commutes for all i ∈I:
Si
ji
/
j′
i
 
S
g

S′
To complete the proof of essential uniqueness, we need only show that g is a bijection.
To do so, we can use the universal mapping property of (S′, j′
i) to construct a candidate for
the inverse of g. Specifically, let T = S and gi = ji in the UMP for (S′, j′
i). The UMP says
that there exists a unique function g′ : S′ →S with ji = g′ ◦j′
i for all i ∈I, so that this
diagram commutes for all i ∈I:
Si
j′
i
/
ji
 
S′
g′

S
Now, idS ◦ji = ji = g′ ◦j′
i = (g′ ◦g)◦ji. Thus, h = idS and h = g′ ◦g are two functions from
S to S with the property that h ◦ji = ji for all i ∈I. But according to the UMP for (S, ji)
(with T = S and gi = ji), there is a unique map h : S →S with this property. Therefore,
g′ ◦g = idS. Visually, we are invoking the fact that exactly one map h makes the following
diagram commute for all i ∈I:
Si
ji
/
ji

S
h

S
Similarly, idS′ ◦j′
i = j′
i = g◦ji = (g◦g′)◦j′
i. So, h = idS′ and h = g◦g′ are two functions
from S′ to S′ such that h ◦j′
i = j′
i for all i ∈I (see the diagram below). But according to
the UMP for (S′, j′
i) (with T = S′ and gi = j′
i), there is a unique map h : S′ →S′ with this
property.
Si
j′
i
/
j′
i
 
S′
h

S′
Introduction to Universal Mapping Properties
563
Therefore, g ◦g′ = idS′. We now see that g′ is the two-sided inverse of g, so both functions
are bijections.
The next chapter further develops the ideas presented here by posing and solving some
universal mapping problems that appear at the foundations of multilinear algebra. For each
UMP, we give an explicit construction showing that a solution to the UMP does exist. In
each case, a proof completely analogous to the one just given proves that our solution is
essentially unique, up to a unique isomorphism compatible with the universal maps. Once
the universal mapping properties are available, we can use them to derive the fundamental
facts about the algebraic structures occurring in multilinear algebra.
19.8
Summary
Here, we summarize the universal mapping properties discussed in this chapter. We state
each result as a diagram completion property and as a bijection between appropriate
collections of functions.
1.
UMP for Basis of a Finite-Dimensional Vector Space. Let X = {x1, . . . , xn} be
a basis of the vector space V over the field F and i : X →V be the inclusion
map. For each F-vector space W, composition with i gives a bijection from the
set HomF (V, W) of all F-linear maps T : V →W onto the set of all functions
f : X →W. So, for each function f : X →W there exists a unique F-linear map
T : V →W extending f (meaning f = T ◦i):
X
i
/
f
 
V
T

W
Explicitly, T(Pn
i=1 cixi) = Pn
i=1 cif(xi) for ci ∈F.
2.
UMP for Basis of a Free Module. Let R be a ring, M be a free left R-module
with basis X, and i : X →M be the inclusion map. For each left R-module N,
composition with i gives a bijection from the set HomR(M, N) of all R-linear
maps T : M →N onto the set of all functions f : X →N. So, for each function
f : X →N there exists a unique R-linear map T : M →N extending f (meaning
f = T ◦i):
X
i
/
f
 
M
T

N
Explicitly, T(P
x∈X cxx) = P
x∈X cxf(x) for all cx ∈R such that only finitely
many cx are nonzero.
3.
UMP for Quotient Modules. Let R be a ring, M be a left R-module, N be
a submodule of M, and p : M →M/N be the canonical projection map.
For each left R-module Q, composition with p gives a bijection from the set
HomR(M/N, Q) of all R-linear maps f ′ : M/N →Q onto the set of those R-
linear maps f : M →Q satisfying f(z) = 0 for all z ∈N. So, for each R-linear
564
Advanced Linear Algebra
map f on M that sends all of N to zero, there exists a unique lifting of f to an
R-linear map f ′ on M/N (meaning f = f ′ ◦p):
M
p /
f
"
M/N
f ′

Q
Explicitly, f ′(x + N) = f(x) for all x ∈M. Moreover, img(f ′) = img(f) and
ker(f ′) = ker(f)/N.
4.
UMP for Direct Product of Two Modules. Let R be a ring, M and N be left
R-modules, and p : M × N →M and q : M × N →N be the canonical
projections. For each left R-module Q, the map h 7→(p ◦h, q ◦h) is a bijection
from HomR(Q, M × N) onto HomR(Q, M) × HomR(Q, N). So, for each pair of
R-linear maps (f, g) with f : Q →M and g : Q →N, there exists a unique
R-linear h : Q →M × N with f = p ◦h and g = q ◦h:
M
M × N
p
o
q
/ N
Q
f
c
g
;
h
O
Explicitly, h(x) = (f(x), g(x)) for x ∈Q.
5.
UMP for Direct Sum of Two Modules. Let R be a ring, M and N be left R-
modules, and i : M →M ⊕N and j : N →M ⊕N be the canonical injections.
For each left R-module Q, the map h 7→(h◦i, h◦j) is a bijection from HomR(M ⊕
N, Q) onto HomR(M, Q)×HomR(N, Q). So, for each pair of R-linear maps (f, g)
with f : M →Q and g : N →Q, there exists a unique R-linear h : M ⊕N →Q
with f = h ◦i and g = h ◦j:
M
i /
f
#
M ⊕N
h

N
jo
g
{
Q
Explicitly, h((m, n)) = f(m) + g(n) for m ∈M and n ∈N.
6.
UMP for Direct Product of a Family of Modules. Let R be a ring, I be an index
set, Mi be a left R-module for each i ∈I, and pi : Q
j∈I Mj →Mi be the
canonical projections. For each left R-module Q, the map f 7→(pi ◦f : i ∈I) is a
bijection from HomR(Q, Q
j∈I Mj) onto Q
j∈I HomR(Q, Mj). So, for each family
of R-linear maps (fi : i ∈I) with fi : Q →Mi for all i ∈I, there exists a unique
R-linear f : Q →Q
j∈I Mj with fi = pi ◦f for all i ∈I:
Mi
Q
j∈I Mj
pi
o
Q
fi
c
f
O
Explicitly, f(x) = (fi(x) : i ∈I) for x ∈Q.
Introduction to Universal Mapping Properties
565
7.
UMP for Direct Sum of a Family of Modules. Let R be a ring, I be an index
set, Mi be a left R-module for each i ∈I, and ji : Mi →L
k∈I Mk be the
canonical injections. For each left R-module Q, the map f 7→(f ◦ji : i ∈I) is a
bijection from HomR(L
k∈I Mk, Q) onto Q
k∈I HomR(Mk, Q). So, for each family
of R-linear maps (fi : i ∈I) with fi : Mi →Q for all i ∈I, there exists a unique
R-linear f : L
k∈I Mk →Q with fi = f ◦ji for all i ∈I:
Mi
fi
$
ji / L
k∈I Mk
f

Q
Explicitly, f(x) = P
i∈I fi(xi) for x = (xi : i ∈I) ∈L
k∈I Mk.
8.
Combining the last two items, we have a bijection
HomR
 M
i∈I
Mi,
Y
k∈K
Nk
!
−→
Y
i∈I
Y
k∈K
HomR(Mi, Nk)
that sends an R-map g : L
i∈I Mi →Q
k∈K Nk to (pk ◦g ◦ji : i ∈I, k ∈K).
9.
UMP for Coproduct of Sets. Let I be an index set and Si be a set for each i ∈I.
Define S = {(i, x) : i ∈I, x ∈Si} and define ji : Si →S by ji(x) = (i, x) for i ∈I
and x ∈Si. For each set T, the map f 7→(f ◦ji : i ∈I) is a bijection from the
set of functions from S to T to the set of families (fi : i ∈I) where fi : Si →T
for i ∈I. So, for each family (fi : i ∈I) of functions from Si to T, there exists a
unique f : S →T with fi = f ◦ji for all i ∈I:
Si
fi

ji
/ S
f

T
Explicitly, f((i, x)) = fi(x) for i ∈I and x ∈Si.
The solution of a universal mapping problem is essentially unique, meaning that for
any two solutions to a UMP, there is a unique bijection between the underlying sets that
respects the associated universal maps.
19.9
Exercises
In these exercises, assume R is a ring unless otherwise stated.
1.
Let V and W be nonzero finite-dimensional vector spaces over a field F. Let
X = {x1, . . . , xn} be a subset of V . Assume X spans V but is linearly dependent
over F. (a) Prove or disprove: for every function f : X →W, there exists an
F-linear map T : V →W extending f. (b) Prove or disprove: for all f : X →W,
there is at most one F-linear map T : V →W extending f.
2.
Repeat Exercise 1, but now assume X is a linearly independent subset of V that
does not span V .
566
Advanced Linear Algebra
3.
Give complete details of the proof of the UMP for bases of a free R-module stated
in §19.1. Do not assume the basis X is finite.
4.
UMP for Quotient Groups. Let (G, ⋆) be a group with normal subgroup N.
Let p : G →G/N be the projection p(x) = x ⋆N for x ∈G. (a) Prove: for every
group L and every group homomorphism f : G →L such that f(x) = eL for
all x ∈N, there exists a unique group homomorphism f ′ : G/N →L such that
f = f ′ ◦p. What are img(f ′) and ker(f ′)? (b) Restate (a) in terms of a bijection
between two collections of functions.
5.
UMP for Quotient Rings. Let (R, +, ·) be a ring with ideal I. Let p : R →R/I
be the projection p(x) = x+I for x ∈R. Formulate and prove a universal mapping
property characterizing the quotient ring R/I and the map p : R →R/I.
6.
Let M and N be left R-modules. Check carefully that the canonical projections
and injections for M×N = M⊕N are R-linear and satisfy the following identities:
p ◦i = idM,
q ◦j = idN,
q ◦i = 0,
p ◦j = 0,
i ◦p + j ◦q = idM⊕N .
7.
Assume R is a commutative ring. Prove that the bijection
ϕ : HomR(Q, M × N) →HomR(Q, M) × HomR(Q, N)
constructed in §19.3 is an R-linear map.
8.
Assume R is commutative. Prove that the bijection
ϕ : HomR(M × N, Q) →HomR(M, Q) × HomR(N, Q)
constructed in §19.4 is an R-linear map.
9.
Assume R is commutative. Prove HomR(Q, Q
i∈I Mi) and Q
i∈I HomR(Q, Mi) are
isomorphic R-modules, via the bijection in §19.5.
10.
Assume R is commutative. Prove HomR(L
i∈I Mi, Q) and Q
i∈I HomR(Mi, Q)
are isomorphic R-modules, via the bijection in §19.6.
11.
For fixed k > 0, let M1, M2, . . . , Mk be left R-modules. Prove that for all
left R-modules Q, there is a bijection from HomR(Q, M1 × M2 × · · · × Mk) to
HomR(Q, M1) × HomR(Q, M2) × · · · × HomR(Q, Mk)
(a) by using induction on k and the bijections in §19.3;
(b) by imitating the construction in §19.3.
12.
For fixed k > 0, let M1, M2, . . . , Mk be left R-modules. Prove that for all
left R-modules Q, there is a bijection from HomR(M1 ⊕M2 ⊕· · · ⊕Mk, Q) to
HomR(M1, Q) × HomR(M2, Q) × · · · × HomR(Mk, Q)
(a) by using induction on k and the bijections in §19.4;
(b) by imitating the construction in §19.4.
13.
Assume R is a commutative ring. With the setup in §19.1, show that the bijection
T 7→T ◦i is an R-module isomorphism between the R-module HomR(M, N) and
the product R-module N X.
14.
With the setup in §19.5, let f : Q →Q
i∈I Mi be the R-map corresponding to a
given family of R-maps fi : Q →Mi. Prove ker(f) = T
i∈I ker(fi).
15.
With the setup in §19.6, let f : L
i∈I Mi →Q be the R-map corresponding to a
given family of R-maps fi : Mi →Q. Prove img(f) = P
i∈I img(fi).
Introduction to Universal Mapping Properties
567
16.
UMP for Products of Sets. Given an index set I and a set Si for each i ∈I,
formulate and prove a universal mapping property satisfied by the Cartesian
product set S = Q
i∈I Si and the canonical projection functions pi : S →Si given
by pi((xk : k ∈I)) = xi for each i ∈I.
17.
UMP for Products of Groups. Given a family of groups {Gi : i ∈I} (not
necessarily commutative), construct a group G and group homomorphisms pi :
G →Gi (for all i ∈I) such that, for any group K, there is a bijection from
the set A of all group homomorphisms f : K →G onto the set B of all families
of group homomorphisms (fi : i ∈I) with fi : K →Gi for all i ∈I, given by
f 7→(pi ◦f : i ∈I) for f ∈A.
18.
UMP for Products of Rings. Repeat the previous exercise, replacing groups
and group homomorphisms by rings and ring homomorphisms.
19.
In §19.4, we showed that for any two left R-modules M and N, the direct sum
M ⊕N and the canonical injections i : M →M ⊕N and j : N →M ⊕N
satisfy the UMP for the coproduct of two modules. (a) Does this construction
of the coproduct still work if we assume M and N are groups (possibly non-
commutative), and demand that all maps be group homomorphisms? (b) Does
this construction of the coproduct still work if we assume M and N are rings
and demand that all maps be ring homomorphisms? What if we only allow
commutative rings?
20.
Let M be a left R-module with submodule N, and suppose there is a left R-
module Z and a map q : M →Z such that q[N] = {0Z}, and Z and q satisfy the
UMP for quotient modules from §19.2. Prove that there exists a unique R-module
isomorphism g : M/N →Z with q = g ◦p, where p : M →M/N is the canonical
projection. (Imitate the essential uniqueness proof in §19.7.)
21.
Give a specific example using Z-modules to show that the result of Exercise 20
might fail without the hypothesis that q[N] = {0Z}.
22.
Let {Mi : i ∈I} be an indexed family of left R-modules. Carefully state what
it means to say that the direct product Q
i∈I Mi and the associated projection
maps are essentially unique, and then prove it.
23.
Let {Mi : i ∈I} be an indexed family of left R-modules. Carefully state what it
means to say that the direct sum L
i∈I Mi and the associated injection maps are
essentially unique, and then prove it.
24.
Given left R-modules M and N, prove that M × N ∼= N × M using the fact that
both modules solve the same universal mapping problem. Then find a formula
for the unique isomorphism compatible with the canonical injections.
25.
We are given an index set I, left R-modules Mi and Ni for each i ∈I, and an R-
map fi : Mi →Ni for each i ∈I. Let pi : Q
k∈I Mk →Mi and qi : Q
k∈I Nk →Ni
be the canonical projection maps, for each i ∈I.
(a) Use the UMP for direct products to show there exists a unique R-module
homomorphism F : Q
k∈I Mk →Q
k∈I Nk such that qi ◦F = fi ◦pi for all i ∈I.
(b) Find an explicit formula for F, and describe ker(F) and img(F).
(c) Write F = F(fi : i ∈I) = F(fi) to indicate the dependence of F on the
given maps fi. Suppose Pi is a left R-module and gi : Ni →Pi is an R-map, for
all i ∈I. Let ri : Q
k∈I Pk →Pi be the canonical projection maps. Prove that
F(gi ◦fi) = F(gi)◦F(fi) in two ways: using the explicit formula in (b), and using
the uniqueness of F proved in (a).
568
Advanced Linear Algebra
26.
With the setup in Exercise 25, state and prove results analogous to (a), (b), and
(c) of that exercise for a map G : L
k∈I Mk →L
k∈I Nk induced by the fi and
compatible with the canonical injections.
27.
Let M, N, and P be free left R-modules with bases X, Y , and Z and inclusion
maps i : X →M, j : Y →N, and k : Z →P.
(a) Show that for each function f : X →Y , there is a unique R-homomorphism
F(f) : M →N with F(f) ◦i = j ◦f. Do not find a formula for F(f).
(b) Use (a) (and similar results for functions from Y to Z, etc.) to show that for
all functions f : X →Y and g : Y →Z, F(g ◦f) = F(g) ◦F(f).
(c) Use (a) to show that F(idX) = idM. [In the language of category theory, F is a
functor from the category of sets and functions to the category of left R-modules
and R-maps.]
28.
UMP for Quotient Sets. Let X be a set, ∼be an equivalence relation on
X, and X/∼be the set of all equivalence classes of ∼. Call a function f with
domain X compatible with ∼iff for all x, y ∈X, x ∼y implies f(x) = f(y).
Let p : X →X/∼be the map that sends each x ∈X to its equivalence class
[x] relative to ∼, given by [x] = {z ∈X : x ∼z}. Note that p is surjective and
compatible with ∼. (a) Prove that for every set Z, the map h 7→h ◦p defines
a bijection from the set A of all functions h : X/ ∼→Z to the set B of all
functions f : X →Z that are compatible with ∼. (b) Prove that X/∼(and the
map p, compatible with ∼) is the essentially unique solution to the UMP in (a).
(c) Explain how quotient modules are a special case of the construction in this
problem (cf. §19.2).
29.
UMP for Quotient Topologies. A topological space is a set X and a family
of subsets of X, called open sets, such that ∅and X are open; the union of any
collection of open sets is open; and the intersection of finitely many open sets is
open. A function f : X →Y between two such spaces is continuous iff for all open
subsets V of Y , f −1[V ] is an open subset of X. Let X be a topological space and ∼
be an equivalence relation on X. Define X/∼and p : X →X/∼as in Exercise 28.
Define V ⊆X/∼to be an open set iff p−1[V ] ⊆X is open in the given topological
space X. (a) Show that this definition makes X/∼into a topological space and
p into a continuous surjective map. (b) Prove that for every topological space Z,
the map h 7→h ◦p defines a bijection from the set of all continuous functions
h : X/∼→Z to the set of all continuous functions f : X →Z that are compatible
with ∼. (Explain why it suffices, using Exercise 28(a), to show that h is continuous
if and only if h ◦p is continuous.)
30.
Products of Topological Spaces. Let {Xi : i ∈I} be a family of topological
spaces. Construct a product of the spaces Xi satisfying a universal mapping
property analogous to the product of modules (all functions are required to be
continuous here).
31.
Coproducts of Topological Spaces. Let {Xi : i ∈I} be a family of topological
spaces. Construct a coproduct of the spaces Xi satisfying a universal mapping
property analogous to direct sums of modules and coproducts of sets (all functions
considered must be continuous here).
32.
UMP for Polynomial Rings. Let R be a commutative ring and i : R →R[x]
be given by i(r) = (r, 0, 0, . . .) for r in R (meaning that i maps r to the constant
polynomial with constant term r). Prove: for each commutative ring S, the map
H 7→(H ◦i, H(x)) is a bijection from the set of ring homomorphisms H : R[x] →
Introduction to Universal Mapping Properties
569
S to the set of pairs (h, c), where h : R →S is a ring homomorphism and c ∈S
(cf. §3.5).
33.
Fix a positive integer m. Generalize the previous exercise to obtain a bijective
formulation of the UMP for the polynomial ring R[x1, . . . , xm] (cf. §3.21).
34.
Localization of a Commutative Ring. For any commutative ring T, we say
t ∈T is a unit of T iff there exists u ∈T with tu = 1T = ut. Let T ∗be the set of
units of T. The goal of this exercise is to solve the following universal mapping
problem. Let R be a commutative ring and S be a subset of R that contains 1R
and is closed under the multiplication of R. Construct a commutative ring L and
a ring homomorphism i : R →L such that i(s) ∈L∗for all s ∈S; and for all
commutative rings T, the map g 7→g ◦i defines a bijection from the set of all ring
homomorphisms g : L →T onto the set of all ring homomorphisms f : R →T
such that f(s) ∈T ∗for all s ∈S. The diagram below visualizes what is needed:
R
f

i
/ L
g

(i[S] ⊆L∗)
T
(f[S] ⊆T ∗)
Intuitively, we need to build a ring L in which all elements of S become invertible,
such that L is as close to R as possible. The idea is to use fractions r/s, with
r ∈R and s ∈S, as the elements of L. Proceed formally as follows.
(a) Let X = R × S = {(r, s) : r ∈R, s ∈S}. Define a binary relation ∼on X by
setting (r, s) ∼(r′, s′) iff there exists t ∈S with t(rs′ −sr′) = 0. Check that ∼is
an equivalence relation on X.
(b) Let L be the set of equivalence classes of ∼on X and r/s be the equivalence
class of (r, s) ∈X. For r/s and u/v in L, define r/s + u/v = (rv + su)/(sv) and
(r/s) · (u/v) = (ru)/(sv). Check that these two operations are well-defined.
(c) Verify that (L, +, ·) is a commutative ring.
(d) Define i : R →L by setting i(r) = r/1R for all r ∈R. Check that i is a ring
homomorphism, and i(s) ∈L∗for all s ∈S.
(e) Verify that L and i solve the UMP described above.
(f) Explain why L and i are essentially unique.
(g) Explain why the construction of Q from Z is a special case of the construction
in this exercise.
35.
Define an inverse chain to be a collection of left R-modules (Mn : n ≥0) and
R-maps fn : Mn →Mn−1 for n > 0:
M0
f1
←−M1
f2
←−M2
f3
←−· · ·
fn−1
←−Mn−1
fn
←−Mn
fn+1
←−· · · .
Solve the following universal mapping problem: construct a left R-module L and
R-maps pn : L →Mn for n ≥0 with fn ◦pn = pn−1 for all n > 0, such that for
any left R-module P and R-maps gn : P →Mn (for n ≥0) with fn ◦gn = gn−1
for all n > 0, there exists a unique R-map h : P →L with pn ◦h = gn for all
n ≥0. L is called the inverse limit of the inverse chain. (Define L to be a certain
submodule of the direct product Q
n≥0 Mn.)
36.
Define a direct chain to be a collection of left R-modules (Mn : n ≥0) and
R-maps fn : Mn →Mn+1 for n ≥0:
M0
f0
−→M1
f1
−→M2
f2
−→· · ·
fn−1
−→Mn
fn
−→Mn+1
fn+1
−→· · · .
570
Advanced Linear Algebra
Solve the following universal mapping problem: construct a left R-module D and
R-maps jn : Mn →D for n ≥0 with jn+1 ◦fn = jn for all n ≥0, such that for
any left R-module P and R-maps gn : Mn →P (n ≥0) with gn+1 ◦fn = gn
for all n ≥0, there exists a unique R-map h : D →P with h ◦jn = gn for all
n ≥0. D is called the direct limit of the direct chain. (Define D to be a certain
quotient module of the direct sum L
n≥0 Mn. Use known UMPs for direct sums
and quotient modules.)
37.
Free Monoid Generated by a Set. A monoid is a pair (M, ⋆) satisfying the
first three group axioms in Table 1.1 (closure, associativity, and identity). Given
monoids M and N, a monoid homomorphism is a map f : M →N such that
f(xy) = f(x)f(y) for all x, y ∈M, and f(1M) = 1N. Given any set X, our goal is
to solve the following universal mapping problem (cf. §19.1): construct a monoid
M and a function i : X →M such that for any monoid N and any function
f : X →N, there exists a unique monoid homomorphism T : M →N with
f = T ◦i. (a) Let M be the set of all finite sequences w = w1w2 · · · wk (called
words) where k ≥0 and each wi ∈X. Note that the empty sequence, denoted ϵ,
is in M. Given w = w1w2 · · · wk and y = y1y2 · · · ym in M, define w ⋆y to be the
concatenation w1w2 · · · wky1y2 · · · ym. Show that (M, ⋆) is a monoid with identity
ϵ. (b) Define i : X →M by letting i(x) be the word (sequence) of length 1 with
sole entry x, for x ∈X. Prove that M and i solve the universal mapping problem
posed above.
38.
Coproduct of Two Groups. Given groups M and N, our goal is to construct
a group M ∗N and group homomorphisms i : M →M ∗N and j : N →M ∗N
satisfying an analog of the universal mapping problem in §19.4 (with modules and
R-maps replaced by groups and group homomorphisms). Let M ′ = M ∼{eM}
be the set of non-identity elements in M; let N ′ = N ∼{eN}. If needed, change
notation so that M ′ and N ′ are disjoint sets. Let the set M ∗N consist of all
words w1w2 · · · wk with k ≥0, wi ∈M ′ ∪N ′ for all i, and (for all i < k)
wi ∈M ′ iff wi+1 ∈N ′ (meaning that letters alternate between M ′ and N ′).
Given w = w1w2 · · · wk and y = y1y2 · · · ym in M ∗N, define w⋆y by the following
recursive rules. If w = ϵ (where ϵ is the empty word), then w ⋆y = y. If y = ϵ,
then w ⋆y = w. If wk ∈M ′ and y1 ∈N ′, or if wk ∈N ′ and y1 ∈M ′, then w ⋆y
is the concatenation w1w2 · · · wky1y2 · · · ym. If wk, y1 ∈M ′ and wky1 = z ̸= eM,
then w ⋆y = w1 · · · wk−1zy2 · · · ym. If wk, y1 ∈M ′ and wky1 = eM, we recursively
define w ⋆y = (w1 · · · wk−1) ⋆(y2 · · · ym). If wk, y1 ∈N ′ and wky1 = z ̸= eN,
then w ⋆y = w1 · · · wk−1zy2 · · · ym. If wk, y1 ∈N ′ and wky1 = eN, we recursively
define w ⋆y = (w1 · · · wk−1) ⋆(y2 · · · ym).
(a) Prove (M ∗N, ⋆) is a group. (The verification of associativity is tricky.)
(b) Define injective group homomorphisms i : M →M ∗N and j : N →M ∗N,
and prove that M ∗N with these maps solves the UMP posed above.
39.
Generalize the construction in the previous exercise by defining a coproduct of a
family of groups {Gi : i ∈I} satisfying a UMP analogous to the one in §19.6.
40.
Free Group Generated by a Set X. Given any set X, construct a group (G, ⋆)
and a function i : X →G such that for any group K and any function f : X →K,
there exists a unique group homomorphism T : G →K with f = T ◦i. (Ideas
from the previous three exercises can help here. Let G consist of certain words
in the alphabet X ∪X′, where X′ is a set with |X| = |X′| and X ∩X′ = ∅; the
elements of X′ represent formal inverses of elements of X.)
20
Universal Mapping Problems in Multilinear Algebra
This chapter introduces a subject called multilinear algebra, which studies functions of
several variables that are linear in each variable. After defining these multilinear maps, as
well as alternating maps and symmetric maps, we formulate and solve universal mapping
problems that convert these less familiar maps to linear maps. The modules that arise in
these constructions are called tensor products, exterior powers, and symmetric powers.
After building the tensor product, we use its universal mapping property to prove some
isomorphisms and other general facts about tensor products. We show that linear maps
between modules induce associated linear maps on tensor products, exterior powers, and
symmetric powers. In the case of free modules, we find bases for these new modules in terms
of bases for the original modules. This leads to a discussion of tensor products of matrices
and the relation between determinants and exterior powers. The chapter ends with the
construction of the tensor algebra of a module. Throughout the whole development, we
stress the use of universal mapping properties as a means of organizing, motivating, and
proving the basic results of multilinear algebra.
To read this chapter, you need to know facts about modules and universal mapping
properties covered in Chapters 17 and 19, as well as properties of permutations from
Chapter 2.
20.1
Multilinear Maps
Throughout this chapter, let R be a fixed commutative ring. Given R-modules M1, . . . , Mn,
consider the product R-module M = M1 × · · · × Mn. We distinguish two special types of
maps from M to another R-module N. On one hand, recall that f : M →N is an R-module
homomorphism or an R-linear map iff f(m + m′) = f(m) + f(m′) and f(rm) = rf(m) for
all m, m′ ∈M and all r ∈R. Writing this condition in terms of components, R-linearity of
f means that for all mk, m′
k ∈Mk and all r ∈R,
f(m1 + m′
1, m2 + m′
2, . . . , mn + m′
n) = f(m1, m2, . . . , mn) + f(m′
1, m′
2, . . . , m′
n)
and f(rm1, rm2, . . . , rmn) = rf(m1, m2, . . . , mn).
On the other hand, define f : M1 × · · · × Mn →N to be R-multilinear iff f is R-linear in
each of its n inputs separately. More precisely, for each i between 1 and n and each fixed
choice of m1, . . . , mi−1, mi+1, . . . , mn, we require that
f(m1, . . . , mi−1, mi +m′
i, mi+1, . . . , mn) = f(m1, . . . , mi, . . . , mn)+f(m1, . . . , m′
i, . . . , mn),
(20.1)
f(m1, . . . , mi−1, rmi, mi+1, . . . , mn) = rf(m1, . . . , mi, . . . , mn)
(20.2)
for all mi, m′
i ∈Mi and all r ∈R. When n = 2, we say f is R-bilinear; when n = 3, we say
f is R-trilinear; when R is understood, we may speak of n-linear maps.
DOI: 10.1201/9781003484561-20
571
572
Advanced Linear Algebra
Let f : M1 × · · · × Mn →N be R-multilinear. By induction on s, we see that
f

m1, . . . , mi−1,
s
X
j=1
rjxj, mi+1, . . . , mn

=
s
X
j=1
rjf(m1, . . . , xj, . . . , mn)
(20.3)
for each i, where mk ∈Mk, r1, . . . , rs ∈R, and x1, . . . , xs ∈Mi. Iterating this formula, we
obtain
f


s1
X
j1=1
r1,j1x1,j1,
s2
X
j2=1
r2,j2x2,j2, . . . ,
sn
X
jn=1
rn,jnxn,jn


=
s1
X
j1=1
s2
X
j2=1
· · ·
sn
X
jn=1
r1,j1r2,j2 · · · rn,jnf(x1,j1, . . . , xn,jn)
(20.4)
for all choices of rij ∈R and xij ∈Mi. Furthermore, if P is an R-module and g : N →
P is any R-linear map, then g ◦f : M1 × · · · × Mn →P is R-multilinear. This follows by
applying g to each side of (20.1) and (20.2).
20.2
Alternating Maps
For R-modules M and P, define a map f : M n →P to be alternating iff f is R-multilinear
and f(m1, . . . , mn) = 0 whenever mi = mj for some i ̸= j. The alternating condition is
related to the following anti-commutativity conditions on an R-multilinear map f:
(AC1)
f(m1, . . . , mi, mi+1, . . . , mn) = −f(m1, . . . , mi+1, mi, . . . , mn) for 1 ≤i < n and
all mk ∈M. In other words, interchanging two adjacent inputs of f multiplies
the output by −1.
(AC2)
f(m1, . . . , mi, . . . , mj, . . . , mn) = −f(m1, . . . , mj, . . . , mi, . . . , mn) for all i < j
and all mk ∈M. In other words, interchanging any two inputs of f multiplies the
output by −1.
(AC3)
f(mw(1), . . . , mw(n)) = sgn(w)f(m1, . . . , mn) for all permutations w ∈Sn and all
mk ∈M. In other words, rearranging the inputs of f according to the permutation
w multiplies the output by sgn(w).
We make the following claims regarding these conditions.
Claim 1: The conditions (AC1), (AC2), and (AC3) are equivalent. Proof: By letting w
be the transposition (i, j), which satisfies sgn((i, j)) = −1, we see that (AC3) implies (AC2).
Evidently (AC2) implies (AC1). To see that (AC1) implies (AC3), recall from Chapter 2 that
the list of inputs (mw(1), . . . , mw(n)) can be sorted into the list (m1, . . . , mn) using exactly
inv(w(1), . . . , w(n)) = inv(w) basic transposition moves, where a basic transposition move
switches two adjacent elements in a list. According to (AC1), each such move multiplies the
value of f by −1. Therefore,
f(m1, . . . , mn) = (−1)inv(w)f(mw(1), . . . , mw(n)) = sgn(w)f(mw(1), . . . , mw(n)).
Since sgn(w) is ±1, this relation is equivalent to (AC3). We say that an R-multilinear map
f : M n →P is anti-commutative iff the equivalent conditions (AC1), (AC2), and (AC3)
hold for f.
Universal Mapping Problems in Multilinear Algebra
573
Claim 2: The alternating condition implies all the anti-commutativity conditions. Proof:
Assume f is alternating; we show that condition (AC1) holds. Fix i < n, and fix the inputs
of f at positions different from i and i+ 1. For any x, y ∈M, the alternating property gives
f(. . . , x + y, x + y, . . .) = f(. . . , x, x, . . .) = f(. . . , y, y, . . .) = 0,
where the displayed inputs occur at positions i and i + 1. On the other hand, linearity of f
in input i and in input i + 1 shows that
f(. . . , x + y, x + y, . . .) = f(. . . , x, x + y, . . .) + f(. . . , y, x + y, . . .)
= f(. . . , x, x, . . .) + f(. . . , x, y, . . .) + f(. . . , y, x, . . .) + f(. . . , y, y, . . .).
(20.5)
Substituting zero in three places and rearranging, we get f(. . . , x, y, . . .) = −f(. . . , y, x, . . .),
as needed.
Claim 3: For rings R such that 1R + 1R is not zero and not a zero divisor, any of the
anti-commutativity conditions implies the alternating condition. (For instance, the result
holds when R is a field or integral domain such that 1R + 1R ̸= 0R.) Proof: We deduce
the alternating condition from condition (AC2). Suppose (m1, . . . , mn) ∈M n is such that
mi = mj where i < j. By (AC2),
f(m1, . . . , mi, . . . , mj, . . . , mn) = −f(m1, . . . , mj, . . . , mi, . . . , mn).
Since mi = mj, this relation gives
f(m1, . . . , mi, . . . , mj, . . . , mn) = −f(m1, . . . , mi, . . . , mj, . . . , mn).
Grouping terms, (1R + 1R)f(m1, . . . , mi, . . . , mj, . . . , mn) = 0 in R. By hypothesis on R, it
follows that f(m1, . . . , mn) = 0.
Claim 4: If g : P →Q is R-linear and f : M n →P is alternating (resp. anti-
commutative), then g ◦f : M n →Q is alternating (resp. anti-commutative). Proof:
This follows by applying g to each side of the identities defining the alternating or anti-
commutative properties.
20.3
Symmetric Maps
For R-modules M and P, we define a map f : M n →P to be symmetric iff f is R-
multilinear and f(m1, . . . , mn) = f(m′
1, . . . , m′
n) whenever the list (m1, . . . , mn) ∈M n is a
rearrangement of the list (m′
1, . . . , m′
n). It is equivalent to require that
f(m1, . . . , mi, . . . , mj, . . . , mn) = f(m1, . . . , mj, . . . , mi, . . . , mn)
for all i < j; i.e., the value of f is unchanged whenever two distinct inputs of f are
interchanged. It is also equivalent to require that
f(m1, . . . , mi, mi+1, . . . , mn) = f(m1, . . . , mi+1, mi, . . . , mn)
for 1 ≤i < n; i.e., the value of f is unchanged whenever two adjacent inputs of f are
interchanged. The equivalence of the conditions follows from the fact that an arbitrary
rearrangement of the list (m1, . . . , mn) can be accomplished by a finite sequence of
interchanges of two adjacent inputs (see §2.6).
If g : P →Q is R-linear and f : M n →P is symmetric, then g ◦f : M n →Q
is symmetric. This follows by applying g to each side of the identities f(m1, . . . , mn) =
f(m′
1, . . . , m′
n) in the definition of a symmetric map.
574
Advanced Linear Algebra
20.4
Tensor Product of Modules
Suppose P and M1, M2, . . . , Mn are R-modules, and let X = M1 × · · · × Mn. Recall the
distinction between R-linear maps from X to P and R-multilinear maps from X to P
(§20.1). It would be convenient if we could somehow reduce the study of multilinear maps
to the study of R-linear maps. This raises the following universal mapping problem.
Problem (UMP for Tensor Products). Given a commutative ring R and R-modules
M1, . . . , Mn, construct an R-module N and an R-multilinear map j : M1 × · · · × Mn →N
satisfying the following UMP: for any R-module P, there is a bijection from the set
A = {R-linear maps g : N →P}
onto the set
B = {R-multilinear maps f : M1 × · · · × Mn →P}
sending g to g ◦j for all g
∈
A. In other words, for each R-multilinear map
f : M1 × · · · × Mn →P, there exists a unique R-linear map g : N →P with f = g ◦j:
M1 × · · · × Mn
j
/
f
&
N
g

P
Construction of Solution to the UMP. We will define N to be the quotient of a certain
free R-module F by a certain submodule K. The main idea of the construction is to fit
together the diagrams describing two previously solved universal mapping problems, as
shown here and in Figure 20.1:
X
i
/
f
 
F
h

ν
/ N
g
~
P
FIGURE 20.1
Bijections between sets of maps used to construct tensor products.
Universal Mapping Problems in Multilinear Algebra
575
To begin the construction, let X be the set M1 ×· · ·×Mn, F be a free R-module with basis
X (see §17.11), and i : X →F be the inclusion map. Recall that each element of F can
be written uniquely as a finite R-linear combination c1x1 + · · · + ckxk, where cj ∈R and
xj ∈X. Also recall the universal mapping property of i (§19.1): for any R-module P, there
is a bijection α from the set
C′ = {all R-linear maps h : F →P}
onto the set
B′ = {all functions f : X →P},
given by α(h) = h ◦i for all h ∈C′.
Next, let K be the R-submodule of F generated by all elements of F of the form
1R(m1, . . . , mk−1, mk + m′
k, mk+1, . . . , mn)
−1R(m1, . . . , mk, . . . , mn) −1R(m1, . . . , m′
k, . . . , mn),
(20.6)
1R(m1, . . . , mk−1, rmk, mk+1, . . . , mn) −r(m1, . . . , mk, . . . , mn),
(20.7)
where 1 ≤k ≤n, mk, m′
k ∈Mk, ms ∈Ms for s ̸= k, and r ∈R. Let N be the R-module
F/K and ν : F →N be the projection map given by ν(z) = z + K for z ∈F. Recall the
universal mapping property of ν (§19.2): for any R-module P, there is a bijection β from
the set
A = {all R-linear maps g : N = F/K →P}
onto the set
C = {all R-linear maps h : F →P such that h(z) = 0 for all z ∈K},
given by β(g) = g ◦ν for all g ∈A.
Note that C ⊆C′. We claim that α[C] = B, the set of all R-multilinear maps from
X = M1 × · · · × Mn to P. Proof: An R-linear map h : F →P in C′ belongs to C iff
h[K] = {0} iff h maps every generator of the submodule K to zero iff
h(m1, . . . , mk−1, mk + m′
k, mk+1, . . . , mn)
−h(m1, . . . , mk, . . . , mn) −h(m1, . . . , m′
k, . . . , mn) = 0
and h(m1, . . . , mk−1, rmk, mk+1, . . . , mn) −rh(m1, . . . , mk, . . . , mn) = 0
for all choices of the variables iff
(h◦i)(m1, . . . , mk+m′
k, . . . , mn) = (h◦i)(m1, . . . , mk, . . . , mn)+(h◦i)(m1, . . . , m′
k, . . . , mn)
and (h ◦i)(m1, . . . , rmk, . . . , mn) = r(h ◦i)(m1, . . . , mk, . . . , mn)
for all choices of the variables iff α(h) = h ◦i : X →P is R-multilinear iff α(h) ∈B.
By the claim, the restriction α|C : C →B is a bijection sending h to h ◦i for h ∈C.
We also have the bijection β : A →C given by β(g) = g ◦ν for g ∈A. Composing these
bijections, we obtain a bijection γ from A to B given by γ(g) = g ◦(ν ◦i) for g ∈A. Letting
j = ν ◦i : X →N, we see that γ : A →B is given by composition with j. See Figure 20.1.
To summarize the construction, we have N = F/K, where F is the free R-module
with basis X = M1 × · · · × Mn and K is the R-submodule generated by all elements of
the form (20.6) and (20.7). The map j : X →N sends (m1, . . . , mn) ∈X to the coset
(m1, . . . , mn) + K in N. By the Coset Equality Theorem, we know that
(m1, . . . , mk +m′
k, . . . , mn)+K = [(m1, . . . , mk, . . . , mn)+K]+[(m1, . . . , m′
k, . . . , mn)+K],
(m1, . . . , rmk, . . . , mn) + K = r[(m1, . . . , mk, . . . , mn) + K],
576
Advanced Linear Algebra
and these observations show that j : X →N is an R-multilinear map from X to N, as
needed. We call N the tensor product over R of the modules M1, . . . , Mn and write
N = M1 ⊗R M2 ⊗R · · · ⊗R Mn.
Given mk ∈Mk, we introduce the tensor notation m1⊗m2⊗· · ·⊗mn = j(m1, . . . , mn) ∈N.
In this notation, R-multilinearity of j translates into the identities
m1⊗· · ·⊗(mk+m′
k)⊗· · ·⊗mn = m1⊗· · ·⊗mk⊗· · ·⊗mn+m1⊗· · ·⊗m′
k⊗· · ·⊗mn; (20.8)
m1 ⊗· · · ⊗(rmk) ⊗· · · ⊗mn = r(m1 ⊗· · · ⊗mk ⊗· · · ⊗mn),
(20.9)
valid for all mk, m′
k ∈Mk, ms ∈Ms, and r ∈R.
Uniqueness of Solution to the UMP. To justify our new notation for N and j, we
show that the solution (N, j) to our universal mapping problem is unique up to a unique
isomorphism compatible with the universal map j. Suppose (N ′, j′) is another solution to
the UMP. The proof involves the following four diagrams of sets and mappings:
X
j
/
j′
 
N
g

N ′
X
j′
/
j
 
N ′
g′

N
X
j
/
j
 
N
h

N
X
j′
/
j′
 
N ′
h′

N ′
Since j′ : X →N ′ is R-multilinear and (N, j) solves the UMP, we get a unique R-linear
map g : N →N ′ with j′ = g ◦j (see the first diagram above). It now suffices to show that
g is an isomorphism. Since j : X →N is R-multilinear and (N ′, j′) solves the UMP, we
get a unique R-linear map g′ : N ′ →N with j = g′ ◦j′ (see the second diagram above). It
follows that idN ◦j = j = (g′ ◦g) ◦j. By the uniqueness assertion in the UMP for (N, j),
there is only one R-linear map h : N →N with j = h ◦j (see the third diagram above).
Therefore, g′ ◦g = idN. Similarly, using the uniqueness of h′ in the fourth diagram, we see
that g ◦g′ = idN ′. So g′ is the two-sided inverse of g, hence both maps are isomorphisms.
Note that this uniqueness proof is essentially identical to the earlier uniqueness proof given
for the coproduct of a family of sets (§19.7). In general, this same proof template can be used
repeatedly to establish the uniqueness of solutions to various universal mapping problems
(up to a unique isomorphism compatible with the universal maps). For future universal
mapping problems, we omit the details of this uniqueness proof, asking the reader to verify
the applicability of the proof template used here.
Let M be an R-module and n a positive integer. Consider the Cartesian product M n =
M × M × · · · × M, where there are n copies of M. By letting each Mk = M in the tensor
product construction, we obtain the nth tensor power of M, denoted
OnM = M ⊗n = M ⊗R M ⊗R · · · ⊗R M.
The associated universal map j sends (m1, . . . , mn) ∈M n to m1 ⊗· · · ⊗mn ∈M ⊗n. The
UMP for tensor products says that there is a bijection γ from the set of R-linear maps (R-
module homomorphisms) g : M ⊗n →P onto the set of R-multilinear maps f : M n →P,
given by γ(g) = g ◦j. In the next two sections, we use the tensor power M ⊗n to solve
universal mapping problems for alternating maps and symmetric maps.
Universal Mapping Problems in Multilinear Algebra
577
20.5
Exterior Powers of a Module
In the last section, we solved a universal mapping problem that converted R-multilinear
maps into R-linear maps. Here, we formulate and solve a similar problem that converts
alternating maps into R-linear maps.
Problem (UMP for Exterior Powers). Given a commutative ring R, an R-module M,
and a positive integer n, construct an R-module N and an alternating map i : M n →N
satisfying the following UMP: for any R-module P, there is a bijection from the set
A = {R-linear maps g : N →P}
onto the set
B = {alternating maps f : M n →P}
that sends g ∈A to g ◦i ∈B. In other words, for each alternating map f : M n →P, there
exists a unique R-linear map g : N →P with f = g ◦i:
M n
i
/
f
!
N
g

P
Construction of Solution to the UMP. As before, the idea is to fit together two
previously solved universal mapping problems, as shown in the following diagram and in
Figure 20.2:
M n
j
/
f
#
M ⊗n
h

ν
/ N
g
|
P
FIGURE 20.2
Bijections between sets of maps used to construct exterior powers.
578
Advanced Linear Algebra
To explain this, first recall the universal mapping property for M ⊗n and j : M n →M ⊗n
(§20.4): for any R-module P, there is a bijection α from the set
C′ = {all R-linear maps h : M ⊗n →P}
onto the set
B′ = {all R-multilinear maps f : M n →P},
given by α(h) = h ◦j for all h ∈C′.
Next, let K be the R-submodule of M ⊗n generated by all elements m1 ⊗m2 ⊗· · · ⊗mn ∈
M ⊗n such that mk = mℓfor some k ̸= ℓ. Let N be the R-module M ⊗n/K and ν : M ⊗n →N
be the projection map given by ν(z) = z + K for z ∈M ⊗n. Recall the universal mapping
property of ν (§19.2): for any R-module P, there is a bijection β from the set
A = {all R-linear maps g : N = M ⊗n/K →P}
onto the set
C = {all R-linear maps h : M ⊗n →P such that h(z) = 0 for all z ∈K},
given by β(g) = g ◦ν for all g ∈A.
Note that C ⊆C′. We claim that α[C] = B, the set of all alternating maps from M n to
P. Proof: An R-linear map h : M ⊗n →P in C′ belongs to C iff h[K] = {0} iff h maps every
generator of the submodule K to zero iff h(m1 ⊗· · ·⊗mn) = 0 whenever mk = mℓfor some
k ̸= ℓiff (h◦j)(m1, . . . , mn) = 0 whenever mk = mℓfor some k ̸= ℓiff α(h) = h◦j : M n →P
is alternating iff α(h) ∈B. By the claim, α restricts to a bijection α|C : C →B sending
h ∈C to h ◦j ∈B. We also have the bijection β : A →C sending g ∈A to g ◦ν ∈C.
Composing these bijections, we obtain a bijection γ : A →B given by γ(g) = g ◦(ν ◦j)
for g ∈A. Letting i = ν ◦j : M n →N, we see that the bijection from A to B is given
by composition with i. Since j is R-multilinear and ν is R-linear, the composite map i is
R-multilinear. See Figure 20.2.
The map i : M n →N sends (m1, . . . , mn) ∈M n to the coset (m1 ⊗· · · ⊗mn) + K in
N. By definition of K, i(m1, . . . , mn) = 0 + K = 0N whenever mk = mℓfor some k ̸= ℓ.
Therefore, i is alternating. The standard argument proves that the solution (N, i) to the
UMP is unique up to a unique R-isomorphism.
We call N the nth exterior power of M and write N = VnM. We also write
m1 ∧m2 ∧· · · ∧mn = i(m1, . . . , mn) = m1 ⊗· · · ⊗mn + K ∈N
and call this element the wedge product of m1, . . . , mn. In this notation, the R-multilinearity
and alternating properties of i translate into the identities:
m1 ∧· · · ∧(mk + m′
k) ∧· · · ∧mn = m1 ∧· · · ∧mk ∧· · · ∧mn + m1 ∧· · · ∧m′
k ∧· · · ∧mn;
m1 ∧· · · ∧(rmk) ∧· · · ∧mn = r(m1 ∧· · · ∧mk ∧· · · ∧mn);
m1 ∧· · · ∧mn = 0 whenever mk = mℓfor some k ̸= ℓ.
The anti-commutativity of i, which follows from the alternating property, translates into
the following facts:
m1 ∧· · · ∧mk ∧mk+1 ∧· · · ∧mn = −m1 ∧· · · ∧mk+1 ∧mk ∧· · · ∧mn;
m1 ∧· · · ∧mk ∧· · · ∧mℓ∧· · · ∧mn = −m1 ∧· · · ∧mℓ∧· · · ∧mk ∧· · · ∧mn;
mw(1) ∧· · · ∧mw(n) = (sgn(w))m1 ∧· · · ∧mn for all w ∈Sn.
Universal Mapping Problems in Multilinear Algebra
579
20.6
Symmetric Powers of a Module
Next, we give a universal construction for converting symmetric maps into R-linear maps.
Problem (UMP for Symmetric Powers). Given a commutative ring R, an R-module
M, and a positive integer n, construct an R-module N and a symmetric map i : M n →N
satisfying the following UMP: for any R-module P, there is a bijection from the set
A = {R-linear maps g : N →P}
onto the set
B = {symmetric maps f : M n →P}
sending g ∈A to g ◦i ∈B. In other words, for each symmetric map f : M n →P, there
exists a unique R-linear map g : N →P with f = g ◦i:
M n
i
/
f
!
N
g

P
Construction of Solution to the UMP. The proof is nearly identical to what we did
for alternating maps (see the diagram below and Figure 20.3).
M n
j
/
f
#
M ⊗n
h

ν
/ N
g
|
P
To start, recall once again the universal mapping property for M ⊗n and j : M n →M ⊗n
(§20.4): for any R-module P, there is a bijection α from the set
C′ = {all R-linear maps h : M ⊗n →P}
FIGURE 20.3
Bijections between sets of maps used to construct symmetric powers.
580
Advanced Linear Algebra
onto the set
B′ = {all R-multilinear maps f : M n →P},
given by α(h) = h ◦j for all h ∈C′.
Next, let K be the R-submodule of M ⊗n generated by all elements of the form
m1 ⊗m2 ⊗· · · ⊗mn −m′
1 ⊗m′
2 ⊗· · · ⊗m′
n,
where the list (m′
1, . . . , m′
n) is a rearrangement of the list (m1, . . . , mn). Let N be the R-
module M ⊗n/K and ν : M ⊗n →N be the projection map given by ν(z) = z + K for
z ∈M ⊗n. Recall once again the universal mapping property of ν (§19.2): for any R-module
P, there is a bijection β from the set
A = {all R-linear maps g : N = M ⊗n/K →P}
onto the set
C = {all R-linear maps h : M ⊗n →P such that h(z) = 0 for all z ∈K},
given by β(g) = g ◦ν for all g ∈A.
Note that C ⊆C′. We claim that α[C] = B, the set of all symmetric maps from M n to P.
Proof: An R-linear map h : M ⊗n →P in C′ is in C iff h[K] = {0} iff h maps every generator
of the submodule K to zero iff h(m1⊗· · ·⊗mn)−h(m′
1⊗· · ·⊗m′
n) = 0 whenever (m′
1, . . . , m′
n)
is a rearrangement of (m1, . . . , mn) iff (h ◦j)(m1, . . . , mn) = (h ◦j)(m′
1, . . . , m′
n) whenever
(m′
1, . . . , m′
n) is a rearrangement of (m1, . . . , mn) iff α(h) = h ◦j : M n →P is symmetric
iff α(h) ∈B. By the claim, α restricts to a bijection α|C : C →B sending h ∈C to
h ◦j ∈B. We also have the bijection β : A →C sending g ∈A to g ◦ν ∈C. Composing
these bijections, we obtain a bijection γ : A →B given by γ(g) = g ◦(ν ◦j) for g ∈A.
Letting i = ν ◦j : M n →N, we see that the bijection from A to B is given by composition
with i. Since j is R-multilinear and ν is R-linear, the composite map i is R-multilinear. See
Figure 20.3.
The map i : M n →N sends (m1, . . . , mn) ∈M n to the coset (m1 ⊗· · · ⊗mn) + K in
N. By definition of K, i(m1, . . . , mn) = i(m′
1, . . . , m′
n) in N whenever (m′
1, . . . , m′
n) is a
rearrangement of (m1, . . . , mn). Therefore, i is symmetric. The standard argument proves
that the solution (N, i) to the UMP is unique up to a unique R-isomorphism.
We call N the nth symmetric power of M and write N = Symn M. We also write
m1m2 · · · mn = i(m1, . . . , mn) = m1 ⊗· · · ⊗mn + K ∈N
and call this element the symmetric product of m1, . . . , mn. In this notation, the R-
multilinearity and symmetric properties of i translate into the identities:
m1m2 · · · (mk + m′
k) · · · mn = m1m2 · · · mk · · · mn + m1m2 · · · m′
k · · · mn;
m1m2 · · · (rmk) · · · mn = r(m1m2 · · · mk · · · mn);
m1m2 · · · mn = m′
1m′
2 · · · m′
n
whenever (m′
1, . . . , m′
n) is a rearrangement of (m1, . . . , mn).
Universal Mapping Problems in Multilinear Algebra
581
20.7
Myths about Tensor Products
The tensor product construction is more subtle than other module constructions such as the
direct product. To help the reader avoid common errors, we discuss some misconceptions
or “myths” about tensor products in this section. For simplicity, let us consider the tensor
product M ⊗R N of two R-modules M and N.
Myth 1: “Every element of M⊗RN has the form x⊗y for some x ∈M and y ∈N.” This
myth arises from the faulty assumption that the underlying set of the module M ⊗RN is the
Cartesian product M × N = {(x, y) : x ∈M, y ∈N}. However, recalling the construction
in §20.4, we see that M ⊗R N was built by starting with a free module F having the set
M × N as a basis, and then taking the quotient by a certain submodule K. For any basis
element (x, y) of F, we wrote x ⊗y for the coset (x, y) + K. Since every element of F is a
finite R-linear combination of basis elements, it is true that every element of M ⊗R N is a
finite R-linear combination of the form Pk
i=1 ri(xi ⊗yi), where k ∈Z≥0, ri ∈R, xi ∈M,
and yi ∈N. In fact, we can use (20.9) to write ri(xi ⊗yi) = (rixi) ⊗yi for each i. This
means that every element of M ⊗R N can be written as a finite sum of “basic” tensors x⊗y
with x ∈M and y ∈N.
Myth 2: “The set {x⊗y : x ∈M, y ∈N} is a basis for the R-module M ⊗R N.” In fact,
the set of all ordered pairs (x, y) with x ∈M and y ∈N is a basis for the free R-module F
used in the construction of M ⊗R N. However, after passing to the quotient module F/K,
the set of cosets (x, y) + K is almost never a basis for F/K. For instance, using (20.8), we
have (x1 + x2) ⊗y −(x1 ⊗y) −(x2 ⊗y) = 0 for all x1, x2 ∈M and y ∈N, which gives a
dependence relation among three basic tensors. On the other hand, as we saw above, it is
true that the set of all tensors x ⊗y forms a generating set for the R-module M ⊗R N. In
the coming sections, we will find bases for tensor products, exterior powers, and symmetric
powers of free modules.
Myth 3: “All we need to do to define a function with domain M ⊗R N is to declare
f(x ⊗y) to be any formula involving x and y.” To illustrate the problems that can arise
here, consider the following proposed proof that the R-modules M ⊗R N and N ⊗R M are
isomorphic: “Define f : M ⊗R N →N ⊗R M by f(x ⊗y) = y ⊗x for all x ∈M and y ∈N.
The map f is evidently an R-linear bijection, so M ⊗R N ∼= N ⊗R M.” The first difficulty
is that the stated formula does not define f on the entire domain M ⊗R N (see Myth 1).
Since most maps of interest preserve addition, we could try to get around this by extending
f additively, i.e., by setting f(Pk
i=1 xi ⊗yi) = Pk
i=1 yi ⊗xi.
However, there is still a problem with this extended definition. When computing f(z)
for any z ∈M ⊗R N, the output appears to depend on the particular representation of z as
a sum of tensors xi ⊗yi. Usually, z has many such representations; how do we know that
the formula for f gives the same answer no matter which representation we use? For similar
reasons, it is not at all clear that f must be one-to-one.
To resolve these difficulties in defining f, we must return to the universal mapping
property characterizing M ⊗R N. We have seen that there is a bijection between the set of
R-linear maps from M ⊗R N to a given R-module P and the set of R-bilinear maps from
M × N to P. In this case, P is the R-module N ⊗R M. To define the required R-linear
map f : M ⊗R N →N ⊗R M, we instead define a function g : M × N →N ⊗R M on the
product set M ×N, by letting g(x, y) = y⊗x for x ∈M and y ∈N. Note that this definition
has none of the problems that we encountered earlier, since every element of the Cartesian
product M × N can be written uniquely as (x, y) for some x ∈M and y ∈N. Furthermore,
using (20.8) and (20.9) in N ⊗R M, we see that g is R-bilinear: for all x, x1, x2 ∈M and
582
Advanced Linear Algebra
y, y1, y2 ∈N and r ∈R,
g(x1 + x2, y) = y ⊗(x1 + x2) = (y ⊗x1) + (y ⊗x2) = g(x1, y) + g(x2, y);
g(x, y1 + y2) = (y1 + y2) ⊗x = (y1 ⊗x) + (y2 ⊗x) = g(x, y1) + g(x, y2);
g(rx, y) = y ⊗(rx) = r(y ⊗x) = rg(x, y);
g(x, ry) = (ry) ⊗x = r(y ⊗x) = rg(x, y).
The UMP applies and gives us a unique (and well-defined) R-linear map f : M ⊗R N →
N ⊗R M with g = f ◦j; i.e., f(x ⊗y) = f(j(x, y)) = g(x, y) = y ⊗x. This is our original
definition of f on generators of M ⊗R N, but now we are sure that f is well-defined and
R-linear. In most situations, this is the method we must use to define maps whose domain
is a tensor product, exterior power, or symmetric power.
Why is f a bijection? Seeing that f is onto is not too hard, but checking the injectivity
of f directly can be difficult. Instead, we show that f is bijective by exhibiting a two-sided
inverse map. Start with the map g1 : N × M →M ⊗R N given by g1(y, x) = x ⊗y for all
y ∈N and x ∈M. As above, we see that g1 is R-bilinear, so the UMP furnishes a unique
R-linear map f1 : N ⊗R M →M ⊗R N given on generators by f1(y ⊗x) = x ⊗y. To check
that f ◦f1 = idN⊗RM, observe that f ◦f1(y ⊗x) = f(x ⊗y) = y ⊗x for every generator
y ⊗x of the R-module N ⊗R M. The R-linear map idN⊗RM also sends y ⊗x to y ⊗x for
all y ∈N and x ∈M. We know that two R-linear maps agreeing on a set of generators for
their common domain must be equal. So f ◦f1 = idN⊗RM, and similarly f1 ◦f = idM⊗RN.
Finally, we have a rigorous proof of the R-module isomorphism M ⊗R N ∼= N ⊗R M.
20.8
Tensor Product Isomorphisms
In this section, we give further illustrations of the technique used above to prove that
M ⊗R N is isomorphic to N ⊗R M. As a first example, we prove that for every R-module
M, R ⊗R M ∼= M.
Given an R-module M, let r⋆m denote the action of a scalar r ∈R on a module element
m ∈M. Define a map g : R × M →M by g(r, m) = r ⋆m for r ∈R and m ∈M. Now, g is
R-bilinear, since for all r, s ∈R and m, n ∈M:
g(r + s, m)
=
(r + s) ⋆m = (r ⋆m) + (s ⋆m) = g(r, m) + g(s, m);
g(rs, m)
=
(rs) ⋆m = r ⋆(s ⋆m) = rg(s, m);
g(r, m + n)
=
r ⋆(m + n) = (r ⋆m) + (r ⋆n) = g(r, m) + g(r, n);
g(r, sm)
=
r ⋆(sm) = (rs) ⋆m = (sr) ⋆m = s ⋆(r ⋆m) = sg(r, m).
Note that commutativity of R is needed to justify the last equality. We therefore get a
unique R-linear map f : R ⊗R M →M defined on generators by f(r ⊗m) = r ⋆m for r ∈R
and m ∈M.
Next, we define h : M →R ⊗R M by h(m) = 1R ⊗m for m ∈M. The map h is R-linear
because h(m + n) = 1 ⊗(m + n) = (1 ⊗m) + (1 ⊗n) = h(m) + h(n) for all m, n ∈M;
and h(rm) = 1 ⊗(rm) = r(1 ⊗m) = rh(m) for all r ∈R and m ∈M. We claim h is the
two-sided inverse of f, so that both maps are R-module isomorphisms. On one hand, for
any m ∈M, f(h(m)) = f(1 ⊗m) = 1 ⋆m = m = idM(m), so f ◦h = idM. On the other
hand, for any r ∈R and m ∈M,
h(f(r ⊗m)) = h(r ⋆m) = 1 ⊗(rm) = r(1 ⊗m) = (r1) ⊗m = r ⊗m.
Universal Mapping Problems in Multilinear Algebra
583
Thus, the R-linear maps h◦f and idR⊗RM have the same effect on all generators of R⊗RM,
so these maps are equal. This completes the proof that R ⊗R M ∼= M as R-modules.
For our next result, fix a ring R and R-modules M, N, and P. We prove that
(M ⊕N) ⊗R P ∼= (M ⊗R P) ⊕(N ⊗R P)
as R-modules. Recall that M ⊕N = M × N is the set of ordered pairs (m, n) with m ∈M
and n ∈N, which is an R-module under componentwise operations.
First, define a map g : (M⊕N)×P →(M⊗RP)⊕(N⊗RP) by g((m, n), p) = (m⊗p, n⊗p)
for all m ∈M, n ∈N, and p ∈P. You can check that g is R-bilinear; for example, if
u = (m1, n1) and v = (m2, n2) are in M ⊕N, we compute:
g(u + v, p)
=
g((m1 + m2, n1 + n2), p) = ((m1 + m2) ⊗p, (n1 + n2) ⊗p)
=
(m1 ⊗p + m2 ⊗p, n1 ⊗p + n2 ⊗p)
=
(m1 ⊗p, n1 ⊗p) + (m2 ⊗p, n2 ⊗p)
=
g((m1, n1), p) + g((m2, n2), p) = g(u, p) + g(v, p).
By the UMP, there is a unique R-linear map f : (M ⊕N) ⊗R P →(M ⊗R P) ⊕(N ⊗R P)
given on generators by
f((m, n) ⊗p) = (m ⊗p, n ⊗p).
To construct the inverse of f, we first define maps g1 : M × P →(M ⊕N) ⊗R P and
g2 : N × P →(M ⊕N) ⊗R P, by setting g1(m, p) = (m, 0) ⊗p and g2(n, p) = (0, n) ⊗p for
all m ∈M, n ∈N, and p ∈P. You can verify that g1 and g2 are R-bilinear, so the UMP
provides R-linear maps h1 : M ⊗R P →(M ⊕N) ⊗R P and h2 : N ⊗R P →(M ⊕N) ⊗R P
defined on generators by
h1(m ⊗p) = (m, 0) ⊗p,
h2(n ⊗p) = (0, n) ⊗p.
Now, using the UMP for the direct sum of two R-modules (§19.4), we can combine h1
and h2 to get an R-linear map h : (M ⊗R P) ⊕(N ⊗R P) →(M ⊕N) ⊗R P given by
h(y, z) = h1(y) + h2(z) for y ∈M ⊗R P and z ∈N ⊗R P.
To finish the proof, we prove that f ◦h and h ◦f are identity maps by checking that
each map sends all relevant generators to themselves. A typical generator of (M ⊕N) ⊗R P
is w = (m, n) ⊗p with m ∈M, n ∈N, and p ∈P. We compute
h(f(w)) = h(m ⊗p, n ⊗p) = h1(m ⊗p) + h2(n ⊗p)
= (m, 0) ⊗p + (0, n) ⊗p = ((m, 0) + (0, n)) ⊗p = (m, n) ⊗p = w.
On the other hand, you can check that elements of the form (m ⊗p, 0) and (0, n ⊗p) (with
m ∈M, n ∈N, and p ∈P) generate the R-module (M ⊗R P) ⊕(N ⊗R P). For generators
of the first type,
f(h(m ⊗p, 0)) = f(h1(m ⊗p)) = f((m, 0) ⊗p) = (m ⊗p, 0 ⊗p) = (m ⊗p, 0).
For generators of the second type,
f(h(0, n ⊗p)) = f(h2(n ⊗p)) = f((0, n) ⊗p) = (0 ⊗p, n ⊗p) = (0, n ⊗p).
These calculations use the fact that 0 ⊗p = 0 for any p ∈P. We conclude that h is the
inverse of f, so both maps are R-module isomorphisms.
584
Advanced Linear Algebra
An analogous proof shows that P ⊗R (M ⊕N) ∼= (P ⊗R M)⊕(P ⊗R N). More generally,
for all R-modules Mi and Pj, the same techniques (Exercise 36) prove that
 M
i∈I
Mi
!
⊗R P2 ⊗R · · · ⊗R Pk ∼=
M
i∈I
(Mi ⊗R P2 ⊗R · · · ⊗R Pk).
(20.10)
Even more generally, there is an R-module isomorphism
 M
i1∈I1
Mi1
!
⊗R· · ·⊗R
 M
ik∈Ik
Mik
!
∼=
M
(i1,...,ik)∈I1×···×Ik
(Mi1⊗RMi2⊗R· · ·⊗RMik). (20.11)
20.9
Associativity of Tensor Products
Next, we prove an associativity result for tensor products. Fix an R-module M and positive
integers k and m. We show that M ⊗k ⊗R M ⊗m ∼= M ⊗(k+m) as R-modules by constructing
isomorphisms in both directions. First, define g : M k+m →M ⊗k ⊗R M ⊗m by
g(x1, . . . , xk, xk+1, . . . , xk+m) = (x1 ⊗· · · ⊗xk) ⊗(xk+1 ⊗· · · ⊗xk+m)
for xi ∈M. It is routine to check (by repeated use of (20.8) and (20.9)) that g is an R-
multilinear map. Hence g induces a unique R-linear map f : M ⊗(k+m) →M ⊗k ⊗R M ⊗m
defined on generators by
f(x1 ⊗· · · ⊗xk ⊗xk+1 ⊗· · · ⊗xk+m) = (x1 ⊗· · · ⊗xk) ⊗(xk+1 ⊗· · · ⊗xk+m).
Constructing f −1 is somewhat tricky, since the domain of f −1 involves three tensor
products. To begin, fix z = (xk+1, . . . , xk+m) ∈M m and define a map p′
z : M k →M ⊗(k+m)
by
p′
z(x1, . . . , xk) = x1 ⊗· · · ⊗xk ⊗xk+1 ⊗· · · ⊗xk+m
for all (x1, . . . , xk) ∈M k. You can check that p′
z is R-multilinear. Invoking the UMP for
M ⊗k, we get an R-linear map pz : M ⊗k →M ⊗(k+m) given on generators by
pz(x1 ⊗· · · ⊗xk) = x1 ⊗· · · ⊗xk ⊗xk+1 ⊗· · · ⊗xk+m.
Next, for each fixed y ∈M ⊗k, we define a map q′
y : M m →M ⊗(k+m) by q′
y(z) = pz(y) for
all z ∈M m. You can check, using the formula for pz on generators, that q′
y is R-multilinear.
Invoking the UMP for M ⊗m, we get an R-linear map qy : M ⊗m →M ⊗(k+m) given on
generators by
qy(xk+1 ⊗· · · ⊗xk+m) = p(xk+1,...,xk+m)(y).
Finally, define a map t′ : M ⊗k × M ⊗m →M ⊗(k+m) by setting t′(y, w) = qy(w) for all
y ∈M ⊗k and all w ∈M ⊗m. You can check that t′ is R-bilinear, so we finally get an R-
linear map t : M ⊗k ⊗R M ⊗m →M ⊗(k+m) given on generators by t(y ⊗w) = qy(w). Tracing
through all the definitions, we find that
t((x1 ⊗· · · ⊗xk) ⊗(xk+1 ⊗· · · ⊗xk+m)) = x1 ⊗· · · ⊗xk ⊗xk+1 ⊗· · · ⊗xk+m
for all xj ∈M. Hence, t◦f = idM ⊗(k+m) since these two R-linear maps agree on a generating
set. You can see similarly that f ◦t is an identity map, after checking that elements of the
form (x1 ⊗· · · ⊗xk) ⊗(xk+1 ⊗· · · ⊗xk+m) generate the R-module M ⊗k ⊗R M ⊗m.
Universal Mapping Problems in Multilinear Algebra
585
By an analogous proof, you can show that
(M1 ⊗R · · · ⊗R Mk) ⊗R (Mk+1 ⊗R · · · ⊗R Mk+m) ∼= (M1 ⊗R · · · ⊗R Mk+m)
(20.12)
for any R-modules M1, . . . , Mk+m (Exercise 40). More generally, no matter how we insert
parentheses into M1 ⊗R · · · ⊗R Mk+m to indicate nested tensor product constructions, we
always obtain a module isomorphic to the original tensor product. Another approach to
proving these isomorphisms is to show that both modules solve the same universal mapping
problem.
20.10
Tensor Product of Maps
Let M1, . . . , Mn and P1, . . . , Pn be R-modules and fk
: Mk
→Pk be an R-linear
map for each k between 1 and n. We show that there exists a unique R-linear map
g : M1 ⊗R · · · ⊗R Mn →P1 ⊗R · · · ⊗R Pn given on generators by
g(x1 ⊗x2 ⊗· · · ⊗xn) = f1(x1) ⊗f2(x2) ⊗· · · ⊗fn(xn)
(20.13)
for all xk ∈Mk. The map g is written f1 ⊗f2 ⊗· · · ⊗fn or Nn
k=1fk : Nn
k=1Mk →Nn
k=1Pk
and called the tensor product of the maps fk.
As in previous sections, to obtain the R-linear map g we must invoke the UMP for tensor
products. Define a map h : M1 × · · · × Mn →P1 ⊗R · · · ⊗R Pn by
h(x1, x2, . . . , xn) = f1(x1) ⊗f2(x2) ⊗· · · ⊗fn(xn)
for all xk ∈Mk. The function h is R-multilinear, since
h(x1, . . . , xk + x′
k, . . . , xn)
=
f1(x1) ⊗· · · ⊗fk(xk + x′
k) ⊗· · · ⊗fn(xn)
=
f1(x1) ⊗· · · ⊗(fk(xk) + fk(x′
k)) ⊗· · · ⊗fn(xn)
=
f1(x1) ⊗· · · ⊗fk(xk) ⊗· · · ⊗fn(xn)
+f1(x1) ⊗· · · ⊗fk(x′
k) ⊗· · · ⊗fn(xn)
=
h(x1, . . . , xk, . . . , xn) + h(x1, . . . , x′
k, . . . , xn);
and, for all r ∈R,
h(x1, . . . , rxk, . . . , xn) = f1(x1) ⊗· · · ⊗fk(rxk) ⊗· · · ⊗fn(xn)
= f1(x1) ⊗· · · ⊗rfk(xk) ⊗· · · ⊗fn(xn)
= r(f1(x1) ⊗· · · ⊗fk(xk) ⊗· · · ⊗fn(xn)) = rh(x1, . . . , xk, . . . , xn).
Applying the UMP for tensor products to the R-multilinear map h, we obtain a unique
R-linear map g satisfying (20.13).
With the same setup as above, suppose also that we have R-modules Q1, . . . , Qn
and R-linear maps gk : Pk →Qk for k between 1 and n. Then we have R-linear
maps Nn
k=1fk : Nn
k=1Mk →Nn
k=1Pk, Nn
k=1gk : Nn
k=1Pk →Nn
k=1Qk, and (for each k)
gk ◦fk : Mk →Qk. We claim that
 n
O
k=1
gk
!
◦
 n
O
k=1
fk
!
=
n
O
k=1
(gk ◦fk).
(20.14)
586
Advanced Linear Algebra
Both sides are R-linear maps from Nn
k=1Mk to Nn
k=1Qk, so it suffices to check that these
functions have the same effect on all generators of Nn
k=1Mk. To check this, note that for
all xk ∈Mk,
(g1 ⊗· · · ⊗gk) ◦(f1 ⊗· · · ⊗fk)(x1 ⊗· · · ⊗xk) = (g1 ⊗· · · ⊗gk)(f1(x1) ⊗· · · ⊗fk(xk))
= g1(f1(x1)) ⊗· · · ⊗gk(fk(xk)) = ((g1 ◦f1) ⊗· · · ⊗(gk ◦fk))(x1 ⊗· · · ⊗xk).
Similarly, we have Nn
k=1 idMk = idNn
k=1Mk because both sides are R-linear and
(idM1 ⊗· · · ⊗idMn)(x1 ⊗· · · ⊗xn)
=
idM1(x1) ⊗· · · ⊗idMn(xn)
=
x1 ⊗· · · ⊗xn = idNn
k=1Mk(x1 ⊗· · · ⊗xn).
We can use a similar construction to define linear maps on exterior and symmetric
powers of R-modules. Suppose f : M →P is an R-linear map between R-modules M and
P, and n ∈Z>0. We claim there exists a unique R-linear map Vnf : VnM →VnP given
on generators by
 ^nf
!
(x1 ∧x2 ∧· · · ∧xn) = f(x1) ∧f(x2) ∧· · · ∧f(xn)
for all xi ∈M. This map is called the nth exterior power of f. To obtain this map, define
h : M n →VnP by h(x1, . . . , xn) = f(x1) ∧· · · ∧f(xn). Using R-linearity of f, you can
check that h is R-multilinear. The map h is alternating as well, since xi = xj for i < j
gives f(xi) = f(xj), hence f(x1) ∧· · · ∧f(xi) ∧· · · ∧f(xj) ∧· · · ∧f(xn) = 0. Applying the
UMP for exterior powers to h, we obtain a unique R-linear map Vnf satisfying the formula
above. If Q is another R-module and g : P →Q is another R-linear map, you can prove
that (Vng) ◦(Vnf) = Vn(g ◦f) by a calculation on generators analogous to the one used
to prove (20.14). Similarly, Vn idM = idVnM.
With the same setup as the previous paragraph, the same method proves the existence
of a unique R-linear map Symn f : Symn M →Symn P given on generators by
(Symn f) (x1x2 · · · xn) = f(x1)f(x2) · · · f(xn)
for xi ∈M. This map is called the nth symmetric power of f. You can check that
(Symn g) ◦(Symn f) = Symn(g ◦f) and Symn idM = idSymn M.
20.11
Bases and Multilinear Maps
In this section, we study a universal mapping property involving multilinear maps defined on
a product of free R-modules. We show that each such multilinear map is uniquely determined
by its effect on the product of bases of the given modules. By comparing this result to the
UMP for tensor products, we obtain an explicit basis for a tensor product of free R-modules.
To begin, assume that M1, . . . , Mn are free R-modules with respective bases X1, . . . , Xn.
Let M = M1 × · · · × Mn, X = X1 × · · · × Xn, and i : X →M be the inclusion map. Then
the following universal mapping property holds.
Universal Mapping Problems in Multilinear Algebra
587
UMP for Multilinear Maps on Free Modules. For any R-module N and any function
f : X →N, there exists a unique R-multilinear map T : M →N such that f = T ◦i.
X
i
/
f
 
M
T

N
Equivalently: for any R-module N, there is a bijection from the set
A = {R-multilinear maps T : M →N}
onto the set
B = {all functions f : X →N},
which sends each T ∈A to T ◦i ∈B. Informally, we say that each function f : X →N
extends by multilinearity to a unique multilinear map T : M →N.
To prove the UMP, fix the module N and the map f : X →N. Suppose T : M →N
is R-multilinear and extends f. Since each Mk is free with basis Xk, each element
mk ∈Mk can be written uniquely as a finite R-linear combination of elements of Xk,
say mk = Psk
jk=1 cjk,kxjk,k with each xjk,k ∈Xk and each cjk,k ∈R. Using (20.4), we see
that T(m1, m2, . . . , mn) must be given by the formula
s1
X
j1=1
s2
X
j2=1
· · ·
sn
X
jn=1
cj1,1cj2,2 · · · cjn,nf(xj1,1, xj2,2, . . . , xjn,n).
This shows that T, if it exists at all, is uniquely determined by f. To prove existence, use the
previous formula as the definition of T(m1, . . . , mn). It is routine to check that T extends f.
To see that T is multilinear, fix an index k, a scalar r ∈R, and m′
k = Psk
jk=1 djk,kxjk,k ∈Mk.
On one hand, since rmk = Psk
jk=1(rcjk,k)xjk,k, we get
T(m1, . . . , rmk, . . . , rn)
=
s1
X
j1=1
· · ·
sn
X
jn=1
cj1,1 · · · (rcjk,k) · · · cjn,nf(xj1,1, . . . , xjn,n)
=
r
s1
X
j1=1
· · ·
sn
X
jn=1
cj1,1 · · · cjk,k · · · cjn,nf(xj1,1, . . . , xjn,n)
=
rT(m1, . . . , mk, . . . , rn),
where the second equality needs commutativity of R. On the other hand, since
mk + m′
k = Psk
jk=1(cjk,k + djk,k)xjk,k, we get
T(m1, . . . , mk + m′
k, . . . , mn) =
s1
X
j1=1
· · ·
sn
X
jn=1
cj1,1 · · · (cjk,k + djk,k) · · · cjn,nf(xj1,1, . . . , xjn,n)
=
s1
X
j1=1
· · ·
sn
X
jn=1
cj1,1 · · · cjk,k · · · cjn,nf(xj1,1, . . . , xjn,n)
+
s1
X
j1=1
· · ·
sn
X
jn=1
cj1,1 · · · djk,k · · · cjn,nf(xj1,1, . . . , xjn,n)
= T(m1, . . . , mk, . . . , mn) + T(m1, . . . , m′
k, . . . , mn).
So T is multilinear, completing the proof of the UMP.
588
Advanced Linear Algebra
20.12
Bases for Tensor Products of Free Modules
We continue to assume that M1, . . . , Mn are free R-modules with respective bases
X1, . . . , Xn. We use the UMP proved in §20.11 to construct a new solution to the universal
mapping problem for multilinear maps from §20.4. Our ultimate goal is to show that
{x1 ⊗x2 ⊗· · · ⊗xn : xk ∈Xk} is a basis of M1 ⊗R M2 ⊗R · · · ⊗R Mn.
We have already constructed an R-module N = M1 ⊗R ⊗· · · ⊗R Mn and a multilinear
map j : M1 × · · · × Mn →N such that (N, j) solves the UMP from §20.4. To build the
second solution, let N ′ be a free R-module with basis X = X1×· · ·×Xn (see §17.11). Define
a multilinear map j′ : M1 ×· · ·×Mn →N ′ as follows. For each x = (x1, . . . , xn) ∈X, define
j′(x) = x ∈N ′. Using the UMP proved in §20.11, j′ extends by multilinearity to a unique
multilinear map with domain M1 × · · · × Mn. Explicitly, we have
j′

X
k1≥1
ck1,1xk1,1, . . . ,
X
kn≥1
ckn,nxkn,n

=
X
k1≥1
· · ·
X
kn≥1
ck1,1 · · · ckn,n(xk1,1, . . . , xkn,n) ∈N ′.
To prove that (N ′, j′) solves the UMP, suppose P is any R-module and f : M1×· · ·×Mn →P
is any multilinear map. We must show there exists a unique R-linear map g′ : N ′ →P with
f = g′ ◦j′.
M1 × · · · × Mn
j′
/
f
&
N ′
g′

P
To see that g′ exists, define g′(x) = f(x) for all x ∈X and extend g′ by linearity to an
R-linear map g′ : N ′ →P (using the UMP for free R-modules). Now f and g′ ◦j′ are
two multilinear maps from M1 × · · · × Mn to P that agree on X = X1 × · · · × Xn, since
f(x) = g′(x) = g′(j′(x)) = (g′ ◦j′)(x) for all x ∈X. By the uniqueness property in the
UMP from §20.11, f = g′ ◦j′ as needed. To see that g′ is unique, suppose we also had
f = h′ ◦j′ for some R-linear h′ : N ′ →P. Then h′(x) = h′(j′(x)) = f(x) = g′(x) for all
x ∈X. Two R-linear maps that agree on a basis of N ′ must be equal, so g′ = h′.
Now we know that (N ′, j′) and (N, j) both solve the same UMP. Thus there exists a
unique R-module isomorphism g′ : N ′ →N such that j = g′ ◦j′.
M1 × · · · × Mn
j′
/
j
)
N ′
g′

M1 ⊗R · · · ⊗R Mn
The isomorphism g′ sends the R-basis X = X1 × · · · × Xn of N ′ onto an R-basis of
the tensor product M1 ⊗R · · · ⊗R Mn. Explicitly, g′ sends x = (x1, . . . , xn) ∈X to
g′(x) = g′(j′(x)) = j(x) = x1 ⊗· · · ⊗xn. We have now proved that
{x1 ⊗x2 ⊗· · · ⊗xn : xk ∈Xk}
is a basis for the R-module M1 ⊗R M2 ⊗R · · · ⊗R Mn. In particular, if dim(Mk) = dk < ∞
for each k, we see that dim(Nn
k=1Mk) = d1d2 · · · dn.
Universal Mapping Problems in Multilinear Algebra
589
20.13
Bases and Alternating Maps
Let M be a free R-module with basis X. We use X to build a basis for the exterior power
VnM. First, we need to establish a universal mapping property for alternating maps. Fix
a total ordering < on X. For instance, if X = {x1, x2, . . . , xm} is finite, we can use the
ordering x1 < x2 < · · · < xm. Let Xn = {(z1, . . . , zn) : zi ∈X} and
Xn
< = {(z1, . . . , zn) ∈Xn : z1 < z2 < · · · < zn}
be the set of strictly increasing sequences of n basis elements. If n > |X|, then Xn
< is empty.
Let i : Xn
< →M n be the inclusion mapping of Xn
< into the product module M n.
UMP for Alternating Maps on Free Modules. For any R-module N and any function
f : Xn
< →N, there exists a unique alternating map T : M n →N such that f = T ◦i.
Xn
<
i
/
f
"
M n
T

N
Equivalently: for any R-module N, there is a bijection from the set
A = {alternating maps T : M n →N}
onto the set
B = {all functions f : Xn
< →N},
which sends each T ∈A to T ◦i ∈B. Intuitively, the UMP says that we can build alternating
maps by deciding where to send each strictly increasing list of n basis elements, and the
alternating map is uniquely determined by these decisions.
To prove the UMP, fix the module N and the map f : Xn
< →N. We first extend
f to a map g : Xn →N. Given z = (z1, z2, . . . , zn) ∈Xn, consider two cases. If
zi = zj for some i ̸= j, let g(z) = 0N. If all zi are distinct, let sort(z) ∈Xn
< be
the unique sequence obtained by rearranging the entries of z into increasing order. We
can write sort(z) = (zw(1), zw(2), . . . , zw(n)) for a unique w ∈Sn. In this case, define
g(z) = sgn(w)f(sort(z)) ∈N. Recall from Chapter 2 that sgn(w) = (−1)inv(w), where
inv(w) is the number of interchanges of adjacent elements needed to pass from sort(z) to
z or vice versa. It follows from this that if z and z′ differ by interchanging two adjacent
elements, then g(z) = −g(z′). In turn, we deduce that if z and z′ differ by interchanging
any two elements, then g(z) = −g(z′).
From §20.11, we know that g : Xn →N extends uniquely by multilinearity to give a
multilinear map T : M n →N. Since T extends g, T also extends f, so f = T ◦i. We must
show that T is alternating. Fix (m1, . . . , mn) ∈M n with mi = mj for some i ̸= j. Write
mk = P
z∈X r(k, z)z for some r(k, z) ∈R. Since mi = mj, r(i, z) = r(j, z) for all z ∈X.
Since T is multilinear,
T(m1, . . . , mn)
=
X
z1∈X
· · ·
X
zn∈X
r(1, z1) · · · r(n, zn)T(z1, . . . , zn)
=
X
z=(z1,...,zn)∈Xn
r(1, z1) · · · r(i, zi) · · · r(j, zj) · · · r(n, zn)g(z1, . . . , zn).
590
Advanced Linear Algebra
By definition of g, we can drop all terms in this sum in which two entries of z
are equal. The other terms can be split into pairs z = (z1, . . . , zi, . . . , zj, . . . , zn) and
z′ = (z1, . . . , zj, . . . , zi, . . . , zn) by switching the basis elements in positions i and j. By
the observations above, g(z′) = −g(z). The term indexed by z is
r(1, z1) · · · r(i, zi) · · · r(i, zj) · · · r(n, zn)g(z),
and the term indexed by z′ is
r(1, z1) · · · r(i, zj) · · · r(i, zi) · · · r(n, zn)g(z′).
Since R is commutative, the sum of these two terms is zero. Thus, adding all these pairs,
the total effect is that T(m1, . . . , mi, . . . , mi, . . . , mn) = 0N.
We prove uniqueness of T. Suppose T ′ : M n →N is another alternating map that
extends f : Xn
< →N. If we can show that T ′ extends g : Xn →N, we can conclude
that T = T ′ using the known uniqueness property from the UMP in §20.11. Suppose z =
(z1, . . . , zn) ∈Xn has two repeated entries. Then T ′(z) = 0 = g(z) since T ′ is alternating.
Otherwise, when all entries of z are distinct, let sort(z) = (zw(1), . . . , zw(n)) for some w ∈Sn.
Property (AC3) in §20.2 holds for the alternating map T ′, so
T ′(z) = sgn(w)T ′(sort(z)) = sgn(w)f(sort(z)) = g(z).
Thus T and T ′ both extend g, hence T = T ′.
20.14
Bases for Exterior Powers of Free Modules
Assume that M is a free R-module with a basis X totally ordered by <. Following the
pattern of §20.12, we prove that
{z1 ∧z2 ∧· · · ∧zn : zi ∈X, z1 < z2 < · · · < zn}
is a basis of the R-module VnM.
We
have
already
constructed
an
R-module
N
=
VnM
and
an
alternat-
ing map i : M n →N
such that (N, i) solves the UMP from §20.5. We construct
a
second
solution
to
this
UMP
by
letting
N ′
be
a
free
R-module
with
basis
Xn
< = {(z1, . . . , zn) ∈Xn : z1 < · · · < zn}. Noting that Xn
< is a subset of both M n
and N ′, we can define an alternating map i′ : M n →N ′ by letting i′(z) = z for each
z ∈Xn
< and extending i′ to M n by the UMP in §20.13. To prove that (N ′, i′) solves the
UMP for exterior powers, suppose P is any R-module and f : M n →P is any alternating
map. We must show there exists a unique R-linear map g′ : N ′ →P with f = g′ ◦i′.
M n
i′
/
f
"
N ′
g′

P
To see that g′ exists, define g′(x) = f(x) for all x ∈Xn
< and extend g′ by linearity to an
R-linear map g′ : N ′ →P (using the UMP for free R-modules). Note f and g′ ◦i′ are two
alternating maps from M n to P that agree on Xn
<, since f(x) = g′(x) = g′(i′(x)) = (g′◦i′)(x)
for all x ∈Xn
<. By the uniqueness property in the UMP from §20.13, f = g′ ◦i′ as needed.
Universal Mapping Problems in Multilinear Algebra
591
To see that g′ is unique, suppose we also have f = h′ ◦i′ for some R-linear h′ : N ′ →P.
Then h′(x) = h′(i′(x)) = f(x) = g′(x) for all x ∈Xn
<. Two R-linear maps that agree on a
basis of N ′ must be equal, so g′ = h′.
Now we know that (N ′, i′) and (N, i) both solve the same UMP. Thus there exists a
unique R-module isomorphism g′ : N ′ →N such that i = g′ ◦i′.
M n
i′
/
i
#
N ′
g′

VnM
The isomorphism g′ sends the R-basis Xn
< of N ′ onto an R-basis of the exterior power
VnM. Explicitly, g′ sends z = (z1, . . . , zn) ∈Xn
< to g′(z) = g′(i′(z)) = i(z) = z1 ∧· · · ∧zn.
We have now proved that
i[Xn
<] = {z1 ∧z2 ∧· · · ∧zn : zi ∈X, z1 < · · · < zn}
is a basis for the R-module VnM. In particular, if dim(M) = d < ∞, we see that
dim(VnM) =
 d
n

=
d!
n!(d−n)! for 0 ≤n ≤d, and dim(VnM) = 0 for n > d.
20.15
Bases for Symmetric Powers of Free Modules
Assume M is a free R-module with totally ordered basis X. Let Xn
≤be the set of all
sequences z = (z1, . . . , zn) ∈Xn with z1 ≤z2 ≤· · · ≤zn relative to the total ordering <
on X. Let i : Xn
≤→M n be the inclusion map given by i(z) = z for z ∈Xn
≤.
UMP for Symmetric Maps on Free Modules. For any R-module N and any function
f : Xn
≤→N, there exists a unique symmetric map T : M n →N such that f = T ◦i.
Xn
≤
i
/
f
!
M n
T

N
Equivalently: for any R-module N, there is a bijection from the set
A = {symmetric maps T : M n →N}
onto the set
B = {all functions f : Xn
≤→N},
which sends each T ∈A to T ◦i ∈B. So, we can build symmetric maps uniquely by
specifying where to send each weakly increasing list of n basis elements.
The proof is similar to the one in §20.13, so we leave certain details as exercises. Fix
the module N and the map f : Xn
≤→N. Define g : Xn →N by g(z) = f(sort(z)) for all
z ∈Xn, where sort(z) is the unique weakly increasing sequence of basis elements that can
be obtained by sorting the entries of z. From §20.11, we know that g : Xn →N extends
uniquely by multilinearity to give a multilinear map T : M n →N. Since T extends g, T
also extends f, so f = T ◦i. Using multilinearity and the definition of g, one checks that T
592
Advanced Linear Algebra
is a symmetric map. For uniqueness of T, suppose T ′ : M n →N is another symmetric map
that extends f : Xn
≤→N. Using symmetry and the definition of g, check that T and T ′
must both extend g : Xn →N. Hence, T = T ′ follows from the known uniqueness property
in the UMP from §20.11.
Next, we show that
i[Xn
≤] = {z1z2 · · · zn : zi ∈X, z1 ≤z2 ≤· · · ≤zn}
is a basis of the R-module Symn M. We have already constructed an R-module N = Symn M
and a symmetric map i : M n →N such that (N, i) solves the UMP from §20.6. We construct
a second solution to this UMP by letting N ′ be a free R-module with basis Xn
≤. Define a
symmetric map i′ : M n →N ′ by sending z to z for each z ∈Xn
≤and extending to M n by
the UMP shown above. You can now verify, as in the case of exterior powers, that (N′, i′)
solves the same UMP that (N, i) does. So there is a unique isomorphism between N ′ and
N compatible with the universal maps, and this isomorphism maps the R-basis Xn
≤of N ′
to the claimed basis of N = Symn M.
In particular, if M is a free module of dimension d < ∞, then for all n ≥0, dim(Symn M)
is the number of weakly increasing sequences of length n drawn from a d-letter totally
ordered alphabet. A counting argument (Exercise 48) shows that dim(Symn M) =
 d+n−1
n

.
20.16
Tensor Product of Matrices
Suppose M is a free R-module with ordered basis X = (x1, . . . , xm) and N is a free R-
module with ordered basis Y = (y1, . . . , yn). We have seen that the list of mn basic tensors
Z = (x1 ⊗y1, x2 ⊗y1, . . . , xm ⊗y1, x1 ⊗y2, x2 ⊗y2, . . . xm ⊗y2, . . . , xm ⊗yn)
is an ordered basis of M ⊗RN. Now suppose f : M →M and g : N →N are R-linear maps.
Let A ∈Mm(R) be the matrix of f relative to the basis X and B ∈Mn(R) be the matrix of g
relative to the basis Y . What is the matrix C of the R-linear map f ⊗g : M ⊗RN →M ⊗RN
relative to the basis Z?
To answer this question, first recall that f(xj) = Pm
i=1 A(i, j)xi for 1 ≤j ≤m and
g(yj) = Pn
i=1 B(i, j)yi for 1 ≤j ≤n. We label the rows and columns of the matrix C with
the elements of Z in the order they appear above. To compute the entries in the column of
C labeled by xi ⊗yj, we apply f ⊗g to this element, obtaining
(f ⊗g)(xi ⊗yj)
=
f(xi) ⊗g(yj) =
 m
X
k=1
A(k, i)xk
!
⊗
 n
X
ℓ=1
B(ℓ, j)yℓ
!
=
m
X
k=1
n
X
ℓ=1
A(k, i)B(ℓ, j)(xk ⊗yℓ).
So, the entry of C in the row labeled (xk⊗yℓ) and the column labeled (xi⊗yj) is A(k, i)B(ℓ, j).
We write C = A ⊗B and call C the tensor product of the matrices A and B.
Note that C is an mn × mn matrix, where each entry is a scalar in R. We can also
think of C as an n × n block matrix where each block is itself an m × m matrix. For
1 ≤i, j ≤n, the i, j-block of C has rows labeled x1 ⊗yi, x2 ⊗yi, . . . , xm ⊗yi and columns
labeled x1 ⊗yj, x2 ⊗yj, . . . , xm ⊗yj. The above calculation shows that for 1 ≤r, s ≤m, the
r, s-entry of the i, j-block of C is A(r, s)B(i, j). Thus, the entire i, j-block of C is found by
Universal Mapping Problems in Multilinear Algebra
593
multiplying the entire matrix A by the scalar bij = B(i, j). Pictorially, the block matrix C
is
C = A ⊗B =


Ab11
Ab12
· · ·
Ab1n
Ab21
Ab22
· · ·
Ab2n
...
...
...
...
Abn1
Abn2
· · ·
Abnn

.
Suppose that f1 : M
→M and g1 : N
→N are also R-linear maps, repre-
sented (relative to the bases X and Y ) by matrices A1 and B1. We have seen that
(f1 ◦f) ⊗(g1 ◦g) = (f1 ⊗g1) ◦(f ⊗g). Passing to matrices, this yields the matrix identity
(A1A) ⊗(B1B) = (A1 ⊗B1)(A ⊗B).
You can check that (f1 + f) ⊗g = (f1 ⊗g) + (f ⊗g) and (rf) ⊗g = r(f ⊗g) for all r ∈R.
We therefore see that
(A1 + A) ⊗B = (A1 ⊗B) + (A ⊗B),
(rA) ⊗B = r(A ⊗B),
and similar identities hold in the second input.
20.17
Determinants and Exterior Powers
Let M be a free R-module with ordered basis X = (x1, . . . , xn). Given an R-linear map
f : M →M, let A ∈Mn(R) be the matrix of f relative to X. For each k with 0 ≤k ≤n,
there is an induced R-linear map Vkf : VkM →VkM. What is the matrix of this map
relative to the basis i[Xk
<] of VkM?
Before answering this question in general, consider the special case k = n. Here, VnM is
a one-dimensional free R-module, since its basis i[Xn
<] consists of the single wedge product
x∗= x1 ∧x2 ∧· · ·∧xn in which all elements of X appear in order. So the matrix of Vnf has
just one entry; we find it by applying Vnf to x∗and seeing which multiple of x∗results.
Recall f(xi) = Pn
k=1 A(k, i)xk for 1 ≤i ≤n. Using this, we compute:
 ^nf
!
(x∗)
=
f(x1) ∧f(x2) ∧· · · ∧f(xn)
=
 
n
X
k1=1
A(k1, 1)xk1
!
∧
 
n
X
k2=1
A(k2, 2)xk2
!
∧· · · ∧
 
n
X
kn=1
A(kn, n)xkn
!
=
n
X
k1=1
n
X
k2=1
· · ·
n
X
kn=1
A(k1, 1)A(k2, 2) · · · A(kn, n)(xk1 ∧xk2 ∧· · · ∧xkn).
The last step uses the multilinearity of wedge products. To continue simplifying, recall that
any wedge product with a repeated term is zero. So instead of summing over all sequences
(k1, . . . , kn), it suffices to sum only over the permutations k = (k1, . . . , kn) ∈Sn. Using the
last identity in §20.5, we obtain
 ^nf
!
(x∗) =
 X
k∈Sn
sgn(k)A(k1, 1)A(k2, 2) · · · A(kn, n)
!
(x1 ∧x2 ∧· · · ∧xn) = det(A)x∗.
594
Advanced Linear Algebra
We see that Vnf has matrix [det(A)] relative to the basis (x∗) of VnM. This computation
shows how the mysterious definition of det(A) (first given in Chapter 5, equation (5.1))
occurs naturally in the theory of exterior powers.
We can now give a one-sentence proof of the product formula for determinants
(see §5.13). Take maps f, g : M →M whose matrices relative to X are A and B, respectively;
since Vn(f ◦g) = (Vnf) ◦(Vng), applying the preceding result to the maps f ◦g, f, and
g gives det(AB) = det(A) det(B).
We return to the question of computing the matrix C of Vkf relative to the basis i[Xk
<],
for any k between 0 and n. Each element of i[Xk
<] has the form xi1 ∧xi2 ∧· · · ∧xik for
a unique k-element subset I = {i1 < i2 < · · · < ik} of {1, 2, . . . , n}. Let us label the
rows and columns of C with these subsets. To find the entries of C in the column labeled
J = {j1 < j2 < · · · < jk}, apply Vkf to xj1 ∧xj2 ∧· · · ∧xjk. We obtain
f(xj1) ∧f(xj2) ∧· · · ∧f(xjk)
=
 
n
X
w1=1
A(w1, j1)xw1
!
∧
 
n
X
w2=1
A(w2, j2)xw2
!
∧· · · ∧
 
n
X
wk=1
A(wk, jk)xwk
!
=
n
X
w1=1
· · ·
n
X
wk=1
A(w1, j1) · · · A(wk, jk)xw1 ∧· · · ∧xwk.
As before, we can discard zero terms to reduce to a sum over w = (w1, . . . , wk) with all
entries distinct. For each k-element subset I = {i1 < i2 < · · · < ik}, there are k! terms in
the sum indexed by words w that are rearrangements of the entries of I. If a permutation
f ∈Sk rearranges I into w, then
A(w1, j1) · · · A(wk, jk)xw1 ∧· · · ∧xwk = sgn(f)A(if(1), j1) · · · A(if(k), jk)xi1 ∧· · · ∧xik.
These k! terms in the sum, and no others, contribute to the coefficient of xi1 ∧· · · ∧xik. It
follows that the entry of C in the row labeled I and the column labeled J is
X
f∈Sk
sgn(f)A(if(1), j1) · · · A(if(k), jk) = det(AI,J),
where det(AI,J) denotes the determinant of the k × k submatrix of A obtained by keeping
only the k rows in I and the k columns in J. Thus, the entries in the matrix for Vkf are
precisely the kth order minors of A, which we studied in §18.11.
20.18
From Modules to Algebras
An R-algebra is a ring (A, +, ⋆) that is also an R-module, such that c(x ⋆y) = (cx) ⋆y =
x ⋆(cy) for all x, y ∈A and all c ∈R. An R-algebra homomorphism is a map between
R-algebras that is both a ring homomorphism and an R-linear map. Given any R-module
M, our goal is to build an R-algebra T(M), called the tensor algebra of M, solving the
following universal mapping problem.
Universal Mapping Problems in Multilinear Algebra
595
UMP for Tensor Algebras. Given an R-module M, construct an R-algebra T(M) and
an R-linear map i : M →T(M) such that, for any R-algebra A and any R-linear map
f : M →A, there exists a unique R-algebra homomorphism g : T(M) →A with f = g ◦i.
M
i /
f
"
T(M)
g

A
Construction of T(M). Let T(M) be the (external) direct sum L∞
k=0 M ⊗k, where
M ⊗0 = R. By definition, an element of T(M) is an infinite sequence z = (z0, z1, . . . , zk, . . .),
where zk ∈M ⊗k and all but finitely many zk are zero. We already know T(M) is an R-
module, and (using Exercise 15) there is an injective R-linear map i : M →T(M) given by
i(x) = (0, x, 0, 0, . . .) for x ∈M. We must define an algebra structure on T(M) by specifying
the ring multiplication ⋆: T(M) × T(M) →T(M).
In §20.9, we constructed (for each m, k ∈Z>0) an R-linear map νk,m : M ⊗k ⊗R M ⊗m →
M ⊗(k+m) given on generators by
νk,m((x1 ⊗· · · ⊗xk) ⊗(xk+1 ⊗· · · ⊗xk+m)) = x1 ⊗· · · ⊗xk ⊗xk+1 ⊗· · · ⊗xk+m.
Recall that νk,m is induced (via the UMP for tensor products) from an R-bilinear map
µk,m : M ⊗k × M ⊗m →M ⊗(k+m), which acts on generators by
µk,m(x1 ⊗· · · ⊗xk, xk+1 ⊗· · · ⊗xk+m) = x1 ⊗· · · ⊗xk ⊗xk+1 ⊗· · · ⊗xk+m.
Similarly, we have R-bilinear maps µ0,m and µk,0 such that (for r ∈R and xi ∈M)
µ0,m(r, x1 ⊗· · · ⊗xm) = (rx1) ⊗· · · ⊗xm;
µk,0(x1 ⊗· · · ⊗xk, r) = (rx1) ⊗· · · ⊗xk.
We can assemble all the maps µk,m to obtain a map ⋆: T(M) × T(M) →T(M), as follows.
Given y = (yk : k ≥0) and z = (zk : k ≥0) in T(M), define
y ⋆z =
 X
k+m=n
µk,m(yk, zm) : n ≥0
!
.
It is tedious but routine to confirm that (T(M), +, ⋆) satisfies the axioms for a ring and
R-algebra. In particular, the left and right distributive laws for ⋆follow from the preceding
definition and bilinearity of each µk,m. Associativity of ⋆follows from the fact that
µk+m,p(µk,m(x1 ⊗· · · ⊗xk, y1 ⊗· · · ⊗ym), z1 ⊗· · · ⊗zp)
= µk,m+p(x1 ⊗· · · ⊗xk, µm,p(y1 ⊗· · · ⊗ym, z1 ⊗· · · ⊗zp)),
which holds since both sides equal
x1 ⊗· · · ⊗xk ⊗y1 ⊗· · · ⊗ym ⊗z1 ⊗· · · ⊗zp.
The multiplicative identity of T(M) is (1R, 0, 0, . . .).
596
Advanced Linear Algebra
Next, we verify that (T(M), i) solves the UMP. Let (A, +, ∗) be any R-algebra and
f : M →A be an R-linear map. We need to build an R-algebra map g : T(M) →A with
f = g ◦i. Define g0 : R →A by g0(r) = r · 1A for all r ∈R. For each k > 0, define
fk : M k →A by fk(x1, x2, . . . , xk) = f(x1) ∗f(x2) ∗· · · ∗f(xk) for all xj ∈M. Since A
is an R-algebra and f is R-linear, fk is R-multilinear. So we get an induced R-linear map
gk : M ⊗k →A given on generators by
gk(x1 ⊗x2 ⊗· · · ⊗xk) = f(x1) ∗f(x2) ∗· · · ∗f(xk).
The UMP for direct sums (§19.6) combines all the maps gk to give an R-linear map g :
T(M) →A such that g(z0, z1, . . . , zk, . . .) = P
k≥0 gk(zk). Note that g ◦i = f, since for all
z1 ∈M, g(i(z1)) = g(0, z1, 0, . . .) = g1(z1) = f(z1). Since g is already known to be R-linear,
we need only check that g(y ⋆z) = g(y) ∗g(z) for all y, z ∈T(M). Using the definition of
⋆and the distributive laws, this verification reduces to the fact that gk+m(µk,m(y, z)) =
gk(y) ∗gm(z) for all y ∈M ⊗k and all z ∈M ⊗m. This fact holds because
gk+m(µk,m(y1 ⊗· · · ⊗yk, z1 ⊗· · · ⊗zm)) = f(y1) ∗f(y2) ∗· · · ∗f(yk) ∗f(z1) ∗· · · ∗f(zm)
= gk(y1 ⊗· · · ⊗yk) ∗gm(z1 ⊗· · · ⊗zm),
µk,m is R-bilinear, and gk, gm, gk+m are R-linear.
We must show that g is unique. Suppose h : T(M) →A is also an R-algebra
homomorphism with h ◦i = f. By the uniqueness property in the UMP for direct sums, it
suffices to check that h ◦jk = g ◦jk for all k ≥0, where jk is the injection of M ⊗k into
T(M) = L
s≥0 M ⊗s. When k = 0, we have
h ◦j0(r) = rh(1T (M)) = r1A = rg0(1T (M)) = g ◦j0(r)
for all r ∈R. When k = 1, we have
h ◦j1(x) = h ◦i(x) = f(x) = g ◦i(x) = g ◦j1(x)
for all x ∈M. When k ≥2, note that x1 ⊗x2 ⊗· · · ⊗xk = x1 ⋆x2 ⋆· · · ⋆xk for all xj ∈M.
Since g and h preserve multiplication and coincide with f on M, we get
h ◦jk(x1 ⊗· · · ⊗xk) = h(x1 ⋆· · · ⋆xk) = f(x1) ∗f(x2) ∗· · · ∗f(xk)
= g(x1 ⋆· · · ⋆xk) = g ◦jk(x1 ⊗· · · ⊗xk).
Thus g = h, completing the proof of the uniqueness assertion in the UMP.
In general, the algebra T(M) is not commutative. By replacing tensor powers of M by
symmetric powers of M throughout the preceding construction, we can build a commutative
algebra Sym(M) such that any R-linear map from M into a commutative algebra A uniquely
extends to an R-algebra map from Sym(M) to A (Exercise 59). Sym(M) is called the
symmetric algebra of M. A similar construction using exterior powers produces an algebra
V(M) solving an appropriate UMP (Exercise 60). V(M) is called the exterior algebra of
M.
20.19
Summary
In this summary, assume R is a commutative ring and all other capital letters are R-modules
unless otherwise stated.
Universal Mapping Problems in Multilinear Algebra
597
1.
Special Maps. A map f : M →N is R-linear iff f(m + m′) = f(m) + f(m′) and
f(rm) = rf(m) for all m, m′ ∈M and r ∈R. A map f : M1 × · · · × Mn →N
is R-multilinear iff for 1 ≤i ≤n, f is R-linear in position i when all other
inputs are held fixed. An R-multilinear map f : M n →N is alternating iff f
has value zero when any two inputs are equal. An R-multilinear map f : M n →
N is anti-commutative iff f(mw(1), . . . , mw(n)) = sgn(w)f(m1, . . . , mn) for all
mk ∈M and all w ∈Sn. To prove anti-commutativity, it suffices to check that f
changes sign when any two adjacent inputs are switched. Alternating maps are
anti-commutative, but the converse only holds when 1R +1R is not a zero divisor.
An R-multilinear map f : M n →N is symmetric iff f(mw(1), . . . , mw(n)) =
f(m1, . . . , mn) for all mk ∈M and all w ∈Sn. To prove symmetry, it suffices to
check that f is unchanged when any two adjacent inputs are switched.
2.
Generators for Nn
k=1Mk, VnM, and Symn M. Every element of M1⊗R· · ·⊗RMn
is a finite sum of basic tensors x1 ⊗· · · ⊗xn with xk ∈Mk. Every element of
VnM is a finite sum of elements x1 ∧· · · ∧xn with xk ∈M. Every element of
Symn M is a finite sum of elements x1 · · · xn with xk ∈M. If Xk generates Mk,
then {x1 ⊗· · · ⊗xn : xk ∈Xk} generates Nn
k=1Mk; similarly for VnM and
Symn M.
3.
Bases for Nn
k=1Mk, VnM, and Symn M. If each Mk is free with basis
Xk, then Nn
k=1Mk is free with basis {x1 ⊗· · · ⊗xn
:
xk
∈
Xk}. If
M
is free with basis X totally ordered by <, then VnM
is free with
basis {x1 ∧· · · ∧xn : xk ∈X, x1 < · · · < xn}, and Symn M is free with basis
{x1 · · · xn : xk ∈X, x1 ≤· · · ≤xn}. In the case where dim(Mk) = dk < ∞
and dim(M) = d < ∞, we have dim(M1 ⊗R · · · ⊗R Mn) = d1d2 · · · dn,
dim(VnM)
=
 d
n

for 0
≤
n
≤
d, dim(VnM)
=
0 for n
>
d, and
dim(Symn M) =
 d+n−1
n

.
4.
Myths about Tensor Products. It is false that every element of M1 ⊗R · · · ⊗R Mn
must have the form x1 ⊗· · · ⊗xn for some xk
∈
Mk. It is false that
{x1 ⊗· · · ⊗xn : xk ∈Mk} must be a basis for M1 ⊗R · · · ⊗R Mn. It is false that
we can define a function on M1⊗R· · ·⊗RMn (with no further work) by specifying
where the function sends basic tensors. It is false that a tensor product of nonzero
modules must be nonzero.
5.
Tensor Product Isomorphisms. Five R-module isomorphisms (defined on genera-
tors) are:
(a)Commutativity: M ⊗R N ∼= N ⊗R M via the map m ⊗n 7→n ⊗m.
(b)Associativity: (M ⊗R N) ⊗R P ∼= M ⊗R (N ⊗R P) via the map
(m ⊗n) ⊗p 7→m ⊗(n ⊗p).
(c)Left Distributivity: (M ⊕N) ⊗R P ∼= (M ⊗R P) ⊕(N ⊗R P) via the map
(m, n) ⊗p 7→(m ⊗p, n ⊗p).
(d)Right Distributivity: P ⊗R (M ⊕N) ∼= (P ⊗R M) ⊕(P ⊗R N) via the map
p ⊗(m, n) 7→(p ⊗m, p ⊗n).
(e)Identity for ⊗R: R ⊗R M ∼= M via the map r ⊗m 7→rm; similarly,
M ⊗R R ∼= M.
598
Advanced Linear Algebra
More generally:
Mw(1) ⊗R · · · ⊗R Mw(n) ∼= M1 ⊗R · · · ⊗R Mn for all w ∈Sn;
(M1 ⊗R · · · ⊗R Mk) ⊗R (Mk+1 ⊗R · · · ⊗R Mn) ∼= M1 ⊗R · · · ⊗R Mn;
 M
i1∈I1
Mi1
!
⊗R · · · ⊗R
 M
in∈In
Min
!
∼=
M
(i1,...,in)∈I1×···×In
Mi1 ⊗R · · · ⊗R Min;
and all factors of R can be deleted in a tensor product to give an isomorphic
tensor product.
6.
Tensor Product of Linear Maps. Given R-linear maps fk : Mk →Nk, there is
an induced R-linear map Nn
k=1fk : Nn
k=1Mk →Nn
k=1Nk given on generators
by (f1 ⊗· · · ⊗fn)(x1 ⊗· · · ⊗xn) = f1(x1) ⊗· · · ⊗fn(xn). Given R-linear maps
gk : Nk →Pk, we have Nn
k=1(gk ◦fk) = (Nn
k=1gk) ◦(Nn
k=1fk).
7.
Exterior Powers and Symmetric Powers of Linear Maps. Given an R-linear
map f : M →N, there are induced R-linear maps Vnf : VnM →VnN and
Symn f : Symn M →Symn N that act on generators by
 ^nf
!
(x1 ∧· · · ∧xn) = f(x1) ∧· · · ∧f(xn),
(Symn f) (x1 · · · xn) = f(x1) · · · f(xn).
For an R-linear map g : N →P, we have Vn(g ◦f) = (Vng) ◦(Vnf) and
Symn(g ◦f) = (Symn g) ◦(Symn f).
8.
Tensor Product of Matrices. Given A ∈Mm(R) and B ∈Mn(R), A ⊗B ∈
Mmn(R) is the n × n block matrix whose i, j-block is B(i, j)A. If A is the matrix
of f and B is the matrix of g relative to certain bases, A⊗B is the matrix of f ⊗g
relative to the tensor product of these bases. For C ∈Mm(R) and D ∈Mn(R)
and r ∈R, (CA) ⊗(DB) = (C ⊗D)(A ⊗B), (C + A) ⊗B = (C ⊗B) + (A ⊗B),
A ⊗(B + D) = (A ⊗B) + (A ⊗D), and (rA) ⊗B = r(A ⊗B) = A ⊗(rB).
9.
Exterior Powers and Determinants. Suppose X is an n-element basis for M and
f : M →M is R-linear. If A is the matrix of f relative to X, then [det(A)] is the
matrix of Vnf relative to the basis i[Xn
<]. Similarly, the matrix of Vkf relative
to the basis i[Xk
<] has entries det(AI,J), where I, J ⊆{1, 2, . . . , n} have size k.
Summary of Universal Mapping Properties
1.
UMP for Tensor Products. Let j : M1 × · · · × Mn →M1 ⊗R · · · ⊗R Mn
send (m1, . . . , mn) to m1 ⊗· · · ⊗mn. For every R-module P and every R-
multilinear map f : M1 × · · · × Mn →P, there exists a unique R-linear map
g : M1 ⊗R · · · ⊗R Mn →P with f = g ◦j.
M1 × · · · × Mn
j /
f
)
M1 ⊗R · · · ⊗R Mn
g

P
So tensor products convert R-multilinear maps to R-linear maps.
Universal Mapping Problems in Multilinear Algebra
599
2.
UMP for Exterior Powers. Let i
:
M n
→
VnM send (m1, . . . , mn) to
m1 ∧· · · ∧mn. For every R-module P and every alternating map f : M n →P,
there exists a unique R-linear map g : VnM →P with f = g ◦i.
M n
i
/
f
#
VnM
g

P
So exterior powers convert alternating maps to R-linear maps.
3.
UMP for Symmetric Powers. Let i : M n →Symn M send (m1, . . . , mn) to
m1 · · · mn. For every R-module P and every symmetric map f : M n →P, there
exists a unique R-linear map g : Symn M →P with f = g ◦i.
M n
i /
f
$
Symn M
g

P
So symmetric powers convert symmetric maps to R-linear maps.
4.
UMP for Multilinear Maps on Free Modules. Let M1, . . . , Mn be free R-modules
with respective bases X1, . . . , Xn. Let i : X1 × · · · × Xn →M1 × · · · × Mn be the
inclusion map. For any R-module N and any function f : X1 × · · · × Xn →N,
there exists a unique R-multilinear map T : M1 × · · · × Mn →N such that
f = T ◦i.
X1 × · · · × Xn
i
/
f
)
M1 × · · · × Mn
T

N
So arbitrary functions on the product of bases extend uniquely to multilinear maps.
5.
UMP for Alternating Maps on Free Modules. Let M be a free R-module with a
basis X totally ordered by <. Let Xn
< be the set of strictly increasing sequences of
n elements of X and i : Xn
< →M n be the inclusion map. For any R-module N and
any function f : Xn
< →N, there exists a unique alternating map T : M n →N
such that f = T ◦i.
Xn
<
i
/
f
"
M n
T

N
So arbitrary functions on Xn
< extend uniquely to alternating maps.
6.
UMP for Symmetric Maps on Free Modules. Let M be a free R-module with a
basis X totally ordered by <. Let Xn
≤be the set of weakly increasing sequences
of n elements of X and i : Xn
≤→M n be the inclusion map. For any R-module N
and any function f : Xn
≤→N, there exists a unique symmetric map T : M n →N
such that f = T ◦i.
Xn
≤
i
/
f
!
M n
T

N
So arbitrary functions on Xn
≤extend uniquely to symmetric maps.
600
Advanced Linear Algebra
7.
UMP for Tensor Algebras. For every R-module M, there is an R-algebra T(M)
and an R-linear injection i : M →T(M) such that for every R-algebra A and
every R-linear map f : M →A, there exists a unique R-algebra homomorphism
g : T(M) →A with f = g ◦i.
M
i /
f
"
T(M)
g

A
So tensor algebras convert R-linear maps (with codomain an R-algebra) into R-
algebra maps.
8.
UMP for Symmetric Algebras. For every R-module M, there is a commutative
R-algebra Sym(M) and an R-linear injection i : M →Sym(M) such that for
every commutative R-algebra A and every R-linear map f : M →A, there exists
a unique R-algebra homomorphism g : Sym(M) →A with f = g ◦i.
M
i /
f
$
Sym(M)
g

A
So symmetric algebras convert R-linear maps (with codomain a commutative R-
algebra) into R-algebra maps.
9.
UMP for Exterior Algebras. For every R-module M, there is an R-algebra V(M)
with z ⋆z = 0 for all z ∈V(M) and an R-linear injection i : M →V(M)
such that for every R-algebra A and every R-linear map f : M →A such that
f(x) ∗f(x) = 0 for all x ∈M, there exists a unique R-algebra homomorphism
g : V(M) →A with f = g ◦i.
M
i /
f
"
V(M)
g

A
So exterior algebras convert R-linear maps (where all images square to zero) into
R-algebra maps.
20.20
Exercises
Unless otherwise stated, assume R is a commutative ring and M, N, P, Mi are R-modules
in these exercises.
1.
Define f : Rn →R by f(r1, r2, . . . , rn) = r1r2 · · · rn. Prove that f is a symmetric
map. Justify each step using the ring axioms.
2.
Decide (with proof) whether each map below is R-linear, R-bilinear, or neither.
For the bilinear maps, say whether the map is symmetric or alternating.
Universal Mapping Problems in Multilinear Algebra
601
(a) f : R2 →R given by f(x, y) = x + y.
(b) f : R2 →R given by f(x, y) = 5xy.
(c) f : R2 →R given by f(x, y) = x2 −y2.
(d) f : M3(R) × M3(R) →M3(R) given by f(A, B) = ABT.
(e) f : R2 × R2 →R given by f((a, b), (c, d)) = ad −bc for a, b, c, d ∈R.
(f) I(g, h) =
R 1
0 tg(t)h(t) dt where g, h : [0, 1] →R are continuous functions.
(g) C(g, h) = g ◦h −h ◦g, where g, h : Rn →Rn are linear maps.
3.
Define f : Rn ×Rn →R by f(v, w) = vTAw, where A ∈Mn(R) is a fixed matrix
and v, w are column vectors. Prove that f is R-bilinear. For which matrices A is
f alternating? symmetric? R-linear?
4.
Prove that every R-bilinear map f : Rn ×Rn →R has the form f(v, w) = vTAw
for some A ∈Mn(R).
5.
Prove (20.3) by induction on s. Deduce (20.4) from (20.3).
6.
Give an example of an anti-commutative bilinear map that is not alternating.
7.
Give an example of a nonzero bilinear map that is symmetric and alternating.
8.
Find conditions on n and R so that every symmetric alternating map f : M n →P
must be zero.
9.
Prove: for all multilinear f : M1 × · · · × Mn →P, if mi = 0 for some i, then
f(m1, . . . , mn) = 0. Deduce that if mi = 0 for some i, then m1 ⊗· · · ⊗mn = 0 in
M1 ⊗R · · · ⊗R Mn.
10.
Prove that 2 ⊗3 = 0 in Z6 ⊗Z Z7.
11.
Prove or disprove: if f : M × N →P is R-linear and R-bilinear, then f must be
the zero map.
12.
Suppose R is any ring, possibly non-commutative. Define R-bilinearity as in the
text. Prove: if M and N are left R-modules and f : M × N →R is an R-bilinear
map such that f(x, y) is nonzero and not a zero divisor for some x ∈M and
y ∈N, then R is commutative.
13.
Show that if (N ′, i′) solves the UMP in §20.5, then there exists a unique R-module
isomorphism g : VnM →N ′ with g ◦i = i′ (where i is the map from M n to
VnM defined in §20.5).
14.
State and prove a result similar to the previous exercise for Symn M.
15.
Prove: for all R-modules M, M ∼= M ⊗1 ∼= V1M ∼= Sym1 M.
16.
Formulate and solve a UMP that converts anti-commutative maps to R-linear
maps.
17.
Assume 1R + 1R is not zero and not a zero divisor. Let K1 be the submodule of
M ⊗n generated by all elements of the form
m1 ⊗· · · ⊗mi ⊗· · · ⊗mj ⊗· · · ⊗mn + m1 ⊗· · · ⊗mj ⊗· · · ⊗mi ⊗· · · ⊗mn
for all mk ∈M and all i < j. Prove that M ⊗n/K1 ∼=
VnM.
18.
Assume 1R + 1R is not zero and not a zero divisor. Let K2 be the submodule of
M ⊗n generated by all elements of the form
m1 ⊗· · · ⊗mi ⊗mi+1 ⊗· · · ⊗mn + m1 ⊗· · · ⊗mi+1 ⊗mi ⊗· · · ⊗mn
for all mk ∈M and all i < n. Prove that M ⊗n/K2 ∼=
VnM.
602
Advanced Linear Algebra
19.
Let K3 be the submodule of M ⊗n generated by all elements of the form
m1 ⊗· · · ⊗mi ⊗mi+1 ⊗· · · ⊗mn −m1 ⊗· · · ⊗mi+1 ⊗mi ⊗· · · ⊗mn
for all mk ∈M and all i < n. Prove that M ⊗n/K3 ∼= Symn M.
20.
Assume n! · 1R is invertible in R. Let L be the submodule of M ⊗n generated by
elements of the form
(n!)−1 X
w∈Sn
mw(1) ⊗mw(2) ⊗· · · ⊗mw(n),
where all mk ∈M. Prove L ∼= Symn M.
21.
Assume n! · 1R is invertible in R. Let L′ be the submodule of M ⊗n generated by
elements of the form
(n!)−1 X
w∈Sn
sgn(w)mw(1) ⊗mw(2) ⊗· · · ⊗mw(n),
where all mk ∈M. Prove L′ ∼= VnM.
22.
Assume X1, . . . , Xn are generating sets for the R-modules M1, . . . , Mn (respec-
tively). Prove that X = {x1 ⊗· · · ⊗xn : xi ∈Xi} generates the R-module
M1 ⊗R · · · ⊗R Mn.
23.
Assume X generates the R-module M. (a) Prove {x1 ∧· · · ∧xn : xi ∈X}
generates the R-module VnM. (b) Suppose < is a total ordering on X. Prove
{x1 ∧· · · ∧xn : xi ∈X, x1 < · · · < xn} generates VnM.
24.
State and prove results similar to (a) and (b) in Exercise 23 for Symn M.
25.
Let V be the free Z2-module Z2
2, which has basis X = {(1, 0), (0, 1)}.
(a) Use X to find bases for V ⊗2, V2V , and Sym2 V .
(b) List all elements z ∈V ⊗Z2 V . For each z, express z in all possible ways as a
basic tensor u ⊗v with u, v ∈V , or explain why this cannot be done.
(c) List all w ∈V2V . For each w, express w in all possible ways in the form u∧v,
or explain why this cannot be done.
(d) List all y ∈Sym2 V . For each y, express y in all possible ways in the form uv,
or explain why this cannot be done.
26.
Myth: “The tensor product of two nonzero modules must be nonzero.” Disprove
this myth by showing that for all a, b ∈Z>0 with gcd(a, b) = 1, Za ⊗Z Zb = {0}.
27.
Myth: “If M ̸= {0}, then M ⊗2 ̸= {0}.” Disprove this myth by considering the
Z-module M = Q/Z.
28.
In the proof that M ⊗R N ∼= N ⊗R M in §20.7, show directly that f is surjective.
29.
Commutativity Isomorphisms for Tensor Products. Prove: for all R-
modules M1, . . . , Mn and all w ∈Sn, M1 ⊗R · · ·⊗R Mn ∼= Mw(1) ⊗R · · ·⊗R Mw(n).
30.
Prove M ⊗RR ∼= M: (a) by using previously proved isomorphisms; (b) by defining
specific isomorphisms in both directions.
31.
Suppose I ⊆{1, 2, . . . , n} and Mi = R for all i between 1 and n with i ̸∈I. Prove
M1 ⊗R · · · ⊗R Mn ∼= N
i∈I Mi.
32.
Prove or disprove: for all commutative rings R, V2R ∼= R as R-modules.
33.
Prove or disprove: for all commutative rings R, Sym2 R ∼= R as R-modules.
Universal Mapping Problems in Multilinear Algebra
603
34.
In the proof of (M ⊕N) ⊗R P ∼= (M ⊗R P) ⊕(N ⊗R P) in §20.8, check that
g, g1, and g2 are R-bilinear; and check that elements of the form (m ⊗p, 0) and
(0, n ⊗p) generate (M ⊗R P) ⊕(N ⊗R P).
35.
Prove that P ⊗R (M ⊕N) ∼= (P ⊗R M) ⊕(P ⊗R N) by showing both sides solve
the same UMP.
36.
Prove (20.10).
37.
Prove (20.11).
38.
Use the isomorphisms in §20.8 to give a new proof that the tensor prod-
uct of free R-modules is free, and that (in the finite-dimensional case)
dim(M1 ⊗R · · · ⊗R Mn) = Qn
k=1 dim(Mk).
39.
In the proof in §20.9, check that p′
z is R-multilinear; q′
y is R-multilinear; t′ is
R-bilinear; and the elements (x1 ⊗· · · ⊗xk) ⊗(xk+1 ⊗· · · ⊗xk+m) generate
M ⊗k ⊗R M ⊗m.
40.
Prove (20.12) by imitating the proof in §20.9.
41.
Prove (20.12) by showing both sides solve the same UMP.
42.
Extension of Scalars. Suppose M is a free R-module with basis X, and assume
R is a subring of a commutative ring S.
(a) Prove M ⊗R S is a free S-module with basis X ⊗1S = {x ⊗1S : x ∈X}.
(b) Suppose N is a free R-module with basis Y and T : M →N is an R-linear map
with matrix A relative to the bases X and Y . Prove T ⊗idS : M ⊗R S →N ⊗R S
is an S-linear map with matrix A relative to the bases X ⊗1S and Y ⊗1S.
43.
Given an R-linear map f : M →P, give the details of the construction of the
induced map Symn f : Symn M →Symn P. Prove: if g : P →Q is also R-linear,
then (Symn g) ◦(Symn f) = Symn(g ◦f) and Symn idM = idSymn M.
44.
The Z5-module V = Z2
5 has a Z5-basis X = {e1, e2}, where e1 = (1, 0) and
e2 = (0, 1). Define f : X × X →Z5 by f(e1, e1) = 3, f(e1, e2) = 1, f(e2, e1) = 4,
and f(e2, e2) = 0.
(a) Extend f by multilinearity to T : V × V →Z5. Compute T((2, 3), (4, 1)).
(b) T induces a Z5-linear map S : V ⊗Z5 V →Z5. Describe the kernel of S.
45.
The R-module V = R4 has the standard ordered basis X = (e1, e2, e3, e4).
(a) Give an R-basis for V ⊗2.
(b) For all k ≥0, give an R-basis for VkV .
(c) Give an R-basis for Sym3 V .
46.
Let X = (e1, e2, e3) be the standard ordered basis of V = R3. Define f : X2
< →R
by f(e1, e2) = 5, f(e1, e3) = −2, and f(e2, e3) = 1. Let T : R3 × R3 →R be
the unique alternating map induced from f. Compute T((a, b, c), (x, y, z)) for all
a, b, c, x, y, z ∈R.
47.
In §20.15: check that T is a symmetric map; prove that T = T ′; and show that
(N ′, i′) solves the same UMP that (N, i) does.
48.
Define a bijection from the set X of weakly increasing sequences of length n using
entries in {1, 2, . . . , d} to the set Y of strictly increasing sequences of length n
using entries in {1, 2, . . . , d + n −1}. Conclude that |X| = |Y | =
 d+n−1
n

.
49.
Let A =
 2
1
0
−1

and B =


−1
−2
−1
2
0
2
1
0
1

. Compute A ⊗B and B ⊗A.
604
Advanced Linear Algebra
50.
Let A =
 1
1
0
1

. Compute A⊗k (an iterated tensor product of matrices) for all
k ≥0. (Label the rows and columns of A⊗k by subsets of {1, 2, . . . , k}.) Compute
the inverses of the matrices A⊗k.
51.
By computing the general entry on each side, prove: for all A, C ∈Mm(R) and
B, D ∈Mn(R), (CA) ⊗(DB) = (C ⊗D)(A ⊗B).
52.
Fix m, n ∈Z>0. Show that the map p : Mm(R) × Mn(R) →Mmn(R) given by
p(A, B) = A⊗B (tensor product of matrices) is R-bilinear. Deduce the existence
of an R-isomorphism Mm(R) ⊗R Mn(R) ∼= Mmn(R).
53.
Suppose M1, . . . , Mn are free R-modules with respective bases X1, . . . , Xn of sizes
d1, . . . , dn. Suppose fk : Mk →Mk is an R-linear map represented by the matrix
Ak ∈Mdk(R), for 1 ≤k ≤n. Let A be the matrix of the map Nn
k=1 fk relative
to the basis X = {x1 ⊗· · · ⊗xn : xk ∈Xk} of Nn
k=1Mk. What is the size of A?
Describe the entries of A in terms of the entries of each Ak.
54.
Let f : R3 →R3 have matrix


2
2
−1
0
1
3
0
−1
1

relative to the standard ordered
basis. Compute the matrix of Vkf for 0 ≤k ≤3.
55.
Let M and N be free R-modules, and suppose an R-linear map f : M →N has
matrix A relative to ordered bases X for M and Y for N. Compute the entries
in the matrix of Vkf relative to the bases derived from Xk
< and Y k
<.
56.
Use Exercise 55 and the relation Vk(g ◦f) =
 Vkg

◦
 Vkf

to deduce a formula
relating the kth order minors of rectangular matrices A, B, and AB.
57.
Deduce the Cauchy–Binet formula (§5.14) from the previous exercise.
58.
In §20.18: check that the structure (T(M), +, ⋆) is a ring and an R-algebra; check
that fk is R-multilinear; and fill in the missing details in the proof that g is a
ring homomorphism.
59.
Given an R-module M, construct a commutative R-algebra Sym(M) and an R-
linear map i : M →Sym(M) solving this UMP: for any commutative R-algebra
(A, +, ∗) and any R-linear map f : M →A, there exists a unique R-algebra
homomorphism g : Sym(M) →A with f = g ◦i. (You can solve this problem in
two ways: either imitate the construction in §20.18, or take the quotient of the
tensor algebra T(M) by an appropriate ideal.)
60.
Given an R-module M, construct an R-algebra V(M) such that z ⋆z = 0 for
all z ∈V(M) and an R-linear map i : M →V(M) solving this UMP: for any
R-algebra (A, +, ∗) and any R-linear map f : M →A such that f(x) ∗f(x) = 0A
for all x ∈M, there exists a unique R-algebra homomorphism g : V(M) →A
with f = g ◦i.
61.
Suppose M, N, and P are R-modules, and f : M →N, g : N →P are R-
linear maps. Show that f extends to a unique R-algebra homomorphism T(f) :
T(M) →T(N). For y = (yk : k ≥0) ∈T(M), give a formula for T(f)(y). Show
T(g ◦f) = T(g) ◦T(f) and T(idM) = idT (M).
62.
Prove results for Sym(M) and V(M) analogous to the results in the previous
exercise.
63.
Tensor Product for Bimodules. Let R, S, and T be rings, possibly non-
commutative. We use the notation RMS to indicate that M is an R, S-bimodule,
which is a left R-module and a right S-module such that (rm)s = r(ms) for all r ∈
Universal Mapping Problems in Multilinear Algebra
605
R, m ∈M, and s ∈S. A bimodule homomorphism between two R, S-bimodules is
a map that is both R-linear and S-linear. Fix bimodules RMS, SNT , and RPT . Say
that a map g : M × N →P is S-biadditive iff g(m + m′, n) = g(m, n) + g(m′, n),
g(m, n + n′) = g(m, n) + g(m, n′), and g(ms, n) = g(m, sn) for all m, m′ ∈M,
n, n′ ∈N, and s ∈S. Call the map g an R, S, T-map iff g is S-biadditive and
g(rm, n) = rg(m, n) and g(m, nt) = g(m, n)t for all m ∈M, n ∈N, r ∈R, and
t ∈T. Construct a bimodule R(M ⊗S N)T (called the tensor product of M and N
over S) and an R, S, T-map i : M ×N →M ⊗S N, solving the following UMP: for
each bimodule RPT , there is a bijection from the set of group homomorphisms
f : M ⊗S N →P onto the set of S-biadditive maps g : M × N →P, that
maps f to f ◦i. Furthermore, show that f is an R, T-bimodule homomorphism
iff g = f ◦i is an R, S, T-map. (Define the commutative group M ⊗S N to be
a certain quotient of the free commutative group with basis M × N. Verify the
UMP for S-biadditive maps. Aided by this, carefully define a left R-action and a
right T-action on M ⊗S N, verify the bimodule axioms, and prove that bimodule
homomorphisms correspond to R, S, T-maps.)
64.
Establish tensor product isomorphisms (analogous to those proved in §20.8) for
tensor products of bimodules.
65.
Given rings R, S, T, U and bimodules RMS, SNT , T PU, prove there is an R, U-
bimodule isomorphism (M ⊗S N) ⊗T P ∼= M ⊗S (N ⊗T P).
66.
Let (A, +, ⋆) be an R-algebra. (a) Show there is a well-defined R-linear map
m : A ⊗R A →A given on generators by m(x ⊗y) = x ⋆y for x, y ∈A.
(b) Identifying (A ⊗R A) ⊗R A and A ⊗R (A ⊗R A) with A⊗3, show that
m ◦(m ⊗idA) = m ◦(idA ⊗m). (c) Let c : A⊗R A →A⊗R A be the isomorphism
given by c(x ⊗y) = y ⊗x for x, y ∈A. Show: if A is commutative, then
m ◦c = m. (d) Define an R-linear map e : R →A by e(r) = r1A for r ∈R.
Let g : R ⊗R A →A and h : A ⊗R R →A be the canonical isomorphisms. Show
g = m ◦(e ⊗idA) and h = m ◦(idA ⊗e).
67.
Let A be an R-module. Suppose m : A ⊗R A →A and e : R →A are R-linear
maps satisfying the equations in (b) and (d) of Exercise 66. Define x⋆y = m(x⊗y)
for x, y ∈A. Show that (A, +, ⋆) is an R-algebra with identity 1A = e(1R). Also
show that if m satisfies the equation in (c) of Exercise 66, then A is commutative.
68.
Given R-modules M and P, call a function f : M 4 →P peculiar iff f is R-
multilinear and for all w, x, y, z ∈M, f(x, x, y, z) = 0, f(x, y, z, z) = 0, and
f(w, x, y, z) = f(y, x, w, z) + f(w, y, x, z)
= f(z, x, y, w) + f(w, z, y, x)
= f(y, z, w, x).
Define an R-module N and a peculiar function i : M 4 →N solving the following
UMP: for each R-module P and each peculiar f : M 4 →P, there exists a unique
R-linear map g : N →P such that f = g ◦i.
69.
Suppose M is a free R-module with ordered basis X = (x1 < x2 < · · · < xn).
Show that the R-module N in Exercise 68 is free with basis consisting of all
elements i(xa, xb, xc, xd) where a, b, c, d ∈{1, 2, . . . , n}, a < b, c < d, a ≤c, and
b ≤d.
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
Appendix: Basic Definitions
This appendix records some general mathematical definitions and notations that occur
throughout the text. See [40] for a more detailed exposition of this material. The word iff
is defined to mean “if and only if.”
Sets
We first review some definitions from set theory. All capital letters used here denote sets.
• Set Membership: x ∈S means x is a member of the set S.
• Set Non-membership: x ̸∈S means x is not a member of the set S.
• Subsets: A ⊆B means for all x, if x ∈A, then x ∈B.
• Binary Union: For all x, x ∈A ∪B iff x ∈A or x ∈B.
• Binary Intersection: For all x, x ∈A ∩B iff x ∈A and x ∈B.
• Set Difference: For all x, x ∈A \ B iff x ∈A and x ̸∈B.
• Empty Set: For all x, x ̸∈∅.
• Indexed Unions: For all x, x ∈S
i∈I Ai iff there exists i ∈I with x ∈Ai.
• Indexed Intersections: For all x, x ∈T
i∈I Ai iff for all i ∈I, x ∈Ai.
• Cartesian Products: A × B is the set of all ordered pairs (a, b) with a ∈A and b ∈B.
A1 × · · · × An is the set of all ordered n-tuples (a1, . . . , an) with ai ∈Ai for 1 ≤i ≤n.
• Number Systems: We write Z≥0
= {0, 1, 2, 3, . . .} for the set of natural numbers,
Z>0 = {1, 2, 3, . . .} for the set of positive integers, Z for the set of integers, Q for the set
of rational numbers, R for the set of real numbers, and C for the set of complex numbers.
Q>0 denotes the set of positive rational numbers. R>0 denotes the set of positive real
numbers. For each positive integer n, we write [n] to denote the finite set {1, 2, . . . , n}.
Functions
Formally, a function is an ordered triple f = (X, Y, G), where X is a set called the domain
of f, Y is a set called the codomain of f, and G ⊆X × Y is a set called the graph of f,
which is required to satisfy this condition: for all x ∈X, there exists a unique y ∈Y with
(x, y) ∈G. For all x ∈X, we write y = f(x) iff (x, y) ∈G.
607
608
Appendix: Basic Definitions
The notation f : X →Y means that f is a function with domain X and codomain
Y . We often introduce a new function by a phrase such as: “Let f : X →Y be given by
f(x) = · · · ,” where · · · is some formula involving x. We must check that for each fixed
x ∈X, this formula always does produce exactly one output, and that this output belongs
to the claimed codomain Y . By our definition, two functions f and g are equal iff they have
the same domain and the same codomain and the same graph. To check equality of the
graphs, we must check that f(x) = g(x) for all x in the common domain of f and g.
Given functions f : X →Y and g : Y →Z, the composite function g ◦f is the function
with domain X, codomain Z, and graph {(x, g(f(x))) : x ∈X}. Thus, g ◦f : X →Z
satisfies (g ◦f)(x) = g(f(x)) for all x ∈X.
Let f : X →Y be any function. We say f is one-to-one (or injective, or an injection)
iff for all x1, x2 ∈X, if f(x1) = f(x2) then x1 = x2. We say f is onto (or surjective, or
a surjection) iff for each y ∈Y , there exists x ∈X with y = f(x). We say f is bijective
(or a bijection) iff f is one-to-one and onto iff for each y ∈Y , there exists a unique x ∈X
with y = f(x). The composition of two injections is an injection. The composition of two
surjections is a surjection. The composition of two bijections is a bijection.
The identity function on any set X is the function idX : X →X given by idX(x) = x
for all x ∈X. Given f : X →Y , we say that a function g : Y →X is the inverse of f iff
f ◦g = idY and g ◦f = idX, in which case we write g = f −1. We can show that the inverse
of f is unique when it exists. Also, f −1 exists iff f is a bijection, in which case f −1 is also
a bijection and (f−1)−1 = f.
Suppose f : X →Y is a function and Z ⊆X. We obtain a new function g : Z →Y with
domain Z by setting g(z) = f(z) for all z ∈Z. We call g the restriction of f to Z, denoted
g = f|Z or f|Z.
Suppose f : X →Y is any function. For all A ⊆X, the image of A under f is the
set f[A] = {f(a) : a ∈A} ⊆Y . For all B ⊆Y , the preimage of B under f is the set
f −1[B] = {x ∈X : f(x) ∈B} ⊆X. This notation does not mean that the inverse function
f −1 must exist. But, when f −1 does exist, the preimage of B under f coincides with the
image of B under f −1, so the notation f −1[B] is not ambiguous. The image of the function
f is the set f[X]; f is a surjection iff f[X] = Y . We use square brackets for images and
preimages to prevent ambiguity. More precisely, if A is both a member of X and a subset
of X, then f(A) is the value of f at the point A in its domain, whereas f[A] is the image
under f of the subset A of the domain.
Relations
A relation from X to Y is a subset R of X ×Y . For x ∈X and y ∈Y , xRy means (x, y) ∈R.
A relation on a set X is a relation R from X to X. R is called reflexive on X iff for all x ∈X,
xRx. R is called symmetric iff for all x, y, if xRy, then yRx. R is called antisymmetric iff
for all x, y, if xRy and yRx, then x = y. R is called transitive iff for all x, y, z, if xRy and
yRz, then xRz. R is called an equivalence relation on X iff R is reflexive on X, symmetric,
and transitive.
Suppose R is an equivalence relation on a set X. For x ∈X, the equivalence class of x
relative to R is the set [x]R = {y ∈X : xRy}. A given equivalence class typically has many
names; more precisely, for all x, z ∈R, [x]R = [z]R iff xRz. The quotient set X modulo R is
the set of all equivalence classes of R, namely X/R = {[x]R : x ∈X}.
A set partition of a given set X is a collection P of nonempty subsets of X such that
for all x ∈X, there exists a unique S ∈P with x ∈S. For every equivalence relation R
Appendix: Basic Definitions
609
on a fixed set X, the quotient set X/R is a set partition of X consisting of the equivalence
classes [x]R for x ∈X. Conversely, given any set partition P of X, the relation R defined
by “xRy iff there exists S ∈P with x ∈S and y ∈S” is an equivalence relation on X with
X/R = P. Formally, letting EQX be the set of all equivalence relations on X and SPX
be the set of all set partitions on X, the map f : EQX →SPX given by f(R) = X/R for
R ∈EQX is a bijection.
Partially Ordered Sets
A partial ordering on a set X is a relation ≤on X that is reflexive on X, antisymmetric,
and transitive. A partially ordered set or poset is a pair (X, ≤) where ≤is a partial ordering
on X. A poset (X, ≤) is totally ordered iff for all x, y ∈X, x ≤y or y ≤x. A subset Y of
a poset (X, ≤) is called a chain iff for all x, y ∈Y , x ≤y or y ≤x. By definition, y ≥x
means x ≤y; x < y means x ≤y and x ̸= y; and x > y means x ≥y and x ̸= y.
Let S be a subset of a poset (X, ≤). An upper bound for S is an element x ∈X such
that for all y ∈S, y ≤x. A greatest element of S is an element x ∈S such that for all
y ∈S, y ≤x; x is unique if it exists. A lower bound for S is an element x ∈X such that for
all y ∈S, x ≤y. A least element of S is an element x ∈S such that for all y ∈S, x ≤y; x
is unique if it exists.
We say x ∈X is a least upper bound for S iff x is the least element of the set of upper
bounds of S in X. In detail, this means y ≤x for all y ∈S; and for any z ∈X such that
y ≤z for all y ∈S, x ≤z. The least upper bound of S is unique if it exists; we write
x = sup S in this case. For S = {y1, . . . , yn}, the notation y1 ∨y2 ∨· · · ∨yn is also used to
denote sup S.
We say x ∈X is a greatest lower bound for S iff x is the greatest element of the set of
lower bounds of S in X. In detail, this means x ≤y for all y ∈S; and for any z ∈X such
that z ≤y for all y ∈S, z ≤x. The greatest lower bound of S is unique if it exists; we write
x = inf S in this case. For S = {y1, . . . , yn}, the notation y1 ∧y2 ∧· · · ∧yn is also used to
denote inf S.
A lattice is a poset (X, ≤) such that for all a, b ∈X, the least upper bound a ∨b and
the greatest lower bound a ∧b exist in X. A complete lattice is a poset (X, ≤) such that for
every nonempty subset S of X, sup S and inf S exist in X.
A maximal element in a poset (X, ≤) is an element x ∈X such that for all y ∈X, if
x ≤y then y = x. A minimal element of X is an element x ∈X such that for all y ∈X, if
y ≤x then y = x. Zorn’s Lemma states that if (X, ≤) is a poset in which every chain Y ⊆X
has an upper bound in X, then X has a maximal element. Zorn’s Lemma is discussed in
detail in §17.13.
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
Further Reading
Chapter 1. There are many introductory accounts of modern algebra, including the
texts by Durbin [15], Fraleigh [16], Gallian [17], and Rotman [51]. For more advanced
treatments of modern algebra, you may consult the textbooks by Dummit and Foote [14],
Hungerford [31], Jacobson [32], and Rotman [50]. Introductions to linear algebra at various
levels abound; among many others, we mention the books by Larson and Falvo [35], Lay [36],
and Strang [59]. Two more advanced linear algebra books that are similar, in some respects,
to the present volume are the texts by Halmos [25] and Hoffman and Kunze [29].
Chapter 2. For basic facts on permutations, you may consult any of the abstract algebra
texts mentioned above. There is a vast literature on permutations and the symmetric
group; we direct the reader to the texts by Bona [7], Rotman [54], and Sagan [57] for
more information.
Chapter 3. Thorough algebraic treatments of polynomials may be found in most texts
on abstract algebra, such as those by Dummit and Foote [14] or Hungerford [31]. For
more details on formal power series, you may consult [38, Chpt. 7]. The matrix reduction
algorithm in §3.10 for computing gcds of polynomials (or integers) comes from an article
by W. Blankinship [6]. Cox, Little, and O’Shea have written an excellent book [12] on
multivariable polynomials and their role in computational algebraic geometry.
Chapter 4. Three classic texts on matrix theory are the books by Gantmacher [19], Horn
and Johnson [30], and Lancaster [34].
Chapter 5. There is a vast mathematical literature on the subject of determinants. Lacking
the space to give tribute to all of these, we only mention the text by Turnbull [62], the book
of Aitken [2], and the treatise of Muir [44]. Muir has also written an extensive four-volume
work chronicling the historical development of the theory of determinants [43].
Chapter 6. This chapter developed a “dictionary” linking abstract concepts defined for
vector spaces and linear maps to concrete concepts defined for column vectors and matrices.
Many texts on matrix theory, such as Horn and Johnson [30], heavily favor matrix-based
descriptions and proofs. Other texts, most notably Bourbaki [8, Chpt. II], prefer a very
abstract development that makes almost no mention of matrices. We think it is advisable to
gain facility with both languages for discussing linear algebra. For alternative developments
of this material, see Halmos [25] and Hoffman and Kunze [29].
Chapter 7. The analogy between complex numbers and complex matrices, including the
theorems on the polar decomposition of a matrix, is based on the exposition in Halmos [25].
A wealth of additional material on properties of Hermitian, unitary, positive definite, and
normal matrices may be found in Horn and Johnson’s text [30].
Chapter 8. We can derive the Jordan canonical form theorem in many different ways. In
abstract algebra, this theorem can be deduced from the rational canonical form theorem [29],
611
612
Further Reading
which in turn is derivable from the classification of finitely generated modules over principal
ideal domains. See Chapter 18 or [32] for this approach. Matrix theorists might prefer a more
algorithmic construction that triangularizes a complex matrix and then gradually reduces
it to Jordan form [13, 30]. Various elementary derivations can be found in [9, 18, 20, 24, 63].
Chapter 9. Further information on QR factorizations can be found in Chapter 5 of Golub
and van Loan [21], part II of Trefethen and Bau [61], and §5.3 of Kincaid and Cheney [33].
For LU factorizations, see [21, Chpt. 3] or [30, Sec. 3.5]. These references also contain a
wealth of information on the numerical stability properties of matrix factorizations and the
associated algorithms.
Chapter 10. Our treatment of iterative algorithms for solving linear systems and
computing eigenvalues is similar to that found in §4.6 and §5.1 of Kincaid and Cheney [33].
For more information on this topic, you may consult the numerical analysis texts authored
by Ackleh, Allen, Kearfott, and Seshaiyer [1, §3.4, Chpt. 5], Cheney and Kincaid [10, §8.2,
§8.4], Trefethen and Bau [61, Chpt. VI], and Golub and Van Loan [21].
Chapter 11. For a very detailed treatment of convex sets and convex functions, the reader
may consult Rockafellar’s text [49]. A wealth of material on convex polytopes can be found
in Gr¨unbaum’s encyclopedic work [22]. The presentation in §11.17 through §11.21 is similar
to [66, Lecture 1].
Chapter 12. Our exposition of ruler and compass constructions is similar to the accounts
found in [32, Vol. 1, Chpt. 4] and [52, App. C]. Treatments of Galois theory may be found
in these two texts, as well as in the books by Cox [11], Dummit and Foote [14], and
Hungerford [31]. See Tignol’s book [60] for a very nice historical account of the development
of Galois theory. Another good reference for geometric constructions and other problems in
field theory is Hadlock [23].
Chapter 13. Another treatment of dual spaces and their relation to complex inner product
spaces appears in Halmos [25]. A good discussion of dual spaces in the context of Banach
spaces is given in Simmons [58, Chpt. 9]. The book of Cox, Little, and O’Shea [12] contains
an excellent exposition of the ideal-variety correspondence and other aspects of affine
algebraic geometry.
Chapter 14. Our coverage of bilinear forms closely follows Chapter 6 of Volume 1
of Jacobson’s algebra text [32]. That chapter contains additional information about the
structure of orthogonal groups. Another superb reference for this material is Artin’s classic
monograph [4].
Chapter 15. Two other introductions to Hilbert spaces at a level similar to ours can be
found in Simmons [58, Chpt. 10] and Rudin [56, Chpt. 4]. Halmos’ book [26] contains an
abundance of problems on Hilbert spaces. For more on metric spaces, see Simmons [58] or
Munkres [45].
Chapter 16. A nice treatment of commutative groups, finitely generated or not, appears
in Chapter 10 of Rotman’s group theory text [54]. Another exposition of the reduction algo-
rithm for integer matrices and its connection to classifying finitely generated commutative
groups is given by Munkres [46, §11].
Further Reading
613
Chapter 17. Four excellent accounts of module theory appear in Anderson and Fuller’s
text [3], Atiyah and Macdonald’s book [5], Jacobson’s Basic Algebra 1 and 2 [32] (especially
the third chapter in each volume), and Rotman’s homological algebra book [55]. Bourbaki [8,
Chpt. II] provides a very thorough and general, but rather difficult, treatment of modules.
Chapter 18. The classification of finitely generated modules over principal ideal domains
is a standard topic covered in advanced abstract algebra texts such as [14, 32]. I hope that
the coverage here may be more quickly accessible to readers with a little less background
in group theory and ring theory. The book by Hartley and Hawkes [28] is an excellent,
concise reference that covers the classification theorem, the necessary background on rings
and modules, and applications to canonical forms. There are two approaches to proving the
rational canonical form for square matrices over a field. The approach adopted here deduces
this result from the general theory for PIDs. The other approach avoids the abstraction
of PIDs by proving all necessary results at the level of finite-dimensional vector spaces,
T-invariant subspaces, and T-cyclic subspaces. See [29] for such a treatment. The author’s
opinion is that proving the special case of the classification theorem for torsion F[x]-modules
is not much simpler than proving the full theorem for all finitely generated modules over
all PIDs. In fact, because of all the extra structure of the ring F[x], focusing on this special
case might even give the reader less intuition for what the proof is doing. To help the reader
build intuition, we chose to cover the much more concrete case of Z-modules in an earlier
chapter.
Chapter 19. Two sources that give due emphasis to the central role of universal mapping
properties in abstract algebra are Jacobson’s two-volume algebra text [32] and Rotman’s
homological algebra book [55]. The appropriate general context for understanding UMP’s
is category theory, the basic elements of which are covered in the two references just cited.
A more comprehensive introduction to category theory is given in Mac Lane’s book [41].
Chapter 20. A nice introduction to multilinear algebra is the text by Northcott [47]. A very
thorough account of the subject, including detailed discussions of tensor algebras, exterior
algebras, and symmetric algebras, appears in [8, Chpt. III].
Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com 
Bibliography
[1] Azmy Ackleh, Edward J. Allen, Ralph Kearfott, and Padmanabhan Seshaiyer, Classical
and Modern Numerical Analysis: Theory, Methods, and Practice, Chapman and
Hall/CRC Press, Boca Raton, FL (2010).
[2] A. C. Aitken, Determinants and Matrices (eighth ed.), Oliver and Boyd Ltd., Edin-
burgh (1954).
[3] Frank W. Anderson and Kent R. Fuller, Rings and Categories of Modules (Graduate
Texts in Mathematics, Vol. 13, second ed.), Springer-Verlag, New York (1992).
[4] Emil Artin, Geometric Algebra, Dover Publications, Mineola, NY (2016).
[5] M. F. Atiyah and I. G. Macdonald, Introduction to Commutative Algebra, Addison-
Wesley, Reading, MA (1969).
[6] W. A. Blankinship, “A new version of the Euclidean algorithm,” Amer. Math. Monthly
70 #7 (1963), 742–745.
[7] Mikl`os B`ona, Combinatorics of Permutations, Chapman and Hall/CRC, Boca Raton,
FL (2004).
[8] Nicolas Bourbaki, Algebra 1, Springer-Verlag, New York (1989).
[9] R. Brualdi, “The Jordan canonical form: an old proof,” Amer. Math. Monthly 94 #3
(1987), 257–267.
[10] E. Ward Cheney and David R. Kincaid, Numerical Mathematics and Computing (sixth
ed.), Brooks/Cole, Pacific Grove, CA (2007).
[11] David A. Cox, Galois Theory (second ed.), John Wiley and Sons, New York (2012).
[12] David A. Cox, John Little, and Donal O’Shea, Ideals, Varieties, and Algorithms: An
Introduction to Computational Algebraic Geometry and Commutative Algebra (third
ed.), Springer-Verlag, New York (2010).
[13] R. Fletcher and D. Sorenson, “An algorithmic derivation of the Jordan canonical form,”
Amer. Math. Monthly 90 #1 (1983), 12–16.
[14] David S. Dummit and Richard M. Foote, Abstract Algebra (third ed.), John Wiley and
Sons, New York (2003).
[15] John R. Durbin, Modern Algebra: An Introduction (sixth ed.), John Wiley and Sons,
New York (2008).
[16] John B. Fraleigh, A First Course in Abstract Algebra (seventh ed.), Addison Wesley,
Reading (2002).
[17] Joseph A. Gallian, Contemporary Abstract Algebra (fifth ed.), Houghton Mifflin, Boston
(2001).
615
616
Bibliography
[18] A. Galperin and Z. Waksman, “An elementary approach to Jordan theory,” Amer.
Math. Monthly 87 #9 (1980), 728–732.
[19] F. R. Gantmacher, The Theory of Matrices (two volumes), Chelsea Publishing Co.,
New York (1960).
[20] I. Gohberg and S. Goldberg, “A simple proof of the Jordan decomposition theorem for
matrices,” Amer. Math. Monthly 103 #2 (1996), 157–159.
[21] Gene Golub and Charles Van Loan, Matrix Computations (third ed.), The Johns
Hopkins University Press, Baltimore (1996).
[22] Branko Gr¨unbaum, Convex Polytopes (Graduate Texts in Mathematics, Vol. 221,
second ed.), Springer-Verlag, New York (2003).
[23] Charles R. Hadlock, Field Theory and Its Classical Problems, Carus Mathematical
Monograph no. 19, Mathematical Association of America, Washington, D.C., (1978).
[24] J. Hall, “Another elementary approach to the Jordan form,” Amer. Math. Monthly 98
#4 (1991), 336–340.
[25] Paul R. Halmos, Finite-Dimensional Vector Spaces, Springer-Verlag, New York (1974).
[26] Paul R. Halmos, A Hilbert Space Problem Book (Graduate Texts in Mathematics, Vol.
19, second ed.), Springer-Verlag, New York (1982).
[27] Paul R. Halmos, Naive Set Theory, Springer-Verlag, New York (1998).
[28] Brian Hartley and Trevor Hawkes, Rings, Modules, and Linear Algebra. Chapman and
Hall, London (1970).
[29] Kenneth Hoffman and Ray Kunze, Linear Algebra (second ed.), Prentice Hall, Upper
Saddle River, NJ (1971).
[30] Roger Horn and Charles Johnson, Matrix Analysis (second ed.), Cambridge University
Press, Cambridge (2012).
[31] Thomas W. Hungerford, Algebra (Graduate Texts in Mathematics, Vol. 73), Springer-
Verlag, New York (1980).
[32] Nathan Jacobson, Basic Algebra I and II (second ed.), Dover Publications, Mineola,
NY (2009).
[33] David Kincaid and Ward Cheney, Numerical Analysis: Mathematics of Scientific
Computing (second ed.), Brooks/Cole, Pacific Grove, CA (1996).
[34] Peter Lancaster, Theory of Matrices, Academic Press, New York (1969).
[35] Ron Larson and David Falvo, Elementary Linear Algebra (sixth ed.), Brooks Cole,
Belmont, CA (2009).
[36] David C. Lay, Linear Algebra and Its Applications (fourth ed.), Addison Wesley,
Reading, MA (2011).
[37] Hans Liebeck, “A proof of the equality of column and row rank of a matrix,” Amer.
Math. Monthly 73 #10 (1966), 1114.
Bibliography
617
[38] Nicholas A. Loehr, Bijective Combinatorics, Chapman and Hall/CRC, Boca Raton, FL
(2011).
[39] Nicholas A. Loehr, “A direct proof that row rank equals column rank,” College Math.
J. 38 #4 (2007), 300–301.
[40] Nicholas A. Loehr, An Introduction to Mathematical Proofs, CRC Press, Boca Raton,
FL (2020).
[41] Saunders Mac Lane, Categories for the Working Mathematician (Graduate Texts in
Mathematics, Vol. 5, second ed.), Springer-Verlag, New York (1998).
[42] J. Donald Monk, Introduction to Set Theory, McGraw-Hill, New York (1969).
[43] Thomas Muir, The Theory of Determinants in the Historical Order of Development
(four volumes), Dover Publications, New York (1960).
[44] Thomas Muir, A Treatise on the Theory of Determinants, revised and enlarged by
William Metzler, Dover Publications, New York (1960).
[45] James R. Munkres, Topology (second ed.), Prentice Hall, Upper Saddle River, NJ
(2000).
[46] James R. Munkres, Elements of Algebraic Topology, Perseus Publishing, Cambridge,
MA (1984).
[47] D. G. Northcott, Multilinear Algebra, Cambridge University Press, Cambridge (1984).
[48] James G. Oxley, Matroid Theory (second ed.), Oxford University Press, Oxford (2011).
[49] R. Tyrrell Rockafellar, Convex Analysis, Princeton University Press, Princeton, NJ
(1972).
[50] Joseph J. Rotman, Advanced Modern Algebra (second ed.), American Mathematical
Society, Providence, RI (2010).
[51] Joseph J. Rotman, A First Course in Abstract Algebra (third ed.), Prentice Hall, Upper
Saddle River, NJ (2005).
[52] Joseph J. Rotman, Galois Theory (second ed.), Springer-Verlag, New York (1998).
[53] Joseph J. Rotman, An Introduction to Algebraic Topology (Graduate Texts in Mathe-
matics, Vol. 119), Springer-Verlag, New York (1988).
[54] Joseph J. Rotman, An Introduction to the Theory of Groups (fourth ed.), Springer-
Verlag, New York (1994).
[55] Joseph J. Rotman, Notes on Homological Algebra, Van Nostrand Reinhold, New York
(1970).
[56] Walter Rudin, Real and Complex Analysis (third ed.), McGraw-Hill, Boston (1987).
[57] Bruce E. Sagan, The Symmetric Group: Representations, Combinatorial Algorithms,
and Symmetric Functions (second ed.), Springer-Verlag, New York (2001).
[58] George F. Simmons, Introduction to Topology and Modern Analysis, Krieger Publishing
Co., Malabar, FL (2003).
618
Bibliography
[59] Gilbert Strang, Introduction to Linear Algebra (fourth ed.), Wellesley Cambridge Press,
Wellesley, MA (2009).
[60] Jean-Pierre Tignol, Galois’ Theory of Algebraic Equations, World Scientific Publishing,
Singapore (2001).
[61] Lloyd N. Trefethen and David Bau III, Numerical Linear Algebra, SIAM, Philadelphia
(1997).
[62] Herbert W. Turnbull, The Theory of Determinants, Matrices, and Invariants (second
ed.), Blackie and Son Ltd., London (1945).
[63] H. Valiaho, “An elementary approach to the Jordan form of a matrix,” Amer. Math.
Monthly 93 #9 (1986), 711–714.
[64] D. J. Welsh, Matroid Theory, Academic Press, New York (1976).
[65] Douglas B. West, Introduction to Graph Theory (second ed.), Prentice Hall, Upper
Saddle River, NJ (2001).
[66] G¨unter M. Ziegler, Lectures on Polytopes (Graduate Texts in Mathematics, Vol. 152),
Springer-Verlag, New York (1995).
Index
0n (zero matrix), 105
0m,n (zero matrix), 74
1-norm, 256
17-gon, 332
2-norm, 256
A[i | j] (delete row i, col. j), 115
A ∩B (intersection), 607
A ∪B (union), 607
A \ B (set difference), 607
A ⊆B (subset), 607
A × B (Cartesian product), 607
A∗(conjugate-transpose), 77, 106, 175
A−1 (inverse), 80
A[j] (jth column of A), 73
AT (transpose), 77, 106
An (alternating group), 34
A[i] (ith row of A), 73
Aech (reduced row-echelon form), 86
B-orthogonal linear map, 384
B(V, W) (bounded linear maps), 421
B|W (restricted bilinear form), 375
Bv (bilinear form), 369
C(P; Q) (circle with center P through Q),
318
D(µ) (partition diagram), 205
E1
p[c; i] (elementary matrix), 85
E2
p[i, j] (elementary matrix), 85
E3
p[c; i, j] (elementary matrix), 85
Ex (evaluation at x), 345
F-algebra, 7, 150
generated by z, 57
homomorphism, 150
F-linear map, 12
F(S) (field ext. gen. by S), 337
F(z) (field extension), 321
F n (space of n-tuples), 6, 73, 134
G/H (quotient set), 9
GLn(F) (general linear group), 81
G[n] (subgroup annihilated by n), 458
G∗(dual group), 471
In (identity matrix), 74, 105
J(c; µ) (Jordan matrix), 205
Jm,n (unit matrix), 74
L(P, Q) (line through P and Q), 318
L(V, W) (space of linear maps), 140
LA (left multiplication by A), 143
LX (linear combination map), 135
Lg (left multiplication by g), 108
Lx (left multiplication map), 350
M + N (sum of submodules), 481
M I (product module), 483
M ⊗n (tensor power), 576
M1 ⊗R · · · ⊗R Mn (tensor product), 576
Mn(F) (matrices over a field), 73
Mn(R) (ring of matrices), 5, 105
Mm,n(F) (space of matrices), 6, 73, 140
Mm,n(R) (space of matrices), 105
O(V ) (orthogonal group), 159
On(R) (orthogonal matrices), 159
R-algebra, 594
R-linear combination, 481, 491
R-linear map, 479, 571
R-map, 479
R-submodule, 480
R(A) (range of matrix), 82
R[[x]] (formal power series), 37
R[[x1, . . . , xm]] (formal power series), 60
R[x] (polynomial ring), 36
R[x1, . . . , xm] (polynomial ring), 60
R∗(units of a ring), 19, 516
Rop (opposite ring), 506
Ry (right multiplication map), 350
S(X) (group of bijections), 4
S ⊥T (orthogonal subsets), 351, 374
S⊥(orthogonal complement), 375, 416
Sn (symmetric group), 23
Sz (reflection determined by z), 385
T-invariant subspace, 153, 533
T(M) (tensor algebra), 594
TX,µ (nilpotent map), 206
U(V ) (unitary group), 160
Un(C) (unitary matrices), 160, 181
V ∗(dual space), 145, 342, 376
V ∗∗(double dual), 345
W ⊥(orthogonal complement), 198
W1 ⊞· · · ⊞Wk (orthogonal direct sum), 376
619
620
Index
W1 ⊕· · · ⊕Wk (direct sum), 376
X/R (quotient set), 608
Xn
< (increasing sequences), 589
[A, B] (commutator), 99
[B]X (matrix of a bilinear form), 371
[K : F] (degree of field extension), 320
[T]X (matrix of T), 150
[n] (the set {1, 2, . . . , n}), 23, 73, 607
[v]X (coordinates of v), 136
[x]R (equivalence class), 608
AC (arith. constructible numbers), 319
Bilin(V ) (space of bilinear forms), 371
C (complex numbers), 4, 607
End(M) (endomorphism ring), 22
GC (geom. constructible numbers), 318
GL(V ) (general linear group of V ), 384
GLn(R) (general linear group), 3, 384
HomF (V, W) (space of linear maps), 342
HomR(M, N) (Hom module), 484
HomZ(G, Z) (dual group), 471
Inv(f) (set of inversions), 28
O2(R) (orthogonal group), 396
OB(V ) (B-orthogonal maps), 384
On(A) (A-orthogonal matrices), 384
On(R) (real orthogonal group), 384
Q (rational numbers), 4, 607
R (real numbers), 4, 607
RC (radically constructible numbers), 334
R[x] (real polynomials), 5
Rk (k-dimensional real space), 4
Rk (k-dimensional real vectors), 6
Rad(V ) (radical of bilinear space), 378
SL(V ) (special linear group), 385
SLn(F) (special linear group), 131
SQC (numbers with square root towers),
322
Sym(M) (symmetric algebra), 596
Symn M (nth symmetric power of M), 580
Zx (cyclic subgroup generated by x), 439
Z (integers), 4, 607
subgroups of, 438
Z-basis, 441
Z-independent list, 441
Z-linear combination, 439
Z-linear extension, 443
Z-linear independence, 441
Z-linear map, 443
matrix of, 447
Smith normal form, 454
sum of, 448
Zn (integers mod n), 4, 438
Z>0 (positive integers), 607
Z≥0 (nonnegative integers), 607
adj(A) (classical adjoint), 117
V(M) (exterior algebra), 596
VnM (nth exterior power of M), 578
L
i∈I Mi (direct sum), 483
NnM (tensor power), 576
B (nondegenerate version of bilinear form
B), 379
colrk(A) (column rank), 83
Col(A) (column space), 83
∼= (isomorphism), 12
deg(p) (degree of p), 36
det(A) (determinant), 81, 106
dim(V ) (dimension of V ), 15
ei (standard basis vector), 75
ℓ2 (Hilbert space), 410
ℓ2(X) (Hilbert space), 410
∅(empty set), 607
⟨S⟩(submodule generated by S), 481
⟨v1, v2, . . . , vk⟩(subgroup generated by vi),
439
idX (identity function on X), 23, 608
img(f) (image), 12, 487
inf S (greatest lower bound of S), 348, 609
inv(f) (inversions of f), 28
ker(f) (kernel), 12, 487
⟨g⟩(subgroup generated by g), 8
lcm(f1, . . . , fn) (least common multiple), 53
len(M) (length of module), 502
⊥S (left orthogonal complement), 375
| (divides), 43
Y [T]X (matrix of T), 140
ord(f) (order of series), 65
A (matrix conjugate), 106
per(A) (permanent of A), 124
Q
i∈I Mi (direct product of modules), 483
rank(A), 88
ρ(A) (spectral radius), 262
rowrk(A) (row rank), 84
Row(A) (row space), 84
sgn(f) (sign of f), 29
sort(k) (sorted list), 121
sup S (least upper bound of S), 348, 609
τz (translation), 397
tor(G) (torsion subgroup), 459
tr(A) (trace), 98
c mod n (reduction mod n), 4
eij (matrix unit), 76
f : X →Y (function notation), 608
f[A] (image), 608
Index
621
f|Z (restricted function), 608
f −1 (inverse function), 608
f −1[B] (preimage), 608
g ◦f (composite function), 608
k-cycle, 25
m1 ⊗m2 ⊗· · · ⊗mn (tensor notation), 576
m1 ∧m2 ∧· · · ∧mn (wedge product), 578
m1m2 · · · mn (symmetric product), 580
n-linear map, 571
n-tuple, 73
nG (subgroup of nth multiples), 458
nZ (integer multiples of n), 8
sℓ(reflection across line ℓ), 385, 397
v ⊥w (orthogonal vectors), 351, 374
x (formal indeterminate), 38
x ∈S (set membership), 607
x ̸∈S (set non-membership), 607
Abelian group, 4
addition modulo n, 4
adjoint, 353, 396, 424
adjunct of a matrix, 117
affine basis, 288
universal mapping property, 291
affine combination, 284
affine dependence, 288
affine dimension, 286
affine hull, 287
affine hyperplane, 286
affine independence, 288
and linear independence, 288
affine line, 286
affine map, 290
affine plane, 286
affine set, 284
characterizations of, 290
affine span, 287
affine transformation, 290
affine variety, 357
algebra, 7
algebra homomorphism, 12
algebraic axioms, 17
algebraic element, 328
in an F-algebra, 58
algebraic systems, 17
algebraically closed field, 213
alternate bilinear form, 368, 381
alternating map, 572
alternating property of det(A), 111
altitude, 317, 325
angle between vectors, 367
angle bisection, 317
angle trisection, 317, 331
anisotropic vector, 383
annihilation of M by I, 486
annihilator, 344, 533
in algebraic geometry, 357
of a module, 530
anti-commutativity, 572
antisymmetric bilinear form, 368
antisymmetric relation, 481, 608
approximation by orthonormal vectors, 227
arithmetic construction sequence, 320
arithmetically constructible numbers, 319
associate polynomials, 44
associates, 61, 516
in a ring, 66
associative axiom, 3
associativity of matrix multiplication, 78
automorphism, 479
Axiom of Choice, 495
axioms
for a vector space, 6
for commutative groups, 4
for groups, 4
for rings, 5
Banach space, 356, 421
barycenter, 313
barycentric coordinates, 289
basic transposition, 28
basis, 14
for R-module, 491
for field extension, 328
of space of functions, 75
orthonormal, 158
over Z, 441
Basis Cardinality Theorem, 14, 498
Bessel’s Inequality, 418
Betti number, 458, 529
bijection, 23, 608
bijective formulation of UMP, 552
bilinear form, 350, 368
alternate, 368
and orthogonal maps, 384
and quadratic forms, 397
antisymmetric, 368
block-diagonalization of alternate form,
381
degenerate, 368
diagonalization theorems, 379
discriminant of, 395
622
Index
Embedding Theorem, 398
left-degenerate, 394
left-nondegenerate, 394
main definitions, 391
matrix relative to a basis, 371
nondegenerate, 368
Parallelogram Law, 393
positive definite, 369
symmetric, 368
universal, 396
bilinear homomorphism, 370
bilinear isomorphism, 370
bilinear map, 156, 571
bilinear pairing, 350
and subspace lattices, 352
left kernel, 351
nondegenerate, 351
right kernel, 351
bilinear space, 368
Cancellation Theorem, 389
Decomposition Theorem, 383
decomposition theorem, 390
direct product, 369
Extension Theorem, 398
external direct sum, 369
reflections of, 385
Witt decomposition, 383
bilinearity, 368
bimodule, 509, 604
binary operation, 3
Binomial Theorem, 64
Block Matrix Addition Rule, 103
Block Matrix Multiplication Rule, 92
block-diagonal matrix, 154, 169
block-triangular matrix, 128, 153, 169
boundary of a set, 429
bounded linear map, 421
bounded subset of Rn, 296
Cancellation Law, 18, 515
Cancellation Theorem
for symmetric bilinear spaces, 389
variant form, 398
canonical injection into coproduct, 559
canonical map to M/N, 485
canonical projection, 557
Carath´eodory’s Theorem, 293
Cartesian decomposition, 175, 179
Cartesian product, 607
Catalan number, 99
Cauchy sequence, 276, 406
Cauchy–Binet Formula, 121, 604
Cauchy–Schwarz Inequality, 132, 274, 408
Cayley–Hamilton Theorem, 59, 537
chain, 495, 609
of fields, 320
of submodules, 500
changing variables in a sum, 107
characteristic polynomial, 59, 122, 537
Chinese Remainder Theorem, 69
Cholesky factorization, 241
classical adjoint, 117
classification
of finite commutative groups, 437
of isomorphisms V ∼= F n, 139
closed ball, 430
closed half-spaces of a hyperplane, 295
closed interval, 402
closed set, 402
closed set in Rn, 296
closure axiom, 3
closure conditions, 7
closure of a set, 429
codomain, 607
column rank, 83
column space, 83
column vector, 73, 105
column-equivalence of matrices, 149
common divisor, 44, 516
common multiple, 53, 516
commutative axiom, 3
commutative group, 4, 437
endomorphism ring of, 507
axioms for, 4
Betti number, 458
classification of, 437
dimension of, 441
elementary divisors, 458
endomorphism ring of, 22
finitely generated, 439
free, 441
invariant factors, 458
rank of, 441
torsion subgroup, 459
commutative ring, 5
commutativity of disjoint cycles, 26
commutator of two matrices, 99
companion matrix, 130, 534
comparable metrics, 428
comparable norms, 258
Comparison Lemma for Finite Independent
Sets and Spanning Sets, 494
Index
623
Comparison Theorem for Independent Sets
and Spanning Sets, 497
compass, 317
complete lattice, 348, 481, 609
complete metric space, 276, 407
complex conjugation, 77, 175
complex inner product space, 227, 354, 408
composite function, 608
composition, 23
concave function, 307
cone, 298
congruence class of matrices, 374
congruent matrices, 157, 374
conical combination, 298
conjugate of group element, 7
in Sn, 32
conjugate-linearity, 354
conjugate-symmetry, 354
conjugate-transpose, 77, 106, 175
determinant of, 109
constant polynomial, 36
Constructibility Theorem, 322
constructing a 17-gon, 332
constructions of modules, 480
continuity properties of a Hilbert space, 414
continuous function, 404, 568
contraction, 430
convergent sequence, 401
convex combination, 292
convex cone generated by a set, 298
convex function, 305
convex hull, 292
convex set, 292, 415
convex span, 292
coordinates
in a free commutative group, 442
of a vector, 136
relative to a basis, 492
coproduct
of modules, 558
of sets, 561
of two groups, 570
Correspondence Theorem for Modules, 489
coset, 9, 395, 438, 485
Coset Equality Theorem, 9, 21
Cramer’s Rule, 118
Crout factorization, 236
Cubic Formula, 334, 340
cubic polynomial, 36
cycle decomposition of a permutation, 25
cyclic group, 439
cyclic module, 482
cyclic subgroup, 8
cyclic subspace, 533
Decomposition Theorem for symmetric
bilinear spaces, 383, 390
definition of xi, 60
definitions, 607
degenerate bilinear form, 368
Degenerate Subspace Embedding Theorem,
398
degree
addition formula, 39
of field extension, 320
of polynomial, 36
of sums and products, 39
Degree Formula for Field Extensions, 321
derivative of formal series, 70
determinant, 81, 106
alternating property, 111
alternate formula, 109
and columns of matrix, 113
and exterior powers, 594
and matrix invertibility, 114
Cauchy–Binet Formula, 121
criterion for positive definite matrices,
194
effect of elementary row operations, 111
generalized Laplace expansion, 130
Laplace expansion, 115
multilinearity in the rows, 110
of AT and A∗, 109
of diagonal matrix, 107
of elementary matrix, 112
of identity matrix, 107
of matrix with two equal rows, 111
of matrix with zero row, 111
of triangular matrix, 107
product formula, 114, 119
Vandermonde, 130
diagonal matrix, 98, 107, 151
determinant of, 107
diagonalizable linear map, 152, 202
diagonalizable part of linear map, 219
diagonalization of symmetric bilinear forms,
379
diagonally dominant matrix, 264
diagram completion property, 551
diagram of a partition, 205, 460
Diamond Isomorphism Theorem, 488
dilation property, 257
624
Index
dimension, 14, 15
of free commutative group, 441
Dimension Theorem for Totally Isotropic
Subspaces, 382
direct chain, 569
direct limit, 570
direct product, 8
of bilinear spaces, 369
of groups, 439
of modules, 482
projection, 557
direct sum, 211, 354, 376, 439
natural injections, 555, 559
of modules, 483
of subspaces, 223
UMP, 556
directed graph of a function, 24
direction subspace, 286
discrete metric, 400
discriminant, 395
disjoint cycles, 26
distance, 367
distance in inner product space, 227
distributive law, 5, 77
divides, 43
divisibility in a ring, 515
division ring, 493
Division Theorem
for integers, 66
for polynomials, 42
generalized version, 43
divisor, 43
domain, 607
Doolittle factorization, 236
dot product, 158, 367
double dual map, 349
double dual space, 345
dual basis, 343
dual basis, 145, 471
effect of elementary operations, 361
in inner product space, 354
dual group, 471
dual map, 349, 434
dual of a Hilbert space, 423
dual space, 145, 342, 376
duplication of the cube, 317, 331
eigenspace, 224
eigenvalue, 212
eigenvector, 212
Eisenstein’s Irreducibility Criterion, 55
elementary column operation, 85
on integer matrices, 449
elementary divisors, 458, 526
of a linear map, 535
elementary matrix, 85
determinant of, 112
elementary operation
and dual bases, 361
effect on matrix of a map, 449
on basis of commutative group, 441
elementary row operation, 85
and determinants, 111
on integer matrices, 449
Embedding Theorem for Degenerate
Subspaces, 398
Embedding Theorem for Totally Isotropic
Subspaces, 382
empty set, 607
endomorphism, 479
endomorphism ring, 22, 507
entry of a matrix, 73
epigraph, 305
epimorphism, 479
equality of functions, 608
equilateral triangle, 324
equivalence class, 608
equivalence relation, 608
equivalent matrices, 149
equivalent metrics, 428
essential uniqueness, 562
Euclid’s algorithm, 44
for integers, 66
Euclidean distance, 367
Euclidean domain, 540
Euclidean isometry, 396
Euclidean norm, 177, 256, 367
Euclidean plane, 318
evaluation at x, 345
evaluation homomorphism, 40
eventually constant sequence, 428
Exchange Lemma, 497
existence of Jordan canonical form, 212
explicit formula for A−1, 118
Exponent Theorem, 19
extending by linearity, 203, 342
extension field, 320
extension of orthonormal basis, 229
extension of scalars, 603
Extension Theorem of Witt, 398
exterior algebra of a module, 596
exterior power
Index
625
and determinants, 594
of a module, 578
of an R-map, 586
external direct sum of bilinear spaces, 369
Extreme Value Theorem, 431
f.g. free module, 519
factorization into basic transpositions, 28
factorization into disjoint cycles, 27
factorization into transpositions, 28
field, 5
extension of, 320
field extension
degree formula, 321
generated by subset, 337
pure, 334
finite length module, 502
finite support, 483
finite-dimensional vector space, 14
finitely generated commutative group, 439
finitely generated free commutative group,
441
finitely generated module, 482
first derivative test for convex functions, 306
Fitting’s Lemma, 211
flag of subspaces, 153, 169
formal derivative, 70
formal polynomial, 37
formal power series, 37
equality of, 37
multivariable, 60
order of, 65
ring operations, 38
Fourier coefficients, 418
free commutative group, 441
freeness of subgroups, 446
isomorphism with Zk, 444
UMP, 443
free group, 570
free module, 491
free monoid, 570
Frobenius matrix norm, 275
function, 607
bijective, 608
composite, 608
composition of, 23
continuous, 568
directed graph of, 24
equality of, 26, 74, 608
one-line form, 23
one-to-one, 23
onto, 23
sign of, 29
surjective, 608
functor, 568
Fundamental Homomorphism Theorem, 13
Fundamental Theorem of Algebra, 51
Fundamental Theorem of Arithmetic, 66
Galois theory, 334
Gauss–Seidel Algorithm, 254
Gaussian elimination, 86
and LU factorization, 238
gcd, see greatest common divisor
general associativity of matrix products, 79
general bilinearity, 368
general linear group, 81
general position, 311
Generalized Associative Law, 79
Generalized Distributive Law, 132
generalized eigenspace, 224
Generalized Fundamental Homomorphism
Theorem, 487
generalized Laplace expansion, 130
generating set, 491
generator, 481
generators of a cone, 298
geometric arithmetic, 326
geometric construction sequence, 319
geometric reflection, 385
geometric square roots, 327
geometrically constructible numbers, 318
Givens rotation matrix, 249
Givens’ QR algorithm, 249
Gram–Schmidt algorithm, 228, 432
modified version, 248
graph of a function, 313, 607
greatest common divisor, 44, 516
by matrix reduction, 47, 67
greatest element, 609
greatest lower bound, 348, 609
group, 3
Abelian, 4
axioms, 4
cancellation law, 18
classification of, 437
commutative, 4, 437
cyclic, 439
direct product, 439
direct sum, 439
Exponent Theorem, 19
free, 570
626
Index
Fundamental Homomorphism
Theorem, 13
homomorphism, 12
Lagrange’s Theorem, 482
of isometries, 397
quotient construction, 9
solvable, 334
Sudoku Theorem, 19
H-cone, 298
H-polyhedron, 315
H¨older’s Inequality, 274
Hadamard product, 97
heptagon, 331
Hermitian decomposition, 179
Hermitian matrix, 177
Hermitian part of A, 179
Hessian matrix, 307
Hilbert Basis Theorem, 357
Hilbert space, 408
adjoint operators, 424
dual, 423
orthonormal set, 417
Hilbert space isomorphism, 419
Hom module, 484
homogeneous linear equation, 282
homomorphism, 17
equality on a generating set, 482
image, 487
kernel, 487
of F-algebras, 150
of algebras, 12
of bilinear spaces, 370
of groups, 12
of left modules, 479
of modules, 12
of right modules, 479
of rings, 12
of vector spaces, 12
Householder matrix Qv, 232
Householder reflection, 232
Householder transformation Tv, 232
Householder’s QR algorithm, 233
hyperbolic pair, 378
hyperbolic plane, 369, 378
Hyperplane Separation Theorem, 296
ideal, 7, 480, 515
maximal, 508
prime, 510
ideal–variety correspondence, 357
idempotent matrix, 155
identity element, 3
identity function, 23, 608
identity matrix, 74, 105
determinant of, 107
iff, 607
image, 12, 283, 443, 487, 608
of a function, 608
of a linear map, 206
imaginary part, 175
impossible geometric constructions, 331
inclusion-preserving map, 489
indefinite matrix, 180
independent list in a module, 491
independent subset of a module, 491
indeterminate xi, 60
index of nilpotence, 204
indexed intersection, 607
indexed union, 607
induced matrix norm, 260, 261
injection, 608
for a direct sum, 555, 559
injective function, 608
inner product axioms, 227
inner product on Cn, 177
inner product space, 353
complex, 160, 354, 408
dual basis, 354
real, 158
integer division with remainder, 66
integer gcds, 66
Integer Matrix Reduction Theorem, 452
integer partition, 460
integers mod n, 4, 438
integral domain, 5
Interlacing Theorem, 192
internal direct sum, 376
intersection, 607
Intersection Lemma for V-cones, 299
intersection of submodules, 481
invariant factors, 458, 525
of a linear map, 535
of a matrix, 528
invariant subspace, 211
inverse, 3
chain, 569
function, 608
limit, 569
mod n, 66
of a matrix, 118
of conjugate-transpose, 81
Index
627
of transpose, 81
inverse power method, 269
inversion, 28
invertibility of a matrix, 89
invertible element, 3
invertible matrix, 80
irreducibility
over C and R, 51
over finite fields, 51
via reducing mod a prime, 55
irreducible element in a ring, 517
irreducible factorization of polynomials, 51
irreducible polynomial, 50, 61
isometric isomorphism, 420
isometry, 419
of Euclidean plane, 396
isomorphism, 12, 479
of bilinear spaces, 370
isotropic vector, 375
iterative algorithms, 252
Jacobi’s Algorithm, 253
Jensen’s Inequality, 306
Jordan block, 202, 536
Jordan canonical form, 202, 536
and ODEs, 216
existence, 212
of a matrix, 537
uniqueness, 213
Jordan Canonical Form Theorem, 202
Jordan–Chevalley Decomposition Theorem,
219
Jordan–H¨older Theorem for Modules, 500
kernel, 12, 206, 282, 443, 487
Kronecker’s algorithm, 56
Lagrange Interpolation Formula, 56, 188
Lagrange’s Theorem, 482
Laplace expansion, 115
generalized, 130
lattice, 348, 481, 609
of submodules, 481
laws of exponents, 99
lcm, see least common multiple
leading coefficient, 36
leading entry of a row, 86
leading monomial, 36
leading term, 36
least common multiple, 53, 516
least element, 609
least squares approximation, 242
least upper bound, 348, 609
left R-module, 477
left R-module homomorphism, 479
left adjoint, 353
left coset, 10
Left Coset Equality Theorem, 10, 21
left ideal, 7, 480
maximal, 508
left inverse, 81
left kernel, 351
left multiplication map, 350
left multiplication map, 108, 143, 507
left-degenerate bilinear form, 394
left-nondegenerate bilinear form, 394
left-nondegenerate pairing, 351
length of a module, 502
length of a vector, 227
limit of a sequence, 401
line segment, 292
linear combination, 14, 282
in an R-module, 481
with integer coefficients, 439
linear combination function, 135
linear dependence, 14
linear functional, 342
representation by bilinear form, 376
linear half-space, 298
linear hyperplane, 282
linear independence
and affine independence, 288
and orthogonality, 227, 375
over Z, 441
linear map, 12, 290
adjoint of, 396
diagonalizable, 152, 202
for commutative groups, 443
idempotent, 155
Jordan–Chevalley decomposition, 219
nilpotent, 202
orthogonal, 158, 384
triangulable, 152
unitarily diagonalizable, 183
unitary, 160
linear polynomial, 36
linear span of a set, 283
linear subspace, 282
linear transformation, 12
linearity in inputs, 368
linearly independent list, 14
linearly independent set, 14
628
Index
localization, 569
Lorentz form, 367
lower bound, 348, 609
lower-triangular matrix, 98, 107, 152
LU factorization
and Gaussian elimination, 238
recursive formula, 235
LU Factorization Theorem, 235
magnitude of a complex number, 175
main diagonal, 88
matrix, 73, 105
A-orthogonal, 384
associativity of multiplication, 78
block-diagonal, 154, 169
block-triangular, 128, 153, 169
characteristic polynomial, 122
column rank, 83
column space, 83
column-equivalent, 149
commutator, 99
conditions for invertibility, 89
congruence, 157, 374
conjugate-transpose, 77
determinant of, 81, 106
diagonal, 98, 107, 151
diagonalizable, 152
diagonally dominant, 264
distributive laws, 77
elementary, 85
elementary column operation, 85
elementary row operation, 85
equality, 74
equivalent, 149
formula for inverse, 118
generalized associativity, 79
Hadamard product, 97
Hermitian, 177
idempotent, 155
invertible, 80
laws of exponents, 99
lower-triangular, 98
minor, 475
negative power, 81
nilpotent, 98
normal, 179
null space, 282
of a bilinear form, 371
orthogonal, 159, 384
orthogonal similarity, 160
permanent of, 124
positive definite, 180
powers of, 80
product of, 76
range of, 82
rank, 88
reduced row-echelon form, 86
row rank, 84
row space, 84
row-equivalence, 149
similar, 182
similarity, 131, 151, 374
singular values, 192
skew-Hermitian, 179
skew-symmetric, 98
Smith normal form, 88
strictly upper-triangular, 98
tensor product of, 592
trace, 98
triangulable, 152, 184
triangular, 151
unitarily diagonalizable, 183
unitarily triangulable, 184
unitary, 160, 181
unitary similarity, 160
unitriangular, 98, 226
upper-triangular, 98
matrix conjugate, 106
matrix factorizations, 226
matrix invariants, 526
matrix inverse and determinants, 114
matrix multiplication, 76
and tensor products, 94
matrix norm, 259
Frobenius, 275
induced by vector norm, 260
matrix of Vkf, 593
matrix of a Z-linear map, 447
matrix of a bilinear map, 156
matrix of a dual map, 349
matrix of linear map relative to bases, 140
matrix product, 106, 146
matrix reduction algorithm for gcds, 47, 67
Matrix Reduction Theorem for PIDs, 523
matrix ring, 5
matrix sum, 106
matrix tensor product, 92
matrix transpose, 77
maximal chain, 500
maximal element, 495, 609
maximal ideal, 495, 518
maximal left ideal, 508
Index
629
maximal orthonormal set, 418
meaning of x, 38
metric, 400
comparable, 428
equivalent, 428
metric space, 257, 400
complete, 407
separable, 432
subspace of, 400
midpoint, 324
minimal element, 609
minimal polynomial, 58, 328, 537
Minkowski’s Inequality, 274
minor, 475, 594
modified Gram–Schmidt algorithm, 248
module, 6
R-independent list, 491
R-independent set, 491
homomorphism, 479
basis of, 491
comparison to vector spaces, 477
constructions, 480, 503
coordinates relative to a basis, 492
coproduct of, 558
Correspondence Theorem, 489
cyclic, 482
definition, 477, 502
Diamond Isomorphism Theorem, 488
direct product, 482
direct sum, 483
finitely generated, 482
free, 491
generated by a subset, 491
informal introduction, 477
intersection of, 481
isomorphic, 479
lattice of submodules, 481
left, 477
length of, 502
Nested Quotient Isomorphism
Theorem, 489
quotient of, 485
Recognition Theorem, 490, 511
right, 478
simple, 481
submodule of, 480
sum of, 481
theorems on free R-modules, 505
theorems on homomorphisms, 504
UMP for direct product, 557
module homomorphism, 12
monic polynomial, 36
monoid, 570
monomorphism, 479
multilinear algebra, 571
multilinear map, 571
multilinearity of determinants, 110
multiple, 43
multiplication mod n, 5
multiset, 531
natural map to M/N, 485
naturality of V ∼= V ∗∗, 350
negative definite matrix, 180
negative power of a matrix, 81
negative semidefinite matrix, 180
Nested Quotient Isomorphism Theorem, 489
nilpotent element, 151
nilpotent linear map, 202
nilpotent matrix, 98
nilpotent part of a linear map, 219
non-isotropic vector, 375, 383
nondegenerate bilinear form, 368
nondegenerate pairing, 351
nonsingular system, 118
normal equations, 243
normal matrix, 179
and unitary diagonalizability, 186
normal operator, 424
normal subgroup, 7
normal vector to a hyperplane, 295
normed vector space, 256, 356, 421
null space, 12, 206, 282
nullity, 15
Nullstellensatz, 357
one-line form, 23
one-to-one function, 23, 608
onto function, 23, 608
open ball, 403
open cover, 406
open half-spaces of a hyperplane, 295
open set, 403, 568
operator norm, 422
opposite ring, 506
order of formal series, 65
ordered Z-basis, 441
ordered affine basis, 288
ordered basis, 14
for R-module, 519
ordinary differential equation, 216
orthogonal complement, 198, 351, 375, 416
630
Index
main theorem, 377
orthogonal direct sum, 376
orthogonal group, 159
orthogonal linear map, 158, 384
orthogonal list, 227, 375, 413
orthogonal maps as products of reflections,
387
orthogonal matrix, 159, 384
orthogonal projection, 227, 418
orthogonal similarity, 160
orthogonal subsets, 374
orthogonal vectors, 227, 367, 374, 408
orthogonality and linear independence, 227,
375
orthogonality for a pairing, 351
orthonormal basis, 158, 229, 367
orthonormal list, 227, 375
orthonormal set, 417
orthonormal vectors, 181
orthonormalization algorithm, 228
pairing, see bilinear pairing
parallel affine sets, 311
parallel line, 317, 325
Parallelogram Law, 393, 414
Parseval’s Identity, 420
partial flag, 169
partial ordering, 609
partially ordered set, 481, 495, 609
partition, 205, 460
diagram, 205, 460
Penrose properties, 251
permanent, 124
symmetry property, 125
permutation, 23
cycle decomposition, 25
factorizations of, 27
matrix, 131
sign of, 29
Permuted LU Factorization Theorem, 240
perpendicular bisector, 324
Pfaffians, 133
PID, see principal ideal domain
points in general position, 311
pointwise addition of functions, 341
pointwise operations, 74
polar decomposition, 175, 190
Polarization Identity, 414
polygon, 317
polynomial, 5
addition, 36
associated function, 40
associates, 44
common divisor, 44
degree of, 36
divisibility definitions, 43
division with remainder, 42
equality of, 35
evaluation at x = c, 40
formal definition, 37
function, 40
gcd, 44
informal definition, 35
irreducible, 50, 61
lcms, 53
minimal, 58
monic, 36
multiplication, 36
multivariable, 60
number of roots, 49
reduction mod p, 55
repeated roots, 70
root of, 49
scalar multiplication, 37
UMP, 41, 60
unique factorization into irreducible
factors, 51
value at c, 40
zero of, 49
polynomial ring UMP, 568
poset, 348, 495, 609
chain, 495
maximal element, 495
upper bound, 495
positive combination, 298
positive definite bilinear form, 369
positive definite inner product, 353
positive definite matrix, 180
determinant criterion, 194
positive definiteness, 354
positive operator, 424
positive semidefinite matrix, 180
power method, 267
powers of a matrix, 80
preimage, 608
preserves inclusions, 489
Primary Decomposition Theorem, 225
prime, 5
prime element in a ring, 517
prime factorization of integers, 66
prime factorization of polynomials, 52
prime ideal, 510, 518
Index
631
principal ideal, 8, 515
principal ideal domain (PID), 67, 515
product formula, 119
via elementary matrices, 113
Product Formula for Determinants, 114
product module
projection, 555
UMP, 557
product of s matrices, 79
product set, 8
projection, 155
for a direct sum, 170
for a product module, 555
for direct product, 557
projection onto M/N, 485
projection property for cones, 301
Projection Theorem, 149
properties of reflections, 385
pseudoinverse, 251
pure field extension, 334
Pythagorean Identity, 227
Pythagorean Theorem, 413
QR algorithm of Givens, 249
QR factorization via Gram–Schmidt, 230
quadratic form, 397
quadratic form associated with A, 177
quadratic polynomial, 36
Quartic Formula, 334, 340
quartic polynomial, 36
quaternions, 20
quintic polynomial, 36
quotient, 42
quotient group, 9, 438
universal mapping property, 566
quotient module, 485
UMP, 488, 553
quotient ring, 11, 486
universal mapping property, 566
quotient set, 9, 568, 608
quotient topology, 568
quotient vector space, 11
Radical Cancellation Lemma, 390
radical construction sequence, 334
radical ideal, 357
radical of bilinear form, 378
radically constructible numbers, 334
Radon’s Theorem, 314
range, 12, 82, 206, 282
rank, 15
of a matrix, 88
of free commutative group, 441
of linear map, 454
Rank–Nullity Theorem, 15
rational canonical form, 534, 535
of a matrix, 537
Rational Root Theorem, 49
Real Bilinear Form Diagonalization
Theorem, 380
real inner product space, 353
real part, 175
Recognition Theorem for Direct Products,
490, 511
reduced row-echelon form, 86
reducible linear map, 155
reducible polynomial, 50
reducing mod p, 55
Reduction Theorem for Integer Matrices,
452
reflection, 231, 396, 397
and orthogonal maps, 387
in R2, 385
in bilinear space, 385
reflexive relation, 481, 608
relation, 608
antisymmetric, 481
reflexive, 481
transitive, 481
relativity, 367
remainder, 42
repeated root, 70
representing a functional by a bilinear form,
376
restriction of a bilinear form, 375
restriction of a function, 608
Richardson’s algorithm, 252
right R-module, 478
right R-module homomorphism, 479
right adjoint, 353, 396
right coset, 11
right ideal, 7, 480
right inverse, 81
right kernel, 351
right multiplication map, 350
right-nondegenerate pairing, 351
ring, 5
axioms for, 5
commutative, 5
Fundamental Homomorphism
Theorem, 13
homomorphism, 12
632
Index
localization, 569
opposite ring, 506
quotient construction, 11
units of, 19
root of a polynomial, 49
rotation, 396
row rank, 84
row space, 84
row vector, 73, 105
row-equivalence of matrices, 149
ruler, 317
scalar, 6, 73, 478
scalar multiplication, 6, 106
second derivative test for convex functions,
307
secret sharing, 69
self-adjoint matrix, 178
self-adjoint operator, 424
separable metric space, 432
sequence, 401
sequential compactness, 296
sequentially compact set, 405
set difference, 607
set membership, 607
set partition, 608
shifted inverse power method, 269
shifted power method, 268
sign, 29
Sign Homomorphism Theorem, 29
similar linear maps, 151
similar matrices, 131, 151, 182, 374
simple module, 481
simplex, 293
simultaneous triangularization, 185
simultaneous unitary diagonalization, 189
singular value decomposition, 192, 243
singular values, 192, 243
skew matrix, 372
skew-Hermitian matrix, 179
skew-Hermitian part of A, 179
skew-symmetric matrix, 98
Smith normal form, 88, 452, 454, 523
solvable group, 334
spacetime, 367
spanning list, 14
spanning set, 14, 481
special linear group, 131
spectral radius, 262
Spectral Theorem, 186
spectrum, 212
splitting matrix, 264
Splitting Theorem for Modules over a PID,
529
square root tower, 322
squaring the circle, 317, 331
stabilizing a flag, 153
standard basis for Mm,n(F), 76
standard ordered basis, 441
standard ordered basis for F n, 75
straightedge, 317
strictly lower-triangular matrix, 152
strictly triangulable linear map, 152
strictly upper-triangular matrix, 98, 152
Structure Theorem for Linear Maps
between Free Modules over PIDs,
525
subalgebra, 8
subfield, 7, 320
subgroup, 7, 438
and submodules, 480
cyclic, 8
generated by a set, 439
normal, 7
of free commutative group, 446
subgroups of Z, 438
submatrix, 527
submodule, 8, 480
generated by a set, 481
generators of, 481
submultiplicative axiom, 259
subring, 7
subsequence, 401
subset, 607
subspace, 8, 282
T-invariant, 153
and submodules, 480
generated by vectors, 22
of a metric space, 400
spanned by a set, 283
Subspace Characterization Theorem, 283
subspace correspondence for V and V ∗, 347
subspace lattice and pairings, 352
subsystem, 7, 17
subtraction, 4
subtraction in a module, 478
subtractive cancellation, 249
Sudoku Theorem, 19
sum of Z-linear maps, 448
sum of submodules, 481
sum of subspaces, 211
sup norm, 256
Index
633
surjection, 608
symmetric algebra of a module, 596
symmetric bilinear form, 368
diagonalization theorems, 379
symmetric group, 23
symmetric map, 573
symmetric matrix, 372
symmetric power of a module, 580
symmetric power of an R-map, 586
symmetric product, 580
symmetric relation, 608
symmetry of permanents, 125
symplectic geometry, 375
symplectic pair, 378
symplectic plane, 378
tensor algebra of a module, 594
tensor power of a module, 576
tensor product
and matrix multiplication, 94
of R-linear maps, 585
of bimodules, 604
of matrices, 92, 592
of modules, 576
testing irreducibility, 55
theorem
characterizing affine sets, 290
characterizing linear subspaces, 283
classifying finite commutative groups,
437
classifying finitely generated
commutative groups, 440
classifying Hilbert spaces, 420
classifying nilpotent linear maps, 203,
208
on a basis for F(z), 329
on a basis of F[{z}], 58
on adjoints, 353
on affine maps, 291
on affine sets and linear subspaces, 286
on algebraic elements and finite-degree
extensions, 329
on alternate bilinear forms, 381
on approximation by orthonormal
vectors, 227
on bases and linear maps, 551
on bases of function spaces, 75
on bilinear pairings, 351
on bilinearity of matrix tensor
products, 93
on building bases, 496
on convex hulls, 292
on convex hulls of finite sets, 304
on dual bases, 343
on evaluation maps, 345
on finite intersections of closed
half-spaces, 303
on gcds and lcms in PIDs, 517
on Hermitian decomposition of a
matrix, 179
on independence of orthogonal vectors,
227
on invariant factors of a matrix, 537
on Jordan canonical forms, 536
on left invertibility, 102
on matrix representation of bilinear
forms, 372
on minimal polynomials, 328
on orthogonal complements, 377, 416
on orthogonal maps and reflections, 387
on pairings and subspace lattices, 352
on QR factorizations, 230
on rational canonical forms, 535
on right invertibility, 102
on subgroups of Z, 438
on submodules of a free module over a
PID, 520
on the conjugate-transpose, 176
topological space, 568
torsion subgroup, 459
torsion submodule, 529
totally bounded set, 431
totally isotropic subspace, 382
totally ordered poset, 609
trace, 98
transition matrix, 147
transitive relation, 481, 608
translate of a set, 286
translation, 397
translation invariance, 257
translation map, 290
transpose, 106
and permanents, 125
determinant of, 109
of a linear map, 349
of a matrix, 77, 145
transposition, 28
Triangle Inequality, 256, 257, 400
for norms, 408
triangulable linear map, 152
triangulable matrix, 184
trilinear map, 571
634
Index
two-sided ideal, 7
UMP, see universal mapping property
uniform continuity, 430
union, 607
unique factorization domain (UFD), 518
unique factorization property, 552
Unique Factorization Theorem
for integers, 66
for polynomials, 51
uniqueness of Betti numbers, 459
uniqueness of elementary divisors, 460
uniqueness of invariant factors, 463
uniqueness of Jordan canonical form, 213
unit, 19, 516
unit matrix, 74
unit vector, 256, 367, 375
unitarily diagonalizable matrix, 183
and normality, 186
unitarily triangulable matrix, 184
unitary linear map, 160
unitary matrix, 160, 181
unitary operator, 424
unitary similarity, 160, 182
unitriangular matrix, 98, 226
universal bilinear form, 396
universal mapping problem, 561
universal mapping property, 551
for Q
i∈I Mi, 557
for affine bases, 291
for basis of a vector space, 552
for coproduct of sets, 561
for direct sum of two modules, 556
for exterior powers, 577
for free R-modules, 491
for free commutative groups, 443
for localization, 569
for polynomial rings, 41, 60, 568
for product of two modules, 555
for products, 567
for quotient groups, 566
for quotient modules, 488, 553
for quotient rings, 566
for quotient sets, 568
for quotient topologies, 568
for symmetric powers, 579
for tensor products, 574
for vector spaces, 342
upper bound, 348, 495, 609
upper-triangular matrix, 98, 107, 151
V-cone, 298
V-polyhedron, 315
value of a polynomial, 40
Vandermonde determinant, 130
variety operator, 357
vector, 478
vector norm, 256
induced matrix norm, 260
vector space, 6
axioms for, 6
dimension of, 14
finite-dimensional, 14
quotient construction, 11
spanning set, 14
vector space homomorphism, 12
wedge product, 578
well-defined operation, 10
Witt decomposition, 383
Witt index, 383
Witt’s Cancellation Theorem, 389
variant form, 398
Witt’s Decomposition Theorem, 383, 390
Witt’s Extension Theorem, 398
zero divisor, 5
zero function, 74
zero matrix, 74, 105
zero of a polynomial, 49
zero polynomial, 36
zero-set, 344
in algebraic geometry, 357
Zorn’s Lemma, 495, 609
